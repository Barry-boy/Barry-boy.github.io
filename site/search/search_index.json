{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes \u8fdb\u9636\u8bad\u7ec3\u8425 \u00b6 \u5317\u6f02\u4e00\u65cf \u4e3b\u8981\u8bb0\u5f55\u5de5\u4f5c\u4ee5\u53ca\u751f\u6d3b\u5185\u5bb9 \u6211\u76842022\u76ee\u6807: \u751f\u6d3b/\u8fdb\u5ea6: \u8df3\u69fd\uff084\u6708\u672b\u5c3e\u5df2\u5b8c\u6210\uff09 \u722c\u957f\u57ce\uff08\u5df2\u5b8c\u6210\uff09 \u4f53\u9a8c\u767e\u5ea6\u65e0\u4eba\u9a7e\u9a76\uff08\u5df2\u5b8c\u6210\uff09 \u732b\u72d7\u53cc\u5168 (\u6709\u70b9\u56f0\u96be\uff0c\u4f30\u8ba1\u53ea\u80fd\u5b8c\u6210\u4e00\u534a) \u8003\u5b8c\u9a7e\u7167\uff08\u8fdb\u884c\u4e2d\uff09 \u62a5\u540d\u5b8c\u6210 \u79d1\u4e00 \u79d1\u4e8c \u79d1\u4e09 \u79d1\u56db","title":"\u9996\u9875"},{"location":"#kubernetes","text":"\u5317\u6f02\u4e00\u65cf \u4e3b\u8981\u8bb0\u5f55\u5de5\u4f5c\u4ee5\u53ca\u751f\u6d3b\u5185\u5bb9 \u6211\u76842022\u76ee\u6807: \u751f\u6d3b/\u8fdb\u5ea6: \u8df3\u69fd\uff084\u6708\u672b\u5c3e\u5df2\u5b8c\u6210\uff09 \u722c\u957f\u57ce\uff08\u5df2\u5b8c\u6210\uff09 \u4f53\u9a8c\u767e\u5ea6\u65e0\u4eba\u9a7e\u9a76\uff08\u5df2\u5b8c\u6210\uff09 \u732b\u72d7\u53cc\u5168 (\u6709\u70b9\u56f0\u96be\uff0c\u4f30\u8ba1\u53ea\u80fd\u5b8c\u6210\u4e00\u534a) \u8003\u5b8c\u9a7e\u7167\uff08\u8fdb\u884c\u4e2d\uff09 \u62a5\u540d\u5b8c\u6210 \u79d1\u4e00 \u79d1\u4e8c \u79d1\u4e09 \u79d1\u56db","title":"Kubernetes \u8fdb\u9636\u8bad\u7ec3\u8425"},{"location":"k8s/","text":"etcd \u5907\u4efd \u00b6 etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u5907\u4efd \u00b6 \u6062\u590d \u00b6","title":"etcd \u5907\u4efd"},{"location":"k8s/#etcd","text":"etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db","title":"etcd \u5907\u4efd"},{"location":"k8s/#_1","text":"","title":"\u5907\u4efd"},{"location":"k8s/#_2","text":"","title":"\u6062\u590d"},{"location":"Debian/Debian/","text":"\u67e5\u770b\u7cfb\u7edf\u7248\u672c\u4fe1\u606f root@nginx-run-685fdf6467-884vz:/config# cat /etc/os-release PRETTY_NAME = \"Debian GNU/Linux 11 (bullseye)\" NAME = \"Debian GNU/Linux\" VERSION_ID = \"11\" VERSION = \"11 (bullseye)\" VERSION_CODENAME = bullseye ID = debian HOME_URL = \"https://www.debian.org/\" SUPPORT_URL = \"https://www.debian.org/support\" BUG_REPORT_URL = \"https://bugs.debian.org/\" Debian \u6e90 \u6e05\u534e\u6e90\uff1ahttps://mirrors.tuna.tsinghua.edu.cn/help/debian/ cat <<EOF>> sources.list deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb http://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb http://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free EOF","title":"Debian\u64cd\u4f5c\u7cfb\u7edf"},{"location":"Helm/helm-install/","text":"Helm \u00b6 Helm \u7b80\u4ecb \u00b6 Helm \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u7ba1\u7406 Kubernetes \u5e94\u7528\u7a0b\u5e8f - Helm Charts \u53ef\u4ee5\u5b9a\u4e49\u3001\u5b89\u88c5\u548c\u5347\u7ea7\u590d\u6742\u7684 Kubernetes \u5e94\u7528\u7a0b\u5e8f\uff0cCharts \u5305\u5f88\u5bb9\u6613\u521b\u5efa\u3001\u7248\u672c\u7ba1\u7406\u3001\u5206\u4eab\u548c\u5206\u5e03. \u7b80\u5355\u53ef\u4ee5\u7406\u89e3Linux\u4e2dyum\u7684\u7684\u611f\u89c9 Helm \u5b89\u88c5 \u00b6 \u83b7\u53d6\u8f6f\u4ef6 \u5b98\u7f51\u5730\u5740\uff1ahttps://github.com/helm/helm/releases \u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u540e\uff0c\u5c06 helm \u4e8c\u8fdb\u5236\u5305\u6587\u4ef6\u79fb\u52a8\u5230\u4efb\u610f\u7684 PATH \u8def\u5f84\u4e0b $ helm version version.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.18.2\"} Linux \u4e0b\u5b89\u88c5 root@k8s-master:/opt# wget https://get.helm.sh/helm-v3.8.1-linux-amd64.tar.gz cni containerd helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt# tar -xf helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt/linux-amd64# mv helm /usr/bin/ \u7ba1\u7406\u914d\u7f6e \u00b6 Chart\u56fd\u5185\u4ed3\u5e93\u914d\u7f6e \u00b6 \u4ed3\u5e93\u914d\u7f6e \u5fae\u8f6f\u7684\u6e90\uff1ahttp://mirror.azure.cn/kubernetes/charts/ \u963f\u91cc\u7684\u6e90\uff1ahttps://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \u5b98\u65b9\u7684\u6e90\uff1ahttps://hub.kubeapps.com/charts/incubator \u6dfb\u52a0chart\u5b58\u50a8\u5e93 \u00b6 helm repo add stable http://mirror.azure.cn/kubernetes/charts helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts root@k8s-master:/opt/linux-amd64# helm repo update # \u66f4\u65b0\u6e90 \u67e5\u770b\u5b58\u50a8\u5e93 \u00b6 $ helm repo list NAME URL stable http://mirror.azure.cn/kubernetes/charts/ openbayes https://dev.openbayes.com/charts \u5220\u9664\u5b58\u50a8\u5e93 \u00b6 root@k8s-master:/opt/linux-amd64# helm repo remove aliyun Helm \u90e8\u7f72\u5e94\u7528: \u00b6 Helm \u90e8\u7f72traefix \u00b6 \u90e8\u7f72traefik \u6587\u7ae0\u53c2\u8003: https://github.com/traefik/traefik-helm-chart git clone https://github.com/traefik/traefik-helm-chart.git \u5b9a\u4e49values\u6587\u4ef6 $ cat sjtu-traefik.yaml image : name : traefik tag : \"2.7\" # values-prod.yaml # Create an IngressRoute for the dashboard ingressRoute : dashboard : enabled : false # \u7981\u7528helm\u4e2d\u6e32\u67d3\u7684dashboard\uff0c\u6211\u4eec\u81ea\u5df1\u624b\u52a8\u521b\u5efa # Configure ports ports : web : port : 8000 hostPort : 80 # \u4f7f\u7528 hostport \u6a21\u5f0f # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure : port : 8443 hostPort : 443 # \u4f7f\u7528 hostport \u6a21\u5f0f # Options for the main traefik service, where the entrypoints traffic comes # from. service : # \u4f7f\u7528 hostport \u6a21\u5f0f\u5c31\u4e0d\u9700\u8981Service\u4e86 enabled : false # Logs # https://docs.traefik.io/observability/logs/ #logs: # general: # level: DEBUG tolerations : # kubeadm \u5b89\u88c5\u7684\u96c6\u7fa4\u9ed8\u8ba4\u60c5\u51b5\u4e0bmaster\u662f\u6709\u6c61\u70b9\uff0c\u9700\u8981\u5bb9\u5fcd\u8fd9\u4e2a\u6c61\u70b9\u624d\u53ef\u4ee5\u90e8\u7f72 - key : \"node-role.kubernetes.io/master\" operator : \"Equal\" effect : \"NoSchedule\" nodeSelector : # \u56fa\u5b9a\u5230master1\u8282\u70b9\uff08\u8be5\u8282\u70b9\u624d\u53ef\u4ee5\u8bbf\u95ee\u5916\u7f51\uff09 kubernetes.io/hostname : \"master\" \u90e8\u7f72traefik helm upgrade --install traefik traefik/traefik -f ./traefik/values/sjtu-traefik.yaml --namespace kube-system \u67e5\u770b\u8fd9\u6b21\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u662f\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\u7684\uff0c-A \u53ef\u4ee5\u67e5\u770b\u6240\u6709helm release $ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik kube-system 5 2022-07-08 19:08:15.393244 +0800 CST deployed traefik-10.24.0 2.8.0 Helm \u5347\u7ea7\u56de\u6eda \u00b6 \u5347\u7ea7 \u00b6 \u4e3e\u4f8b\u8bf4\u660e \u4f8b\u5982\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6765\u5347\u7ea7grafana\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7--set \u5728\u540e\u9762\u4f20\u53c2\u6570 helm upgrade --install grafana grafana/grafana \u6ce8\u610f\u4e8b\u9879 \u5728\u5347\u7ea7\u5e94\u7528\u7a0b\u5e8f\u4e4b\u524d\u53ef\u4ee5\u901a\u8fc7diff\u7684\u65b9\u5f0f\u6765\u67e5\u770b\u4e24\u4e2a\u7248\u672c\u7684\u533a\u522b\uff0c\u786e\u5b9a\u6ca1\u95ee\u9898\uff0c\u518d\u5347\u7ea7 $ helm diff upgrade --install grafana grafana/grafana -f ./grafana.yaml -n infra \u56de\u6eda \u00b6 helm rollback version-id \u65e0\u8bba\u662f\u5347\u7ea7\u8fd8\u662f\u56de\u6eda\u90fd\u662f\u6709\u4e00\u4e2a\u7684\u98ce\u9669\u7684\uff0c\u6ce8\u610f\u9632\u6b62\u8bef\u64cd\u4f5c\u3002","title":"Helm \u7b80\u4ecb\u4e0e\u5b89\u88c5"},{"location":"Helm/helm-install/#helm","text":"","title":"Helm"},{"location":"Helm/helm-install/#helm_1","text":"Helm \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u7ba1\u7406 Kubernetes \u5e94\u7528\u7a0b\u5e8f - Helm Charts \u53ef\u4ee5\u5b9a\u4e49\u3001\u5b89\u88c5\u548c\u5347\u7ea7\u590d\u6742\u7684 Kubernetes \u5e94\u7528\u7a0b\u5e8f\uff0cCharts \u5305\u5f88\u5bb9\u6613\u521b\u5efa\u3001\u7248\u672c\u7ba1\u7406\u3001\u5206\u4eab\u548c\u5206\u5e03. \u7b80\u5355\u53ef\u4ee5\u7406\u89e3Linux\u4e2dyum\u7684\u7684\u611f\u89c9","title":"Helm \u7b80\u4ecb"},{"location":"Helm/helm-install/#helm_2","text":"\u83b7\u53d6\u8f6f\u4ef6 \u5b98\u7f51\u5730\u5740\uff1ahttps://github.com/helm/helm/releases \u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u540e\uff0c\u5c06 helm \u4e8c\u8fdb\u5236\u5305\u6587\u4ef6\u79fb\u52a8\u5230\u4efb\u610f\u7684 PATH \u8def\u5f84\u4e0b $ helm version version.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.18.2\"} Linux \u4e0b\u5b89\u88c5 root@k8s-master:/opt# wget https://get.helm.sh/helm-v3.8.1-linux-amd64.tar.gz cni containerd helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt# tar -xf helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt/linux-amd64# mv helm /usr/bin/","title":"Helm \u5b89\u88c5"},{"location":"Helm/helm-install/#_1","text":"","title":"\u7ba1\u7406\u914d\u7f6e"},{"location":"Helm/helm-install/#chart","text":"\u4ed3\u5e93\u914d\u7f6e \u5fae\u8f6f\u7684\u6e90\uff1ahttp://mirror.azure.cn/kubernetes/charts/ \u963f\u91cc\u7684\u6e90\uff1ahttps://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \u5b98\u65b9\u7684\u6e90\uff1ahttps://hub.kubeapps.com/charts/incubator","title":"Chart\u56fd\u5185\u4ed3\u5e93\u914d\u7f6e"},{"location":"Helm/helm-install/#chart_1","text":"helm repo add stable http://mirror.azure.cn/kubernetes/charts helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts root@k8s-master:/opt/linux-amd64# helm repo update # \u66f4\u65b0\u6e90","title":"\u6dfb\u52a0chart\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#_2","text":"$ helm repo list NAME URL stable http://mirror.azure.cn/kubernetes/charts/ openbayes https://dev.openbayes.com/charts","title":"\u67e5\u770b\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#_3","text":"root@k8s-master:/opt/linux-amd64# helm repo remove aliyun","title":"\u5220\u9664\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#helm_3","text":"","title":"Helm \u90e8\u7f72\u5e94\u7528:"},{"location":"Helm/helm-install/#helm-traefix","text":"\u90e8\u7f72traefik \u6587\u7ae0\u53c2\u8003: https://github.com/traefik/traefik-helm-chart git clone https://github.com/traefik/traefik-helm-chart.git \u5b9a\u4e49values\u6587\u4ef6 $ cat sjtu-traefik.yaml image : name : traefik tag : \"2.7\" # values-prod.yaml # Create an IngressRoute for the dashboard ingressRoute : dashboard : enabled : false # \u7981\u7528helm\u4e2d\u6e32\u67d3\u7684dashboard\uff0c\u6211\u4eec\u81ea\u5df1\u624b\u52a8\u521b\u5efa # Configure ports ports : web : port : 8000 hostPort : 80 # \u4f7f\u7528 hostport \u6a21\u5f0f # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure : port : 8443 hostPort : 443 # \u4f7f\u7528 hostport \u6a21\u5f0f # Options for the main traefik service, where the entrypoints traffic comes # from. service : # \u4f7f\u7528 hostport \u6a21\u5f0f\u5c31\u4e0d\u9700\u8981Service\u4e86 enabled : false # Logs # https://docs.traefik.io/observability/logs/ #logs: # general: # level: DEBUG tolerations : # kubeadm \u5b89\u88c5\u7684\u96c6\u7fa4\u9ed8\u8ba4\u60c5\u51b5\u4e0bmaster\u662f\u6709\u6c61\u70b9\uff0c\u9700\u8981\u5bb9\u5fcd\u8fd9\u4e2a\u6c61\u70b9\u624d\u53ef\u4ee5\u90e8\u7f72 - key : \"node-role.kubernetes.io/master\" operator : \"Equal\" effect : \"NoSchedule\" nodeSelector : # \u56fa\u5b9a\u5230master1\u8282\u70b9\uff08\u8be5\u8282\u70b9\u624d\u53ef\u4ee5\u8bbf\u95ee\u5916\u7f51\uff09 kubernetes.io/hostname : \"master\" \u90e8\u7f72traefik helm upgrade --install traefik traefik/traefik -f ./traefik/values/sjtu-traefik.yaml --namespace kube-system \u67e5\u770b\u8fd9\u6b21\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u662f\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\u7684\uff0c-A \u53ef\u4ee5\u67e5\u770b\u6240\u6709helm release $ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik kube-system 5 2022-07-08 19:08:15.393244 +0800 CST deployed traefik-10.24.0 2.8.0","title":"Helm \u90e8\u7f72traefix"},{"location":"Helm/helm-install/#helm_4","text":"","title":"Helm \u5347\u7ea7\u56de\u6eda"},{"location":"Helm/helm-install/#_4","text":"\u4e3e\u4f8b\u8bf4\u660e \u4f8b\u5982\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6765\u5347\u7ea7grafana\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7--set \u5728\u540e\u9762\u4f20\u53c2\u6570 helm upgrade --install grafana grafana/grafana \u6ce8\u610f\u4e8b\u9879 \u5728\u5347\u7ea7\u5e94\u7528\u7a0b\u5e8f\u4e4b\u524d\u53ef\u4ee5\u901a\u8fc7diff\u7684\u65b9\u5f0f\u6765\u67e5\u770b\u4e24\u4e2a\u7248\u672c\u7684\u533a\u522b\uff0c\u786e\u5b9a\u6ca1\u95ee\u9898\uff0c\u518d\u5347\u7ea7 $ helm diff upgrade --install grafana grafana/grafana -f ./grafana.yaml -n infra","title":"\u5347\u7ea7"},{"location":"Helm/helm-install/#_5","text":"helm rollback version-id \u65e0\u8bba\u662f\u5347\u7ea7\u8fd8\u662f\u56de\u6eda\u90fd\u662f\u6709\u4e00\u4e2a\u7684\u98ce\u9669\u7684\uff0c\u6ce8\u610f\u9632\u6b62\u8bef\u64cd\u4f5c\u3002","title":"\u56de\u6eda"},{"location":"Helm/helm/","text":"Helm \u914d\u7f6egrafana \u00b6 \u83b7\u53d6Token \u00b6 \u53c2\u8003: \u5b98\u7f51 curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}% \u6253\u5f00dashboardProviders \u00b6 \u6ce8\u610f \u6ce8\u610f\u53bb\u6389dashboardProviders\u540e\u9762\u7684{} dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default # \u6dfb\u52a0\u5b98\u7f51dashboard dashboards : default : ceph-cluster : gnetId : 2842 revision : 14 datasource : Prometheus ceph-osd : gnetId : 5336 revision : 5 datasource : Prometheus ceph-pools : gnetId : 5342 revision : 5 datasource : Prometheus token : 'eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ=='","title":"Helm \u914d\u7f6egrafana"},{"location":"Helm/helm/#helm-grafana","text":"","title":"Helm \u914d\u7f6egrafana"},{"location":"Helm/helm/#token","text":"\u53c2\u8003: \u5b98\u7f51 curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}%","title":"\u83b7\u53d6Token"},{"location":"Helm/helm/#dashboardproviders","text":"\u6ce8\u610f \u6ce8\u610f\u53bb\u6389dashboardProviders\u540e\u9762\u7684{} dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default # \u6dfb\u52a0\u5b98\u7f51dashboard dashboards : default : ceph-cluster : gnetId : 2842 revision : 14 datasource : Prometheus ceph-osd : gnetId : 5336 revision : 5 datasource : Prometheus ceph-pools : gnetId : 5342 revision : 5 datasource : Prometheus token : 'eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ=='","title":"\u6253\u5f00dashboardProviders"},{"location":"Storage/Juicefs/","text":"Juicefs\u673a\u5668\u5b66\u4e60\u5b58\u50a8\u65b9\u6848 \u00b6 \u73af\u5883\u8981\u6c42 docker\u73af\u5883 k3s \u73af\u5883\u5b89\u88c5 redis\u6570\u636e\u5e93 rook-ceph\u5b58\u50a8 juicefs \u73af\u5883\u90e8\u7f72 \u00b6 \u53c2\u8003\u6587\u7ae0: Juicefs\u5b98\u7f51 \u8fdb\u884c\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u5982\u679c\u662f\u751f\u4ea7\u73af\u5883\u9700\u8981\u8003\u8651redis\u7684\u9ad8\u53ef\u7528\uff0c\u4ee5\u53ca\u539f\u6570\u636e\u7684\u5907\u4efd\u3002 Cephfs\u548cjuicefs\u6027\u80fd\u5bf9\u6bd4 \u00b6 \u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4 \u00b6 cephfs \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 816 : Mon Aug 15 09 :48:26 2022 read: IOPS = 33 .5k, BW = 131MiB/s ( 137MB/s )( 200MiB/1527msec ) clat ( nsec ) : min = 570 , max = 248033k, avg = 29131 .89, stdev = 2094821 .44 lat ( nsec ) : min = 605 , max = 248033k, avg = 29167 .63, stdev = 2094821 .43 clat percentiles ( nsec ) : | 1 .00th =[ 644 ] , 5 .00th =[ 708 ] , 10 .00th =[ 732 ] , | 20 .00th =[ 748 ] , 30 .00th =[ 756 ] , 40 .00th =[ 780 ] , | 50 .00th =[ 796 ] , 60 .00th =[ 828 ] , 70 .00th =[ 892 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1160 ] , | 99 .00th =[ 1416 ] , 99 .50th =[ 1752 ] , 99 .90th =[ 23168 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 93847552 ] bw ( KiB/s ) : min = 73728 , max = 196608 , per = 21 .37%, avg = 135168 .00, stdev = 86889 .28, samples = 2 iops : min = 18432 , max = 49152 , avg = 33792 .00, stdev = 21722 .32, samples = 2 lat ( nsec ) : 750 = 23 .25%, 1000 = 63 .18% lat ( usec ) : 2 = 13 .14%, 4 = 0 .15%, 10 = 0 .12%, 20 = 0 .04%, 50 = 0 .02% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01% cpu : usr = 0 .72%, sys = 6 .55%, ctx = 81 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 823 : Mon Aug 15 09 :48:31 2022 read: IOPS = 41 .1k, BW = 161MiB/s ( 168MB/s )( 200MiB/1246msec ) clat ( nsec ) : min = 620 , max = 346202k, avg = 23787 .02, stdev = 1805527 .05 lat ( nsec ) : min = 654 , max = 346202k, avg = 23822 .73, stdev = 1805527 .05 clat percentiles ( nsec ) : | 1 .00th =[ 692 ] , 5 .00th =[ 740 ] , 10 .00th =[ 748 ] , | 20 .00th =[ 764 ] , 30 .00th =[ 780 ] , 40 .00th =[ 804 ] , | 50 .00th =[ 828 ] , 60 .00th =[ 876 ] , 70 .00th =[ 924 ] , | 80 .00th =[ 980 ] , 90 .00th =[ 1128 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 1464 ] , 99 .50th =[ 1672 ] , 99 .90th =[ 11712 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 63700992 ] bw ( KiB/s ) : min = 159960 , max = 224614 , per = 26 .87%, avg = 192287 .00, stdev = 45717 .28, samples = 2 iops : min = 39990 , max = 56153 , avg = 48071 .50, stdev = 11428 .97, samples = 2 lat ( nsec ) : 750 = 9 .15%, 1000 = 72 .77% lat ( usec ) : 2 = 17 .78%, 4 = 0 .09%, 10 = 0 .10%, 20 = 0 .04%, 50 = 0 .01% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 500 = 0 .01% cpu : usr = 0 .88%, sys = 8 .19%, ctx = 88 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 824 : Mon Aug 15 09 :48:31 2022 jufice \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) Jobs: 5 (f=5): [R(5)][66.7%][r=304MiB/s][r=77.8k IOPS][eta 00m:02s] big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=831: Mon Aug 15 09:50:58 2022 read: IOPS=14.9k, BW=58.0MiB/s (60.9MB/s)(200MiB/3446msec) clat (nsec): min=373, max=533198k, avg=67006.12, stdev=4299023.59 lat (nsec): min=406, max=533198k, avg=67043.41, stdev=4299023.63 clat percentiles (nsec): | 1.00th=[ 390], 5.00th=[ 418], 10.00th=[ 462], | 20.00th=[ 532], 30.00th=[ 548], 40.00th=[ 564], | 50.00th=[ 572], 60.00th=[ 580], 70.00th=[ 596], | 80.00th=[ 620], 90.00th=[ 692], 95.00th=[ 860], | 99.00th=[ 58624], 99.50th=[ 86528], 99.90th=[ 220160], | 99.95th=[ 3031040], 99.99th=[248512512] bw ( KiB/s): min=24576, max=90112, per=22.61%, avg=64140.67, stdev=22219.00, samples=6 iops : min= 6144, max=22528, avg=16035.17, stdev=5554.75, samples=6 lat (nsec) : 500=14.08%, 750=78.30%, 1000=4.37% lat (usec) : 2=0.36%, 4=0.65%, 10=0.30%, 20=0.09%, 50=0.28% lat (usec) : 100=1.21%, 250=0.26%, 500=0.03%, 750=0.01%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 20=0.01%, 50=0.01%, 100=0.01% lat (msec) : 250=0.01%, 500=0.01%, 750=0.01% cpu : usr=0.70%, sys=1.92%, ctx=874, majf=0, minf=16 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=838: Mon Aug 15 09:51:22 2022 read: IOPS=157k, BW=613MiB/s (643MB/s)(200MiB/326msec) clat (nsec): min=379, max=7481.0k, avg=5697.24, stdev=75385.63 lat (nsec): min=411, max=7481.1k, avg=5733.65, stdev=75387.68 clat percentiles (nsec): | 1.00th=[ 458], 5.00th=[ 498], 10.00th=[ 524], | 20.00th=[ 556], 30.00th=[ 572], 40.00th=[ 580], | 50.00th=[ 596], 60.00th=[ 612], 70.00th=[ 636], | 80.00th=[ 684], 90.00th=[ 852], 95.00th=[ 964], | 99.00th=[ 102912], 99.50th=[ 280576], 99.90th=[ 962560], | 99.95th=[1302528], 99.99th=[2539520] lat (nsec) : 500=5.63%, 750=79.66%, 1000=10.50% lat (usec) : 2=1.80%, 4=0.02%, 10=0.21%, 20=0.06%, 50=0.26% lat (usec) : 100=0.84%, 250=0.45%, 500=0.38%, 750=0.07%, 1000=0.03% lat (msec) : 2=0.08%, 4=0.01%, 10=0.01% cpu : usr=5.85%, sys=27.08%, ctx=1348, majf=0, minf=17 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=839: Mon Aug 15 09:51:22 2022","title":"Juicefs\u5b58\u50a8"},{"location":"Storage/Juicefs/#juicefs","text":"\u73af\u5883\u8981\u6c42 docker\u73af\u5883 k3s \u73af\u5883\u5b89\u88c5 redis\u6570\u636e\u5e93 rook-ceph\u5b58\u50a8","title":"Juicefs\u673a\u5668\u5b66\u4e60\u5b58\u50a8\u65b9\u6848"},{"location":"Storage/Juicefs/#juicefs_1","text":"\u53c2\u8003\u6587\u7ae0: Juicefs\u5b98\u7f51 \u8fdb\u884c\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u5982\u679c\u662f\u751f\u4ea7\u73af\u5883\u9700\u8981\u8003\u8651redis\u7684\u9ad8\u53ef\u7528\uff0c\u4ee5\u53ca\u539f\u6570\u636e\u7684\u5907\u4efd\u3002","title":"juicefs \u73af\u5883\u90e8\u7f72"},{"location":"Storage/Juicefs/#cephfsjuicefs","text":"","title":"Cephfs\u548cjuicefs\u6027\u80fd\u5bf9\u6bd4"},{"location":"Storage/Juicefs/#_1","text":"cephfs \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 816 : Mon Aug 15 09 :48:26 2022 read: IOPS = 33 .5k, BW = 131MiB/s ( 137MB/s )( 200MiB/1527msec ) clat ( nsec ) : min = 570 , max = 248033k, avg = 29131 .89, stdev = 2094821 .44 lat ( nsec ) : min = 605 , max = 248033k, avg = 29167 .63, stdev = 2094821 .43 clat percentiles ( nsec ) : | 1 .00th =[ 644 ] , 5 .00th =[ 708 ] , 10 .00th =[ 732 ] , | 20 .00th =[ 748 ] , 30 .00th =[ 756 ] , 40 .00th =[ 780 ] , | 50 .00th =[ 796 ] , 60 .00th =[ 828 ] , 70 .00th =[ 892 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1160 ] , | 99 .00th =[ 1416 ] , 99 .50th =[ 1752 ] , 99 .90th =[ 23168 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 93847552 ] bw ( KiB/s ) : min = 73728 , max = 196608 , per = 21 .37%, avg = 135168 .00, stdev = 86889 .28, samples = 2 iops : min = 18432 , max = 49152 , avg = 33792 .00, stdev = 21722 .32, samples = 2 lat ( nsec ) : 750 = 23 .25%, 1000 = 63 .18% lat ( usec ) : 2 = 13 .14%, 4 = 0 .15%, 10 = 0 .12%, 20 = 0 .04%, 50 = 0 .02% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01% cpu : usr = 0 .72%, sys = 6 .55%, ctx = 81 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 823 : Mon Aug 15 09 :48:31 2022 read: IOPS = 41 .1k, BW = 161MiB/s ( 168MB/s )( 200MiB/1246msec ) clat ( nsec ) : min = 620 , max = 346202k, avg = 23787 .02, stdev = 1805527 .05 lat ( nsec ) : min = 654 , max = 346202k, avg = 23822 .73, stdev = 1805527 .05 clat percentiles ( nsec ) : | 1 .00th =[ 692 ] , 5 .00th =[ 740 ] , 10 .00th =[ 748 ] , | 20 .00th =[ 764 ] , 30 .00th =[ 780 ] , 40 .00th =[ 804 ] , | 50 .00th =[ 828 ] , 60 .00th =[ 876 ] , 70 .00th =[ 924 ] , | 80 .00th =[ 980 ] , 90 .00th =[ 1128 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 1464 ] , 99 .50th =[ 1672 ] , 99 .90th =[ 11712 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 63700992 ] bw ( KiB/s ) : min = 159960 , max = 224614 , per = 26 .87%, avg = 192287 .00, stdev = 45717 .28, samples = 2 iops : min = 39990 , max = 56153 , avg = 48071 .50, stdev = 11428 .97, samples = 2 lat ( nsec ) : 750 = 9 .15%, 1000 = 72 .77% lat ( usec ) : 2 = 17 .78%, 4 = 0 .09%, 10 = 0 .10%, 20 = 0 .04%, 50 = 0 .01% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 500 = 0 .01% cpu : usr = 0 .88%, sys = 8 .19%, ctx = 88 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 824 : Mon Aug 15 09 :48:31 2022 jufice \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) Jobs: 5 (f=5): [R(5)][66.7%][r=304MiB/s][r=77.8k IOPS][eta 00m:02s] big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=831: Mon Aug 15 09:50:58 2022 read: IOPS=14.9k, BW=58.0MiB/s (60.9MB/s)(200MiB/3446msec) clat (nsec): min=373, max=533198k, avg=67006.12, stdev=4299023.59 lat (nsec): min=406, max=533198k, avg=67043.41, stdev=4299023.63 clat percentiles (nsec): | 1.00th=[ 390], 5.00th=[ 418], 10.00th=[ 462], | 20.00th=[ 532], 30.00th=[ 548], 40.00th=[ 564], | 50.00th=[ 572], 60.00th=[ 580], 70.00th=[ 596], | 80.00th=[ 620], 90.00th=[ 692], 95.00th=[ 860], | 99.00th=[ 58624], 99.50th=[ 86528], 99.90th=[ 220160], | 99.95th=[ 3031040], 99.99th=[248512512] bw ( KiB/s): min=24576, max=90112, per=22.61%, avg=64140.67, stdev=22219.00, samples=6 iops : min= 6144, max=22528, avg=16035.17, stdev=5554.75, samples=6 lat (nsec) : 500=14.08%, 750=78.30%, 1000=4.37% lat (usec) : 2=0.36%, 4=0.65%, 10=0.30%, 20=0.09%, 50=0.28% lat (usec) : 100=1.21%, 250=0.26%, 500=0.03%, 750=0.01%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 20=0.01%, 50=0.01%, 100=0.01% lat (msec) : 250=0.01%, 500=0.01%, 750=0.01% cpu : usr=0.70%, sys=1.92%, ctx=874, majf=0, minf=16 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=838: Mon Aug 15 09:51:22 2022 read: IOPS=157k, BW=613MiB/s (643MB/s)(200MiB/326msec) clat (nsec): min=379, max=7481.0k, avg=5697.24, stdev=75385.63 lat (nsec): min=411, max=7481.1k, avg=5733.65, stdev=75387.68 clat percentiles (nsec): | 1.00th=[ 458], 5.00th=[ 498], 10.00th=[ 524], | 20.00th=[ 556], 30.00th=[ 572], 40.00th=[ 580], | 50.00th=[ 596], 60.00th=[ 612], 70.00th=[ 636], | 80.00th=[ 684], 90.00th=[ 852], 95.00th=[ 964], | 99.00th=[ 102912], 99.50th=[ 280576], 99.90th=[ 962560], | 99.95th=[1302528], 99.99th=[2539520] lat (nsec) : 500=5.63%, 750=79.66%, 1000=10.50% lat (usec) : 2=1.80%, 4=0.02%, 10=0.21%, 20=0.06%, 50=0.26% lat (usec) : 100=0.84%, 250=0.45%, 500=0.38%, 750=0.07%, 1000=0.03% lat (msec) : 2=0.08%, 4=0.01%, 10=0.01% cpu : usr=5.85%, sys=27.08%, ctx=1348, majf=0, minf=17 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=839: Mon Aug 15 09:51:22 2022","title":"\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4"},{"location":"Storage/osd/","text":"osd \u6027\u80fd\u6d4b\u8bd5 \u00b6 \u73af\u5883\u8981\u6c42 \u51c6\u5907\u4e00\u4e2ak8s\u96c6\u7fa4 \u51c6\u5907\u4e00\u4e2aceph\u96c6\u7fa4 \u673a\u68b0\u78c1\u76d8\u6d4b\u8bd5 \u00b6 \u80cc\u666f \u6d4b\u8bd5\u673a\u68b0\u78c1\u76d8\u66f4\u6362\u56fa\u6001\u78c1\u76d8\u540e\uff0cosd\u7684\u8bfb\u5199\u901f\u5ea6 \u65b9\u6cd5\u4e00\uff1aFio \u538b\u529b\u6d4b\u8bd5 \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u00b6 fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest filename : \u538b\u6d4b\u7684\u6587\u4ef6\uff08\u6302\u5728ceph\u7684\u76ee\u5f55\u4e0b\uff09 -iodepth : \u961f\u5217\u6df1\u5ea6 -size : \u6307\u5b9a\u5199\u591a\u5927\u7684\u6570\u636e rw : I/O\u6a21\u5f0f\uff0c\u968f\u673a\u8bfb\u5199\uff0c\u987a\u5e8f\u8bfb\u5199\u7b49\u7b49 bs : I/O block\u5927\u5c0f \u793a\u4f8b\uff1a \u00b6 4K\u968f\u673a\u5199-iops fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randwrite, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ w ( 6 )][ 100 .0% ][ w = 15 .0MiB/s ][ w = 4089 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 96930 : Mon Aug 1 14 :34:37 2022 write: IOPS = 4104 , BW = 16 .0MiB/s ( 16 .8MB/s )( 963MiB/60054msec ) ; 0 zone resets slat ( nsec ) : min = 1755 , max = 48814k, avg = 10925 .38, stdev = 316419 .53 clat ( msec ) : min = 5 , max = 191 , avg = 46 .76, stdev = 13 .55 lat ( msec ) : min = 5 , max = 191 , avg = 46 .77, stdev = 13 .54 clat percentiles ( msec ) : | 1 .00th =[ 25 ] , 5 .00th =[ 29 ] , 10 .00th =[ 31 ] , 20 .00th =[ 33 ] , | 30 .00th =[ 37 ] , 40 .00th =[ 44 ] , 50 .00th =[ 49 ] , 60 .00th =[ 53 ] , | 70 .00th =[ 55 ] , 80 .00th =[ 58 ] , 90 .00th =[ 61 ] , 95 .00th =[ 64 ] , | 99 .00th =[ 85 ] , 99 .50th =[ 96 ] , 99 .90th =[ 130 ] , 99 .95th =[ 142 ] , | 99 .99th =[ 163 ] bw ( KiB/s ) : min = 14912 , max = 19106 , per = 100 .00%, avg = 16419 .87, stdev = 80 .74, samples = 720 iops : min = 3728 , max = 4776 , avg = 4104 .83, stdev = 20 .18, samples = 720 lat ( msec ) : 10 = 0 .04%, 20 = 0 .38%, 50 = 53 .46%, 100 = 45 .67%, 250 = 0 .45% cpu : usr = 0 .19%, sys = 0 .71%, ctx = 182573 , majf = 1 , minf = 11 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,246515,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 16 .0MiB/s ( 16 .8MB/s ) , 16 .0MiB/s-16.0MiB/s ( 16 .8MB/s-16.8MB/s ) , io = 963MiB ( 1010MB ) , run = 60054 -60054msec Disk stats ( read/write ) : rbd0: ios = 4 /245332, merge = 0 /637, ticks = 81 /11412705, in_queue = 10919116 , util = 57 .39% 4k\u968f\u673a\u8bfb-iops [ ucloud ] root@master0:~# fio -filename = /data/fio2.img -direct = 1 -iodepth 32 -thread -rw = randread -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randread, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ r ( 6 )][ 88 .9% ][ r = 200MiB/s ][ r = 51 .2k IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 101153 : Mon Aug 1 14 :40:36 2022 read: IOPS = 36 .1k, BW = 141MiB/s ( 148MB/s )( 1200MiB/8508msec ) slat ( nsec ) : min = 1151 , max = 14537k, avg = 19572 .33, stdev = 101622 .30 clat ( nsec ) : min = 455 , max = 79415k, avg = 5280191 .11, stdev = 7133988 .93 lat ( usec ) : min = 64 , max = 79418 , avg = 5299 .94, stdev = 7130 .13 clat percentiles ( usec ) : | 1 .00th =[ 363 ] , 5 .00th =[ 865 ] , 10 .00th =[ 1237 ] , 20 .00th =[ 1778 ] , | 30 .00th =[ 2245 ] , 40 .00th =[ 2704 ] , 50 .00th =[ 3163 ] , 60 .00th =[ 3720 ] , | 70 .00th =[ 4490 ] , 80 .00th =[ 5866 ] , 90 .00th =[ 10290 ] , 95 .00th =[ 19268 ] , | 99 .00th =[ 40109 ] , 99 .50th =[ 44303 ] , 99 .90th =[ 52167 ] , 99 .95th =[ 56361 ] , | 99 .99th =[ 65274 ] bw ( KiB/s ) : min = 45752 , max = 234048 , per = 98 .88%, avg = 142805 .42, stdev = 12299 .59, samples = 98 iops : min = 11438 , max = 58512 , avg = 35701 .11, stdev = 3074 .86, samples = 98 lat ( nsec ) : 500 = 0 .01% lat ( usec ) : 20 = 0 .01%, 50 = 0 .01%, 100 = 0 .03%, 250 = 0 .38%, 500 = 1 .39% lat ( usec ) : 750 = 2 .05%, 1000 = 2 .80% lat ( msec ) : 2 = 18 .03%, 4 = 39 .21%, 10 = 25 .81%, 20 = 5 .53%, 50 = 4 .61% lat ( msec ) : 100 = 0 .14% cpu : usr = 1 .03%, sys = 3 .66%, ctx = 239175 , majf = 0 , minf = 198 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 307200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 141MiB/s ( 148MB/s ) , 141MiB/s-141MiB/s ( 148MB/s-148MB/s ) , io = 1200MiB ( 1258MB ) , run = 8508 -8508msec Disk stats ( read/write ) : rbd0: ios = 298435 /0, merge = 1141 /0, ticks = 1470086 /0, in_queue = 887248 , util = 98 .83% 4k\u968f\u673a\u8bfb\u5199-iops [ ucloud ] root@master0:~# fio -filename = /data/fio3.img -direct = 1 -iodepth 32 -thread -rw = randrw -rwmixread = 70 -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randrw, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 6 ( f = 6 ) : [ m ( 6 )][ 100 .0% ][ r = 26 .7MiB/s,w = 11 .6MiB/s ][ r = 6835 ,w = 2976 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 104592 : Mon Aug 1 14 :45:40 2022 read: IOPS = 6434 , BW = 25 .1MiB/s ( 26 .4MB/s )( 838MiB/33355msec ) slat ( nsec ) : min = 1268 , max = 512155 , avg = 5128 .49, stdev = 4908 .50 clat ( usec ) : min = 108 , max = 237284 , avg = 17218 .47, stdev = 13454 .22 lat ( usec ) : min = 112 , max = 237286 , avg = 17223 .74, stdev = 13454 .22 clat percentiles ( usec ) : | 1 .00th =[ 857 ] , 5 .00th =[ 1696 ] , 10 .00th =[ 2507 ] , 20 .00th =[ 4293 ] , | 30 .00th =[ 6980 ] , 40 .00th =[ 12256 ] , 50 .00th =[ 17695 ] , 60 .00th =[ 20579 ] , | 70 .00th =[ 22938 ] , 80 .00th =[ 25560 ] , 90 .00th =[ 31589 ] , 95 .00th =[ 41157 ] , | 99 .00th =[ 55313 ] , 99 .50th =[ 64226 ] , 99 .90th =[ 94897 ] , 99 .95th =[ 154141 ] , | 99 .99th =[ 227541 ] bw ( KiB/s ) : min = 18128 , max = 31360 , per = 99 .94%, avg = 25724 .41, stdev = 445 .55, samples = 396 iops : min = 4532 , max = 7840 , avg = 6430 .97, stdev = 111 .38, samples = 396 write: IOPS = 2775 , BW = 10 .8MiB/s ( 11 .4MB/s )( 362MiB/33355msec ) ; 0 zone resets slat ( nsec ) : min = 1519 , max = 586139 , avg = 5581 .99, stdev = 5697 .03 clat ( usec ) : min = 735 , max = 234047 , avg = 29153 .24, stdev = 13547 .74 lat ( usec ) : min = 740 , max = 234050 , avg = 29158 .97, stdev = 13547 .75 clat percentiles ( msec ) : | 1 .00th =[ 5 ] , 5 .00th =[ 14 ] , 10 .00th =[ 19 ] , 20 .00th =[ 21 ] , | 30 .00th =[ 23 ] , 40 .00th =[ 24 ] , 50 .00th =[ 26 ] , 60 .00th =[ 28 ] , | 70 .00th =[ 33 ] , 80 .00th =[ 40 ] , 90 .00th =[ 46 ] , 95 .00th =[ 52 ] , | 99 .00th =[ 67 ] , 99 .50th =[ 77 ] , 99 .90th =[ 153 ] , 99 .95th =[ 215 ] , | 99 .99th =[ 230 ] bw ( KiB/s ) : min = 7264 , max = 13640 , per = 99 .93%, avg = 11092 .62, stdev = 204 .64, samples = 396 iops : min = 1816 , max = 3410 , avg = 2773 .11, stdev = 51 .15, samples = 396 lat ( usec ) : 250 = 0 .01%, 500 = 0 .12%, 750 = 0 .35%, 1000 = 0 .56% lat ( msec ) : 2 = 3 .77%, 4 = 8 .37%, 10 = 13 .38%, 20 = 18 .51%, 50 = 52 .05% lat ( msec ) : 100 = 2 .75%, 250 = 0 .11% cpu : usr = 0 .42%, sys = 1 .37%, ctx = 262129 , majf = 1 , minf = 6 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 214635 ,92565,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 25 .1MiB/s ( 26 .4MB/s ) , 25 .1MiB/s-25.1MiB/s ( 26 .4MB/s-26.4MB/s ) , io = 838MiB ( 879MB ) , run = 33355 -33355msec WRITE: bw = 10 .8MiB/s ( 11 .4MB/s ) , 10 .8MiB/s-10.8MiB/s ( 11 .4MB/s-11.4MB/s ) , io = 362MiB ( 379MB ) , run = 33355 -33355msec Disk stats ( read/write ) : rbd0: ios = 212275 /91760, merge = 1000 /200, ticks = 3650146 /2659754, in_queue = 5701392 , util = 86 .01% 1M\u987a\u5e8f\u5199-\u541e\u5410 [ ucloud ] root@master0:~# fio -filename = /data/fio4.img -direct = 1 -iodepth 32 -thread -rw = write -ioengine = libaio -bs = 1M -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = write, bs =( R ) 1024KiB-1024KiB, ( W ) 1024KiB-1024KiB, ( T ) 1024KiB-1024KiB, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 3 ( f = 3 ) : [ _ ( 2 ) ,W ( 2 ) ,_ ( 1 ) ,W ( 1 )][ 90 .0% ][ w = 177MiB/s ][ w = 177 IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 107996 : Mon Aug 1 14 :51:00 2022 write: IOPS = 131 , BW = 131MiB/s ( 138MB/s )( 1200MiB/9131msec ) ; 0 zone resets slat ( usec ) : min = 41 , max = 13194 , avg = 208 .41, stdev = 756 .44 clat ( msec ) : min = 20 , max = 3855 , avg = 1392 .05, stdev = 816 .66 lat ( msec ) : min = 20 , max = 3855 , avg = 1392 .26, stdev = 816 .65 clat percentiles ( msec ) : | 1 .00th =[ 284 ] , 5 .00th =[ 351 ] , 10 .00th =[ 435 ] , 20 .00th =[ 625 ] , | 30 .00th =[ 885 ] , 40 .00th =[ 1099 ] , 50 .00th =[ 1234 ] , 60 .00th =[ 1418 ] , | 70 .00th =[ 1620 ] , 80 .00th =[ 1938 ] , 90 .00th =[ 2769 ] , 95 .00th =[ 2903 ] , | 99 .00th =[ 3608 ] , 99 .50th =[ 3675 ] , 99 .90th =[ 3809 ] , 99 .95th =[ 3842 ] , | 99 .99th =[ 3842 ] bw ( KiB/s ) : min = 24554 , max = 273468 , per = 99 .58%, avg = 134013 .71, stdev = 11587 .87, samples = 93 iops : min = 22 , max = 267 , avg = 130 .28, stdev = 11 .35, samples = 93 lat ( msec ) : 50 = 0 .25%, 250 = 0 .33%, 500 = 13 .33%, 750 = 11 .50%, 1000 = 10 .08% lat ( msec ) : 2000 = 44 .92%, > = 2000 = 19 .58% cpu : usr = 0 .18%, sys = 0 .09%, ctx = 792 , majf = 1 , minf = 6 IO depths : 1 = 0 .5%, 2 = 1 .0%, 4 = 2 .0%, 8 = 4 .0%, 16 = 8 .0%, 32 = 84 .5%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 99 .4%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .6%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,1200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 1200MiB ( 1258MB ) , run = 9131 -9131msec Disk stats ( read/write ) : rbd0: ios = 0 /887, merge = 0 /280, ticks = 0 /1112233, in_queue = 1110460 , util = 16 .67% \u65b9\u6cd5\u4e8c\uff1aRBD bench \u538b\u529b\u6d4b\u8bd5 \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u00b6 [ ucloud ] root@master0:~# rbd help bench usage : rbd bench [--pool <pool>] [--namespace <namespace>] [--image <image>] [--io-size <io-size>] [--io-threads <io-threads>] [--io-total <io-total>] [--io-pattern <io-pattern>] [--rw-mix-read <rw-mix-read>] --io-type <io-type> <image-spec> Simple benchmark. Positional arguments <image-spec> image specification (example : [ <pool-name>/ [ <namespace>/ ]] <image-name>) Optional arguments -p [ --pool ] arg pool name # \u6307\u5b9apool\u7684\u540d\u79f0 --namespace arg namespace name # \u6307\u5b9anamespace --image arg image name --io-size arg IO size (in B/K/M/G/T) [default : 4K] # \u6307\u5b9aIO\u5927\u5c0f --io-threads arg ios in flight [default : 16] # \u6307\u5b9a\u5e76\u53d1 --io-total arg total size for IO (in B/K/M/G/T) [default : 1G] # \u6570\u636e\u7684\u5927\u5c0f --io-pattern arg IO pattern (rand, seq, or full-seq) [default : seq] # iops\uff08rand\u4e3a\u968f\u673a\uff0cseq\u987a\u5e8f\uff09 --rw-mix-read arg read proportion in readwrite (<= 100) [default : 50] # --rw-mix-read \u6df7\u5408\u8bfb\u5199\u8bfb\u7684\u5360\u6bd4 --io-type arg IO type (read, write, or readwrite(rw)) # \u7c7b\u578b\uff0c\u8981\u4ee5\u4ec0\u4e48\u65b9\u5f0f\u538b\u6d4b\uff0c\u8bfb\u6216\u8005\u5199 \u793a\u4f8b\uff1a \u00b6 \u6e29\u99a8\u63d0\u793a \u538b\u6d4b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7iostat -x 1 \u5bf9\u78c1\u76d8\u8fdb\u884c\u76d1\u63a7 - ceph osd perf \u53ef\u4ee5\u4f7f\u7528\u8be5\u547d\u4ee4\u67e5\u770bosd\u5ef6\u8fdf\u60c5\u51b5 4K\u968f\u673a\u5199 rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type write bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 5536 5574 .28 22 MiB/s 2 9664 4849 .68 19 MiB/s 3 13776 4603 .46 18 MiB/s 4 17968 4500 .49 18 MiB/s ...... 64 261440 3935 .99 15 MiB/s elapsed: 64 ops: 262144 ops/sec: 4064 .99 bytes/sec: 16 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a4064.99 4K\u968f\u673a\u8bfb [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type read bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 26816 26939 .7 105 MiB/s 2 53728 26925 .8 105 MiB/s 3 81648 27257 .6 106 MiB/s 4 90896 22569 .9 88 MiB/s 5 115328 23087 .2 90 MiB/s 6 142416 23119 .9 90 MiB/s 7 170048 23263 .9 91 MiB/s 8 197104 23091 .1 90 MiB/s 9 225264 27046 .6 106 MiB/s 10 253360 27606 .3 108 MiB/s elapsed: 10 ops: 262144 ops/sec: 25391 .6 bytes/sec: 99 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a25391.6 4K\u968f\u673a\u6df7\u5408\u8bfb\u5199 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type readwrite --rw-mix-read 70 bench type readwrite read:write = 70 :30 io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 12144 12208 .8 48 MiB/s 2 23488 11775 .5 46 MiB/s 3 34624 11562 45 MiB/s 4 45552 11403 .4 45 MiB/s 5 56528 11317 .8 44 MiB/s 6 67664 11104 43 MiB/s 7 78976 11097 .6 43 MiB/s 8 89872 11049 .6 43 MiB/s 9 101216 11132 .8 43 MiB/s 10 112608 11216 44 MiB/s ..... elapsed: 23 ops: 262144 ops/sec: 11012 .6 bytes/sec: 43 MiB/s read_ops: 183730 read_ops/sec: 7718 .43 read_bytes/sec: 30 MiB/s write_ops: 78414 write_ops/sec: 3294 .14 write_bytes/sec: 13 MiB/s 1M\u987a\u5e8f\u5199\uff08\u6d4b\u541e\u5410\u91cf\uff09 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 1M --io-threads 16 --io-total 200M --io-pattern seq --io-type write bench type write io_size 1048576 io_threads 16 bytes 209715200 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 160 175 .298 175 MiB/s elapsed: 1 ops: 200 ops/sec: 136 .986 bytes/sec: 137 MiB/s \u56fa\u6001\u78c1\u76d8\u6d4b\u8bd5","title":"osd \u6027\u80fd\u6d4b\u8bd5"},{"location":"Storage/osd/#osd","text":"\u73af\u5883\u8981\u6c42 \u51c6\u5907\u4e00\u4e2ak8s\u96c6\u7fa4 \u51c6\u5907\u4e00\u4e2aceph\u96c6\u7fa4","title":"osd \u6027\u80fd\u6d4b\u8bd5"},{"location":"Storage/osd/#_1","text":"\u80cc\u666f \u6d4b\u8bd5\u673a\u68b0\u78c1\u76d8\u66f4\u6362\u56fa\u6001\u78c1\u76d8\u540e\uff0cosd\u7684\u8bfb\u5199\u901f\u5ea6","title":"\u673a\u68b0\u78c1\u76d8\u6d4b\u8bd5"},{"location":"Storage/osd/#fio","text":"","title":"\u65b9\u6cd5\u4e00\uff1aFio \u538b\u529b\u6d4b\u8bd5"},{"location":"Storage/osd/#_2","text":"fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest filename : \u538b\u6d4b\u7684\u6587\u4ef6\uff08\u6302\u5728ceph\u7684\u76ee\u5f55\u4e0b\uff09 -iodepth : \u961f\u5217\u6df1\u5ea6 -size : \u6307\u5b9a\u5199\u591a\u5927\u7684\u6570\u636e rw : I/O\u6a21\u5f0f\uff0c\u968f\u673a\u8bfb\u5199\uff0c\u987a\u5e8f\u8bfb\u5199\u7b49\u7b49 bs : I/O block\u5927\u5c0f","title":"\u53c2\u6570\u8bf4\u660e\uff1a"},{"location":"Storage/osd/#_3","text":"4K\u968f\u673a\u5199-iops fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randwrite, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ w ( 6 )][ 100 .0% ][ w = 15 .0MiB/s ][ w = 4089 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 96930 : Mon Aug 1 14 :34:37 2022 write: IOPS = 4104 , BW = 16 .0MiB/s ( 16 .8MB/s )( 963MiB/60054msec ) ; 0 zone resets slat ( nsec ) : min = 1755 , max = 48814k, avg = 10925 .38, stdev = 316419 .53 clat ( msec ) : min = 5 , max = 191 , avg = 46 .76, stdev = 13 .55 lat ( msec ) : min = 5 , max = 191 , avg = 46 .77, stdev = 13 .54 clat percentiles ( msec ) : | 1 .00th =[ 25 ] , 5 .00th =[ 29 ] , 10 .00th =[ 31 ] , 20 .00th =[ 33 ] , | 30 .00th =[ 37 ] , 40 .00th =[ 44 ] , 50 .00th =[ 49 ] , 60 .00th =[ 53 ] , | 70 .00th =[ 55 ] , 80 .00th =[ 58 ] , 90 .00th =[ 61 ] , 95 .00th =[ 64 ] , | 99 .00th =[ 85 ] , 99 .50th =[ 96 ] , 99 .90th =[ 130 ] , 99 .95th =[ 142 ] , | 99 .99th =[ 163 ] bw ( KiB/s ) : min = 14912 , max = 19106 , per = 100 .00%, avg = 16419 .87, stdev = 80 .74, samples = 720 iops : min = 3728 , max = 4776 , avg = 4104 .83, stdev = 20 .18, samples = 720 lat ( msec ) : 10 = 0 .04%, 20 = 0 .38%, 50 = 53 .46%, 100 = 45 .67%, 250 = 0 .45% cpu : usr = 0 .19%, sys = 0 .71%, ctx = 182573 , majf = 1 , minf = 11 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,246515,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 16 .0MiB/s ( 16 .8MB/s ) , 16 .0MiB/s-16.0MiB/s ( 16 .8MB/s-16.8MB/s ) , io = 963MiB ( 1010MB ) , run = 60054 -60054msec Disk stats ( read/write ) : rbd0: ios = 4 /245332, merge = 0 /637, ticks = 81 /11412705, in_queue = 10919116 , util = 57 .39% 4k\u968f\u673a\u8bfb-iops [ ucloud ] root@master0:~# fio -filename = /data/fio2.img -direct = 1 -iodepth 32 -thread -rw = randread -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randread, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ r ( 6 )][ 88 .9% ][ r = 200MiB/s ][ r = 51 .2k IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 101153 : Mon Aug 1 14 :40:36 2022 read: IOPS = 36 .1k, BW = 141MiB/s ( 148MB/s )( 1200MiB/8508msec ) slat ( nsec ) : min = 1151 , max = 14537k, avg = 19572 .33, stdev = 101622 .30 clat ( nsec ) : min = 455 , max = 79415k, avg = 5280191 .11, stdev = 7133988 .93 lat ( usec ) : min = 64 , max = 79418 , avg = 5299 .94, stdev = 7130 .13 clat percentiles ( usec ) : | 1 .00th =[ 363 ] , 5 .00th =[ 865 ] , 10 .00th =[ 1237 ] , 20 .00th =[ 1778 ] , | 30 .00th =[ 2245 ] , 40 .00th =[ 2704 ] , 50 .00th =[ 3163 ] , 60 .00th =[ 3720 ] , | 70 .00th =[ 4490 ] , 80 .00th =[ 5866 ] , 90 .00th =[ 10290 ] , 95 .00th =[ 19268 ] , | 99 .00th =[ 40109 ] , 99 .50th =[ 44303 ] , 99 .90th =[ 52167 ] , 99 .95th =[ 56361 ] , | 99 .99th =[ 65274 ] bw ( KiB/s ) : min = 45752 , max = 234048 , per = 98 .88%, avg = 142805 .42, stdev = 12299 .59, samples = 98 iops : min = 11438 , max = 58512 , avg = 35701 .11, stdev = 3074 .86, samples = 98 lat ( nsec ) : 500 = 0 .01% lat ( usec ) : 20 = 0 .01%, 50 = 0 .01%, 100 = 0 .03%, 250 = 0 .38%, 500 = 1 .39% lat ( usec ) : 750 = 2 .05%, 1000 = 2 .80% lat ( msec ) : 2 = 18 .03%, 4 = 39 .21%, 10 = 25 .81%, 20 = 5 .53%, 50 = 4 .61% lat ( msec ) : 100 = 0 .14% cpu : usr = 1 .03%, sys = 3 .66%, ctx = 239175 , majf = 0 , minf = 198 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 307200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 141MiB/s ( 148MB/s ) , 141MiB/s-141MiB/s ( 148MB/s-148MB/s ) , io = 1200MiB ( 1258MB ) , run = 8508 -8508msec Disk stats ( read/write ) : rbd0: ios = 298435 /0, merge = 1141 /0, ticks = 1470086 /0, in_queue = 887248 , util = 98 .83% 4k\u968f\u673a\u8bfb\u5199-iops [ ucloud ] root@master0:~# fio -filename = /data/fio3.img -direct = 1 -iodepth 32 -thread -rw = randrw -rwmixread = 70 -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randrw, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 6 ( f = 6 ) : [ m ( 6 )][ 100 .0% ][ r = 26 .7MiB/s,w = 11 .6MiB/s ][ r = 6835 ,w = 2976 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 104592 : Mon Aug 1 14 :45:40 2022 read: IOPS = 6434 , BW = 25 .1MiB/s ( 26 .4MB/s )( 838MiB/33355msec ) slat ( nsec ) : min = 1268 , max = 512155 , avg = 5128 .49, stdev = 4908 .50 clat ( usec ) : min = 108 , max = 237284 , avg = 17218 .47, stdev = 13454 .22 lat ( usec ) : min = 112 , max = 237286 , avg = 17223 .74, stdev = 13454 .22 clat percentiles ( usec ) : | 1 .00th =[ 857 ] , 5 .00th =[ 1696 ] , 10 .00th =[ 2507 ] , 20 .00th =[ 4293 ] , | 30 .00th =[ 6980 ] , 40 .00th =[ 12256 ] , 50 .00th =[ 17695 ] , 60 .00th =[ 20579 ] , | 70 .00th =[ 22938 ] , 80 .00th =[ 25560 ] , 90 .00th =[ 31589 ] , 95 .00th =[ 41157 ] , | 99 .00th =[ 55313 ] , 99 .50th =[ 64226 ] , 99 .90th =[ 94897 ] , 99 .95th =[ 154141 ] , | 99 .99th =[ 227541 ] bw ( KiB/s ) : min = 18128 , max = 31360 , per = 99 .94%, avg = 25724 .41, stdev = 445 .55, samples = 396 iops : min = 4532 , max = 7840 , avg = 6430 .97, stdev = 111 .38, samples = 396 write: IOPS = 2775 , BW = 10 .8MiB/s ( 11 .4MB/s )( 362MiB/33355msec ) ; 0 zone resets slat ( nsec ) : min = 1519 , max = 586139 , avg = 5581 .99, stdev = 5697 .03 clat ( usec ) : min = 735 , max = 234047 , avg = 29153 .24, stdev = 13547 .74 lat ( usec ) : min = 740 , max = 234050 , avg = 29158 .97, stdev = 13547 .75 clat percentiles ( msec ) : | 1 .00th =[ 5 ] , 5 .00th =[ 14 ] , 10 .00th =[ 19 ] , 20 .00th =[ 21 ] , | 30 .00th =[ 23 ] , 40 .00th =[ 24 ] , 50 .00th =[ 26 ] , 60 .00th =[ 28 ] , | 70 .00th =[ 33 ] , 80 .00th =[ 40 ] , 90 .00th =[ 46 ] , 95 .00th =[ 52 ] , | 99 .00th =[ 67 ] , 99 .50th =[ 77 ] , 99 .90th =[ 153 ] , 99 .95th =[ 215 ] , | 99 .99th =[ 230 ] bw ( KiB/s ) : min = 7264 , max = 13640 , per = 99 .93%, avg = 11092 .62, stdev = 204 .64, samples = 396 iops : min = 1816 , max = 3410 , avg = 2773 .11, stdev = 51 .15, samples = 396 lat ( usec ) : 250 = 0 .01%, 500 = 0 .12%, 750 = 0 .35%, 1000 = 0 .56% lat ( msec ) : 2 = 3 .77%, 4 = 8 .37%, 10 = 13 .38%, 20 = 18 .51%, 50 = 52 .05% lat ( msec ) : 100 = 2 .75%, 250 = 0 .11% cpu : usr = 0 .42%, sys = 1 .37%, ctx = 262129 , majf = 1 , minf = 6 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 214635 ,92565,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 25 .1MiB/s ( 26 .4MB/s ) , 25 .1MiB/s-25.1MiB/s ( 26 .4MB/s-26.4MB/s ) , io = 838MiB ( 879MB ) , run = 33355 -33355msec WRITE: bw = 10 .8MiB/s ( 11 .4MB/s ) , 10 .8MiB/s-10.8MiB/s ( 11 .4MB/s-11.4MB/s ) , io = 362MiB ( 379MB ) , run = 33355 -33355msec Disk stats ( read/write ) : rbd0: ios = 212275 /91760, merge = 1000 /200, ticks = 3650146 /2659754, in_queue = 5701392 , util = 86 .01% 1M\u987a\u5e8f\u5199-\u541e\u5410 [ ucloud ] root@master0:~# fio -filename = /data/fio4.img -direct = 1 -iodepth 32 -thread -rw = write -ioengine = libaio -bs = 1M -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = write, bs =( R ) 1024KiB-1024KiB, ( W ) 1024KiB-1024KiB, ( T ) 1024KiB-1024KiB, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 3 ( f = 3 ) : [ _ ( 2 ) ,W ( 2 ) ,_ ( 1 ) ,W ( 1 )][ 90 .0% ][ w = 177MiB/s ][ w = 177 IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 107996 : Mon Aug 1 14 :51:00 2022 write: IOPS = 131 , BW = 131MiB/s ( 138MB/s )( 1200MiB/9131msec ) ; 0 zone resets slat ( usec ) : min = 41 , max = 13194 , avg = 208 .41, stdev = 756 .44 clat ( msec ) : min = 20 , max = 3855 , avg = 1392 .05, stdev = 816 .66 lat ( msec ) : min = 20 , max = 3855 , avg = 1392 .26, stdev = 816 .65 clat percentiles ( msec ) : | 1 .00th =[ 284 ] , 5 .00th =[ 351 ] , 10 .00th =[ 435 ] , 20 .00th =[ 625 ] , | 30 .00th =[ 885 ] , 40 .00th =[ 1099 ] , 50 .00th =[ 1234 ] , 60 .00th =[ 1418 ] , | 70 .00th =[ 1620 ] , 80 .00th =[ 1938 ] , 90 .00th =[ 2769 ] , 95 .00th =[ 2903 ] , | 99 .00th =[ 3608 ] , 99 .50th =[ 3675 ] , 99 .90th =[ 3809 ] , 99 .95th =[ 3842 ] , | 99 .99th =[ 3842 ] bw ( KiB/s ) : min = 24554 , max = 273468 , per = 99 .58%, avg = 134013 .71, stdev = 11587 .87, samples = 93 iops : min = 22 , max = 267 , avg = 130 .28, stdev = 11 .35, samples = 93 lat ( msec ) : 50 = 0 .25%, 250 = 0 .33%, 500 = 13 .33%, 750 = 11 .50%, 1000 = 10 .08% lat ( msec ) : 2000 = 44 .92%, > = 2000 = 19 .58% cpu : usr = 0 .18%, sys = 0 .09%, ctx = 792 , majf = 1 , minf = 6 IO depths : 1 = 0 .5%, 2 = 1 .0%, 4 = 2 .0%, 8 = 4 .0%, 16 = 8 .0%, 32 = 84 .5%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 99 .4%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .6%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,1200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 1200MiB ( 1258MB ) , run = 9131 -9131msec Disk stats ( read/write ) : rbd0: ios = 0 /887, merge = 0 /280, ticks = 0 /1112233, in_queue = 1110460 , util = 16 .67%","title":"\u793a\u4f8b\uff1a"},{"location":"Storage/osd/#rbd-bench","text":"","title":"\u65b9\u6cd5\u4e8c\uff1aRBD bench \u538b\u529b\u6d4b\u8bd5"},{"location":"Storage/osd/#_4","text":"[ ucloud ] root@master0:~# rbd help bench usage : rbd bench [--pool <pool>] [--namespace <namespace>] [--image <image>] [--io-size <io-size>] [--io-threads <io-threads>] [--io-total <io-total>] [--io-pattern <io-pattern>] [--rw-mix-read <rw-mix-read>] --io-type <io-type> <image-spec> Simple benchmark. Positional arguments <image-spec> image specification (example : [ <pool-name>/ [ <namespace>/ ]] <image-name>) Optional arguments -p [ --pool ] arg pool name # \u6307\u5b9apool\u7684\u540d\u79f0 --namespace arg namespace name # \u6307\u5b9anamespace --image arg image name --io-size arg IO size (in B/K/M/G/T) [default : 4K] # \u6307\u5b9aIO\u5927\u5c0f --io-threads arg ios in flight [default : 16] # \u6307\u5b9a\u5e76\u53d1 --io-total arg total size for IO (in B/K/M/G/T) [default : 1G] # \u6570\u636e\u7684\u5927\u5c0f --io-pattern arg IO pattern (rand, seq, or full-seq) [default : seq] # iops\uff08rand\u4e3a\u968f\u673a\uff0cseq\u987a\u5e8f\uff09 --rw-mix-read arg read proportion in readwrite (<= 100) [default : 50] # --rw-mix-read \u6df7\u5408\u8bfb\u5199\u8bfb\u7684\u5360\u6bd4 --io-type arg IO type (read, write, or readwrite(rw)) # \u7c7b\u578b\uff0c\u8981\u4ee5\u4ec0\u4e48\u65b9\u5f0f\u538b\u6d4b\uff0c\u8bfb\u6216\u8005\u5199","title":"\u53c2\u6570\u8bf4\u660e\uff1a"},{"location":"Storage/osd/#_5","text":"\u6e29\u99a8\u63d0\u793a \u538b\u6d4b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7iostat -x 1 \u5bf9\u78c1\u76d8\u8fdb\u884c\u76d1\u63a7 - ceph osd perf \u53ef\u4ee5\u4f7f\u7528\u8be5\u547d\u4ee4\u67e5\u770bosd\u5ef6\u8fdf\u60c5\u51b5 4K\u968f\u673a\u5199 rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type write bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 5536 5574 .28 22 MiB/s 2 9664 4849 .68 19 MiB/s 3 13776 4603 .46 18 MiB/s 4 17968 4500 .49 18 MiB/s ...... 64 261440 3935 .99 15 MiB/s elapsed: 64 ops: 262144 ops/sec: 4064 .99 bytes/sec: 16 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a4064.99 4K\u968f\u673a\u8bfb [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type read bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 26816 26939 .7 105 MiB/s 2 53728 26925 .8 105 MiB/s 3 81648 27257 .6 106 MiB/s 4 90896 22569 .9 88 MiB/s 5 115328 23087 .2 90 MiB/s 6 142416 23119 .9 90 MiB/s 7 170048 23263 .9 91 MiB/s 8 197104 23091 .1 90 MiB/s 9 225264 27046 .6 106 MiB/s 10 253360 27606 .3 108 MiB/s elapsed: 10 ops: 262144 ops/sec: 25391 .6 bytes/sec: 99 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a25391.6 4K\u968f\u673a\u6df7\u5408\u8bfb\u5199 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type readwrite --rw-mix-read 70 bench type readwrite read:write = 70 :30 io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 12144 12208 .8 48 MiB/s 2 23488 11775 .5 46 MiB/s 3 34624 11562 45 MiB/s 4 45552 11403 .4 45 MiB/s 5 56528 11317 .8 44 MiB/s 6 67664 11104 43 MiB/s 7 78976 11097 .6 43 MiB/s 8 89872 11049 .6 43 MiB/s 9 101216 11132 .8 43 MiB/s 10 112608 11216 44 MiB/s ..... elapsed: 23 ops: 262144 ops/sec: 11012 .6 bytes/sec: 43 MiB/s read_ops: 183730 read_ops/sec: 7718 .43 read_bytes/sec: 30 MiB/s write_ops: 78414 write_ops/sec: 3294 .14 write_bytes/sec: 13 MiB/s 1M\u987a\u5e8f\u5199\uff08\u6d4b\u541e\u5410\u91cf\uff09 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 1M --io-threads 16 --io-total 200M --io-pattern seq --io-type write bench type write io_size 1048576 io_threads 16 bytes 209715200 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 160 175 .298 175 MiB/s elapsed: 1 ops: 200 ops/sec: 136 .986 bytes/sec: 137 MiB/s \u56fa\u6001\u78c1\u76d8\u6d4b\u8bd5","title":"\u793a\u4f8b\uff1a"},{"location":"Storage/pv-rep/","text":"\u4e0a\u6d77\u4ea4\u5927\u4fee\u590dpv\u6b65\u9aa4 \u00b6 \u8fd9\u91cc\u4e3b\u8981\u662fkubeadm\u90e8\u7f72\u7684k8s \u73b0\u8c61 \u00b6 PV\u4e0d\u77e5\u9053\u56e0\u4e3a\u4ec0\u4e48\u539f\u56e0\u5904\u4e8eTerminating. \u4fee\u590dPV \u00b6 \u6587\u7ae0\u53c2\u8003\uff1ahttps://github.com/jianz/k8s-reset-terminating-pv \u7aef\u53e3\u8f6c\u53d1\u5230\u672c\u5730 \u00b6 kubectl port-forward pods/etcd-master 2379 :2379 -n kube-system \u4fee\u590dpv \u00b6 ./resetpv-linux-x86-64 --k8s-key-prefix registry pvc-bd426570-7fc2-4270-bd41-9512262b0790 --etcd-ca = /etc/kubernetes/pki/etcd/ca.crt --etcd-cert = /etc/kubernetes/pki/etcd/server.crt --etcd-key = /etc/kubernetes/pki/etcd/server.key Warning pvc-bd426570-7fc2-4270-bd41-9512262b0790 \u662f\u5904\u4e8eTerminating\u7684pv\uff0c\u4fee\u590d\u5b8c\u6210pv\u5904\u4e8eBound\u72b6\u6001 k8s\u5347\u7ea7\u5bfc\u81f4sc\u65e0\u6cd5\u6b63\u5e38work \u00b6 \u53c2\u8003\u5730\u5740: sc\u65e0\u6cd5\u6b63\u5e38work Bug Using Kubernetes v1.20.0, getting \"unexpected error getting claim reference: selfLink was empty, can't make reference \u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u7f16\u8f91 /etc/kubernetes/manifests/kube-apiserver.yaml \u6dfb\u52a0\u8fd9\u4e00\u884c\uff1a ---feature-gates = RemoveSelfLink = false \u4eb2\u548c\u6027\u5bfc\u81f4rook-ceph\u65e0\u6cd5\u6b63\u5e38\u4f7f\u7528 \u00b6 Bug unable to get monitor info from DNS SRV with service name: ceph-mon \u53d6\u6d88\u4eb2\u548c\u6027\u7684\u914d\u7f6e","title":"pv/pvc\u7b80\u4ecb"},{"location":"Storage/pv-rep/#pv","text":"\u8fd9\u91cc\u4e3b\u8981\u662fkubeadm\u90e8\u7f72\u7684k8s","title":"\u4e0a\u6d77\u4ea4\u5927\u4fee\u590dpv\u6b65\u9aa4"},{"location":"Storage/pv-rep/#_1","text":"PV\u4e0d\u77e5\u9053\u56e0\u4e3a\u4ec0\u4e48\u539f\u56e0\u5904\u4e8eTerminating.","title":"\u73b0\u8c61"},{"location":"Storage/pv-rep/#pv_1","text":"\u6587\u7ae0\u53c2\u8003\uff1ahttps://github.com/jianz/k8s-reset-terminating-pv","title":"\u4fee\u590dPV"},{"location":"Storage/pv-rep/#_2","text":"kubectl port-forward pods/etcd-master 2379 :2379 -n kube-system","title":"\u7aef\u53e3\u8f6c\u53d1\u5230\u672c\u5730"},{"location":"Storage/pv-rep/#pv_2","text":"./resetpv-linux-x86-64 --k8s-key-prefix registry pvc-bd426570-7fc2-4270-bd41-9512262b0790 --etcd-ca = /etc/kubernetes/pki/etcd/ca.crt --etcd-cert = /etc/kubernetes/pki/etcd/server.crt --etcd-key = /etc/kubernetes/pki/etcd/server.key Warning pvc-bd426570-7fc2-4270-bd41-9512262b0790 \u662f\u5904\u4e8eTerminating\u7684pv\uff0c\u4fee\u590d\u5b8c\u6210pv\u5904\u4e8eBound\u72b6\u6001","title":"\u4fee\u590dpv"},{"location":"Storage/pv-rep/#k8sscwork","text":"\u53c2\u8003\u5730\u5740: sc\u65e0\u6cd5\u6b63\u5e38work Bug Using Kubernetes v1.20.0, getting \"unexpected error getting claim reference: selfLink was empty, can't make reference \u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u7f16\u8f91 /etc/kubernetes/manifests/kube-apiserver.yaml \u6dfb\u52a0\u8fd9\u4e00\u884c\uff1a ---feature-gates = RemoveSelfLink = false","title":"k8s\u5347\u7ea7\u5bfc\u81f4sc\u65e0\u6cd5\u6b63\u5e38work"},{"location":"Storage/pv-rep/#rook-ceph","text":"Bug unable to get monitor info from DNS SRV with service name: ceph-mon \u53d6\u6d88\u4eb2\u548c\u6027\u7684\u914d\u7f6e","title":"\u4eb2\u548c\u6027\u5bfc\u81f4rook-ceph\u65e0\u6cd5\u6b63\u5e38\u4f7f\u7528"},{"location":"Storage/rook-ceph-update/","text":"rook-ceph \u7248\u672c\u5347\u7ea7 \u00b6 https://blog.51cto.com/foxhound/2553979","title":"rook-ceph \u7248\u672c\u5347\u7ea7"},{"location":"Storage/rook-ceph-update/#rook-ceph","text":"https://blog.51cto.com/foxhound/2553979","title":"rook-ceph \u7248\u672c\u5347\u7ea7"},{"location":"Storage/rook-ceph/","text":"rook-ceph\u7b80\u4ecb\u548c\u90e8\u7f72 \u00b6 rook-ceph\u7b80\u4ecb \u00b6 \u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u8c61\u5b58\u50a8\uff0c\u5757\u8bbe\u5907\uff0c\u6587\u4ef6\u7cfb\u7edf \u5757\u5b58\u50a8 CephFs \u5bf9\u8c61\u5b58\u50a8 ceph \u7684\u7248\u672c\u5386\u53f2 \u00b6 x.0.z - \u5f00\u53d1\u7248 x.1.z - \u5019\u9009\u7248 x.2.z - \u7a33\u5b9a\uff0c\u4fee\u6b63\u7248 ceph\u96c6\u7fa4\u89d2\u8272\u5b9a\u4e49 \u00b6 \u6ce8\u610f ceph\u96c6\u7fa4\u7684osd\u8282\u70b9\u4e00\u822c\u4fdd\u8bc1>=3\u4e2a\uff0c\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u9ad8\u53ef\u7528\u6027\u3002 mon : ceph \u76d1\u89c6\u5668,\u5728\u4e00\u4e2a\u4e3b\u673a\u4e0a\u8fd0\u884c\u7684\u4e00\u4e2a\u5b88\u62a4\u8fdb\u7a0b\uff0c\u7528\u4e8e\u7ef4\u62a4\u96c6\u7fa4\u72b6\u6001\u6620\u5c04\u5173\u7cfb mgr : \u8d1f\u8d23\u8ddf\u8e2a\u8fd0\u884c\u65f6\u6307\u6807\u548cceph\u96c6\u7fa4\u7684\u5f53\u524d\u72b6\u6001 osd : \u78c1\u76d8\uff08\u771f\u6b63\u5b58\u50a8\u6570\u636e\u7684\u5730\u65b9\uff09 ceph \u9762\u8bd5\u9898 ceph rook-ceph\u90e8\u7f72 \u00b6 \u73af\u5883\u8981\u6c42 \u4e00\u4e2ak8s\u96c6\u7fa4\uff0cnode\u8282\u70b9\u6700\u5c11\u4e09\u4e2a\u8282\u70b9 mon: 8C 8G/200G 16C 16g/32-200G \u666e\u901a\u6d4b\u8bd5\u90e8\u7f72\uff1a \u00b6 \u90e8\u7f72crds\uff0ccommon\uff0coperator \u00b6 [ucloud] root@master0:~# git clone --single-branch --branch v1.5.5 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml \u955c\u50cf\u5217\u8868\uff1a \u00b6 # ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.4.0\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.3.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.3.0\" CSI\u83b7\u53d6\u955c\u50cf\u811a\u672c\uff1a \u00b6 [ ucloud ] root@node0:/home/lixie# cat <<EOF>> image-sci.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF \u5b9a\u5236\u5316\u90e8\u7f72\uff1a \u00b6 \u5b9a\u5236mon\u8c03\u5ea6\u53c2\u6570 \u00b6 \u80cc\u666f \u2f63\u4ea7\u73af\u5883\u6709\u2f00\u4e9b\u4e13\u2ed4\u7684\u8282\u70b9\u2f64\u4e8emon\u3001mgr\uff0c\u5b58\u50a8\u8282\u70b9\u8282\u70b9\u4f7f\u2f64\u5355\u72ec\u7684\u8282\u70b9\u627f\u62c5,\u5229\u2f64\u8c03\u5ea6\u673a\u5236\u5b9e \u73b0 placement : mon : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mon operator : In values : - enabled #\u8bbe\u7f6e\u78c1\u76d8\u7684\u53c2\u6570\uff0c\u8c03\u6574\u4e3afalse\uff0c\u2f45\u4fbf\u540e\u2faf\u5b9a\u5236 214 useAllNodes : false 215 useAllDevices : false \u5206\u522b\u7ed9\u8282\u70b9\u6253\u4e0a\u6807\u7b7e [ucloud] root@master0:~# kubectl label node node0 ceph-mon=enabled node/node0 labeled [ucloud] root@master0:~# kubectl label node node1 ceph-mon=enabled node/node1 labeled [ucloud] root@master0:~# kubectl label node node2 ceph-mon=enabled node/node2 labeled \u83b7\u53d6\u955c\u50cf\u811a\u672c $ cat image-rook-ceph-sci-v1.7.11.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} #docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF \u5b9a\u5236mgr\u8c03\u5ea6\u53c2\u6570 \u00b6 \u6e29\u99a8\u63d0\u793a \u4fee\u6539mgr\u7684\u8c03\u5ea6\u53c2\u6570,\u4fee\u6539\u5b8c\u4e4b\u540e\u91cd\u65b0apply cluster.yaml\u914d\u7f6e\u4f7f\u5176\u52a0\u8f7d\u5230\u96c6\u7fa4\u4e2d mgr : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mgr operator : In values : - enabled \u6b64\u65f6\u8c03\u5ea6\u4f1a\u5931\u8d25\uff0c\u7ed9node-1\u548cnode-2\u6253\u4e0a ceph-mgr=enabled \u7684\u6807\u7b7e $ kubectl label nodes node0 ceph-mgr=enabled node/node0 labeled $ kubectl label nodes node1 ceph-mgr=enabled node/node1 labeled \u5b9a\u5236msd\u8c03\u5ea6\u53c2\u6570 \u00b6 \u8bbe\u7f6eosd\u7684\u8c03\u5ea6\u53c2\u6570 osd : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-osd operator : In values : - enabled \u5b9a\u5236osd\u7684\u78c1\u76d8\u53c2\u6570 nodes : - name : \"node0\" devices : # specific devices to use for storage can be specified for each node - name : \"vdb\" - name : \"node1\" devices : # specific devices to use for storage can be specified for each node - name : \"vdc\" toolbox\u5ba2\u6237\u7aef \u00b6 apply\u4ee5\u4e0b\u4e2d\u4e24\u4e2a\u6587\u4ef6\u7684\u4e00\u4e2a\u5c31\u53ef\u4ee5\uff0c\u4e00\u822c\u9009\u62e9toolbox.yaml $ ll tool* -rw-r--r-- 1 beiyiwangdejiyi staff 1.7K 7 19 17:26 toolbox-job.yaml # \u4e00\u6b21\u6027\u4efb\u52a1 -rw-r--r-- 1 beiyiwangdejiyi staff 1.4K 7 19 17:26 toolbox.yaml kubectl apply -f toolbox.yaml k8s\u8bbf\u95eeceph \u00b6 centos\u7cfb\u7edf\uff1a 1. \u914d\u7f6eCeph yum\u6e90 \u00b6 [root@node-1 ~]# cat /etc/yum.repos.d/ceph.repo [ceph] name=ceph baseurl=https://mirrors.aliyun.com/ceph/rpm-octopus/el8/x86_64/ enabled=1 gpgcheck=0 2.\u5b89\u88c5ceph-common \u00b6 [root@node-1 ~]# yum -y install ceph-common 3. \u521b\u5efaceph\u914d\u7f6e\u6587\u4ef6 \u00b6 [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/ceph.conf # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [global] mon_host = 10.43.248.216:6789,10.43.174.200:6789,10.43.9.21:6789 [client.admin] keyring = /etc/ceph/keyring [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/keyring # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [client.admin] key = AQCRS+NijUeiIxAAhFtv6je2FmMEAVHAJJqPwg== ubuntu\u7cfb\u7edf\uff1a apt install ceph-common \u8bbf\u95eeRBD\u5757\u5b58\u50a8 \u00b6 1.\u521b\u5efa\u4e00\u4e2apool \u00b6 [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # ceph osd pool create rook 16 16 pool 'rook' created [root@rook-ceph-tools-65c94d77bb-xg6xs /]# ceph osd lspools # \u67e5\u770bpools 1 device_health_metrics 2 replicapool 3 rook 2. \u5728pool\u4e0a\u521b\u5efa\u5757\u8bbe\u5907 \u00b6 [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd create -p rook --image rook-rbd.img --size 10G [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd ls -p rook rook-rbd.img [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd info rook/rook-rbd.img # \u67e5\u770b\u8be6\u7ec6\u4fe1\u606f rbd image 'rook-rbd.img' : size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count : 0 id : 50a7fcf85890 block_name_prefix : rbd_data.50a7fcf85890 format : 2 features : layering op_features : flags : create_timestamp : Fri Jul 29 05:40:05 2022 access_timestamp : Fri Jul 29 05:40:05 2022 modify_timestamp : Fri Jul 29 05:40:05 2022 3\u3001\u5ba2\u6237\u6302\u8f7dRBD\u5757 \u00b6 [ ucloud ] root@master0:/home/lixie# rbd map rook/rook-rbd.img /dev/rbd0 [ ucloud ] root@master0:/home/lixie# rbd showmapped id pool namespace image snap device 0 rook rook-rbd.img - /dev/rbd0 [ ucloud ] root@master0:/home/lixie# mkfs.xfs /dev/rbd0 meta-data = /dev/rbd1 isize = 512 agcount = 16 , agsize = 163840 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 1 , sparse = 1 , rmapbt = 0 = reflink = 1 data = bsize = 4096 blocks = 2621440 , imaxpct = 25 = sunit = 16 swidth = 16 blks naming = version 2 bsize = 4096 ascii-ci = 0 , ftype = 1 log = internal log bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 16 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 \u95ee\u9898\u4e00\uff1a\u52a0\u8f7d rbd \u5185\u6838\u6a21\u5757\u5931\u8d25 [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd map rook/rook-rbd.img modinfo: ERROR: Module alias rbd not found. modprobe: FATAL: Module rbd not found in directory /lib/modules/5.4.0-48-generic rbd: failed to load rbd kernel module (1) rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (2) No such file or directory \u89e3\u51b3\u65b9\u6cd5\uff1a [ucloud] root@node0:/home/lixie# modprobe rbd [ucloud] root@node0:/home/lixie# lsmod |grep rbd rbd 106496 0 libceph 327680 1 rbd \u95ee\u9898\u4e8c\uff1amap rdb [root@rook-ceph-tools-65c94d77bb-b9b2h /]# rbd map rook/rook-rbd.img rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". \u89e3\u51b3\u65b9\u6cd5\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\u6267\u884c\uff0c\u8be5\u547d\u4ee4\u3002 \u95ee\u9898\u4e09: \u5185\u6838\u6a21\u5757\u4e0d\u652f\u6301\u8fd9\u4e48\u591a\u7684\u7279\u6027 [dev] root@master0:/home/lixie# rbd map rook/rook-rbd1.img rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \"rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten\". In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (6) No such device or address \u89e3\u51b3\u65b9\u6cd5\uff1a rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten # \u6309\u7167\u4ed6\u7684\u63d0\u793a\uff0c\u5148\u7981\u6b62\u8fd9\u4e9b\u7279\u6027\u518dmap Dashbaard \u56fe\u5f62\u7ba1\u7406 \u00b6 \u6e29\u99a8\u63d0\u793a \u6ce8\u610f\u9700\u8981\u5c06\u4e3b\u673a\u66b4\u6f0f\u7aef\u53e3\u7684\u5b89\u5168\u7ec4\u6253\u5f00\uff0c\u5b89\u5168\u7ec4\u6253\u5f00 31926 \u7aef\u53e3 \u542f\u2f64\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230rook-ceph-mgr-dashboard-external-http\u7684service\uff0c\u5176\u7c7b\u578b\u662fNodePort\uff0c /Users/beiyiwangdejiyi/k8s-data/rook-v1.6.11/cluster/examples/kubernetes/ceph k apply -f dashboard-external-http.yaml $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 579d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 579d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 579d rook-ceph-mgr-dashboard ClusterIP 10.97.4.98 <none> 7000/TCP 579d rook-ceph-mgr-dashboard-external-http NodePort 10.97.10.172 <none> 7000:31926/TCP 8m18s \u9ed8\u8ba4mgr\u521b\u5efa\u4e86\u2f00\u4e2aadmin\u7684\u2f64\u6237\uff0c\u5176\u5bc6\u7801\u5b58\u653e\u5728rook-ceph-dashboard-password\u7684secrets\u5bf9\u8c61\u4e2d\uff0c\u901a\u8fc7\u5982\u4e0b\u2f45\u5f0f\u53ef\u4ee5\u83b7\u53d6\u5230 kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -oyaml apiVersion : v1 data : password : XTBndS0iREN1bE9UMGpQY2JQSSE= # \u91c7\u7528base64\u52a0\u5bc6 kind : Secret metadata : creationTimestamp : \"2020-12-16T18:01:16Z\" name : rook-ceph-dashboard-password namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"103972533\" uid : 7082d4ad-47bb-46e2-b439-624016cc5f81 type : kubernetes.io/rook base64 \u89e3\u5bc6 $ echo XTBndS0iREN1bE9UMGpQY2JQSSE= | base64 -d ]0gu-\"DCulOT0jPcbPI!% \u6d4b\u8bd5\u767b\u9646 URL: http://106.75.119.241:31926/ \u5e10\u53f7: admin \u5bc6\u7801: ]0gu-\"DCulOT0jPcbPI! \u8fdb\u5165\u5c31\u53ef\u4ee5\u770b\u5230\u4e00\u4e0b\u754c\u9762 Dashboard \u76d1\u63a7ceph \u00b6 $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 580d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 580d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 580d # \u7ed9prometheus\u4f5c\u4e3a\u5ba2\u6237\u7aef\u4f7f\u7528\u7684 ...... \u90e8\u7f72Prometheus Operator \u00b6 kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml \u786e\u8ba4prometheus-operator\u5904\u4e8erun\u72b6\u6001 $ k get pod prometheus-operator-7ccf6dfc8-d9dmm 1/1 Running 0 142 \u90e8\u7f72Prometheus Instances \u00b6 $ git clone --single-branch --branch v1.6.11 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph/monitoring \u521b\u5efa\u670d\u52a1\u76d1\u89c6\u5668\u4ee5\u53ca Prometheus \u670d\u52a1\u5668 pod \u548c\u670d\u52a1 kubectl create -f service-monitor.yaml kubectl create -f prometheus.yaml kubectl create -f prometheus-service.yaml \u672c\u5730\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/prometheus-rook-prometheus-0 -n rook-ceph 9090 9090 \u8bbf\u95ee\uff1ahttp://localhost:9090/ grafana\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/grafana-cc568dbd8-4nvlq -n infra 3000 80 \u8bbf\u95ee\uff1ahttp://localhost:3000/ \u5e10\u53f7\uff1aadmin \u5bc6\u7801\uff1astrongpassword \u76d1\u63a7\u5c55\u677f Ceph - Cluster\uff1ahttps://grafana.com/grafana/dashboards/2842 Ceph - OSD \uff1a https://grafana.com/grafana/dashboards/5336 Ceph - Pools\uff1a https://grafana.com/grafana/dashboards/5342 \u5bf9\u8c61\u5b58\u50a8 \u00b6 \u90e8\u7f72RGW\u5bf9\u8c61\u5b58\u50a8 \u00b6 $ k apply -f object.yaml [ ucloud ] root@master0:~/.kube# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-rbdplugin-metrics ClusterIP 10 .43.40.164 <none> 8080 /TCP,8081/TCP 7d4h csi-cephfsplugin-metrics ClusterIP 10 .43.170.151 <none> 8080 /TCP,8081/TCP 7d4h rook-ceph-mon-a ClusterIP 10 .43.248.216 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-b ClusterIP 10 .43.174.200 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-c ClusterIP 10 .43.9.21 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mgr-dashboard ClusterIP 10 .43.193.31 <none> 8443 /TCP 7d4h rook-ceph-mgr ClusterIP 10 .43.114.15 <none> 9283 /TCP 7d4h wordpress-mysql ClusterIP None <none> 3306 /TCP 7d3h rook-prometheus NodePort 10 .43.128.1 <none> 9090 :30900/TCP 3d prometheus-operated ClusterIP None <none> 9090 /TCP 3d rook-ceph-rgw-my-store ClusterIP 10 .43.73.235 <none> 80 /TCP 18m [ ucloud ] root@master0:~/.kube# curl http://10.43.73.235 <?xml version = \"1.0\" encoding = \"UTF-8\" ?><ListAllMyBucketsResult xmlns = \"http://s3.amazonaws.com/doc/2006-03-01/\" ><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult> [ ucloud ] root@master0:~/.kube# RGW\u9ad8\u53ef\u7528 \u00b6 $ vim object.yaml 54 instances: 2 \u521b\u5efaBucket \u00b6 \u521b\u5efastorageclass $ k apply -f storageclass-bucket-delete.yaml storageclass.storage.k8s.io/rook-ceph-delete-bucket created \u521b\u5efabucket $ k apply -f object-bucket-claim-delete.yaml objectbucketclaim.objectbucket.io/ceph-delete-bucket created \u5bb9\u5668\u8bbf\u95ee\u5bf9\u8c61\u5b58\u50a8 \u00b6 \u83b7\u53d6ceph-rgw\u7684\u8bbf\u95ee\u5730\u5740 $ k get cm ceph-delete-bucket -o yaml apiVersion : v1 data : BUCKET_HOST : rook-ceph-rgw-my-store.rook-ceph.svc BUCKET_NAME : ceph-bkt-148b1fa5-7868-42e5-8135-383d357c41cd BUCKET_PORT : \"80\" BUCKET_REGION : us-east-1 BUCKET_SUBREGION : \"\" kind : ConfigMap metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282724\" uid : 26786f4b-9371-46fa-8ca9-f470e5e92cb2 \u62ff\u5230secrets $ k get secrets ceph-delete-bucket -o yaml apiVersion : v1 data : AWS_ACCESS_KEY_ID : Rk9JSjBJNlg0NDVNUVVMVkpGMzc= AWS_SECRET_ACCESS_KEY : SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA== kind : Secret metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282723\" uid : 1b0af759-7c7d-4fe4-9a80-ea2615fa8fef type : Opaque base64 \u89e3\u5bc6 $ echo Rk9JSjBJNlg0NDVNUVVMVkpGMzc = | base64 -d FOIJ0I6X445MQULVJF37% # beiyiwangdejiyi @ beiyiwangdejiyideMacBook-Pro in ~/note-work/hugo on git:main x [14:28:13] $ echo SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA == | base64 -d IWchIZyWTm3F6DrgoTqD4GX39Yas4xKVfXLDDsXx% fio --name=sequential-read --directory=/config --rw=read --refill_buffers --bs=4K --size=200M root@nginx-run-685fdf6467-mdl9v:/# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: Laying out IO file ( 1 file / 200MiB ) sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 723 : Wed Aug 10 06 :28:37 2022 read: IOPS = 98 .8k, BW = 386MiB/s ( 405MB/s )( 200MiB/518msec ) clat ( nsec ) : min = 378 , max = 86956k, avg = 9833 .73, stdev = 534902 .03 lat ( nsec ) : min = 411 , max = 86956k, avg = 9868 .39, stdev = 534902 .42 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 486 ] , 10 .00th =[ 532 ] , | 20 .00th =[ 556 ] , 30 .00th =[ 580 ] , 40 .00th =[ 604 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 708 ] , 90 .00th =[ 804 ] , 95 .00th =[ 932 ] , | 99 .00th =[ 70144 ] , 99 .50th =[ 102912 ] , 99 .90th =[ 456704 ] , | 99 .95th =[ 1253376 ] , 99 .99th =[ 26083328 ] bw ( KiB/s ) : min = 401376 , max = 401376 , per = 100 .00%, avg = 401376 .00, stdev = 0 .00, samples = 1 iops : min = 100344 , max = 100344 , avg = 100344 .00, stdev = 0 .00, samples = 1 lat ( nsec ) : 500 = 5 .72%, 750 = 80 .66%, 1000 = 9 .78% lat ( usec ) : 2 = 1 .74%, 4 = 0 .06%, 10 = 0 .17%, 20 = 0 .06%, 50 = 0 .11% lat ( usec ) : 100 = 1 .17%, 250 = 0 .37%, 500 = 0 .07%, 750 = 0 .02%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01% cpu : usr = 3 .29%, sys = 17 .02%, ctx = 571 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 386MiB/s ( 405MB/s ) , 386MiB/s-386MiB/s ( 405MB/s-405MB/s ) , io = 200MiB ( 210MB ) , run = 518 -518msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 729 : Wed Aug 10 06 :30:48 2022 read: IOPS = 344k, BW = 1342MiB/s ( 1407MB/s )( 200MiB/149msec ) clat ( nsec ) : min = 375 , max = 7097 .8k, avg = 2339 .11, stdev = 39079 .52 lat ( nsec ) : min = 406 , max = 7097 .8k, avg = 2372 .97, stdev = 39080 .14 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 588 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 700 ] , 90 .00th =[ 748 ] , 95 .00th =[ 868 ] , | 99 .00th =[ 67072 ] , 99 .50th =[ 73216 ] , 99 .90th =[ 114176 ] , | 99 .95th =[ 156672 ] , 99 .99th =[ 1122304 ] lat ( nsec ) : 500 = 2 .29%, 750 = 87 .79%, 1000 = 7 .05% lat ( usec ) : 2 = 0 .96%, 4 = 0 .01%, 10 = 0 .15%, 20 = 0 .04%, 50 = 0 .11% lat ( usec ) : 100 = 1 .42%, 250 = 0 .15%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01% cpu : usr = 25 .68%, sys = 49 .32%, ctx = 335 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 1342MiB/s ( 1407MB/s ) , 1342MiB/s-1342MiB/s ( 1407MB/s-1407MB/s ) , io = 200MiB ( 210MB ) , run = 149 -149msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) Jobs: 2 ( f = 2 ) : [ _ ( 4 ) ,R ( 2 )][ 100 .0% ][ r = 264MiB/s ][ r = 67 .7k IOPS ][ eta 00m:00s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 732 : Wed Aug 10 06 :32:40 2022 read: IOPS = 11 .1k, BW = 43 .5MiB/s ( 45 .6MB/s )( 200MiB/4602msec ) clat ( nsec ) : min = 377 , max = 522247k, avg = 89494 .70, stdev = 4682750 .76 lat ( nsec ) : min = 408 , max = 522247k, avg = 89553 .19, stdev = 4682751 .34 clat percentiles ( nsec ) : | 1 .00th =[ 414 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 796 ] , | 80 .00th =[ 884 ] , 90 .00th =[ 1020 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 83456 ] , 99 .50th =[ 218112 ] , 99 .90th =[ 5144576 ] , | 99 .95th =[ 20578304 ] , 99 .99th =[ 240123904 ] bw ( KiB/s ) : min = 14080 , max = 90112 , per = 18 .59%, avg = 45625 .50, stdev = 27571 .34, samples = 8 iops : min = 3520 , max = 22528 , avg = 11406 .37, stdev = 6892 .84, samples = 8 lat ( nsec ) : 500 = 3 .47%, 750 = 60 .05%, 1000 = 25 .67% lat ( usec ) : 2 = 8 .49%, 4 = 0 .21%, 10 = 0 .15%, 20 = 0 .08%, 50 = 0 .07% lat ( usec ) : 100 = 1 .00%, 250 = 0 .33%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .05%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .59%, sys = 2 .00%, ctx = 671 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 733 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .7k, BW = 41 .9MiB/s ( 43 .9MB/s )( 200MiB/4776msec ) clat ( nsec ) : min = 376 , max = 734234k, avg = 92901 .06, stdev = 5934361 .60 lat ( nsec ) : min = 411 , max = 734234k, avg = 92951 .67, stdev = 5934362 .28 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 588 ] , 30 .00th =[ 636 ] , 40 .00th =[ 684 ] , | 50 .00th =[ 732 ] , 60 .00th =[ 788 ] , 70 .00th =[ 860 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1080 ] , 95 .00th =[ 1272 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 193536 ] , 99 .90th =[ 3981312 ] , | 99 .95th =[ 27131904 ] , 99 .99th =[ 233832448 ] bw ( KiB/s ) : min = 8192 , max = 98304 , per = 19 .42%, avg = 47655 .75, stdev = 31431 .05, samples = 8 iops : min = 2048 , max = 24576 , avg = 11913 .88, stdev = 7857 .79, samples = 8 lat ( nsec ) : 500 = 3 .87%, 750 = 50 .45%, 1000 = 31 .27% lat ( usec ) : 2 = 12 .03%, 4 = 0 .18%, 10 = 0 .15%, 20 = 0 .07%, 50 = 0 .06% lat ( usec ) : 100 = 1 .02%, 250 = 0 .48%, 500 = 0 .15%, 750 = 0 .05%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .05%, 4 = 0 .02%, 10 = 0 .03%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .44%, sys = 2 .07%, ctx = 845 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 734 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .9k, BW = 42 .6MiB/s ( 44 .7MB/s )( 200MiB/4696msec ) clat ( nsec ) : min = 379 , max = 607583k, avg = 91313 .18, stdev = 4982310 .93 lat ( nsec ) : min = 412 , max = 607583k, avg = 91361 .07, stdev = 4982311 .06 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 700 ] , 60 .00th =[ 740 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 130560 ] , 99 .90th =[ 4882432 ] , | 99 .95th =[ 14483456 ] , 99 .99th =[ 254803968 ] bw ( KiB/s ) : min = 7680 , max = 90112 , per = 17 .01%, avg = 41739 .89, stdev = 24407 .57, samples = 9 iops : min = 1920 , max = 22528 , avg = 10434 .89, stdev = 6101 .90, samples = 9 lat ( nsec ) : 500 = 3 .93%, 750 = 58 .07%, 1000 = 25 .64% lat ( usec ) : 2 = 10 .13%, 4 = 0 .22%, 10 = 0 .13%, 20 = 0 .04%, 50 = 0 .08% lat ( usec ) : 100 = 1 .10%, 250 = 0 .34%, 500 = 0 .08%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .04%, 20 = 0 .02%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .72%, sys = 1 .81%, ctx = 502 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 735 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .6k, BW = 41 .5MiB/s ( 43 .5MB/s )( 200MiB/4822msec ) clat ( nsec ) : min = 374 , max = 700599k, avg = 93757 .53, stdev = 5099889 .71 lat ( nsec ) : min = 413 , max = 700599k, avg = 93803 .83, stdev = 5099890 .00 clat percentiles ( nsec ) : | 1 .00th =[ 430 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1256 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 226304 ] , 99 .90th =[ 5931008 ] , | 99 .95th =[ 24248320 ] , 99 .99th =[ 235929600 ] bw ( KiB/s ) : min = 6520 , max = 65536 , per = 15 .07%, avg = 36989 .89, stdev = 18379 .13, samples = 9 iops : min = 1630 , max = 16384 , avg = 9247 .44, stdev = 4594 .77, samples = 9 lat ( nsec ) : 500 = 3 .74%, 750 = 59 .39%, 1000 = 24 .25% lat ( usec ) : 2 = 10 .19%, 4 = 0 .22%, 10 = 0 .18%, 20 = 0 .08%, 50 = 0 .08% lat ( usec ) : 100 = 0 .96%, 250 = 0 .43%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .06%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .21%, sys = 2 .24%, ctx = 874 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 736 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .2k, BW = 39 .9MiB/s ( 41 .9MB/s )( 200MiB/5008msec ) clat ( nsec ) : min = 377 , max = 806541k, avg = 97234 .58, stdev = 6320832 .41 lat ( nsec ) : min = 414 , max = 806541k, avg = 97345 .92, stdev = 6320844 .03 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 652 ] , | 50 .00th =[ 684 ] , 60 .00th =[ 724 ] , 70 .00th =[ 772 ] , | 80 .00th =[ 868 ] , 90 .00th =[ 1032 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 80384 ] , 99 .50th =[ 154624 ] , 99 .90th =[ 5275648 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 200278016 ] bw ( KiB/s ) : min = 8192 , max = 49152 , per = 12 .40%, avg = 30434 .00, stdev = 15445 .75, samples = 9 iops : min = 2048 , max = 12288 , avg = 7608 .44, stdev = 3861 .51, samples = 9 lat ( nsec ) : 500 = 3 .77%, 750 = 62 .98%, 1000 = 22 .07% lat ( usec ) : 2 = 8 .67%, 4 = 0 .18%, 10 = 0 .26%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .05%, 250 = 0 .40%, 500 = 0 .11%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .05%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .46%, sys = 1 .96%, ctx = 787 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 737 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .3k, BW = 40 .2MiB/s ( 42 .1MB/s )( 200MiB/4977msec ) clat ( nsec ) : min = 378 , max = 894860k, avg = 96613 .11, stdev = 5799729 .12 lat ( nsec ) : min = 413 , max = 894860k, avg = 96658 .78, stdev = 5799729 .19 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 506 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 644 ] , | 50 .00th =[ 676 ] , 60 .00th =[ 716 ] , 70 .00th =[ 764 ] , | 80 .00th =[ 852 ] , 90 .00th =[ 988 ] , 95 .00th =[ 1176 ] , | 99 .00th =[ 87552 ] , 99 .50th =[ 216064 ] , 99 .90th =[ 6848512 ] , | 99 .95th =[ 31064064 ] , 99 .99th =[ 231735296 ] bw ( KiB/s ) : min = 16929 , max = 69632 , per = 14 .42%, avg = 35383 .25, stdev = 17459 .47, samples = 8 iops : min = 4232 , max = 17408 , avg = 8845 .75, stdev = 4364 .90, samples = 8 lat ( nsec ) : 500 = 4 .66%, 750 = 63 .16%, 1000 = 22 .55% lat ( usec ) : 2 = 7 .10%, 4 = 0 .20%, 10 = 0 .27%, 20 = 0 .09%, 50 = 0 .11% lat ( usec ) : 100 = 0 .99%, 250 = 0 .44%, 500 = 0 .12%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .32%, sys = 2 .05%, ctx = 1006 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 240MiB/s ( 251MB/s ) , 39 .9MiB/s-43.5MiB/s ( 41 .9MB/s-45.6MB/s ) , io = 1200MiB ( 1258MB ) , run = 4602 -5008msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes Jobs: 3 ( f = 3 ) : [ _ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 )][ 80 .0% ][ r = 172MiB/s ][ r = 44 .1k IOPS ][ eta 00m:02s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 740 : Wed Aug 10 06 :34:00 2022 read: IOPS = 8276 , BW = 32 .3MiB/s ( 33 .9MB/s )( 200MiB/6186msec ) clat ( nsec ) : min = 378 , max = 805881k, avg = 120250 .60, stdev = 7449083 .91 lat ( nsec ) : min = 410 , max = 805881k, avg = 120301 .98, stdev = 7449084 .14 clat percentiles ( nsec ) : | 1 .00th =[ 422 ] , 5 .00th =[ 532 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 612 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 660 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 996 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 132096 ] , 99 .90th =[ 1318912 ] , | 99 .95th =[ 10289152 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8192 , max = 65536 , per = 23 .93%, avg = 35045 .82, stdev = 25507 .11, samples = 11 iops : min = 2048 , max = 16384 , avg = 8761 .45, stdev = 6376 .78, samples = 11 lat ( nsec ) : 500 = 2 .40%, 750 = 77 .33%, 1000 = 15 .36% lat ( usec ) : 2 = 2 .64%, 4 = 0 .06%, 10 = 0 .18%, 20 = 0 .11%, 50 = 0 .13% lat ( usec ) : 100 = 1 .11%, 250 = 0 .39%, 500 = 0 .09%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .03%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .34%, sys = 1 .62%, ctx = 845 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 741 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6102 , BW = 23 .8MiB/s ( 24 .0MB/s )( 200MiB/8390msec ) clat ( nsec ) : min = 379 , max = 1190 .5M, avg = 162758 .58, stdev = 11051920 .43 lat ( nsec ) : min = 413 , max = 1190 .5M, avg = 162807 .90, stdev = 11051920 .69 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 700 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1004 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1073152 ] , | 99 .95th =[ 5275648 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 512 , max = 73728 , per = 20 .69%, avg = 30307 .20, stdev = 21673 .33, samples = 10 iops : min = 128 , max = 18432 , avg = 7576 .80, stdev = 5418 .33, samples = 10 lat ( nsec ) : 500 = 2 .76%, 750 = 73 .76%, 1000 = 18 .35% lat ( usec ) : 2 = 2 .79%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .13%, 50 = 0 .14% lat ( usec ) : 100 = 1 .05%, 250 = 0 .42%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .02%, 20 = 0 .01%, 100 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .33%, sys = 1 .10%, ctx = 982 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 742 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6906 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7413msec ) clat ( nsec ) : min = 380 , max = 1034 .3M, avg = 144218 .03, stdev = 9148025 .05 lat ( nsec ) : min = 414 , max = 1034 .3M, avg = 144254 .80, stdev = 9148025 .00 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 596 ] , 40 .00th =[ 620 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 676 ] , 70 .00th =[ 708 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 988 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 146432 ] , 99 .90th =[ 1253376 ] , | 99 .95th =[ 4620288 ] , 99 .99th =[ 522190848 ] bw ( KiB/s ) : min = 16384 , max = 73728 , per = 26 .85%, avg = 39318 .40, stdev = 23491 .33, samples = 10 iops : min = 4096 , max = 18432 , avg = 9829 .60, stdev = 5872 .83, samples = 10 lat ( nsec ) : 500 = 3 .29%, 750 = 76 .66%, 1000 = 15 .22% lat ( usec ) : 2 = 2 .43%, 4 = 0 .10%, 10 = 0 .23%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .13%, 250 = 0 .37%, 500 = 0 .13%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .32%, sys = 1 .32%, ctx = 864 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 743 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6106 , BW = 23 .9MiB/s ( 25 .0MB/s )( 200MiB/8385msec ) clat ( nsec ) : min = 380 , max = 1507 .9M, avg = 162772 .24, stdev = 13174473 .96 lat ( nsec ) : min = 414 , max = 1507 .9M, avg = 162810 .08, stdev = 13174473 .94 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 510 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 692 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 79360 ] , 99 .50th =[ 136192 ] , 99 .90th =[ 897024 ] , | 99 .95th =[ 2506752 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8 , max = 81920 , per = 20 .70%, avg = 30310 .40, stdev = 22461 .38, samples = 10 iops : min = 2 , max = 20480 , avg = 7577 .60, stdev = 5615 .35, samples = 10 lat ( nsec ) : 500 = 4 .29%, 750 = 73 .14%, 1000 = 17 .30% lat ( usec ) : 2 = 2 .94%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .07%, 50 = 0 .19% lat ( usec ) : 100 = 1 .10%, 250 = 0 .44%, 500 = 0 .11%, 750 = 0 .05%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .03%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .05%, sys = 1 .40%, ctx = 993 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 744 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6911 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7408msec ) clat ( nsec ) : min = 381 , max = 1179 .3M, avg = 143994 .58, stdev = 11079777 .48 lat ( nsec ) : min = 413 , max = 1179 .3M, avg = 144029 .70, stdev = 11079777 .45 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 580 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 636 ] , 60 .00th =[ 668 ] , 70 .00th =[ 692 ] , | 80 .00th =[ 732 ] , 90 .00th =[ 836 ] , 95 .00th =[ 956 ] , | 99 .00th =[ 76288 ] , 99 .50th =[ 111104 ] , 99 .90th =[ 995328 ] , | 99 .95th =[ 3227648 ] , 99 .99th =[ 742391808 ] bw ( KiB/s ) : min = 5040 , max = 73728 , per = 22 .93%, avg = 33587 .20, stdev = 27492 .28, samples = 10 iops : min = 1260 , max = 18432 , avg = 8396 .80, stdev = 6873 .07, samples = 10 lat ( nsec ) : 500 = 3 .81%, 750 = 78 .93%, 1000 = 13 .08% lat ( usec ) : 2 = 2 .02%, 4 = 0 .02%, 10 = 0 .21%, 20 = 0 .07%, 50 = 0 .12% lat ( usec ) : 100 = 1 .15%, 250 = 0 .36%, 500 = 0 .07%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .35%, ctx = 724 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 745 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6238 , BW = 24 .4MiB/s ( 25 .6MB/s )( 200MiB/8207msec ) clat ( nsec ) : min = 379 , max = 1170 .8M, avg = 159639 .78, stdev = 10672683 .50 lat ( nsec ) : min = 413 , max = 1170 .8M, avg = 159675 .27, stdev = 10672683 .47 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 596 ] , 40 .00th =[ 628 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1810432 ] , | 99 .95th =[ 5865472 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 6400 , max = 74752 , per = 18 .50%, avg = 27096 .62, stdev = 22310 .47, samples = 13 iops : min = 1600 , max = 18688 , avg = 6774 .15, stdev = 5577 .62, samples = 13 lat ( nsec ) : 500 = 4 .00%, 750 = 74 .22%, 1000 = 16 .52% lat ( usec ) : 2 = 3 .01%, 4 = 0 .05%, 10 = 0 .19%, 20 = 0 .07%, 50 = 0 .11% lat ( usec ) : 100 = 1 .06%, 250 = 0 .40%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .03%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .22%, ctx = 949 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 143MiB/s ( 150MB/s ) , 23 .8MiB/s-32.3MiB/s ( 24 .0MB/s-33.9MB/s ) , io = 1200MiB ( 1258MB ) , run = 6186 -8390msec fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=6 fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-write: Laying out IO file ( 1 file / 200MiB ) Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 756 : Wed Aug 10 06 :39:40 2022 write: IOPS = 33 .6k, BW = 131MiB/s ( 138MB/s )( 200MiB/1525msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 7420 , avg = 27 .69, stdev = 125 .85 lat ( usec ) : min = 7 , max = 7420 , avg = 27 .76, stdev = 125 .85 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 12 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 28 ] , 95 .00th =[ 37 ] , | 99 .00th =[ 118 ] , 99 .50th =[ 285 ] , 99 .90th =[ 1860 ] , 99 .95th =[ 3097 ] , | 99 .99th =[ 4752 ] bw ( KiB/s ) : min = 132286 , max = 138088 , per = 100 .00%, avg = 135187 .00, stdev = 4102 .63, samples = 2 iops : min = 33071 , max = 34522 , avg = 33796 .50, stdev = 1026 .01, samples = 2 lat ( usec ) : 10 = 9 .54%, 20 = 24 .43%, 50 = 63 .08%, 100 = 1 .75%, 250 = 0 .66% lat ( usec ) : 500 = 0 .22%, 750 = 0 .09%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .10%, 4 = 0 .07%, 10 = 0 .03% cpu : usr = 9 .84%, sys = 31 .04%, ctx = 51905 , majf = 0 , minf = 12 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 200MiB ( 210MB ) , run = 1525 -1525msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 759 : Wed Aug 10 06 :41:20 2022 write: IOPS = 31 .3k, BW = 122MiB/s ( 128MB/s )( 200MiB/1637msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 9234 , avg = 30 .16, stdev = 137 .54 lat ( usec ) : min = 7 , max = 9234 , avg = 30 .21, stdev = 137 .54 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 18 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 29 ] , 95 .00th =[ 41 ] , | 99 .00th =[ 147 ] , 99 .50th =[ 379 ] , 99 .90th =[ 2311 ] , 99 .95th =[ 3064 ] , | 99 .99th =[ 4490 ] bw ( KiB/s ) : min = 119544 , max = 132640 , per = 100 .00%, avg = 128018 .67, stdev = 7349 .32, samples = 3 iops : min = 29886 , max = 33160 , avg = 32004 .67, stdev = 1837 .33, samples = 3 lat ( usec ) : 10 = 7 .32%, 20 = 25 .16%, 50 = 63 .95%, 100 = 2 .08%, 250 = 0 .85% lat ( usec ) : 500 = 0 .23%, 750 = 0 .09%, 1000 = 0 .08% lat ( msec ) : 2 = 0 .12%, 4 = 0 .11%, 10 = 0 .02% cpu : usr = 7 .21%, sys = 32 .64%, ctx = 51971 , majf = 0 , minf = 13 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 122MiB/s ( 128MB/s ) , 122MiB/s-122MiB/s ( 128MB/s-128MB/s ) , io = 200MiB ( 210MB ) , run = 1637 -1637msec fio --name=big-file-multi-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --numjobs=6 --end_fsync=1 fio -filename=/config/fio.img -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=mytest \u56fa\u6001\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes Jobs: 6 ( f = 6 ) : [ R ( 6 )][ 88 .9% ][ r = 130MiB/s ][ r = 33 .2k IOPS ][ eta 00m:01s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 225237 : Fri Aug 12 14 :09:04 2022 read: IOPS = 5925 , BW = 23 .1MiB/s ( 24 .3MB/s )( 200MiB/8641msec ) clat ( nsec ) : min = 434 , max = 21757k, avg = 168221 .11, stdev = 1662876 .10 lat ( nsec ) : min = 471 , max = 21757k, avg = 168260 .51, stdev = 1662876 .63 clat percentiles ( nsec ) : | 1 .00th =[ 524 ] , 5 .00th =[ 540 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , | 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , 70 .00th =[ 628 ] , | 80 .00th =[ 660 ] , 90 .00th =[ 732 ] , 95 .00th =[ 884 ] , | 99 .00th =[ 3948544 ] , 99 .50th =[ 19005440 ] , 99 .90th =[ 20054016 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 20054016 ] \u5185\u5b58\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 12095 : Fri Aug 12 15 :05:27 2022 read: IOPS = 966k, BW = 3774MiB/s ( 3957MB/s )( 200MiB/53msec ) clat ( nsec ) : min = 520 , max = 221610 , avg = 757 .38, stdev = 2561 .09 lat ( nsec ) : min = 553 , max = 221646 , avg = 792 .50, stdev = 2561 .18 clat percentiles ( nsec ) : | 1 .00th =[ 540 ] , 5 .00th =[ 556 ] , 10 .00th =[ 556 ] , 20 .00th =[ 564 ] , | 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , | 70 .00th =[ 644 ] , 80 .00th =[ 724 ] , 90 .00th =[ 908 ] , 95 .00th =[ 940 ] , | 99 .00th =[ 3344 ] , 99 .50th =[ 3728 ] , 99 .90th =[ 19072 ] , 99 .95th =[ 43264 ] , | 99 .99th =[ 115200 ] fio --name=small-file-multi-read \\ --directory=/config \\ --rw=read --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=500 \\ --numjobs=2","title":"rook-ceph\u7b80\u4ecb"},{"location":"Storage/rook-ceph/#rook-ceph","text":"","title":"rook-ceph\u7b80\u4ecb\u548c\u90e8\u7f72"},{"location":"Storage/rook-ceph/#rook-ceph_1","text":"\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u8c61\u5b58\u50a8\uff0c\u5757\u8bbe\u5907\uff0c\u6587\u4ef6\u7cfb\u7edf \u5757\u5b58\u50a8 CephFs \u5bf9\u8c61\u5b58\u50a8","title":"rook-ceph\u7b80\u4ecb"},{"location":"Storage/rook-ceph/#ceph","text":"x.0.z - \u5f00\u53d1\u7248 x.1.z - \u5019\u9009\u7248 x.2.z - \u7a33\u5b9a\uff0c\u4fee\u6b63\u7248","title":"ceph \u7684\u7248\u672c\u5386\u53f2"},{"location":"Storage/rook-ceph/#ceph_1","text":"\u6ce8\u610f ceph\u96c6\u7fa4\u7684osd\u8282\u70b9\u4e00\u822c\u4fdd\u8bc1>=3\u4e2a\uff0c\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u9ad8\u53ef\u7528\u6027\u3002 mon : ceph \u76d1\u89c6\u5668,\u5728\u4e00\u4e2a\u4e3b\u673a\u4e0a\u8fd0\u884c\u7684\u4e00\u4e2a\u5b88\u62a4\u8fdb\u7a0b\uff0c\u7528\u4e8e\u7ef4\u62a4\u96c6\u7fa4\u72b6\u6001\u6620\u5c04\u5173\u7cfb mgr : \u8d1f\u8d23\u8ddf\u8e2a\u8fd0\u884c\u65f6\u6307\u6807\u548cceph\u96c6\u7fa4\u7684\u5f53\u524d\u72b6\u6001 osd : \u78c1\u76d8\uff08\u771f\u6b63\u5b58\u50a8\u6570\u636e\u7684\u5730\u65b9\uff09 ceph \u9762\u8bd5\u9898 ceph","title":"ceph\u96c6\u7fa4\u89d2\u8272\u5b9a\u4e49"},{"location":"Storage/rook-ceph/#rook-ceph_2","text":"\u73af\u5883\u8981\u6c42 \u4e00\u4e2ak8s\u96c6\u7fa4\uff0cnode\u8282\u70b9\u6700\u5c11\u4e09\u4e2a\u8282\u70b9 mon: 8C 8G/200G 16C 16g/32-200G","title":"rook-ceph\u90e8\u7f72"},{"location":"Storage/rook-ceph/#_1","text":"","title":"\u666e\u901a\u6d4b\u8bd5\u90e8\u7f72\uff1a"},{"location":"Storage/rook-ceph/#crdscommonoperator","text":"[ucloud] root@master0:~# git clone --single-branch --branch v1.5.5 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml","title":"\u90e8\u7f72crds\uff0ccommon\uff0coperator"},{"location":"Storage/rook-ceph/#_2","text":"# ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.4.0\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.3.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.3.0\"","title":"\u955c\u50cf\u5217\u8868\uff1a"},{"location":"Storage/rook-ceph/#csi","text":"[ ucloud ] root@node0:/home/lixie# cat <<EOF>> image-sci.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF","title":"CSI\u83b7\u53d6\u955c\u50cf\u811a\u672c\uff1a"},{"location":"Storage/rook-ceph/#_3","text":"","title":"\u5b9a\u5236\u5316\u90e8\u7f72\uff1a"},{"location":"Storage/rook-ceph/#mon","text":"\u80cc\u666f \u2f63\u4ea7\u73af\u5883\u6709\u2f00\u4e9b\u4e13\u2ed4\u7684\u8282\u70b9\u2f64\u4e8emon\u3001mgr\uff0c\u5b58\u50a8\u8282\u70b9\u8282\u70b9\u4f7f\u2f64\u5355\u72ec\u7684\u8282\u70b9\u627f\u62c5,\u5229\u2f64\u8c03\u5ea6\u673a\u5236\u5b9e \u73b0 placement : mon : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mon operator : In values : - enabled #\u8bbe\u7f6e\u78c1\u76d8\u7684\u53c2\u6570\uff0c\u8c03\u6574\u4e3afalse\uff0c\u2f45\u4fbf\u540e\u2faf\u5b9a\u5236 214 useAllNodes : false 215 useAllDevices : false \u5206\u522b\u7ed9\u8282\u70b9\u6253\u4e0a\u6807\u7b7e [ucloud] root@master0:~# kubectl label node node0 ceph-mon=enabled node/node0 labeled [ucloud] root@master0:~# kubectl label node node1 ceph-mon=enabled node/node1 labeled [ucloud] root@master0:~# kubectl label node node2 ceph-mon=enabled node/node2 labeled \u83b7\u53d6\u955c\u50cf\u811a\u672c $ cat image-rook-ceph-sci-v1.7.11.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} #docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF","title":"\u5b9a\u5236mon\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#mgr","text":"\u6e29\u99a8\u63d0\u793a \u4fee\u6539mgr\u7684\u8c03\u5ea6\u53c2\u6570,\u4fee\u6539\u5b8c\u4e4b\u540e\u91cd\u65b0apply cluster.yaml\u914d\u7f6e\u4f7f\u5176\u52a0\u8f7d\u5230\u96c6\u7fa4\u4e2d mgr : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mgr operator : In values : - enabled \u6b64\u65f6\u8c03\u5ea6\u4f1a\u5931\u8d25\uff0c\u7ed9node-1\u548cnode-2\u6253\u4e0a ceph-mgr=enabled \u7684\u6807\u7b7e $ kubectl label nodes node0 ceph-mgr=enabled node/node0 labeled $ kubectl label nodes node1 ceph-mgr=enabled node/node1 labeled","title":"\u5b9a\u5236mgr\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#msd","text":"\u8bbe\u7f6eosd\u7684\u8c03\u5ea6\u53c2\u6570 osd : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-osd operator : In values : - enabled \u5b9a\u5236osd\u7684\u78c1\u76d8\u53c2\u6570 nodes : - name : \"node0\" devices : # specific devices to use for storage can be specified for each node - name : \"vdb\" - name : \"node1\" devices : # specific devices to use for storage can be specified for each node - name : \"vdc\"","title":"\u5b9a\u5236msd\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#toolbox","text":"apply\u4ee5\u4e0b\u4e2d\u4e24\u4e2a\u6587\u4ef6\u7684\u4e00\u4e2a\u5c31\u53ef\u4ee5\uff0c\u4e00\u822c\u9009\u62e9toolbox.yaml $ ll tool* -rw-r--r-- 1 beiyiwangdejiyi staff 1.7K 7 19 17:26 toolbox-job.yaml # \u4e00\u6b21\u6027\u4efb\u52a1 -rw-r--r-- 1 beiyiwangdejiyi staff 1.4K 7 19 17:26 toolbox.yaml kubectl apply -f toolbox.yaml","title":"toolbox\u5ba2\u6237\u7aef"},{"location":"Storage/rook-ceph/#k8sceph","text":"centos\u7cfb\u7edf\uff1a","title":"k8s\u8bbf\u95eeceph"},{"location":"Storage/rook-ceph/#1-ceph-yum","text":"[root@node-1 ~]# cat /etc/yum.repos.d/ceph.repo [ceph] name=ceph baseurl=https://mirrors.aliyun.com/ceph/rpm-octopus/el8/x86_64/ enabled=1 gpgcheck=0","title":"1. \u914d\u7f6eCeph yum\u6e90"},{"location":"Storage/rook-ceph/#2ceph-common","text":"[root@node-1 ~]# yum -y install ceph-common","title":"2.\u5b89\u88c5ceph-common"},{"location":"Storage/rook-ceph/#3-ceph","text":"[root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/ceph.conf # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [global] mon_host = 10.43.248.216:6789,10.43.174.200:6789,10.43.9.21:6789 [client.admin] keyring = /etc/ceph/keyring [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/keyring # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [client.admin] key = AQCRS+NijUeiIxAAhFtv6je2FmMEAVHAJJqPwg== ubuntu\u7cfb\u7edf\uff1a apt install ceph-common","title":"3. \u521b\u5efaceph\u914d\u7f6e\u6587\u4ef6"},{"location":"Storage/rook-ceph/#rbd","text":"","title":"\u8bbf\u95eeRBD\u5757\u5b58\u50a8"},{"location":"Storage/rook-ceph/#1pool","text":"[ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # ceph osd pool create rook 16 16 pool 'rook' created [root@rook-ceph-tools-65c94d77bb-xg6xs /]# ceph osd lspools # \u67e5\u770bpools 1 device_health_metrics 2 replicapool 3 rook","title":"1.\u521b\u5efa\u4e00\u4e2apool"},{"location":"Storage/rook-ceph/#2-pool","text":"[ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd create -p rook --image rook-rbd.img --size 10G [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd ls -p rook rook-rbd.img [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd info rook/rook-rbd.img # \u67e5\u770b\u8be6\u7ec6\u4fe1\u606f rbd image 'rook-rbd.img' : size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count : 0 id : 50a7fcf85890 block_name_prefix : rbd_data.50a7fcf85890 format : 2 features : layering op_features : flags : create_timestamp : Fri Jul 29 05:40:05 2022 access_timestamp : Fri Jul 29 05:40:05 2022 modify_timestamp : Fri Jul 29 05:40:05 2022","title":"2. \u5728pool\u4e0a\u521b\u5efa\u5757\u8bbe\u5907"},{"location":"Storage/rook-ceph/#3rbd","text":"[ ucloud ] root@master0:/home/lixie# rbd map rook/rook-rbd.img /dev/rbd0 [ ucloud ] root@master0:/home/lixie# rbd showmapped id pool namespace image snap device 0 rook rook-rbd.img - /dev/rbd0 [ ucloud ] root@master0:/home/lixie# mkfs.xfs /dev/rbd0 meta-data = /dev/rbd1 isize = 512 agcount = 16 , agsize = 163840 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 1 , sparse = 1 , rmapbt = 0 = reflink = 1 data = bsize = 4096 blocks = 2621440 , imaxpct = 25 = sunit = 16 swidth = 16 blks naming = version 2 bsize = 4096 ascii-ci = 0 , ftype = 1 log = internal log bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 16 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 \u95ee\u9898\u4e00\uff1a\u52a0\u8f7d rbd \u5185\u6838\u6a21\u5757\u5931\u8d25 [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd map rook/rook-rbd.img modinfo: ERROR: Module alias rbd not found. modprobe: FATAL: Module rbd not found in directory /lib/modules/5.4.0-48-generic rbd: failed to load rbd kernel module (1) rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (2) No such file or directory \u89e3\u51b3\u65b9\u6cd5\uff1a [ucloud] root@node0:/home/lixie# modprobe rbd [ucloud] root@node0:/home/lixie# lsmod |grep rbd rbd 106496 0 libceph 327680 1 rbd \u95ee\u9898\u4e8c\uff1amap rdb [root@rook-ceph-tools-65c94d77bb-b9b2h /]# rbd map rook/rook-rbd.img rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". \u89e3\u51b3\u65b9\u6cd5\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\u6267\u884c\uff0c\u8be5\u547d\u4ee4\u3002 \u95ee\u9898\u4e09: \u5185\u6838\u6a21\u5757\u4e0d\u652f\u6301\u8fd9\u4e48\u591a\u7684\u7279\u6027 [dev] root@master0:/home/lixie# rbd map rook/rook-rbd1.img rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \"rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten\". In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (6) No such device or address \u89e3\u51b3\u65b9\u6cd5\uff1a rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten # \u6309\u7167\u4ed6\u7684\u63d0\u793a\uff0c\u5148\u7981\u6b62\u8fd9\u4e9b\u7279\u6027\u518dmap","title":"3\u3001\u5ba2\u6237\u6302\u8f7dRBD\u5757"},{"location":"Storage/rook-ceph/#dashbaard","text":"\u6e29\u99a8\u63d0\u793a \u6ce8\u610f\u9700\u8981\u5c06\u4e3b\u673a\u66b4\u6f0f\u7aef\u53e3\u7684\u5b89\u5168\u7ec4\u6253\u5f00\uff0c\u5b89\u5168\u7ec4\u6253\u5f00 31926 \u7aef\u53e3 \u542f\u2f64\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230rook-ceph-mgr-dashboard-external-http\u7684service\uff0c\u5176\u7c7b\u578b\u662fNodePort\uff0c /Users/beiyiwangdejiyi/k8s-data/rook-v1.6.11/cluster/examples/kubernetes/ceph k apply -f dashboard-external-http.yaml $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 579d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 579d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 579d rook-ceph-mgr-dashboard ClusterIP 10.97.4.98 <none> 7000/TCP 579d rook-ceph-mgr-dashboard-external-http NodePort 10.97.10.172 <none> 7000:31926/TCP 8m18s \u9ed8\u8ba4mgr\u521b\u5efa\u4e86\u2f00\u4e2aadmin\u7684\u2f64\u6237\uff0c\u5176\u5bc6\u7801\u5b58\u653e\u5728rook-ceph-dashboard-password\u7684secrets\u5bf9\u8c61\u4e2d\uff0c\u901a\u8fc7\u5982\u4e0b\u2f45\u5f0f\u53ef\u4ee5\u83b7\u53d6\u5230 kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -oyaml apiVersion : v1 data : password : XTBndS0iREN1bE9UMGpQY2JQSSE= # \u91c7\u7528base64\u52a0\u5bc6 kind : Secret metadata : creationTimestamp : \"2020-12-16T18:01:16Z\" name : rook-ceph-dashboard-password namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"103972533\" uid : 7082d4ad-47bb-46e2-b439-624016cc5f81 type : kubernetes.io/rook base64 \u89e3\u5bc6 $ echo XTBndS0iREN1bE9UMGpQY2JQSSE= | base64 -d ]0gu-\"DCulOT0jPcbPI!% \u6d4b\u8bd5\u767b\u9646 URL: http://106.75.119.241:31926/ \u5e10\u53f7: admin \u5bc6\u7801: ]0gu-\"DCulOT0jPcbPI! \u8fdb\u5165\u5c31\u53ef\u4ee5\u770b\u5230\u4e00\u4e0b\u754c\u9762","title":"Dashbaard \u56fe\u5f62\u7ba1\u7406"},{"location":"Storage/rook-ceph/#dashboard-ceph","text":"$ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 580d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 580d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 580d # \u7ed9prometheus\u4f5c\u4e3a\u5ba2\u6237\u7aef\u4f7f\u7528\u7684 ......","title":"Dashboard \u76d1\u63a7ceph"},{"location":"Storage/rook-ceph/#prometheus-operator","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml \u786e\u8ba4prometheus-operator\u5904\u4e8erun\u72b6\u6001 $ k get pod prometheus-operator-7ccf6dfc8-d9dmm 1/1 Running 0 142","title":"\u90e8\u7f72Prometheus Operator"},{"location":"Storage/rook-ceph/#prometheus-instances","text":"$ git clone --single-branch --branch v1.6.11 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph/monitoring \u521b\u5efa\u670d\u52a1\u76d1\u89c6\u5668\u4ee5\u53ca Prometheus \u670d\u52a1\u5668 pod \u548c\u670d\u52a1 kubectl create -f service-monitor.yaml kubectl create -f prometheus.yaml kubectl create -f prometheus-service.yaml \u672c\u5730\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/prometheus-rook-prometheus-0 -n rook-ceph 9090 9090 \u8bbf\u95ee\uff1ahttp://localhost:9090/ grafana\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/grafana-cc568dbd8-4nvlq -n infra 3000 80 \u8bbf\u95ee\uff1ahttp://localhost:3000/ \u5e10\u53f7\uff1aadmin \u5bc6\u7801\uff1astrongpassword \u76d1\u63a7\u5c55\u677f Ceph - Cluster\uff1ahttps://grafana.com/grafana/dashboards/2842 Ceph - OSD \uff1a https://grafana.com/grafana/dashboards/5336 Ceph - Pools\uff1a https://grafana.com/grafana/dashboards/5342","title":"\u90e8\u7f72Prometheus Instances"},{"location":"Storage/rook-ceph/#_4","text":"","title":"\u5bf9\u8c61\u5b58\u50a8"},{"location":"Storage/rook-ceph/#rgw","text":"$ k apply -f object.yaml [ ucloud ] root@master0:~/.kube# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-rbdplugin-metrics ClusterIP 10 .43.40.164 <none> 8080 /TCP,8081/TCP 7d4h csi-cephfsplugin-metrics ClusterIP 10 .43.170.151 <none> 8080 /TCP,8081/TCP 7d4h rook-ceph-mon-a ClusterIP 10 .43.248.216 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-b ClusterIP 10 .43.174.200 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-c ClusterIP 10 .43.9.21 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mgr-dashboard ClusterIP 10 .43.193.31 <none> 8443 /TCP 7d4h rook-ceph-mgr ClusterIP 10 .43.114.15 <none> 9283 /TCP 7d4h wordpress-mysql ClusterIP None <none> 3306 /TCP 7d3h rook-prometheus NodePort 10 .43.128.1 <none> 9090 :30900/TCP 3d prometheus-operated ClusterIP None <none> 9090 /TCP 3d rook-ceph-rgw-my-store ClusterIP 10 .43.73.235 <none> 80 /TCP 18m [ ucloud ] root@master0:~/.kube# curl http://10.43.73.235 <?xml version = \"1.0\" encoding = \"UTF-8\" ?><ListAllMyBucketsResult xmlns = \"http://s3.amazonaws.com/doc/2006-03-01/\" ><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult> [ ucloud ] root@master0:~/.kube#","title":"\u90e8\u7f72RGW\u5bf9\u8c61\u5b58\u50a8"},{"location":"Storage/rook-ceph/#rgw_1","text":"$ vim object.yaml 54 instances: 2","title":"RGW\u9ad8\u53ef\u7528"},{"location":"Storage/rook-ceph/#bucket","text":"\u521b\u5efastorageclass $ k apply -f storageclass-bucket-delete.yaml storageclass.storage.k8s.io/rook-ceph-delete-bucket created \u521b\u5efabucket $ k apply -f object-bucket-claim-delete.yaml objectbucketclaim.objectbucket.io/ceph-delete-bucket created","title":"\u521b\u5efaBucket"},{"location":"Storage/rook-ceph/#_5","text":"\u83b7\u53d6ceph-rgw\u7684\u8bbf\u95ee\u5730\u5740 $ k get cm ceph-delete-bucket -o yaml apiVersion : v1 data : BUCKET_HOST : rook-ceph-rgw-my-store.rook-ceph.svc BUCKET_NAME : ceph-bkt-148b1fa5-7868-42e5-8135-383d357c41cd BUCKET_PORT : \"80\" BUCKET_REGION : us-east-1 BUCKET_SUBREGION : \"\" kind : ConfigMap metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282724\" uid : 26786f4b-9371-46fa-8ca9-f470e5e92cb2 \u62ff\u5230secrets $ k get secrets ceph-delete-bucket -o yaml apiVersion : v1 data : AWS_ACCESS_KEY_ID : Rk9JSjBJNlg0NDVNUVVMVkpGMzc= AWS_SECRET_ACCESS_KEY : SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA== kind : Secret metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282723\" uid : 1b0af759-7c7d-4fe4-9a80-ea2615fa8fef type : Opaque base64 \u89e3\u5bc6 $ echo Rk9JSjBJNlg0NDVNUVVMVkpGMzc = | base64 -d FOIJ0I6X445MQULVJF37% # beiyiwangdejiyi @ beiyiwangdejiyideMacBook-Pro in ~/note-work/hugo on git:main x [14:28:13] $ echo SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA == | base64 -d IWchIZyWTm3F6DrgoTqD4GX39Yas4xKVfXLDDsXx% fio --name=sequential-read --directory=/config --rw=read --refill_buffers --bs=4K --size=200M root@nginx-run-685fdf6467-mdl9v:/# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: Laying out IO file ( 1 file / 200MiB ) sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 723 : Wed Aug 10 06 :28:37 2022 read: IOPS = 98 .8k, BW = 386MiB/s ( 405MB/s )( 200MiB/518msec ) clat ( nsec ) : min = 378 , max = 86956k, avg = 9833 .73, stdev = 534902 .03 lat ( nsec ) : min = 411 , max = 86956k, avg = 9868 .39, stdev = 534902 .42 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 486 ] , 10 .00th =[ 532 ] , | 20 .00th =[ 556 ] , 30 .00th =[ 580 ] , 40 .00th =[ 604 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 708 ] , 90 .00th =[ 804 ] , 95 .00th =[ 932 ] , | 99 .00th =[ 70144 ] , 99 .50th =[ 102912 ] , 99 .90th =[ 456704 ] , | 99 .95th =[ 1253376 ] , 99 .99th =[ 26083328 ] bw ( KiB/s ) : min = 401376 , max = 401376 , per = 100 .00%, avg = 401376 .00, stdev = 0 .00, samples = 1 iops : min = 100344 , max = 100344 , avg = 100344 .00, stdev = 0 .00, samples = 1 lat ( nsec ) : 500 = 5 .72%, 750 = 80 .66%, 1000 = 9 .78% lat ( usec ) : 2 = 1 .74%, 4 = 0 .06%, 10 = 0 .17%, 20 = 0 .06%, 50 = 0 .11% lat ( usec ) : 100 = 1 .17%, 250 = 0 .37%, 500 = 0 .07%, 750 = 0 .02%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01% cpu : usr = 3 .29%, sys = 17 .02%, ctx = 571 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 386MiB/s ( 405MB/s ) , 386MiB/s-386MiB/s ( 405MB/s-405MB/s ) , io = 200MiB ( 210MB ) , run = 518 -518msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 729 : Wed Aug 10 06 :30:48 2022 read: IOPS = 344k, BW = 1342MiB/s ( 1407MB/s )( 200MiB/149msec ) clat ( nsec ) : min = 375 , max = 7097 .8k, avg = 2339 .11, stdev = 39079 .52 lat ( nsec ) : min = 406 , max = 7097 .8k, avg = 2372 .97, stdev = 39080 .14 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 588 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 700 ] , 90 .00th =[ 748 ] , 95 .00th =[ 868 ] , | 99 .00th =[ 67072 ] , 99 .50th =[ 73216 ] , 99 .90th =[ 114176 ] , | 99 .95th =[ 156672 ] , 99 .99th =[ 1122304 ] lat ( nsec ) : 500 = 2 .29%, 750 = 87 .79%, 1000 = 7 .05% lat ( usec ) : 2 = 0 .96%, 4 = 0 .01%, 10 = 0 .15%, 20 = 0 .04%, 50 = 0 .11% lat ( usec ) : 100 = 1 .42%, 250 = 0 .15%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01% cpu : usr = 25 .68%, sys = 49 .32%, ctx = 335 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 1342MiB/s ( 1407MB/s ) , 1342MiB/s-1342MiB/s ( 1407MB/s-1407MB/s ) , io = 200MiB ( 210MB ) , run = 149 -149msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) Jobs: 2 ( f = 2 ) : [ _ ( 4 ) ,R ( 2 )][ 100 .0% ][ r = 264MiB/s ][ r = 67 .7k IOPS ][ eta 00m:00s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 732 : Wed Aug 10 06 :32:40 2022 read: IOPS = 11 .1k, BW = 43 .5MiB/s ( 45 .6MB/s )( 200MiB/4602msec ) clat ( nsec ) : min = 377 , max = 522247k, avg = 89494 .70, stdev = 4682750 .76 lat ( nsec ) : min = 408 , max = 522247k, avg = 89553 .19, stdev = 4682751 .34 clat percentiles ( nsec ) : | 1 .00th =[ 414 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 796 ] , | 80 .00th =[ 884 ] , 90 .00th =[ 1020 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 83456 ] , 99 .50th =[ 218112 ] , 99 .90th =[ 5144576 ] , | 99 .95th =[ 20578304 ] , 99 .99th =[ 240123904 ] bw ( KiB/s ) : min = 14080 , max = 90112 , per = 18 .59%, avg = 45625 .50, stdev = 27571 .34, samples = 8 iops : min = 3520 , max = 22528 , avg = 11406 .37, stdev = 6892 .84, samples = 8 lat ( nsec ) : 500 = 3 .47%, 750 = 60 .05%, 1000 = 25 .67% lat ( usec ) : 2 = 8 .49%, 4 = 0 .21%, 10 = 0 .15%, 20 = 0 .08%, 50 = 0 .07% lat ( usec ) : 100 = 1 .00%, 250 = 0 .33%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .05%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .59%, sys = 2 .00%, ctx = 671 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 733 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .7k, BW = 41 .9MiB/s ( 43 .9MB/s )( 200MiB/4776msec ) clat ( nsec ) : min = 376 , max = 734234k, avg = 92901 .06, stdev = 5934361 .60 lat ( nsec ) : min = 411 , max = 734234k, avg = 92951 .67, stdev = 5934362 .28 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 588 ] , 30 .00th =[ 636 ] , 40 .00th =[ 684 ] , | 50 .00th =[ 732 ] , 60 .00th =[ 788 ] , 70 .00th =[ 860 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1080 ] , 95 .00th =[ 1272 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 193536 ] , 99 .90th =[ 3981312 ] , | 99 .95th =[ 27131904 ] , 99 .99th =[ 233832448 ] bw ( KiB/s ) : min = 8192 , max = 98304 , per = 19 .42%, avg = 47655 .75, stdev = 31431 .05, samples = 8 iops : min = 2048 , max = 24576 , avg = 11913 .88, stdev = 7857 .79, samples = 8 lat ( nsec ) : 500 = 3 .87%, 750 = 50 .45%, 1000 = 31 .27% lat ( usec ) : 2 = 12 .03%, 4 = 0 .18%, 10 = 0 .15%, 20 = 0 .07%, 50 = 0 .06% lat ( usec ) : 100 = 1 .02%, 250 = 0 .48%, 500 = 0 .15%, 750 = 0 .05%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .05%, 4 = 0 .02%, 10 = 0 .03%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .44%, sys = 2 .07%, ctx = 845 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 734 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .9k, BW = 42 .6MiB/s ( 44 .7MB/s )( 200MiB/4696msec ) clat ( nsec ) : min = 379 , max = 607583k, avg = 91313 .18, stdev = 4982310 .93 lat ( nsec ) : min = 412 , max = 607583k, avg = 91361 .07, stdev = 4982311 .06 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 700 ] , 60 .00th =[ 740 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 130560 ] , 99 .90th =[ 4882432 ] , | 99 .95th =[ 14483456 ] , 99 .99th =[ 254803968 ] bw ( KiB/s ) : min = 7680 , max = 90112 , per = 17 .01%, avg = 41739 .89, stdev = 24407 .57, samples = 9 iops : min = 1920 , max = 22528 , avg = 10434 .89, stdev = 6101 .90, samples = 9 lat ( nsec ) : 500 = 3 .93%, 750 = 58 .07%, 1000 = 25 .64% lat ( usec ) : 2 = 10 .13%, 4 = 0 .22%, 10 = 0 .13%, 20 = 0 .04%, 50 = 0 .08% lat ( usec ) : 100 = 1 .10%, 250 = 0 .34%, 500 = 0 .08%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .04%, 20 = 0 .02%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .72%, sys = 1 .81%, ctx = 502 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 735 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .6k, BW = 41 .5MiB/s ( 43 .5MB/s )( 200MiB/4822msec ) clat ( nsec ) : min = 374 , max = 700599k, avg = 93757 .53, stdev = 5099889 .71 lat ( nsec ) : min = 413 , max = 700599k, avg = 93803 .83, stdev = 5099890 .00 clat percentiles ( nsec ) : | 1 .00th =[ 430 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1256 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 226304 ] , 99 .90th =[ 5931008 ] , | 99 .95th =[ 24248320 ] , 99 .99th =[ 235929600 ] bw ( KiB/s ) : min = 6520 , max = 65536 , per = 15 .07%, avg = 36989 .89, stdev = 18379 .13, samples = 9 iops : min = 1630 , max = 16384 , avg = 9247 .44, stdev = 4594 .77, samples = 9 lat ( nsec ) : 500 = 3 .74%, 750 = 59 .39%, 1000 = 24 .25% lat ( usec ) : 2 = 10 .19%, 4 = 0 .22%, 10 = 0 .18%, 20 = 0 .08%, 50 = 0 .08% lat ( usec ) : 100 = 0 .96%, 250 = 0 .43%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .06%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .21%, sys = 2 .24%, ctx = 874 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 736 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .2k, BW = 39 .9MiB/s ( 41 .9MB/s )( 200MiB/5008msec ) clat ( nsec ) : min = 377 , max = 806541k, avg = 97234 .58, stdev = 6320832 .41 lat ( nsec ) : min = 414 , max = 806541k, avg = 97345 .92, stdev = 6320844 .03 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 652 ] , | 50 .00th =[ 684 ] , 60 .00th =[ 724 ] , 70 .00th =[ 772 ] , | 80 .00th =[ 868 ] , 90 .00th =[ 1032 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 80384 ] , 99 .50th =[ 154624 ] , 99 .90th =[ 5275648 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 200278016 ] bw ( KiB/s ) : min = 8192 , max = 49152 , per = 12 .40%, avg = 30434 .00, stdev = 15445 .75, samples = 9 iops : min = 2048 , max = 12288 , avg = 7608 .44, stdev = 3861 .51, samples = 9 lat ( nsec ) : 500 = 3 .77%, 750 = 62 .98%, 1000 = 22 .07% lat ( usec ) : 2 = 8 .67%, 4 = 0 .18%, 10 = 0 .26%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .05%, 250 = 0 .40%, 500 = 0 .11%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .05%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .46%, sys = 1 .96%, ctx = 787 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 737 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .3k, BW = 40 .2MiB/s ( 42 .1MB/s )( 200MiB/4977msec ) clat ( nsec ) : min = 378 , max = 894860k, avg = 96613 .11, stdev = 5799729 .12 lat ( nsec ) : min = 413 , max = 894860k, avg = 96658 .78, stdev = 5799729 .19 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 506 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 644 ] , | 50 .00th =[ 676 ] , 60 .00th =[ 716 ] , 70 .00th =[ 764 ] , | 80 .00th =[ 852 ] , 90 .00th =[ 988 ] , 95 .00th =[ 1176 ] , | 99 .00th =[ 87552 ] , 99 .50th =[ 216064 ] , 99 .90th =[ 6848512 ] , | 99 .95th =[ 31064064 ] , 99 .99th =[ 231735296 ] bw ( KiB/s ) : min = 16929 , max = 69632 , per = 14 .42%, avg = 35383 .25, stdev = 17459 .47, samples = 8 iops : min = 4232 , max = 17408 , avg = 8845 .75, stdev = 4364 .90, samples = 8 lat ( nsec ) : 500 = 4 .66%, 750 = 63 .16%, 1000 = 22 .55% lat ( usec ) : 2 = 7 .10%, 4 = 0 .20%, 10 = 0 .27%, 20 = 0 .09%, 50 = 0 .11% lat ( usec ) : 100 = 0 .99%, 250 = 0 .44%, 500 = 0 .12%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .32%, sys = 2 .05%, ctx = 1006 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 240MiB/s ( 251MB/s ) , 39 .9MiB/s-43.5MiB/s ( 41 .9MB/s-45.6MB/s ) , io = 1200MiB ( 1258MB ) , run = 4602 -5008msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes Jobs: 3 ( f = 3 ) : [ _ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 )][ 80 .0% ][ r = 172MiB/s ][ r = 44 .1k IOPS ][ eta 00m:02s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 740 : Wed Aug 10 06 :34:00 2022 read: IOPS = 8276 , BW = 32 .3MiB/s ( 33 .9MB/s )( 200MiB/6186msec ) clat ( nsec ) : min = 378 , max = 805881k, avg = 120250 .60, stdev = 7449083 .91 lat ( nsec ) : min = 410 , max = 805881k, avg = 120301 .98, stdev = 7449084 .14 clat percentiles ( nsec ) : | 1 .00th =[ 422 ] , 5 .00th =[ 532 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 612 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 660 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 996 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 132096 ] , 99 .90th =[ 1318912 ] , | 99 .95th =[ 10289152 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8192 , max = 65536 , per = 23 .93%, avg = 35045 .82, stdev = 25507 .11, samples = 11 iops : min = 2048 , max = 16384 , avg = 8761 .45, stdev = 6376 .78, samples = 11 lat ( nsec ) : 500 = 2 .40%, 750 = 77 .33%, 1000 = 15 .36% lat ( usec ) : 2 = 2 .64%, 4 = 0 .06%, 10 = 0 .18%, 20 = 0 .11%, 50 = 0 .13% lat ( usec ) : 100 = 1 .11%, 250 = 0 .39%, 500 = 0 .09%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .03%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .34%, sys = 1 .62%, ctx = 845 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 741 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6102 , BW = 23 .8MiB/s ( 24 .0MB/s )( 200MiB/8390msec ) clat ( nsec ) : min = 379 , max = 1190 .5M, avg = 162758 .58, stdev = 11051920 .43 lat ( nsec ) : min = 413 , max = 1190 .5M, avg = 162807 .90, stdev = 11051920 .69 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 700 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1004 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1073152 ] , | 99 .95th =[ 5275648 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 512 , max = 73728 , per = 20 .69%, avg = 30307 .20, stdev = 21673 .33, samples = 10 iops : min = 128 , max = 18432 , avg = 7576 .80, stdev = 5418 .33, samples = 10 lat ( nsec ) : 500 = 2 .76%, 750 = 73 .76%, 1000 = 18 .35% lat ( usec ) : 2 = 2 .79%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .13%, 50 = 0 .14% lat ( usec ) : 100 = 1 .05%, 250 = 0 .42%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .02%, 20 = 0 .01%, 100 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .33%, sys = 1 .10%, ctx = 982 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 742 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6906 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7413msec ) clat ( nsec ) : min = 380 , max = 1034 .3M, avg = 144218 .03, stdev = 9148025 .05 lat ( nsec ) : min = 414 , max = 1034 .3M, avg = 144254 .80, stdev = 9148025 .00 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 596 ] , 40 .00th =[ 620 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 676 ] , 70 .00th =[ 708 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 988 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 146432 ] , 99 .90th =[ 1253376 ] , | 99 .95th =[ 4620288 ] , 99 .99th =[ 522190848 ] bw ( KiB/s ) : min = 16384 , max = 73728 , per = 26 .85%, avg = 39318 .40, stdev = 23491 .33, samples = 10 iops : min = 4096 , max = 18432 , avg = 9829 .60, stdev = 5872 .83, samples = 10 lat ( nsec ) : 500 = 3 .29%, 750 = 76 .66%, 1000 = 15 .22% lat ( usec ) : 2 = 2 .43%, 4 = 0 .10%, 10 = 0 .23%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .13%, 250 = 0 .37%, 500 = 0 .13%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .32%, sys = 1 .32%, ctx = 864 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 743 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6106 , BW = 23 .9MiB/s ( 25 .0MB/s )( 200MiB/8385msec ) clat ( nsec ) : min = 380 , max = 1507 .9M, avg = 162772 .24, stdev = 13174473 .96 lat ( nsec ) : min = 414 , max = 1507 .9M, avg = 162810 .08, stdev = 13174473 .94 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 510 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 692 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 79360 ] , 99 .50th =[ 136192 ] , 99 .90th =[ 897024 ] , | 99 .95th =[ 2506752 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8 , max = 81920 , per = 20 .70%, avg = 30310 .40, stdev = 22461 .38, samples = 10 iops : min = 2 , max = 20480 , avg = 7577 .60, stdev = 5615 .35, samples = 10 lat ( nsec ) : 500 = 4 .29%, 750 = 73 .14%, 1000 = 17 .30% lat ( usec ) : 2 = 2 .94%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .07%, 50 = 0 .19% lat ( usec ) : 100 = 1 .10%, 250 = 0 .44%, 500 = 0 .11%, 750 = 0 .05%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .03%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .05%, sys = 1 .40%, ctx = 993 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 744 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6911 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7408msec ) clat ( nsec ) : min = 381 , max = 1179 .3M, avg = 143994 .58, stdev = 11079777 .48 lat ( nsec ) : min = 413 , max = 1179 .3M, avg = 144029 .70, stdev = 11079777 .45 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 580 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 636 ] , 60 .00th =[ 668 ] , 70 .00th =[ 692 ] , | 80 .00th =[ 732 ] , 90 .00th =[ 836 ] , 95 .00th =[ 956 ] , | 99 .00th =[ 76288 ] , 99 .50th =[ 111104 ] , 99 .90th =[ 995328 ] , | 99 .95th =[ 3227648 ] , 99 .99th =[ 742391808 ] bw ( KiB/s ) : min = 5040 , max = 73728 , per = 22 .93%, avg = 33587 .20, stdev = 27492 .28, samples = 10 iops : min = 1260 , max = 18432 , avg = 8396 .80, stdev = 6873 .07, samples = 10 lat ( nsec ) : 500 = 3 .81%, 750 = 78 .93%, 1000 = 13 .08% lat ( usec ) : 2 = 2 .02%, 4 = 0 .02%, 10 = 0 .21%, 20 = 0 .07%, 50 = 0 .12% lat ( usec ) : 100 = 1 .15%, 250 = 0 .36%, 500 = 0 .07%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .35%, ctx = 724 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 745 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6238 , BW = 24 .4MiB/s ( 25 .6MB/s )( 200MiB/8207msec ) clat ( nsec ) : min = 379 , max = 1170 .8M, avg = 159639 .78, stdev = 10672683 .50 lat ( nsec ) : min = 413 , max = 1170 .8M, avg = 159675 .27, stdev = 10672683 .47 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 596 ] , 40 .00th =[ 628 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1810432 ] , | 99 .95th =[ 5865472 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 6400 , max = 74752 , per = 18 .50%, avg = 27096 .62, stdev = 22310 .47, samples = 13 iops : min = 1600 , max = 18688 , avg = 6774 .15, stdev = 5577 .62, samples = 13 lat ( nsec ) : 500 = 4 .00%, 750 = 74 .22%, 1000 = 16 .52% lat ( usec ) : 2 = 3 .01%, 4 = 0 .05%, 10 = 0 .19%, 20 = 0 .07%, 50 = 0 .11% lat ( usec ) : 100 = 1 .06%, 250 = 0 .40%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .03%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .22%, ctx = 949 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 143MiB/s ( 150MB/s ) , 23 .8MiB/s-32.3MiB/s ( 24 .0MB/s-33.9MB/s ) , io = 1200MiB ( 1258MB ) , run = 6186 -8390msec fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=6 fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-write: Laying out IO file ( 1 file / 200MiB ) Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 756 : Wed Aug 10 06 :39:40 2022 write: IOPS = 33 .6k, BW = 131MiB/s ( 138MB/s )( 200MiB/1525msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 7420 , avg = 27 .69, stdev = 125 .85 lat ( usec ) : min = 7 , max = 7420 , avg = 27 .76, stdev = 125 .85 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 12 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 28 ] , 95 .00th =[ 37 ] , | 99 .00th =[ 118 ] , 99 .50th =[ 285 ] , 99 .90th =[ 1860 ] , 99 .95th =[ 3097 ] , | 99 .99th =[ 4752 ] bw ( KiB/s ) : min = 132286 , max = 138088 , per = 100 .00%, avg = 135187 .00, stdev = 4102 .63, samples = 2 iops : min = 33071 , max = 34522 , avg = 33796 .50, stdev = 1026 .01, samples = 2 lat ( usec ) : 10 = 9 .54%, 20 = 24 .43%, 50 = 63 .08%, 100 = 1 .75%, 250 = 0 .66% lat ( usec ) : 500 = 0 .22%, 750 = 0 .09%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .10%, 4 = 0 .07%, 10 = 0 .03% cpu : usr = 9 .84%, sys = 31 .04%, ctx = 51905 , majf = 0 , minf = 12 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 200MiB ( 210MB ) , run = 1525 -1525msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 759 : Wed Aug 10 06 :41:20 2022 write: IOPS = 31 .3k, BW = 122MiB/s ( 128MB/s )( 200MiB/1637msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 9234 , avg = 30 .16, stdev = 137 .54 lat ( usec ) : min = 7 , max = 9234 , avg = 30 .21, stdev = 137 .54 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 18 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 29 ] , 95 .00th =[ 41 ] , | 99 .00th =[ 147 ] , 99 .50th =[ 379 ] , 99 .90th =[ 2311 ] , 99 .95th =[ 3064 ] , | 99 .99th =[ 4490 ] bw ( KiB/s ) : min = 119544 , max = 132640 , per = 100 .00%, avg = 128018 .67, stdev = 7349 .32, samples = 3 iops : min = 29886 , max = 33160 , avg = 32004 .67, stdev = 1837 .33, samples = 3 lat ( usec ) : 10 = 7 .32%, 20 = 25 .16%, 50 = 63 .95%, 100 = 2 .08%, 250 = 0 .85% lat ( usec ) : 500 = 0 .23%, 750 = 0 .09%, 1000 = 0 .08% lat ( msec ) : 2 = 0 .12%, 4 = 0 .11%, 10 = 0 .02% cpu : usr = 7 .21%, sys = 32 .64%, ctx = 51971 , majf = 0 , minf = 13 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 122MiB/s ( 128MB/s ) , 122MiB/s-122MiB/s ( 128MB/s-128MB/s ) , io = 200MiB ( 210MB ) , run = 1637 -1637msec fio --name=big-file-multi-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --numjobs=6 --end_fsync=1 fio -filename=/config/fio.img -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=mytest \u56fa\u6001\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes Jobs: 6 ( f = 6 ) : [ R ( 6 )][ 88 .9% ][ r = 130MiB/s ][ r = 33 .2k IOPS ][ eta 00m:01s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 225237 : Fri Aug 12 14 :09:04 2022 read: IOPS = 5925 , BW = 23 .1MiB/s ( 24 .3MB/s )( 200MiB/8641msec ) clat ( nsec ) : min = 434 , max = 21757k, avg = 168221 .11, stdev = 1662876 .10 lat ( nsec ) : min = 471 , max = 21757k, avg = 168260 .51, stdev = 1662876 .63 clat percentiles ( nsec ) : | 1 .00th =[ 524 ] , 5 .00th =[ 540 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , | 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , 70 .00th =[ 628 ] , | 80 .00th =[ 660 ] , 90 .00th =[ 732 ] , 95 .00th =[ 884 ] , | 99 .00th =[ 3948544 ] , 99 .50th =[ 19005440 ] , 99 .90th =[ 20054016 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 20054016 ] \u5185\u5b58\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 12095 : Fri Aug 12 15 :05:27 2022 read: IOPS = 966k, BW = 3774MiB/s ( 3957MB/s )( 200MiB/53msec ) clat ( nsec ) : min = 520 , max = 221610 , avg = 757 .38, stdev = 2561 .09 lat ( nsec ) : min = 553 , max = 221646 , avg = 792 .50, stdev = 2561 .18 clat percentiles ( nsec ) : | 1 .00th =[ 540 ] , 5 .00th =[ 556 ] , 10 .00th =[ 556 ] , 20 .00th =[ 564 ] , | 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , | 70 .00th =[ 644 ] , 80 .00th =[ 724 ] , 90 .00th =[ 908 ] , 95 .00th =[ 940 ] , | 99 .00th =[ 3344 ] , 99 .50th =[ 3728 ] , 99 .90th =[ 19072 ] , 99 .95th =[ 43264 ] , | 99 .99th =[ 115200 ] fio --name=small-file-multi-read \\ --directory=/config \\ --rw=read --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=500 \\ --numjobs=2","title":"\u5bb9\u5668\u8bbf\u95ee\u5bf9\u8c61\u5b58\u50a8"},{"location":"ansible/ansible-case1/","text":"Ansible \u6848\u4f8b \u00b6 \u51c6\u5907\u5de5\u4f5c \u914d\u7f6einventory/pve [ pve ] master.pve hostname = master ansible_host = 192 .168.1.11 ansible_ssh_user = root ansible_ssh_pass = \"pass@word1\" \u6e29\u99a8\u63d0\u793a \u53ef\u80fd\u51fa\u73b0sudo\u6743\u9650\u4e0d\u591f\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u5728inventory\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9 gtx3090.c1 hostname = ubuntu ansible_ssh_user = xxx ansible_sudo_pass = xxx ansible_ssh_pass = \"password\" Ansible \u5e38\u7528\u9009\u9879 -C //\u9884\u6267\u884c --list-hosts //\u5217\u51fa\u6b63\u5728\u8fd0\u884c\u4efb\u52a1\u7684\u4e3b\u673a --list-tasks //\u5217\u51fatasks --limit \u4e3b\u673a\u5217\u8868 //\u53ea\u9488\u5bf9\u7279\u5b9a\u4e3b\u673a\u6267\u884c \u6848\u4f8b\u4e00\uff1acreate_user.yaml \u00b6 Ansible \u6279\u91cf\u521b\u5efa\u7528\u6237 \u8fd9\u91cc\u8bbe\u7f6epassword\u9700\u8981\u4f7f\u7528python\u547d\u4ee4\u751f\u6210sha-512\u7b97\u6cd5,pw\u662f\u8981\u52a0\u5bc6\u7684\u5bc6\u7801 a=$(python3 -c 'import crypt,getpass;pw=\"123456\";print(crypt.crypt(pw))') $ echo $a QUbhMQ9BAGQBE \u793a\u4f8b\u4e00 \u7528\u6237\u5bc6\u7801 - hosts : pve tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - name : add user user : name={{ item }} groups=admin password=d6F1o2yfgTymU shell=/bin/bash with_items : - user1 - user2 - user3 \u793a\u4f8b\u4e8c: \u516c\u94a5\u5f62\u5f0f tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - user : name : \"{{ item }}\" groups : admin shell : /bin/bash with_items : - lixie - user : name : \"{{ item }}\" state : absent with_items : - user # - ubuntu - authorized_key : user : \"{{ item.u }}\" key : \"https://github.com/{{ item.k }}.keys\" # \u516c\u94a5\u4e0a\u4f20\u5230github with_items : - { u : lixie , k : lixie021 } \u6267\u884cansible-playbook ansible-playbook -i inventory/pve pve.yaml","title":"Ansible \u6848\u4f8b"},{"location":"ansible/ansible-case1/#ansible","text":"\u51c6\u5907\u5de5\u4f5c \u914d\u7f6einventory/pve [ pve ] master.pve hostname = master ansible_host = 192 .168.1.11 ansible_ssh_user = root ansible_ssh_pass = \"pass@word1\" \u6e29\u99a8\u63d0\u793a \u53ef\u80fd\u51fa\u73b0sudo\u6743\u9650\u4e0d\u591f\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u5728inventory\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9 gtx3090.c1 hostname = ubuntu ansible_ssh_user = xxx ansible_sudo_pass = xxx ansible_ssh_pass = \"password\" Ansible \u5e38\u7528\u9009\u9879 -C //\u9884\u6267\u884c --list-hosts //\u5217\u51fa\u6b63\u5728\u8fd0\u884c\u4efb\u52a1\u7684\u4e3b\u673a --list-tasks //\u5217\u51fatasks --limit \u4e3b\u673a\u5217\u8868 //\u53ea\u9488\u5bf9\u7279\u5b9a\u4e3b\u673a\u6267\u884c","title":"Ansible \u6848\u4f8b"},{"location":"ansible/ansible-case1/#create_useryaml","text":"Ansible \u6279\u91cf\u521b\u5efa\u7528\u6237 \u8fd9\u91cc\u8bbe\u7f6epassword\u9700\u8981\u4f7f\u7528python\u547d\u4ee4\u751f\u6210sha-512\u7b97\u6cd5,pw\u662f\u8981\u52a0\u5bc6\u7684\u5bc6\u7801 a=$(python3 -c 'import crypt,getpass;pw=\"123456\";print(crypt.crypt(pw))') $ echo $a QUbhMQ9BAGQBE \u793a\u4f8b\u4e00 \u7528\u6237\u5bc6\u7801 - hosts : pve tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - name : add user user : name={{ item }} groups=admin password=d6F1o2yfgTymU shell=/bin/bash with_items : - user1 - user2 - user3 \u793a\u4f8b\u4e8c: \u516c\u94a5\u5f62\u5f0f tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - user : name : \"{{ item }}\" groups : admin shell : /bin/bash with_items : - lixie - user : name : \"{{ item }}\" state : absent with_items : - user # - ubuntu - authorized_key : user : \"{{ item.u }}\" key : \"https://github.com/{{ item.k }}.keys\" # \u516c\u94a5\u4e0a\u4f20\u5230github with_items : - { u : lixie , k : lixie021 } \u6267\u884cansible-playbook ansible-playbook -i inventory/pve pve.yaml","title":"\u6848\u4f8b\u4e00\uff1acreate_user.yaml"},{"location":"basic/cncf/","text":"\u4e91\u539f\u751f\u53caCNCF \u00b6 \u4e91\u539f\u751f\u7684\u5b9a\u4e49 \u00b6 https://github.com/cncf/toc/blob/main/DEFINITION.md#%E4%B8%AD%E6 \u4e91\u539f\u751f\u6280\u672f\u6709\u5229\u4e8e\u5404\u7ec4\u7ec7\u5728\u516c\u6709\u4e91\u3001\u79c1\u6709\u4e91\u548c\u6df7\u5408\u4e91\u7b49\u65b0\u578b\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u6784\u5efa\u548c\u8fd0\u884c\u53ef\u5f39\u6027\u6269\u5c55\u7684\u5e94\u7528\u3002\u4e91\u539f\u751f\u7684\u4ee3\u8868\u6280\u672f\u5305\u62ec\u5bb9\u5668\u3001\u670d\u52a1\u7f51\u683c\u3001\u5fae\u670d\u52a1\u3001\u4e0d\u53ef\u53d8\u57fa\u7840\u8bbe\u65bd\u548c\u58f0\u660e\u5f0fAPI\u3002 \u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u6784\u5efa\u5bb9\u9519\u6027\u597d\u3001\u6613\u4e8e\u7ba1\u7406\u548c\u4fbf\u4e8e\u89c2\u5bdf\u7684\u677e\u8026\u5408\u7cfb\u7edf\u3002\u7ed3\u5408\u53ef\u9760\u7684\u81ea\u52a8\u5316\u624b\u6bb5\uff0c\u4e91\u539f\u751f\u6280\u672f\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u8f7b\u677e\u5730\u5bf9\u7cfb\u7edf\u4f5c\u51fa\u9891\u7e41\u548c\u53ef\u9884\u6d4b\u7684\u91cd\u5927\u53d8\u66f4\u3002 \u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u81f4\u529b\u4e8e\u57f9\u80b2\u548c\u7ef4\u62a4\u4e00\u4e2a\u5382\u5546\u4e2d\u7acb\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u6765\u63a8\u5e7f\u4e91\u539f\u751f\u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u6700\u524d\u6cbf\u7684\u6a21\u5f0f\u6c11\u4e3b\u5316\uff0c\u8ba9\u8fd9\u4e9b\u521b\u65b0\u4e3a\u5927\u4f17\u6240\u7528\u3002 CNCF \u4e91\u539f\u751f\u5bb9\u5668\u751f\u6001\u7cfb\u7edf\u6982\u8981 \u00b6 \u7f51\u7ad9\uff1ahttp://dockone.io/article/3006 \u6587\u6863\u53c2\u8003\uff1a \u7b2c\u4e00\u671f\u6587\u6863\uff1a \u7b2c\u4e8c\u671f\u6587\u6863\uff1a \u7b2c\u4e09\u671f\u6587\u6863\uff1a","title":"\u4e91\u539f\u751f\u7b80\u4ecb"},{"location":"basic/cncf/#cncf","text":"","title":"\u4e91\u539f\u751f\u53caCNCF"},{"location":"basic/cncf/#_1","text":"https://github.com/cncf/toc/blob/main/DEFINITION.md#%E4%B8%AD%E6 \u4e91\u539f\u751f\u6280\u672f\u6709\u5229\u4e8e\u5404\u7ec4\u7ec7\u5728\u516c\u6709\u4e91\u3001\u79c1\u6709\u4e91\u548c\u6df7\u5408\u4e91\u7b49\u65b0\u578b\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u6784\u5efa\u548c\u8fd0\u884c\u53ef\u5f39\u6027\u6269\u5c55\u7684\u5e94\u7528\u3002\u4e91\u539f\u751f\u7684\u4ee3\u8868\u6280\u672f\u5305\u62ec\u5bb9\u5668\u3001\u670d\u52a1\u7f51\u683c\u3001\u5fae\u670d\u52a1\u3001\u4e0d\u53ef\u53d8\u57fa\u7840\u8bbe\u65bd\u548c\u58f0\u660e\u5f0fAPI\u3002 \u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u6784\u5efa\u5bb9\u9519\u6027\u597d\u3001\u6613\u4e8e\u7ba1\u7406\u548c\u4fbf\u4e8e\u89c2\u5bdf\u7684\u677e\u8026\u5408\u7cfb\u7edf\u3002\u7ed3\u5408\u53ef\u9760\u7684\u81ea\u52a8\u5316\u624b\u6bb5\uff0c\u4e91\u539f\u751f\u6280\u672f\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u8f7b\u677e\u5730\u5bf9\u7cfb\u7edf\u4f5c\u51fa\u9891\u7e41\u548c\u53ef\u9884\u6d4b\u7684\u91cd\u5927\u53d8\u66f4\u3002 \u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u81f4\u529b\u4e8e\u57f9\u80b2\u548c\u7ef4\u62a4\u4e00\u4e2a\u5382\u5546\u4e2d\u7acb\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u6765\u63a8\u5e7f\u4e91\u539f\u751f\u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u6700\u524d\u6cbf\u7684\u6a21\u5f0f\u6c11\u4e3b\u5316\uff0c\u8ba9\u8fd9\u4e9b\u521b\u65b0\u4e3a\u5927\u4f17\u6240\u7528\u3002","title":"\u4e91\u539f\u751f\u7684\u5b9a\u4e49"},{"location":"basic/cncf/#cncf_1","text":"\u7f51\u7ad9\uff1ahttp://dockone.io/article/3006 \u6587\u6863\u53c2\u8003\uff1a \u7b2c\u4e00\u671f\u6587\u6863\uff1a \u7b2c\u4e8c\u671f\u6587\u6863\uff1a \u7b2c\u4e09\u671f\u6587\u6863\uff1a","title":"CNCF \u4e91\u539f\u751f\u5bb9\u5668\u751f\u6001\u7cfb\u7edf\u6982\u8981"},{"location":"basic/etcd/","text":"Etcd \u5907\u4efd \u00b6 https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ Info \u4ee5\u4e0b\u5907\u4efd\u4e3b\u8981\u662f\u4ee5kubeadm\u5b89\u88c5\u7684k8s \u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84 \u00b6 [cka] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.159.81:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.159.81:2380 - --initial-cluster=master0=https://192.168.159.81:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls=https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls=http://127.0.0.1:2381 - --listen-peer-urls=https://192.168.159.81:2380 - --name=master0 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \u624b\u52a8\u5907\u4efd \u00b6 \u5b89\u88c5etcd\u5ba2\u6237\u7aef \u00b6 apt install etcd-client \u6307\u5b9a\u8def\u5f84\u5907\u4efd \u00b6 \u8fd9\u91cc\u5c06\u5907\u4efd\u6307\u5b9a\u5907\u4efd\u5230 /srv/data/\uff0c\u6307\u5b9a\u7684\u4e09\u4e2a\u6587\u4ef6\u5c31\u662f\u4ee5\u4e0a\u6807\u6ce8\u7684\u4e09\u4e2a\u4f4d\u7f6e export ETCDCTL_API=3 # \u58f0\u660eetcd-api mkdir /srv/data/ etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u6821\u9a8c\u7ed3\u679c\uff1a etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot status /srv/data/etcd-snapshot.db \u81ea\u52a8\u5907\u4efd \u00b6 \u8fd8\u539f\uff08kubeadm \u5b89\u88c5\u7684k8s\uff09 \u00b6 \u6a21\u62df\u6545\u969c \u00b6 \u8fd9\u91cc\u6211\u6a21\u62df\u5220\u9664\u4e86\u8fd9\u4e2a\u8d44\u6e90\uff0c\u770b\u4e00\u4f1a\u662f\u5426\u53ef\u4ee5\u8fd8\u539f [ucloud] root@master0:~# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d [ucloud] root@master0:~# k delete ds nginx-ds daemonset.apps \"nginx-ds\" deleted \u786e\u8ba4k8s\u7ec4\u4ef6\u7684\u4f4d\u7f6e \u00b6 \u67e5\u770bkubelet\u7684\u72b6\u6001 [ucloud] root@master0:~# systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d # \u67e5\u770b\u8fd9\u4e2a\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6 \u2514\u250010-kubeadm.conf Active: active (running) since Mon 2022-07-04 15:09:15 CST; 1 day 23h ago Docs: https://kubernetes.io/docs/home/ Main PID: 39279 (kubelet) Tasks: 17 (limit: 4390) Memory: 63.3M CGroup: /system.slice/kubelet.service \u67e5\u770b\u6587\u4ef6\uff1a/etc/systemd/system/kubelet.service.d/10-kubeadm.conf [ucloud] root@master0:~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # \u67e5\u770b\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS \u67e5\u770b\u6587\u4ef6\uff1a/var/lib/kubelet/config.yaml [ucloud] root@master0:~# cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: ...... staticPodPath: /etc/kubernetes/manifests # \u770b\u5230\u8fd9\u884c\uff0c\u5c31\u8bf4\u660e\u914d\u7f6e\u6587\u4ef6\u8fd9\u4e2amanifests\u4e0b streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s \u505c\u6b62api-server \u00b6 [ucloud] root@master0:~# mkdir /opt/backup/ -p [ucloud] root@master0:~# cd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# ll total 24 drwxr-xr-x 2 root root 4096 Jul 3 22:01 ./ drwxr-xr-x 5 root root 4096 Jul 3 21:15 ../ -rw------- 1 root root 2294 Jul 3 22:00 etcd.yaml -rw------- 1 root root 4036 Jul 3 22:00 kube-apiserver.yaml -rw------- 1 root root 3541 Jul 3 22:00 kube-controller-manager.yaml -rw------- 1 root root 1464 Jul 3 22:00 kube-scheduler.yaml [ucloud] root@master0:/etc/kubernetes/manifests# mv kube-* /opt/backup/ # \u79fb\u8d70\u4e4b\u540e\u6211\u4eec\u4f1a\u53d1\u73b0\u4e0d\u80fd\u4f7f\u7528kubectl\u547d\u4ee4 \u8fdb\u884c\u8fd8\u539f \u00b6 \u5982\u679c\u6062\u590d\u5931\u8d25\u9700\u8981\u6dfb\u52a0--skip-hash-check\u53c2\u6570 [ucloud] root@master0:~# etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot restore /srv/data/etcd-snapshot.db --data-dir=/var/lib/etcd-restore --skip-hash-check \u4fee\u6539etcd\u7684\u914d\u7f6e\u6587\u4ef6 \u5c06 volume \u914d\u7f6e\u7684 path: /var/lib/etcd \u6539\u6210/var/lib/etcd-restore [ucloud] root@master0:/etc/kubernetes/manifests# pwd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# vim etcd.yaml - hostPath: path: /var/lib/etcd-restore # \u5c06\u6539\u76ee\u5f55\u4fee\u6539\u4e3a\u8fd8\u539fetcd\u7684\u4f4d\u7f6e type: DirectoryOrCreate \u8fd8\u539fk8s\u7ec4\u4ef6 mv /opt/backup/* /etc/kubernetes/manifests systemctl restart kubelet \u6821\u9a8c\u7ed3\u679c\uff1a \u53d1\u73b0\u8bef\u5220\u9664\u7684pod\u8fd8\u539f\u6210\u529f [ucloud] root@master0:/etc/kubernetes/manifests# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d","title":"ETCD\u5907\u4efd\u8fd8\u539f"},{"location":"basic/etcd/#etcd","text":"https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ Info \u4ee5\u4e0b\u5907\u4efd\u4e3b\u8981\u662f\u4ee5kubeadm\u5b89\u88c5\u7684k8s","title":"Etcd \u5907\u4efd"},{"location":"basic/etcd/#_1","text":"[cka] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.159.81:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.159.81:2380 - --initial-cluster=master0=https://192.168.159.81:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls=https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls=http://127.0.0.1:2381 - --listen-peer-urls=https://192.168.159.81:2380 - --name=master0 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt","title":"\u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84"},{"location":"basic/etcd/#_2","text":"","title":"\u624b\u52a8\u5907\u4efd"},{"location":"basic/etcd/#etcd_1","text":"apt install etcd-client","title":"\u5b89\u88c5etcd\u5ba2\u6237\u7aef"},{"location":"basic/etcd/#_3","text":"\u8fd9\u91cc\u5c06\u5907\u4efd\u6307\u5b9a\u5907\u4efd\u5230 /srv/data/\uff0c\u6307\u5b9a\u7684\u4e09\u4e2a\u6587\u4ef6\u5c31\u662f\u4ee5\u4e0a\u6807\u6ce8\u7684\u4e09\u4e2a\u4f4d\u7f6e export ETCDCTL_API=3 # \u58f0\u660eetcd-api mkdir /srv/data/ etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u6821\u9a8c\u7ed3\u679c\uff1a etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot status /srv/data/etcd-snapshot.db","title":"\u6307\u5b9a\u8def\u5f84\u5907\u4efd"},{"location":"basic/etcd/#_4","text":"","title":"\u81ea\u52a8\u5907\u4efd"},{"location":"basic/etcd/#kubeadm-k8s","text":"","title":"\u8fd8\u539f\uff08kubeadm \u5b89\u88c5\u7684k8s\uff09"},{"location":"basic/etcd/#_5","text":"\u8fd9\u91cc\u6211\u6a21\u62df\u5220\u9664\u4e86\u8fd9\u4e2a\u8d44\u6e90\uff0c\u770b\u4e00\u4f1a\u662f\u5426\u53ef\u4ee5\u8fd8\u539f [ucloud] root@master0:~# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d [ucloud] root@master0:~# k delete ds nginx-ds daemonset.apps \"nginx-ds\" deleted","title":"\u6a21\u62df\u6545\u969c"},{"location":"basic/etcd/#k8s","text":"\u67e5\u770bkubelet\u7684\u72b6\u6001 [ucloud] root@master0:~# systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d # \u67e5\u770b\u8fd9\u4e2a\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6 \u2514\u250010-kubeadm.conf Active: active (running) since Mon 2022-07-04 15:09:15 CST; 1 day 23h ago Docs: https://kubernetes.io/docs/home/ Main PID: 39279 (kubelet) Tasks: 17 (limit: 4390) Memory: 63.3M CGroup: /system.slice/kubelet.service \u67e5\u770b\u6587\u4ef6\uff1a/etc/systemd/system/kubelet.service.d/10-kubeadm.conf [ucloud] root@master0:~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # \u67e5\u770b\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS \u67e5\u770b\u6587\u4ef6\uff1a/var/lib/kubelet/config.yaml [ucloud] root@master0:~# cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: ...... staticPodPath: /etc/kubernetes/manifests # \u770b\u5230\u8fd9\u884c\uff0c\u5c31\u8bf4\u660e\u914d\u7f6e\u6587\u4ef6\u8fd9\u4e2amanifests\u4e0b streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s","title":"\u786e\u8ba4k8s\u7ec4\u4ef6\u7684\u4f4d\u7f6e"},{"location":"basic/etcd/#api-server","text":"[ucloud] root@master0:~# mkdir /opt/backup/ -p [ucloud] root@master0:~# cd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# ll total 24 drwxr-xr-x 2 root root 4096 Jul 3 22:01 ./ drwxr-xr-x 5 root root 4096 Jul 3 21:15 ../ -rw------- 1 root root 2294 Jul 3 22:00 etcd.yaml -rw------- 1 root root 4036 Jul 3 22:00 kube-apiserver.yaml -rw------- 1 root root 3541 Jul 3 22:00 kube-controller-manager.yaml -rw------- 1 root root 1464 Jul 3 22:00 kube-scheduler.yaml [ucloud] root@master0:/etc/kubernetes/manifests# mv kube-* /opt/backup/ # \u79fb\u8d70\u4e4b\u540e\u6211\u4eec\u4f1a\u53d1\u73b0\u4e0d\u80fd\u4f7f\u7528kubectl\u547d\u4ee4","title":"\u505c\u6b62api-server"},{"location":"basic/etcd/#_6","text":"\u5982\u679c\u6062\u590d\u5931\u8d25\u9700\u8981\u6dfb\u52a0--skip-hash-check\u53c2\u6570 [ucloud] root@master0:~# etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot restore /srv/data/etcd-snapshot.db --data-dir=/var/lib/etcd-restore --skip-hash-check \u4fee\u6539etcd\u7684\u914d\u7f6e\u6587\u4ef6 \u5c06 volume \u914d\u7f6e\u7684 path: /var/lib/etcd \u6539\u6210/var/lib/etcd-restore [ucloud] root@master0:/etc/kubernetes/manifests# pwd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# vim etcd.yaml - hostPath: path: /var/lib/etcd-restore # \u5c06\u6539\u76ee\u5f55\u4fee\u6539\u4e3a\u8fd8\u539fetcd\u7684\u4f4d\u7f6e type: DirectoryOrCreate \u8fd8\u539fk8s\u7ec4\u4ef6 mv /opt/backup/* /etc/kubernetes/manifests systemctl restart kubelet \u6821\u9a8c\u7ed3\u679c\uff1a \u53d1\u73b0\u8bef\u5220\u9664\u7684pod\u8fd8\u539f\u6210\u529f [ucloud] root@master0:/etc/kubernetes/manifests# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d","title":"\u8fdb\u884c\u8fd8\u539f"},{"location":"basic/k8s-systeminit/","text":"k8s\u96c6\u7fa4\u90e8\u7f72 \u00b6 kubeadm \u521b\u5efa\u96c6\u7fa4 \u00b6 \u6ce8\u610f\u4ee5\u4e0b\u662fcentos\u642d\u5efa \u96c6\u7fa4\u521d\u59cb\u5316 \u00b6 # \u5c06 SELinux \u8bbe\u7f6e\u4e3a permissive \u6a21\u5f0f\uff08\u76f8\u5f53\u4e8e\u5c06\u5176\u7981\u7528\uff09 sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #\u5173\u95edswap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #\u5141\u8bb8 iptables \u68c0\u67e5\u6865\u63a5\u6d41\u91cf cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system \u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes = kubernetes # \u6240\u6709\u8282\u70b9 sudo systemctl enable --now kubelet \u6e29\u99a8\u63d0\u793a kubelet \u73b0\u5728\u6bcf\u9694\u51e0\u79d2\u5c31\u4f1a\u91cd\u542f\uff0c\u56e0\u4e3a\u5b83\u9677\u5165\u4e86\u4e00\u4e2a\u7b49\u5f85 kubeadm \u6307\u4ee4\u7684\u6b7b\u5faa\u73af \u67e5\u770bkubeadm\u7684\u7248\u672c\uff0c\u5e76\u62c9\u53d6\u8be5\u7248\u672c\u7684images kubeadm config images list --kubernetes-version v1.20.9 > k8s.images root@ubuntu:/home/ubuntu/script# cat k8s.images |awk -F'/' '{print $2}' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 \u811a\u672c\u4e0b\u8f7d\u955c\u50cf root@ubuntu:/home/ubuntu/script# cat ubuntu-k8s-images.sh #!/bin/bash images=' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 ' for i in $images ; do docker pull registry.aliyuncs.com/google_containers/$i done \u521d\u59cb\u5316master\u8282\u70b9\uff1a #\u6240\u6709\u673a\u5668\u6dfb\u52a0master\u57df\u540d\u6620\u5c04\uff0c\u4ee5\u4e0b\u9700\u8981\u4fee\u6539\u4e3a\u81ea\u5df1\u7684 echo \"192.168.8.70 cluster-endpoint\" >> /etc/hosts # \u4e3b\u8282\u70b9\u521d\u59cb\u5316 kubeadm init \\ --apiserver-advertise-address=192.168.159.201 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=172.16.0.0/16 #\u6240\u6709\u7f51\u7edc\u8303\u56f4\u4e0d\u91cd\u53e0 \u521d\u59cb\u5316\u53d1\u73b0\u6240\u6709\u72b6\u6001\u90fd\u662f NotReady \u5b89\u88c5\u7f51\u7edc\u7ec4\u4ef6: calico Node\u8282\u70b9\u52a0\u5165\u96c6\u7fa4 \u00b6 Warning \u65b0\u4ee4\u724c,\u9ed8\u8ba4\u7684\u4ee4\u724c24\u5c0f\u65f6\u5019\u5931\u6548 kubeadm token create --print-join-command Mac\u8fde\u63a5\u96c6\u7fa4\u62a5\u9519 \u00b6 Mac \u8fde\u63a5k8s\u96c6\u7fa4\u62a5\u9519 x509: certificate signed by unknown authority \u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\u6ca1\u6709\u6267\u884c\u5916\u7f51\u5730\u5740.\u5bfc\u81f4\u8bc1\u4e66\u4e0d\u80fd\u6b63\u5e38\u4f7f\u7528 \u5c06 master \u7684\u5916\u7f51\u5730\u5740\u548c\u4e3b\u673a\u540d\u89e3\u6790\u5230\u672c\u5730 hosts \u6e05\u7406k8s\u96c6\u7fa4 \u00b6 Success [root@k8s-master ~]# kubeadm reset [root@k8s-master ~]# kubectl delete node 192.168.200.112 [root@k8s-node01 ~]# docker rm -f $(docker ps -aq) [root@k8s-node01 ~]# systemctl stop kubelet [root@k8s-node01 ~]# rm -rf /etc/kubernetes/* [root@k8s-node01 ~]# rm -rf /var/lib/kubelet/* \u5f3a\u5236\u5220\u9664namespace \u00b6 delete namespace \u6253\u5f00\u4e00\u4e2a\u65b0\u7a97\u53e3\uff1aroot@master30:~# kubectl proxy --port=8001 \u65b9\u6cd5\u4e8c $ NAMESPACE_NAME=rook-ceph cat <<EOF | curl -X PUT \\ 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE_NAME/finalize \\ -H \"Content-Type : application/json\" \\ --data-binary @- { \"kind\" : \"Namespace\" , \"apiVersion\" : \"v1\" , \"metadata\" : { \"name\" : \"$NAMESPACE_NAME\" }, \"spec\" : { \"finalizers\" : null } } EOF System OOM encountered \u539f\u56e0\u63cf\u8ff0\uff1a \u4e0a\u8ff0\u4e24\u79cdOOM\uff08\u8fdb\u7a0bOOM\uff0c\u5bb9\u5668OOM\uff09\u53d1\u751f\u540e\uff0c\u90fd\u53ef\u80fd\u4f1a\u4f34\u968f\u4e00\u4e2a\u7cfb\u7edfOOM\u4e8b\u4ef6\uff0c\u8be5\u4e8b\u4ef6\u7684\u539f\u56e0\u662f\u7531\u4e0a\u8ff0OOM\u4e8b\u4ef6\u4f34\u968f\u5bfc\u81f4\u3002 \u89e3\u51b3\u65b9\u6848\uff1a \u9700\u8981\u89e3\u51b3\u4e0a\u9762\u8fdb\u7a0bOOM\u6216\u8005\u5bb9\u5668CgroupOOM\u7684\u95ee\u9898 \u5c0f\u6280\u5de7 \u00b6 \u5c0f\u6280\u5de7 Mac \u7ba1\u7406kubernetes\uff0c\u5408\u5e76yaml https://aisensiy.me/kubeconfig-management","title":"\u96c6\u7fa4\u90e8\u7f72"},{"location":"basic/k8s-systeminit/#k8s","text":"","title":"k8s\u96c6\u7fa4\u90e8\u7f72"},{"location":"basic/k8s-systeminit/#kubeadm","text":"\u6ce8\u610f\u4ee5\u4e0b\u662fcentos\u642d\u5efa","title":"kubeadm \u521b\u5efa\u96c6\u7fa4"},{"location":"basic/k8s-systeminit/#_1","text":"# \u5c06 SELinux \u8bbe\u7f6e\u4e3a permissive \u6a21\u5f0f\uff08\u76f8\u5f53\u4e8e\u5c06\u5176\u7981\u7528\uff09 sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #\u5173\u95edswap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #\u5141\u8bb8 iptables \u68c0\u67e5\u6865\u63a5\u6d41\u91cf cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system \u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes = kubernetes # \u6240\u6709\u8282\u70b9 sudo systemctl enable --now kubelet \u6e29\u99a8\u63d0\u793a kubelet \u73b0\u5728\u6bcf\u9694\u51e0\u79d2\u5c31\u4f1a\u91cd\u542f\uff0c\u56e0\u4e3a\u5b83\u9677\u5165\u4e86\u4e00\u4e2a\u7b49\u5f85 kubeadm \u6307\u4ee4\u7684\u6b7b\u5faa\u73af \u67e5\u770bkubeadm\u7684\u7248\u672c\uff0c\u5e76\u62c9\u53d6\u8be5\u7248\u672c\u7684images kubeadm config images list --kubernetes-version v1.20.9 > k8s.images root@ubuntu:/home/ubuntu/script# cat k8s.images |awk -F'/' '{print $2}' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 \u811a\u672c\u4e0b\u8f7d\u955c\u50cf root@ubuntu:/home/ubuntu/script# cat ubuntu-k8s-images.sh #!/bin/bash images=' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 ' for i in $images ; do docker pull registry.aliyuncs.com/google_containers/$i done \u521d\u59cb\u5316master\u8282\u70b9\uff1a #\u6240\u6709\u673a\u5668\u6dfb\u52a0master\u57df\u540d\u6620\u5c04\uff0c\u4ee5\u4e0b\u9700\u8981\u4fee\u6539\u4e3a\u81ea\u5df1\u7684 echo \"192.168.8.70 cluster-endpoint\" >> /etc/hosts # \u4e3b\u8282\u70b9\u521d\u59cb\u5316 kubeadm init \\ --apiserver-advertise-address=192.168.159.201 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=172.16.0.0/16 #\u6240\u6709\u7f51\u7edc\u8303\u56f4\u4e0d\u91cd\u53e0 \u521d\u59cb\u5316\u53d1\u73b0\u6240\u6709\u72b6\u6001\u90fd\u662f NotReady \u5b89\u88c5\u7f51\u7edc\u7ec4\u4ef6: calico","title":"\u96c6\u7fa4\u521d\u59cb\u5316"},{"location":"basic/k8s-systeminit/#node","text":"Warning \u65b0\u4ee4\u724c,\u9ed8\u8ba4\u7684\u4ee4\u724c24\u5c0f\u65f6\u5019\u5931\u6548 kubeadm token create --print-join-command","title":"Node\u8282\u70b9\u52a0\u5165\u96c6\u7fa4"},{"location":"basic/k8s-systeminit/#mac","text":"Mac \u8fde\u63a5k8s\u96c6\u7fa4\u62a5\u9519 x509: certificate signed by unknown authority \u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\u6ca1\u6709\u6267\u884c\u5916\u7f51\u5730\u5740.\u5bfc\u81f4\u8bc1\u4e66\u4e0d\u80fd\u6b63\u5e38\u4f7f\u7528 \u5c06 master \u7684\u5916\u7f51\u5730\u5740\u548c\u4e3b\u673a\u540d\u89e3\u6790\u5230\u672c\u5730 hosts","title":"Mac\u8fde\u63a5\u96c6\u7fa4\u62a5\u9519"},{"location":"basic/k8s-systeminit/#k8s_1","text":"Success [root@k8s-master ~]# kubeadm reset [root@k8s-master ~]# kubectl delete node 192.168.200.112 [root@k8s-node01 ~]# docker rm -f $(docker ps -aq) [root@k8s-node01 ~]# systemctl stop kubelet [root@k8s-node01 ~]# rm -rf /etc/kubernetes/* [root@k8s-node01 ~]# rm -rf /var/lib/kubelet/*","title":"\u6e05\u7406k8s\u96c6\u7fa4"},{"location":"basic/k8s-systeminit/#namespace","text":"delete namespace \u6253\u5f00\u4e00\u4e2a\u65b0\u7a97\u53e3\uff1aroot@master30:~# kubectl proxy --port=8001 \u65b9\u6cd5\u4e8c $ NAMESPACE_NAME=rook-ceph cat <<EOF | curl -X PUT \\ 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE_NAME/finalize \\ -H \"Content-Type : application/json\" \\ --data-binary @- { \"kind\" : \"Namespace\" , \"apiVersion\" : \"v1\" , \"metadata\" : { \"name\" : \"$NAMESPACE_NAME\" }, \"spec\" : { \"finalizers\" : null } } EOF System OOM encountered \u539f\u56e0\u63cf\u8ff0\uff1a \u4e0a\u8ff0\u4e24\u79cdOOM\uff08\u8fdb\u7a0bOOM\uff0c\u5bb9\u5668OOM\uff09\u53d1\u751f\u540e\uff0c\u90fd\u53ef\u80fd\u4f1a\u4f34\u968f\u4e00\u4e2a\u7cfb\u7edfOOM\u4e8b\u4ef6\uff0c\u8be5\u4e8b\u4ef6\u7684\u539f\u56e0\u662f\u7531\u4e0a\u8ff0OOM\u4e8b\u4ef6\u4f34\u968f\u5bfc\u81f4\u3002 \u89e3\u51b3\u65b9\u6848\uff1a \u9700\u8981\u89e3\u51b3\u4e0a\u9762\u8fdb\u7a0bOOM\u6216\u8005\u5bb9\u5668CgroupOOM\u7684\u95ee\u9898","title":"\u5f3a\u5236\u5220\u9664namespace"},{"location":"basic/k8s-systeminit/#_2","text":"\u5c0f\u6280\u5de7 Mac \u7ba1\u7406kubernetes\uff0c\u5408\u5e76yaml https://aisensiy.me/kubeconfig-management","title":"\u5c0f\u6280\u5de7"},{"location":"basic/k8s-update/","text":"k8s \u7248\u672c\u5347\u7ea7 \u00b6 \u6ce8\u610f \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5347\u7ea7\u662fkubeadm\u5b89\u88c5\u7684k8s\u5662 \u7248\u672c\u9700\u6c42 \u00b6 k8s \u7684 \u7248\u672c\u4e3av1.19.16\uff0c\u5982\u679c\u8981\u5347\u7ea7\u5230v1.23.8 Warning \u6ce8\u610f\uff1a \u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c,\u56e0\u6b64\u9700\u8981\u5347\u7ea7\u52064\u6b21\u8fdb\u884c\u5347\u7ea7\uff0c\u9700\u89811.20.15--1.21.14--1.22.11--1.23.8 \uff08\u5347\u7ea7\u56db\u6b21\uff09 \u6240\u6709\u8282\u70b9\u6267\u884c\uff1aapt update \u4e3b\u8282\u70b9: \u00b6 [ ucloud ] root@master0:~# apt-cache policy kubeadm | grep 1 .20. 1 .20.15-00 500 # ************\u6240\u6709\u8282\u70b9\uff0cmaster\u548cnode\u8282\u70b9\u90fd\u9700\u8981\u5347\u7ea7********* apt-get install kubeadm = 1 .20.15-00 # \u67e5\u770bk8s\u7248\u672c\u9700\u8981\u7684\u955c\u50cf kubeadm config images list --kubernetes-version v1.20.15 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 # \u4e3b\u8282\u70b9\u5347\u7ea7\uff0c\u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u6dfb\u52a0# --etcd-upgrade=false \u53c2\u6570\u5ffd\u7565\u5373\u53ef kubeadm upgrade apply v1.20.15 \u6ce8\u610f\uff1a \u8fd9\u65f6\u6211\u4eec\u53d1\u73b0k8s\u7684\u7248\u672c\u8fd8\u662f\u6ca1\u6709\u53d8\u5316\uff0c\u66f4\u65b0kubelet\u3002 \u4e3b\u8282\u70b9\uff1a apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 Error \u8fd9\u79cd\u60c5\u51b5\u53ef\u80fd\u662f\u4e4b\u524d\u5347\u7ea7\u6ca1\u6709\u8fdb\u884cnode\u8282\u70b9\u7684kubeadm\u5347\u7ea7\u5bfc\u81f4\u7684. [sjtu] root@master:/home/lixie# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade/health] FATAL: [preflight] Some fatal errors occurred: [ERROR ControlPlaneNodesReady]: there are NotReady control-planes in the cluster: [master] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher \u89e3\u51b3\uff1a\uff08\u91cd\u65b0\u5b89\u88c5\u8be5\u7248\u672ckubelet\u548ckubectl\uff09 apt install kubelet=1.19.16-00 apt install kubectl=1.19.16-00 \u4ece\u8282\u70b9: \u00b6 node\u8282\u70b9\u5347\u7ea7kubelet\u7684\u914d\u7f6e\uff08\u6240\u6709node\u8282\u70b9\uff09 sudo kubeadm upgrade node \u5b89\u88c5kubelet\u548ckubectl apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 kubelet --version # \u67e5\u770b\u7248\u672c Kubernetes v1.23.8 \u9644\u4ef6 \u00b6 k8s\u8bc1\u4e66\u8fc7\u671f\u5904\u7406 \u00b6 \u95ee\u9898\uff1a \u5904\u7406k8s\u8bc1\u4e66\u8fc7\u671f","title":"K8S\u5347\u7ea7"},{"location":"basic/k8s-update/#k8s","text":"\u6ce8\u610f \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5347\u7ea7\u662fkubeadm\u5b89\u88c5\u7684k8s\u5662","title":"k8s \u7248\u672c\u5347\u7ea7"},{"location":"basic/k8s-update/#_1","text":"k8s \u7684 \u7248\u672c\u4e3av1.19.16\uff0c\u5982\u679c\u8981\u5347\u7ea7\u5230v1.23.8 Warning \u6ce8\u610f\uff1a \u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c,\u56e0\u6b64\u9700\u8981\u5347\u7ea7\u52064\u6b21\u8fdb\u884c\u5347\u7ea7\uff0c\u9700\u89811.20.15--1.21.14--1.22.11--1.23.8 \uff08\u5347\u7ea7\u56db\u6b21\uff09 \u6240\u6709\u8282\u70b9\u6267\u884c\uff1aapt update","title":"\u7248\u672c\u9700\u6c42"},{"location":"basic/k8s-update/#_2","text":"[ ucloud ] root@master0:~# apt-cache policy kubeadm | grep 1 .20. 1 .20.15-00 500 # ************\u6240\u6709\u8282\u70b9\uff0cmaster\u548cnode\u8282\u70b9\u90fd\u9700\u8981\u5347\u7ea7********* apt-get install kubeadm = 1 .20.15-00 # \u67e5\u770bk8s\u7248\u672c\u9700\u8981\u7684\u955c\u50cf kubeadm config images list --kubernetes-version v1.20.15 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 # \u4e3b\u8282\u70b9\u5347\u7ea7\uff0c\u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u6dfb\u52a0# --etcd-upgrade=false \u53c2\u6570\u5ffd\u7565\u5373\u53ef kubeadm upgrade apply v1.20.15 \u6ce8\u610f\uff1a \u8fd9\u65f6\u6211\u4eec\u53d1\u73b0k8s\u7684\u7248\u672c\u8fd8\u662f\u6ca1\u6709\u53d8\u5316\uff0c\u66f4\u65b0kubelet\u3002 \u4e3b\u8282\u70b9\uff1a apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 Error \u8fd9\u79cd\u60c5\u51b5\u53ef\u80fd\u662f\u4e4b\u524d\u5347\u7ea7\u6ca1\u6709\u8fdb\u884cnode\u8282\u70b9\u7684kubeadm\u5347\u7ea7\u5bfc\u81f4\u7684. [sjtu] root@master:/home/lixie# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade/health] FATAL: [preflight] Some fatal errors occurred: [ERROR ControlPlaneNodesReady]: there are NotReady control-planes in the cluster: [master] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher \u89e3\u51b3\uff1a\uff08\u91cd\u65b0\u5b89\u88c5\u8be5\u7248\u672ckubelet\u548ckubectl\uff09 apt install kubelet=1.19.16-00 apt install kubectl=1.19.16-00","title":"\u4e3b\u8282\u70b9:"},{"location":"basic/k8s-update/#_3","text":"node\u8282\u70b9\u5347\u7ea7kubelet\u7684\u914d\u7f6e\uff08\u6240\u6709node\u8282\u70b9\uff09 sudo kubeadm upgrade node \u5b89\u88c5kubelet\u548ckubectl apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 kubelet --version # \u67e5\u770b\u7248\u672c Kubernetes v1.23.8","title":"\u4ece\u8282\u70b9:"},{"location":"basic/k8s-update/#_4","text":"","title":"\u9644\u4ef6"},{"location":"basic/k8s-update/#k8s_1","text":"\u95ee\u9898\uff1a \u5904\u7406k8s\u8bc1\u4e66\u8fc7\u671f","title":"k8s\u8bc1\u4e66\u8fc7\u671f\u5904\u7406"},{"location":"basic/overview/","text":"K8S\u7b80\u4ecb \u00b6","title":"K8S\u7b80\u4ecb"},{"location":"basic/overview/#k8s","text":"","title":"K8S\u7b80\u4ecb"},{"location":"cks/cks/","text":"CKS\u8003\u9898 \u00b6 1. \u5bb9\u5668\u8fd0\u884c\u65f6\uff08runtimeclass \u8003\u9898\uff09 \u00b6 1.1.1 \u8003\u9898 \u00b6 1.1.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.1.3 \u8003\u9898\u89e3\u7b54 \u00b6 \u53c2\u8003\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/containers/runtime-class/ \u521b\u5efa RuntimeClass \u8d44\u6e90 \u00b6 # RuntimeClass \u5b9a\u4e49\u4e8e node.k8s.io API \u7ec4 apiVersion : node.k8s.io/v1 kind : RuntimeClass metadata : # RuntimeClass \u662f\u4e00\u4e2a\u96c6\u7fa4\u5c42\u9762\u7684\u8d44\u6e90 name : untrusted # \u7528\u6765\u5f15\u7528 RuntimeClass \u7684\u540d\u5b57 handler : runsc # \u5bf9\u5e94\u7684 CRI \u914d\u7f6e\u7684\u540d\u79f0 \u6dfb\u52a0 runtimeClassName \u00b6 apiVersion : v1 kind : Pod metadata : name : mypod spec : runtimeClassName : untrusted # \u6ce8\u610f\u8fd9\u4e2auntrusted \u540d\u79f0 # ... 2022-01 \u8003\u9898\u66f4\u65b0 \u00b6 2. ServiceAccount\u8003\u9898 \u00b6 \u53c2\u8003\u5730\u5740 https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ 1.2.1 \u8003\u9898 \u00b6 1.2.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.2.3 \u8003\u9898\u89e3\u7b54 \u00b6 apiVersion : v1 kind : ServiceAccount metadata : name : build-robot automountServiceAccountToken : false apiVersion : v1 kind : Pod metadata : name : my-pod spec : serviceAccountName : build-robot automountServiceAccountToken : false 3. kube-bench \u8003\u9898 \u00b6 1.3.1 \u8003\u9898 \u00b6 1.3.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.3.3 \u8003\u9898\u89e3\u7b54 \u00b6 4. NetworkPolicy \u8003\u9898 \u00b6 \u53c2\u8003\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/ 1.4.1 \u8003\u9898 \u00b6 1.4.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.4.3 \u8003\u9898\u89e3\u7b54 \u00b6 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : defaultdeny # \u540d\u79f0\u53ef\u80fd\u4e0d\u4e00\u6837 spec : podSelector : {} policyTypes : - Ingress 5. PSP \u8003\u9898 \u00b6 \u53c2\u6570\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-policy/ 1.5.1 \u8003\u9898 \u00b6 1.5.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.5.3 \u8003\u9898\u89e3\u7b54 \u00b6 6. RBAC \u8003\u9898 \u00b6 1.6.1 \u8003\u9898 \u00b6 1.6.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.6.3 \u8003\u9898\u89e3\u7b54 \u00b6 7. \u5ba1\u8ba1\u65e5\u5fd7 \u8003\u9898 \u00b6 1.7.1 \u8003\u9898 \u00b6 1.7.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883 \u00b6 1.7.3 \u8003\u9898\u89e3\u7b54 \u00b6","title":"CKA/CKS\u8003\u9898"},{"location":"cks/cks/#cks","text":"","title":"CKS\u8003\u9898"},{"location":"cks/cks/#1-runtimeclass","text":"","title":"1. \u5bb9\u5668\u8fd0\u884c\u65f6\uff08runtimeclass \u8003\u9898\uff09"},{"location":"cks/cks/#111","text":"","title":"1.1.1 \u8003\u9898"},{"location":"cks/cks/#112","text":"","title":"1.1.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#113","text":"\u53c2\u8003\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/containers/runtime-class/","title":"1.1.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#runtimeclass","text":"# RuntimeClass \u5b9a\u4e49\u4e8e node.k8s.io API \u7ec4 apiVersion : node.k8s.io/v1 kind : RuntimeClass metadata : # RuntimeClass \u662f\u4e00\u4e2a\u96c6\u7fa4\u5c42\u9762\u7684\u8d44\u6e90 name : untrusted # \u7528\u6765\u5f15\u7528 RuntimeClass \u7684\u540d\u5b57 handler : runsc # \u5bf9\u5e94\u7684 CRI \u914d\u7f6e\u7684\u540d\u79f0","title":"\u521b\u5efa RuntimeClass \u8d44\u6e90"},{"location":"cks/cks/#runtimeclassname","text":"apiVersion : v1 kind : Pod metadata : name : mypod spec : runtimeClassName : untrusted # \u6ce8\u610f\u8fd9\u4e2auntrusted \u540d\u79f0 # ...","title":"\u6dfb\u52a0 runtimeClassName"},{"location":"cks/cks/#2022-01","text":"","title":"2022-01 \u8003\u9898\u66f4\u65b0"},{"location":"cks/cks/#2-serviceaccount","text":"\u53c2\u8003\u5730\u5740 https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/","title":"2. ServiceAccount\u8003\u9898"},{"location":"cks/cks/#121","text":"","title":"1.2.1 \u8003\u9898"},{"location":"cks/cks/#122","text":"","title":"1.2.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#123","text":"apiVersion : v1 kind : ServiceAccount metadata : name : build-robot automountServiceAccountToken : false apiVersion : v1 kind : Pod metadata : name : my-pod spec : serviceAccountName : build-robot automountServiceAccountToken : false","title":"1.2.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#3-kube-bench","text":"","title":"3. kube-bench \u8003\u9898"},{"location":"cks/cks/#131","text":"","title":"1.3.1 \u8003\u9898"},{"location":"cks/cks/#132","text":"","title":"1.3.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#133","text":"","title":"1.3.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#4-networkpolicy","text":"\u53c2\u8003\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/services-networking/network-policies/","title":"4. NetworkPolicy \u8003\u9898"},{"location":"cks/cks/#141","text":"","title":"1.4.1 \u8003\u9898"},{"location":"cks/cks/#142","text":"","title":"1.4.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#143","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : defaultdeny # \u540d\u79f0\u53ef\u80fd\u4e0d\u4e00\u6837 spec : podSelector : {} policyTypes : - Ingress","title":"1.4.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#5-psp","text":"\u53c2\u6570\u5730\u5740 https://kubernetes.io/zh-cn/docs/concepts/security/pod-security-policy/","title":"5. PSP \u8003\u9898"},{"location":"cks/cks/#151","text":"","title":"1.5.1 \u8003\u9898"},{"location":"cks/cks/#152","text":"","title":"1.5.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#153","text":"","title":"1.5.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#6-rbac","text":"","title":"6. RBAC \u8003\u9898"},{"location":"cks/cks/#161","text":"","title":"1.6.1 \u8003\u9898"},{"location":"cks/cks/#162","text":"","title":"1.6.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#163","text":"","title":"1.6.3 \u8003\u9898\u89e3\u7b54"},{"location":"cks/cks/#7","text":"","title":"7. \u5ba1\u8ba1\u65e5\u5fd7 \u8003\u9898"},{"location":"cks/cks/#171","text":"","title":"1.7.1 \u8003\u9898"},{"location":"cks/cks/#172","text":"","title":"1.7.2 \u6a21\u62df\u8003\u8bd5\u73af\u5883"},{"location":"cks/cks/#173","text":"","title":"1.7.3 \u8003\u9898\u89e3\u7b54"},{"location":"config-admin/RBAC/","text":"RBAC \u9274\u6743 \u00b6 \u6838\u5fc3\u6982\u5ff5 Role : Role \u603b\u662f\u7528\u6765\u5728\u67d0\u4e2a\u540d\u5b57\u7a7a\u95f4\u5185\u8bbe\u7f6e\u8bbf\u95ee\u6743\u9650\uff1b \u5728\u4f60\u521b\u5efa Role \u65f6\uff0c\u4f60\u5fc5\u987b\u6307\u5b9a\u8be5 Role \u6240\u5c5e\u7684\u540d\u5b57\u7a7a\u95f4\u3002(\u6709\u547d\u4ee4\u7a7a\u95f4\u9650\u5236) ClusterRole: ClusterRole \u5219\u662f\u4e00\u4e2a\u96c6\u7fa4\u4f5c\u7528\u57df\u7684\u8d44\u6e90(\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236) RoleBinding: \u89d2\u8272\u7ed1\u5b9a(\u6709\u547d\u4ee4\u7a7a\u95f4\u9650\u5236) ClusterRoleBinding: \u89d2\u8272\u7ed1\u5b9a(\u65e0\u547d\u4ee4\u7a7a\u95f4\u9650\u5236) \u7ec6\u7c92\u5ea6\u7684\u6743\u9650\u5212\u5206 \u00b6 Role\u793a\u4f8b \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default # \u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236 name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" \u6807\u660e core API \u7ec4 resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] ClusterRole \u793a\u4f8b \u00b6 apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" \u88ab\u5ffd\u7565\uff0c\u56e0\u4e3a ClusterRoles \u4e0d\u53d7\u540d\u5b57\u7a7a\u95f4\u9650\u5236 name : secret-reader rules : - apiGroups : [ \"\" ] # \u5728 HTTP \u5c42\u9762\uff0c\u7528\u6765\u8bbf\u95ee Secret \u8d44\u6e90\u7684\u540d\u79f0\u4e3a \"secrets\" resources : [ \"secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] \u4e24\u8005\u7684\u533a\u522b \u4e3b\u8981\u5728\u547d\u540d\u7a7a\u95f4\u9650\u5236\u4e0a\uff0cclusterRole\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\uff0cRole\u6709\u9650\u5236","title":"RBAC\u9274\u6743"},{"location":"config-admin/RBAC/#rbac","text":"\u6838\u5fc3\u6982\u5ff5 Role : Role \u603b\u662f\u7528\u6765\u5728\u67d0\u4e2a\u540d\u5b57\u7a7a\u95f4\u5185\u8bbe\u7f6e\u8bbf\u95ee\u6743\u9650\uff1b \u5728\u4f60\u521b\u5efa Role \u65f6\uff0c\u4f60\u5fc5\u987b\u6307\u5b9a\u8be5 Role \u6240\u5c5e\u7684\u540d\u5b57\u7a7a\u95f4\u3002(\u6709\u547d\u4ee4\u7a7a\u95f4\u9650\u5236) ClusterRole: ClusterRole \u5219\u662f\u4e00\u4e2a\u96c6\u7fa4\u4f5c\u7528\u57df\u7684\u8d44\u6e90(\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236) RoleBinding: \u89d2\u8272\u7ed1\u5b9a(\u6709\u547d\u4ee4\u7a7a\u95f4\u9650\u5236) ClusterRoleBinding: \u89d2\u8272\u7ed1\u5b9a(\u65e0\u547d\u4ee4\u7a7a\u95f4\u9650\u5236)","title":"RBAC \u9274\u6743"},{"location":"config-admin/RBAC/#_1","text":"","title":"\u7ec6\u7c92\u5ea6\u7684\u6743\u9650\u5212\u5206"},{"location":"config-admin/RBAC/#role","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default # \u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236 name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" \u6807\u660e core API \u7ec4 resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ]","title":"Role\u793a\u4f8b"},{"location":"config-admin/RBAC/#clusterrole","text":"apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" \u88ab\u5ffd\u7565\uff0c\u56e0\u4e3a ClusterRoles \u4e0d\u53d7\u540d\u5b57\u7a7a\u95f4\u9650\u5236 name : secret-reader rules : - apiGroups : [ \"\" ] # \u5728 HTTP \u5c42\u9762\uff0c\u7528\u6765\u8bbf\u95ee Secret \u8d44\u6e90\u7684\u540d\u79f0\u4e3a \"secrets\" resources : [ \"secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] \u4e24\u8005\u7684\u533a\u522b \u4e3b\u8981\u5728\u547d\u540d\u7a7a\u95f4\u9650\u5236\u4e0a\uff0cclusterRole\u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\uff0cRole\u6709\u9650\u5236","title":"ClusterRole \u793a\u4f8b"},{"location":"docker/lxcfs/","text":"https://www.vvave.net/archives/introduction-to-linux-kernel-control-groups-v2.html \u4fee\u6539\u914d\u7f6e\u6587\u4ef6\uff1a/etc/default/grub GRUB_CMDLINE_LINUX_DEFAULT = \"quiet splash systemd.unified_cgroup_hierarchy=no systemd.legacy_systemd_cgroup_controller=no\" sudo grub-mkconfig -o /boot/grub/grub.cfg","title":"lxcfs \u4ecb\u7ecd"},{"location":"easy/mac-easy/","text":"Mac \u73af\u5883\u914d\u7f6e \u00b6 \u80cc\u666f \u00b6 \u5728\u7ef4\u62a4\u548c\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4\u65f6\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u90fd\u6709\u81ea\u5df1\u5bf9\u5e94\u7684config\u6587\u4ef6\uff0c\u90a3\u4e48\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u5728 ~/.kube \u76ee\u5f55\u4e0b\u5c31\u4f1a\u6709\u4e00\u5927\u5806\u5404\u79cd\u73af\u5883\u7684 yaml\uff0c\u5bf9\u4e8e\u7ba1\u7406\u6765\u8bf4\u4e0d\u662f\u7279\u522b\u7684\u53cb\u597d\u3002 \u66f4\u6709\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u96c6\u7fa4\u5207\u6765\u5207\u53bb\uff0c\u9020\u6210\u8fd0\u7ef4\u4e8b\u6545\u3002 kubecm \u7528\u6cd5 \u89e3\u51b3\u7684\u75db\u70b9\u95ee\u9898\uff1a \u7edf\u4e00\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4 \u6765\u56de\u5207\u6362\u6307\u5b9anamespace\u7e41\u7410 \u5f88\u591a\u65f6\u5019\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u54ea\u4e2a\u96c6\u7fa4\u4e0b \u5b89\u88c5 \u00b6 Mac\u5b89\u88c5\u5730\u5740\uff1a kubecm https://formulae.brew.sh/formula/kubecm \u4e0b\u8f7d\u597d\u4e86\uff0c\u5982\u4f55\u4f7f\u7528\uff1f \u4f7f\u7528\u8bf4\u660e\uff1a kubecm github\u5730\u5740 kubecm \u6848\u4f8b \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u6211\u6700\u5e38\u7528\u7684\u5c31\u662f\u6dfb\u52a0\uff0c\u5220\u9664\uff0c\u5207\u6362\u96c6\u7fa4 Manage your kubeconfig more easily. \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 Tips Find more information at: https://kubecm.cloud Usage: kubecm [ command ] Available Commands: add Add KubeConfig to $HOME /.kube/config # \u6dfb\u52a0 alias Generate alias for all contexts clear Clear lapsed context, cluster and user cloud manage kubeconfig from cloud completion Generate completion script create Create new KubeConfig ( experiment ) delete Delete the specified context from the kubeconfig # \u5220\u9664 help Help about any command list List KubeConfig merge Merge multiple kubeconfig files into one namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively # \u5207\u6362 version Print version info Flags: --config string path of kubeconfig ( default \" $HOME /.kube/config\" ) -h, --help help for kubecm --ui-size int number of list items to show in menu at once ( default 4 ) Use \"kubecm [command] --help\" for more information about a command. \u9996\u5148\u6211\u4eec\u5148\u56de\u5230\uff1a\u5bb6\u76ee\u5f55\u4e0b\u7684 .kube \u76ee\u5f55 $ pwd /Users/beiyiwangdejiyi/.kube \u6dfb\u52a0\u4e00\u4e2a\u96c6\u7fa4 \u00b6 kubecm add -f enflame.yaml \u5220\u9664\u4e00\u4e2a\u96c6\u7fa4 \u00b6 $ kubecm delete # \u9009\u62e9\u5220\u9664\u7684\u96c6\u7fa4\uff0c\u9009\u62e9True Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select The Delete Kube Context \ud83d\ude3c ucloud-k3s ( * ) enflame produce \u2193 pve \u5207\u6362\u4e00\u4e2a\u96c6\u7fa4 \u00b6 $ kubecm switch # \u53ef\u4ee5\u4e0a\u4e0b\u5207\u6362\uff0c\u9009\u62e9\u96c6\u7fa4\u73af\u5883 Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select Kube Context ucloud-k3s ( * ) bj \ud83d\ude3c dev \u2193 produce \u4ee5\u4e0a\u5c31\u662fkubecm \u7684\u4e00\u4e9b\u5b89\u88c5\u548c\u4f7f\u7528\u65b9\u6cd5\uff0c\u5176\u4ed6\u7684\u5982\u679c\u611f\u5174\u8da3\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u8bd5\u3002 kubens kubectx \u6587\u7ae0\u5730\u5740\uff1a kubenc kubectx\u5b89\u88c5 \u00b6 \u5982\u679c\u4f60\u4f7f\u7528Homebrew\uff0c\u4f60\u53ef\u4ee5\u50cf\u8fd9\u6837\u5b89\u88c5\uff1a brew install kubectx \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6765\u8fd9\u6837\u4f7f\u7528kubens\u5c31\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u5207\u6362namespace\u3002 fzf\u5b89\u88c5 \u00b6 fzf \u5b89\u88c5 fzf\u5b98\u7f51 \u8fd9\u91cc\u9762\u6bd4\u8f83\u597d\u73a9\u7684\u8fd8\u6709\u4e00\u4e2a\u5e26\u6709\u6a21\u7cca\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u83dc\u5355\uff0c\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u518d\u4f7f\u7528kubens\u5c31\u9999\u7684\u5f88\u554a brew install fzf $( brew --prefix ) /opt/fzf/install \u6e29\u99a8\u63d0\u793a \u5982\u679c\u4e0d\u80fd\u4f7f\u7528\uff0c\u9700\u8981\u5173\u95ed\u7ec8\u7aef\uff0c\u91cd\u65b0\u6253\u5f00 fzf\u9664\u4e86\u8fd9\u4e9b\uff0c\u8fd8\u6709\u5f88\u591a\u7684\u9a9a\u64cd\u4f5c\uff0cshell\u547d\u4ee4\u8865\u5168\uff0c\u53e6\u5916fzf \u91cd\u5199\u4e86 ctrl+r \u641c\u7d22\u5386\u53f2\u547d\u4ee4 word\u6587\u4ef6\u4fee\u590d \u00b6 \u53c2\u8003\u5730\u5740 https://www.51cto.com/article/708448.html","title":"Mac \u5c0f\u529f\u80fd"},{"location":"easy/mac-easy/#mac","text":"","title":"Mac \u73af\u5883\u914d\u7f6e"},{"location":"easy/mac-easy/#_1","text":"\u5728\u7ef4\u62a4\u548c\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4\u65f6\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u90fd\u6709\u81ea\u5df1\u5bf9\u5e94\u7684config\u6587\u4ef6\uff0c\u90a3\u4e48\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u5728 ~/.kube \u76ee\u5f55\u4e0b\u5c31\u4f1a\u6709\u4e00\u5927\u5806\u5404\u79cd\u73af\u5883\u7684 yaml\uff0c\u5bf9\u4e8e\u7ba1\u7406\u6765\u8bf4\u4e0d\u662f\u7279\u522b\u7684\u53cb\u597d\u3002 \u66f4\u6709\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u96c6\u7fa4\u5207\u6765\u5207\u53bb\uff0c\u9020\u6210\u8fd0\u7ef4\u4e8b\u6545\u3002 kubecm \u7528\u6cd5 \u89e3\u51b3\u7684\u75db\u70b9\u95ee\u9898\uff1a \u7edf\u4e00\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4 \u6765\u56de\u5207\u6362\u6307\u5b9anamespace\u7e41\u7410 \u5f88\u591a\u65f6\u5019\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u54ea\u4e2a\u96c6\u7fa4\u4e0b","title":"\u80cc\u666f"},{"location":"easy/mac-easy/#_2","text":"Mac\u5b89\u88c5\u5730\u5740\uff1a kubecm https://formulae.brew.sh/formula/kubecm \u4e0b\u8f7d\u597d\u4e86\uff0c\u5982\u4f55\u4f7f\u7528\uff1f \u4f7f\u7528\u8bf4\u660e\uff1a kubecm github\u5730\u5740","title":"\u5b89\u88c5"},{"location":"easy/mac-easy/#kubecm","text":"\u53c2\u6570\u8bf4\u660e\uff1a \u6211\u6700\u5e38\u7528\u7684\u5c31\u662f\u6dfb\u52a0\uff0c\u5220\u9664\uff0c\u5207\u6362\u96c6\u7fa4 Manage your kubeconfig more easily. \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 Tips Find more information at: https://kubecm.cloud Usage: kubecm [ command ] Available Commands: add Add KubeConfig to $HOME /.kube/config # \u6dfb\u52a0 alias Generate alias for all contexts clear Clear lapsed context, cluster and user cloud manage kubeconfig from cloud completion Generate completion script create Create new KubeConfig ( experiment ) delete Delete the specified context from the kubeconfig # \u5220\u9664 help Help about any command list List KubeConfig merge Merge multiple kubeconfig files into one namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively # \u5207\u6362 version Print version info Flags: --config string path of kubeconfig ( default \" $HOME /.kube/config\" ) -h, --help help for kubecm --ui-size int number of list items to show in menu at once ( default 4 ) Use \"kubecm [command] --help\" for more information about a command. \u9996\u5148\u6211\u4eec\u5148\u56de\u5230\uff1a\u5bb6\u76ee\u5f55\u4e0b\u7684 .kube \u76ee\u5f55 $ pwd /Users/beiyiwangdejiyi/.kube","title":"kubecm \u6848\u4f8b"},{"location":"easy/mac-easy/#_3","text":"kubecm add -f enflame.yaml","title":"\u6dfb\u52a0\u4e00\u4e2a\u96c6\u7fa4"},{"location":"easy/mac-easy/#_4","text":"$ kubecm delete # \u9009\u62e9\u5220\u9664\u7684\u96c6\u7fa4\uff0c\u9009\u62e9True Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select The Delete Kube Context \ud83d\ude3c ucloud-k3s ( * ) enflame produce \u2193 pve","title":"\u5220\u9664\u4e00\u4e2a\u96c6\u7fa4"},{"location":"easy/mac-easy/#_5","text":"$ kubecm switch # \u53ef\u4ee5\u4e0a\u4e0b\u5207\u6362\uff0c\u9009\u62e9\u96c6\u7fa4\u73af\u5883 Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select Kube Context ucloud-k3s ( * ) bj \ud83d\ude3c dev \u2193 produce \u4ee5\u4e0a\u5c31\u662fkubecm \u7684\u4e00\u4e9b\u5b89\u88c5\u548c\u4f7f\u7528\u65b9\u6cd5\uff0c\u5176\u4ed6\u7684\u5982\u679c\u611f\u5174\u8da3\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u8bd5\u3002 kubens kubectx \u6587\u7ae0\u5730\u5740\uff1a kubenc","title":"\u5207\u6362\u4e00\u4e2a\u96c6\u7fa4"},{"location":"easy/mac-easy/#kubectx","text":"\u5982\u679c\u4f60\u4f7f\u7528Homebrew\uff0c\u4f60\u53ef\u4ee5\u50cf\u8fd9\u6837\u5b89\u88c5\uff1a brew install kubectx \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6765\u8fd9\u6837\u4f7f\u7528kubens\u5c31\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u5207\u6362namespace\u3002","title":"kubectx\u5b89\u88c5"},{"location":"easy/mac-easy/#fzf","text":"fzf \u5b89\u88c5 fzf\u5b98\u7f51 \u8fd9\u91cc\u9762\u6bd4\u8f83\u597d\u73a9\u7684\u8fd8\u6709\u4e00\u4e2a\u5e26\u6709\u6a21\u7cca\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u83dc\u5355\uff0c\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u518d\u4f7f\u7528kubens\u5c31\u9999\u7684\u5f88\u554a brew install fzf $( brew --prefix ) /opt/fzf/install \u6e29\u99a8\u63d0\u793a \u5982\u679c\u4e0d\u80fd\u4f7f\u7528\uff0c\u9700\u8981\u5173\u95ed\u7ec8\u7aef\uff0c\u91cd\u65b0\u6253\u5f00 fzf\u9664\u4e86\u8fd9\u4e9b\uff0c\u8fd8\u6709\u5f88\u591a\u7684\u9a9a\u64cd\u4f5c\uff0cshell\u547d\u4ee4\u8865\u5168\uff0c\u53e6\u5916fzf \u91cd\u5199\u4e86 ctrl+r \u641c\u7d22\u5386\u53f2\u547d\u4ee4","title":"fzf\u5b89\u88c5"},{"location":"easy/mac-easy/#word","text":"\u53c2\u8003\u5730\u5740 https://www.51cto.com/article/708448.html","title":"word\u6587\u4ef6\u4fee\u590d"},{"location":"easy/mac-usb-system/","text":"Mac \u5236\u4f5cubuntu\u955c\u50cf\uff08\u5176\u4ed6\u53d1\u884c\u7248\u90fd\u53ef\u4ee5\uff09 \u00b6 Convert ISO file to RAW img file $ hdiutil convert -format UDRW -o target.img ~/Downloads/CentOS-7-x86_64-Minimal-2009.iso \u6b63\u5728\u8bfb\u53d6Master Boot Record\uff08MBR\uff1a0\uff09\u2026 \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a1\uff09\u2026 \u6b63\u5728\u8bfb\u53d6\uff08Type EF\uff1a2\uff09\u2026 . \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a3\uff09\u2026 ............................................................................... \u5df2\u8017\u65f6\uff1a 1 .235s \u901f\u5ea6\uff1a787.3MB/\u79d2 \u8282\u7701\uff1a0.0% created: /Users/bougou/target.img.dmg Get USB dev id $ diskutil list /dev/disk0 ( internal ) : #: TYPE NAME SIZE IDENTIFIER 0 : GUID_partition_scheme 1 .0 TB disk0 1 : Apple_APFS_ISC 524 .3 MB disk0s1 2 : Apple_APFS Container disk3 994 .7 GB disk0s2 3 : Apple_APFS_Recovery 5 .4 GB disk0s3 /dev/disk3 ( synthesized ) : #: TYPE NAME SIZE IDENTIFIER 0 : APFS Container Scheme - +994.7 GB disk3 Physical Store disk0s2 1 : APFS Volume Macintosh HD 15 .2 GB disk3s1 2 : APFS Snapshot com.apple.os.update-... 15 .2 GB disk3s1s1 3 : APFS Volume Preboot 614 .7 MB disk3s2 4 : APFS Volume Recovery 1 .6 GB disk3s3 5 : APFS Volume Data 120 .9 GB disk3s5 6 : APFS Volume VM 20 .5 KB disk3s6 /dev/disk4 ( external, physical ) : #: TYPE NAME SIZE IDENTIFIER 0 : FDisk_partition_scheme *15.5 GB disk4 1 : 0xEF 9 .0 MB disk4s2 unmountDisk $ diskutil umountDisk /dev/disk4 Unmount of all volumes on disk4 was successful dd and eject USB $ sudo dd if = ~/target.img.dmg of = /dev/disk4 bs = 1m Password: 972 +1 records in 972 +1 records out 1019942912 bytes transferred in 47 .017375 secs ( 21692894 bytes/sec )","title":"Mac \u5236\u4f5cubuntu\u7cfb\u7edf\u76d8"},{"location":"easy/mac-usb-system/#mac-ubuntu","text":"Convert ISO file to RAW img file $ hdiutil convert -format UDRW -o target.img ~/Downloads/CentOS-7-x86_64-Minimal-2009.iso \u6b63\u5728\u8bfb\u53d6Master Boot Record\uff08MBR\uff1a0\uff09\u2026 \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a1\uff09\u2026 \u6b63\u5728\u8bfb\u53d6\uff08Type EF\uff1a2\uff09\u2026 . \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a3\uff09\u2026 ............................................................................... \u5df2\u8017\u65f6\uff1a 1 .235s \u901f\u5ea6\uff1a787.3MB/\u79d2 \u8282\u7701\uff1a0.0% created: /Users/bougou/target.img.dmg Get USB dev id $ diskutil list /dev/disk0 ( internal ) : #: TYPE NAME SIZE IDENTIFIER 0 : GUID_partition_scheme 1 .0 TB disk0 1 : Apple_APFS_ISC 524 .3 MB disk0s1 2 : Apple_APFS Container disk3 994 .7 GB disk0s2 3 : Apple_APFS_Recovery 5 .4 GB disk0s3 /dev/disk3 ( synthesized ) : #: TYPE NAME SIZE IDENTIFIER 0 : APFS Container Scheme - +994.7 GB disk3 Physical Store disk0s2 1 : APFS Volume Macintosh HD 15 .2 GB disk3s1 2 : APFS Snapshot com.apple.os.update-... 15 .2 GB disk3s1s1 3 : APFS Volume Preboot 614 .7 MB disk3s2 4 : APFS Volume Recovery 1 .6 GB disk3s3 5 : APFS Volume Data 120 .9 GB disk3s5 6 : APFS Volume VM 20 .5 KB disk3s6 /dev/disk4 ( external, physical ) : #: TYPE NAME SIZE IDENTIFIER 0 : FDisk_partition_scheme *15.5 GB disk4 1 : 0xEF 9 .0 MB disk4s2 unmountDisk $ diskutil umountDisk /dev/disk4 Unmount of all volumes on disk4 was successful dd and eject USB $ sudo dd if = ~/target.img.dmg of = /dev/disk4 bs = 1m Password: 972 +1 records in 972 +1 records out 1019942912 bytes transferred in 47 .017375 secs ( 21692894 bytes/sec )","title":"Mac \u5236\u4f5cubuntu\u955c\u50cf\uff08\u5176\u4ed6\u53d1\u884c\u7248\u90fd\u53ef\u4ee5\uff09"},{"location":"k3s/k3s-install/","text":"Docker \u8fd0\u884c\u65f6\u90e8\u7f72k3s \u00b6 \u57fa\u7840\u73af\u5883 \u00b6 \u73af\u5883\u8981\u6c42 Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB. \u65f6\u95f4\u540c\u6b65\uff0c\u65f6\u533a \u5173\u95ed\u9632\u706b\u5899 docker\u73af\u5883 \u5916\u7f6e\u73af\u5883\u8981\u6c42 \u9700\u8981\u5b89\u88c5docker\u73af\u5883\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u9ed8\u8ba4\u4e0d\u662fdocker. \u5b89\u88c5docker\uff1a curl https://releases.rancher.com/install-docker/19.03.sh | sh k3s\u5b89\u88c5server\u7aef \u00b6 Warning --docker \u662f\u4f7f\u7528docker\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker \u6307\u5b9a\u5b89\u88c5\uff1aINSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_VERSION=v1.22.5+k3s1 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker k3s\u90e8\u7f72agent\u7aef \u00b6 \u5148\u5728server\u7aef\u62ff\u5230token\uff1a root@ubuntu:~# cat /var/lib/rancher/k3s/server/node-token K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR = cn K3S_URL = https://192.168.0.202:6443 K3S_TOKEN = K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 INSTALL_K3S_EXEC = \"--docker\" sh - \u6307\u5b9a\u5b89\u88c5\uff1a curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10e3cfbb8fc176b66cb7957997cd7d01958fbbdd507d3c49b75ff819cd0a93905b::server:cba4412e36cc16e5a92137bf987d575d INSTALL_K3S_EXEC=\"--docker\" sh - \u66f4\u65b0\u7248\uff1a \u00b6 k3s\u5b89\u88c5server\u7aef \u00b6 curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker k3s \u90e8\u7f72agent\u7aef \u00b6 curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10ab8ec9db721e53cf65e395d44b2b59a48ee1744dcf7c691733f907af47c88630::server:806d6ca499e7ae52c75c89a7f47961b4 sh -s - --docker \u5378\u8f7dk3s \u00b6 \u8fd9\u91cc\u53ef\u4ee5\u901a\u8fc7 k3s \u5b98\u65b9\u65b9\u5f0f\u5220\u9664 \u6587\u7ae0\u53c2\u8003: - \u53c2\u8003\u5730\u5740\uff1ahttps://docs.rancher.cn/docs/k3s/quick-start/_index","title":"K3S\u5b89\u88c5"},{"location":"k3s/k3s-install/#docker-k3s","text":"","title":"Docker \u8fd0\u884c\u65f6\u90e8\u7f72k3s"},{"location":"k3s/k3s-install/#_1","text":"\u73af\u5883\u8981\u6c42 Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB. \u65f6\u95f4\u540c\u6b65\uff0c\u65f6\u533a \u5173\u95ed\u9632\u706b\u5899 docker\u73af\u5883 \u5916\u7f6e\u73af\u5883\u8981\u6c42 \u9700\u8981\u5b89\u88c5docker\u73af\u5883\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u9ed8\u8ba4\u4e0d\u662fdocker. \u5b89\u88c5docker\uff1a curl https://releases.rancher.com/install-docker/19.03.sh | sh","title":"\u57fa\u7840\u73af\u5883"},{"location":"k3s/k3s-install/#k3sserver","text":"Warning --docker \u662f\u4f7f\u7528docker\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker \u6307\u5b9a\u5b89\u88c5\uff1aINSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_VERSION=v1.22.5+k3s1 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker","title":"k3s\u5b89\u88c5server\u7aef"},{"location":"k3s/k3s-install/#k3sagent","text":"\u5148\u5728server\u7aef\u62ff\u5230token\uff1a root@ubuntu:~# cat /var/lib/rancher/k3s/server/node-token K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR = cn K3S_URL = https://192.168.0.202:6443 K3S_TOKEN = K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 INSTALL_K3S_EXEC = \"--docker\" sh - \u6307\u5b9a\u5b89\u88c5\uff1a curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10e3cfbb8fc176b66cb7957997cd7d01958fbbdd507d3c49b75ff819cd0a93905b::server:cba4412e36cc16e5a92137bf987d575d INSTALL_K3S_EXEC=\"--docker\" sh -","title":"k3s\u90e8\u7f72agent\u7aef"},{"location":"k3s/k3s-install/#_2","text":"","title":"\u66f4\u65b0\u7248\uff1a"},{"location":"k3s/k3s-install/#k3sserver_1","text":"curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker","title":"k3s\u5b89\u88c5server\u7aef"},{"location":"k3s/k3s-install/#k3s-agent","text":"curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10ab8ec9db721e53cf65e395d44b2b59a48ee1744dcf7c691733f907af47c88630::server:806d6ca499e7ae52c75c89a7f47961b4 sh -s - --docker","title":"k3s \u90e8\u7f72agent\u7aef"},{"location":"k3s/k3s-install/#k3s","text":"\u8fd9\u91cc\u53ef\u4ee5\u901a\u8fc7 k3s \u5b98\u65b9\u65b9\u5f0f\u5220\u9664 \u6587\u7ae0\u53c2\u8003: - \u53c2\u8003\u5730\u5740\uff1ahttps://docs.rancher.cn/docs/k3s/quick-start/_index","title":"\u5378\u8f7dk3s"},{"location":"k8s/base/k8s-overview/","text":"k8s \u7b80\u4ecb \u00b6 \u5f00\u6e90\u5bb9\u5668\u5316\u7f16\u6392\u5f15\u64ce\uff0c\u4e13\u7528\u4e8e\u7ba1\u7406\u5bb9\u5668\u5316\u5e94\u7528\u548c\u670d\u52a1\u96c6\u7fa4\u3002 \u6838\u5fc3\u7ec4\u4ef6\u4ecb\u7ecd \u00b6 \u63a7\u5236\u5e73\u9762\u7ec4\u4ef6 \u00b6 kube-apiserver: \u5b98\u65b9\u89e3\u91ca: API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u7ec4\u4ef6\uff0c \u8be5\u7ec4\u4ef6\u8d1f\u8d23\u516c\u5f00\u4e86 Kubernetes API\uff0c\u8d1f\u8d23\u5904\u7406\u63a5\u53d7\u8bf7\u6c42\u7684\u5de5\u4f5c\u3002 API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u524d\u7aef\u3002 Kubernetes API \u670d\u52a1\u5668\u7684\u4e3b\u8981\u5b9e\u73b0\u662f kube-apiserver\u3002 kube-apiserver \u8bbe\u8ba1\u4e0a\u8003\u8651\u4e86\u6c34\u5e73\u6269\u7f29\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u53ef\u901a\u8fc7\u90e8\u7f72\u591a\u4e2a\u5b9e\u4f8b\u6765\u8fdb\u884c\u6269\u7f29\u3002 \u4f60\u53ef\u4ee5\u8fd0\u884c kube-apiserver \u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u5728\u8fd9\u4e9b\u5b9e\u4f8b\u4e4b\u95f4\u5e73\u8861\u6d41\u91cf\u3002 \u4e2a\u4eba\u7406\u89e3: k8s\u7684\u8bbf\u95ee\u5165\u53e3\uff0c\u5982\u679c\u60f3\u64cd\u4f5c\u5bb9\u5668\uff0c\u5fc5\u987b\u8981\u901a\u8fc7api-server\u624d\u53ef\u4ee5\u3002k8s\u96c6\u7fa4\u6240\u6709\u7684node\u90fd\u8981\u4e0eapi-server\u8fdb\u884c\u901a\u4fe1\uff0c\u6240\u4ee5node\u7684\u6570\u91cf\u8d8a\u591a\u7ed9api-server\u7684\u538b\u529b\u5c31\u4f1a\u8d8a\u5927\u3002\u6240\u4ee5\u4e00\u822ck8s-master\u90fd\u662f\u96c6\u7fa4\u6765\u9ad8\u53ef\u7528\u7684\uff0c\u901a\u5e38\u6765\u8bf4\u90fd\u662f\u4e09\u4e2a\u8282\u70b9\u3002 kube-scheduler: \u5b98\u65b9\u89e3\u91ca: \u662f\u8d1f\u8d23\u8d44\u6e90\u8c03\u5ea6\u7684\u8fdb\u7a0b\uff0c\u76d1\u89c6\u65b0\u521b\u5efa\u4e14\u6ca1\u6709\u5206\u914d\u5230Node\u7684Pod\uff0c\u4e3aPod \u9009\u62e9\u4e00\u4e2aNode\uff1b \u8c03\u5ea6\u51b3\u7b56\u8003\u8651\u7684\u56e0\u7d20\u5305\u62ec\u5355\u4e2a Pod \u53ca Pods \u96c6\u5408\u7684\u8d44\u6e90\u9700\u6c42\u3001 \u8f6f\u786c\u4ef6\u53ca\u7b56\u7565\u7ea6\u675f\u3001 \u4eb2\u548c\u6027\u53ca\u53cd\u4eb2\u548c\u6027\u89c4\u8303\u3001\u6570\u636e\u4f4d\u7f6e\u3001\u5de5\u4f5c\u8d1f\u8f7d\u95f4\u7684\u5e72\u6270\u53ca\u6700\u540e\u65f6\u9650 . \u4e2a\u4eba\u7406\u89e3: \u7ba1\u7406\u5458\u53d1\u9001\u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\u7684\u6307\u4ee4\u5230api-server\uff0capi-server\u6536\u5230\u8fd9\u4e2a\u6307\u4ee4\u5c31\u5b58\u50a8\u5728etcd\u4e2d\uff0ckube-scheduler\u4f1a\u76d1\u542capi-server\u770b\u770b\u6709\u6ca1\u6709pod\u7684\u64cd\u4f5c\u4e8b\u4ef6\uff0c\u5982\u679c\u6ca1\u6709\u4e0b\u6b21\u63a5\u7740\u67e5\u8be2\uff0c\u5982\u679c\u6709\u7684\u8bdd\uff0ckube-scheduler\u4f1a\u901a\u8fc7api-server\u62ff\u5230etcd\u8fd9\u4e2a\u4e8b\u4ef6\u3002\u7136\u540ekube-scheduler\u4f1a\u6839\u636enode\u7684\u8d44\u6e90\u5229\u7528\u7387\u6765\u8fdb\u884c\u8c03\u5ea6\u3002\u8c03\u5ea6\u4e4b\u540ekube-scheduler\u4f1a\u628a\u7ed3\u679c\u8fd4\u56de\u7ed9api-server\uff0capi-server\u518d\u628a\u7ed3\u679c\u5199\u5230etcd\u3002 kube-controller-manager: \u5b98\u65b9\u89e3\u91ca: \u8fd0\u884c\u7ba1\u7406\u63a7\u5236\u5668\uff0c\u662f\u96c6\u7fa4\u4e2d\u5904\u7406\u5e38\u89c4\u4efb\u52a1\u7684\u540e\u53f0\u8fdb\u7a0b\uff0c\u662fKubernetes \u91cc\u6240\u6709\u8d44\u6e90\u5bf9\u8c61\u7684\u81ea\u52a8\u5316\u63a7\u5236\u4e2d\u5fc3\u3002\u903b\u8f91\u4e0a\uff0c\u6bcf\u4e2a\u63a7\u5236\u5668\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u8fdb\u7a0b\uff0c\u4f46\u4e3a\u4e86\u964d\u4f4e\u590d\u6742\u6027\uff0c\u5b83\u4eec\u90fd\u88ab\u7f16\u8bd1\u6210\u5355\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5e76\u5728\u5355\u4e2a\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u3002\u8fd9\u4e9b\u63a7\u5236\u5668\u4e3b\u8981\u5305\u62ec\u3002 \u8282\u70b9\u63a7\u5236\u5668\uff08Node Controller\uff09\uff1a\u8d1f\u8d23\u5728Node\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\u53ca\u65f6\u53d1\u73b0\u548c\u54cd\u5e94\uff1b \u590d\u5236\u63a7\u5236\u5668\uff08Replication Controller\uff09\uff1a\u8d1f\u8d23\u7ef4\u62a4\u6b63\u786e\u6570\u91cf\u7684Pod\uff1b \u7aef\u70b9\u63a7\u5236\u5668\uff08Endpoints Controller\uff09\uff1a\u586b\u5145\u7aef\u70b9\u5bf9\u8c61\uff08\u5373\u8fde\u63a5Services\u548cPods\uff09\uff1b \u670d\u52a1\u5e10\u6237\u548c\u4ee4\u724c\u63a7\u5236\u5668\uff08Service Account & Token Controllers\uff09\uff1a\u4e3a\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u9ed8\u8ba4\u5e10\u6237\u548cAPI\u8bbf\u95ee\u4ee4\u724c\u3002 \u4e2a\u4eba\u7406\u89e3: etcd: \u5b98\u65b9\u89e3\u91ca: \u662fKubernetes \u63d0\u4f9b\u7684\u9ed8\u8ba4\u5b58\u50a8\uff0c\u6240\u6709\u96c6\u7fa4\u6570\u636e\u90fd\u4fdd\u5b58\u5728Etcd\u4e2d\uff0c\u4f7f\u7528\u65f6\u5efa\u8bae\u4e3aEtcd \u6570\u636e\u63d0\u4f9b\u5907\u4efd\u8ba1\u5212\uff1b \u4e2a\u4eba\u7406\u89e3: \u4e00\u5806\u63a7\u5236\u5668\uff08\u526f\u672c\u63a7\u5236\u5668\uff0c\u8282\u70b9\u63a7\u5236\u5668\uff0c\u547d\u4ee4\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u670d\u52a1\u5668\u8d26\u53f7\u63a7\u5236\u5668\u7b49\uff09\uff0c\u63a7\u5236\u5668\u505a\u4e3a\u96c6\u7fa4\u5185\u90e8\u7ba1\u7406\u63a7\u5236\u4e2d\u5fc3\uff0c\u8d1f\u8d23\u96c6\u7fa4\u5185\u7684node\u3001pod\u3001\u670d\u52a1\u7aef\u70b9\u3001\u547d\u540d\u7a7a\u95f4\u3001\u670d\u52a1\u5668\u8d26\u53f7\uff0c\u8d44\u6e90\u5b9a\u989d\u7684\u7ba1\u7406\uff0c\u5f53\u67d0\u4e2anode\u610f\u5916\u5b95\u673a\uff0cController-manager\u4f1a\u81ea\u52a8\u53d1\u73b0\u5e76\u6267\u884c\u81ea\u52a8\u6062\u590d\u6d41\u7a0b\u3002\u786e\u4fdd\u96c6\u7fa4\u4e2dpod\u7684\u526f\u672c\u59cb\u7ec8\u4fdd\u6301\u9884\u671f\u5de5\u4f5c\u7684\u72b6\u6001\u3002 Node \u7ec4\u4ef6 \u00b6 kubelet\uff1a \u5b98\u65b9\u89e3\u91ca: \u8d1f\u8d23Pod\u5bf9\u5e94\u5bb9\u5668\u7684\u521b\u5efa\u3001\u8d77\u505c\u7b49\u4efb\u52a1\uff0c\u540c\u65f6\u4e0eMaster \u8282\u70b9\u5bc6\u5207\u534f\u4f5c\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7ba1\u7406\u7684\u57fa\u672c\u529f\u80fd\u3002 \u4e2a\u4eba\u7406\u89e3: kubelet\u662f\u7ef4\u62a4node\u4e0apod\u7684\u72b6\u6001\uff0c\u5982\u679cpod\u6302\u4e86\uff0c\u4ed6\u4f1a\u5c06\u4e8b\u4ef6\u53d1\u9001\u7ed9api-server\uff0capi-server\u5c06\u4e8b\u4ef6\u5199\u5165etcd\uff0ckube-scheduler\u6216\u8005Controller-manager\u4f1a\u62ff\u5230api-server\u4e0a\u7684\u4e8b\u4ef6\uff0c\u5bf9pod\u8fdb\u884c\u91cd\u5efa\u7b49\u64cd\u4f5c\u3002 kube-proxy\uff1a \u5b98\u65b9\u89e3\u91ca: \u5b98\u7f51\u89e3\u91ca\uff1a\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u7f51\u7edc\u4ee3\u7406\uff0c \u5b9e\u73b0 Kubernetes \u670d\u52a1\uff08Service\uff09 \u6982\u5ff5\u7684\u4e00\u90e8\u5206 \u4e2a\u4eba\u7406\u89e3: \u4e2a\u4eba\u7406\u89e3\uff1a\u7ef4\u62a4\u5f53\u524d\u4e3b\u673a\u7684\u7f51\u7edc\u89c4\u5219 \u5176\u4ed6\u7ec4\u4ef6 \u00b6 helm install mysql presslabs/mysql-operator \\ -f 1-config.yaml \\ -n infra helm install mysql-operator bitpoke/mysql-operator \\ -f 1-config.yaml \\ -n infra","title":"K8S\u7b80\u4ecb"},{"location":"k8s/base/k8s-overview/#k8s","text":"\u5f00\u6e90\u5bb9\u5668\u5316\u7f16\u6392\u5f15\u64ce\uff0c\u4e13\u7528\u4e8e\u7ba1\u7406\u5bb9\u5668\u5316\u5e94\u7528\u548c\u670d\u52a1\u96c6\u7fa4\u3002","title":"k8s \u7b80\u4ecb"},{"location":"k8s/base/k8s-overview/#_1","text":"","title":"\u6838\u5fc3\u7ec4\u4ef6\u4ecb\u7ecd"},{"location":"k8s/base/k8s-overview/#_2","text":"kube-apiserver: \u5b98\u65b9\u89e3\u91ca: API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u7ec4\u4ef6\uff0c \u8be5\u7ec4\u4ef6\u8d1f\u8d23\u516c\u5f00\u4e86 Kubernetes API\uff0c\u8d1f\u8d23\u5904\u7406\u63a5\u53d7\u8bf7\u6c42\u7684\u5de5\u4f5c\u3002 API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u524d\u7aef\u3002 Kubernetes API \u670d\u52a1\u5668\u7684\u4e3b\u8981\u5b9e\u73b0\u662f kube-apiserver\u3002 kube-apiserver \u8bbe\u8ba1\u4e0a\u8003\u8651\u4e86\u6c34\u5e73\u6269\u7f29\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u53ef\u901a\u8fc7\u90e8\u7f72\u591a\u4e2a\u5b9e\u4f8b\u6765\u8fdb\u884c\u6269\u7f29\u3002 \u4f60\u53ef\u4ee5\u8fd0\u884c kube-apiserver \u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u5728\u8fd9\u4e9b\u5b9e\u4f8b\u4e4b\u95f4\u5e73\u8861\u6d41\u91cf\u3002 \u4e2a\u4eba\u7406\u89e3: k8s\u7684\u8bbf\u95ee\u5165\u53e3\uff0c\u5982\u679c\u60f3\u64cd\u4f5c\u5bb9\u5668\uff0c\u5fc5\u987b\u8981\u901a\u8fc7api-server\u624d\u53ef\u4ee5\u3002k8s\u96c6\u7fa4\u6240\u6709\u7684node\u90fd\u8981\u4e0eapi-server\u8fdb\u884c\u901a\u4fe1\uff0c\u6240\u4ee5node\u7684\u6570\u91cf\u8d8a\u591a\u7ed9api-server\u7684\u538b\u529b\u5c31\u4f1a\u8d8a\u5927\u3002\u6240\u4ee5\u4e00\u822ck8s-master\u90fd\u662f\u96c6\u7fa4\u6765\u9ad8\u53ef\u7528\u7684\uff0c\u901a\u5e38\u6765\u8bf4\u90fd\u662f\u4e09\u4e2a\u8282\u70b9\u3002 kube-scheduler: \u5b98\u65b9\u89e3\u91ca: \u662f\u8d1f\u8d23\u8d44\u6e90\u8c03\u5ea6\u7684\u8fdb\u7a0b\uff0c\u76d1\u89c6\u65b0\u521b\u5efa\u4e14\u6ca1\u6709\u5206\u914d\u5230Node\u7684Pod\uff0c\u4e3aPod \u9009\u62e9\u4e00\u4e2aNode\uff1b \u8c03\u5ea6\u51b3\u7b56\u8003\u8651\u7684\u56e0\u7d20\u5305\u62ec\u5355\u4e2a Pod \u53ca Pods \u96c6\u5408\u7684\u8d44\u6e90\u9700\u6c42\u3001 \u8f6f\u786c\u4ef6\u53ca\u7b56\u7565\u7ea6\u675f\u3001 \u4eb2\u548c\u6027\u53ca\u53cd\u4eb2\u548c\u6027\u89c4\u8303\u3001\u6570\u636e\u4f4d\u7f6e\u3001\u5de5\u4f5c\u8d1f\u8f7d\u95f4\u7684\u5e72\u6270\u53ca\u6700\u540e\u65f6\u9650 . \u4e2a\u4eba\u7406\u89e3: \u7ba1\u7406\u5458\u53d1\u9001\u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\u7684\u6307\u4ee4\u5230api-server\uff0capi-server\u6536\u5230\u8fd9\u4e2a\u6307\u4ee4\u5c31\u5b58\u50a8\u5728etcd\u4e2d\uff0ckube-scheduler\u4f1a\u76d1\u542capi-server\u770b\u770b\u6709\u6ca1\u6709pod\u7684\u64cd\u4f5c\u4e8b\u4ef6\uff0c\u5982\u679c\u6ca1\u6709\u4e0b\u6b21\u63a5\u7740\u67e5\u8be2\uff0c\u5982\u679c\u6709\u7684\u8bdd\uff0ckube-scheduler\u4f1a\u901a\u8fc7api-server\u62ff\u5230etcd\u8fd9\u4e2a\u4e8b\u4ef6\u3002\u7136\u540ekube-scheduler\u4f1a\u6839\u636enode\u7684\u8d44\u6e90\u5229\u7528\u7387\u6765\u8fdb\u884c\u8c03\u5ea6\u3002\u8c03\u5ea6\u4e4b\u540ekube-scheduler\u4f1a\u628a\u7ed3\u679c\u8fd4\u56de\u7ed9api-server\uff0capi-server\u518d\u628a\u7ed3\u679c\u5199\u5230etcd\u3002 kube-controller-manager: \u5b98\u65b9\u89e3\u91ca: \u8fd0\u884c\u7ba1\u7406\u63a7\u5236\u5668\uff0c\u662f\u96c6\u7fa4\u4e2d\u5904\u7406\u5e38\u89c4\u4efb\u52a1\u7684\u540e\u53f0\u8fdb\u7a0b\uff0c\u662fKubernetes \u91cc\u6240\u6709\u8d44\u6e90\u5bf9\u8c61\u7684\u81ea\u52a8\u5316\u63a7\u5236\u4e2d\u5fc3\u3002\u903b\u8f91\u4e0a\uff0c\u6bcf\u4e2a\u63a7\u5236\u5668\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u8fdb\u7a0b\uff0c\u4f46\u4e3a\u4e86\u964d\u4f4e\u590d\u6742\u6027\uff0c\u5b83\u4eec\u90fd\u88ab\u7f16\u8bd1\u6210\u5355\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5e76\u5728\u5355\u4e2a\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u3002\u8fd9\u4e9b\u63a7\u5236\u5668\u4e3b\u8981\u5305\u62ec\u3002 \u8282\u70b9\u63a7\u5236\u5668\uff08Node Controller\uff09\uff1a\u8d1f\u8d23\u5728Node\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\u53ca\u65f6\u53d1\u73b0\u548c\u54cd\u5e94\uff1b \u590d\u5236\u63a7\u5236\u5668\uff08Replication Controller\uff09\uff1a\u8d1f\u8d23\u7ef4\u62a4\u6b63\u786e\u6570\u91cf\u7684Pod\uff1b \u7aef\u70b9\u63a7\u5236\u5668\uff08Endpoints Controller\uff09\uff1a\u586b\u5145\u7aef\u70b9\u5bf9\u8c61\uff08\u5373\u8fde\u63a5Services\u548cPods\uff09\uff1b \u670d\u52a1\u5e10\u6237\u548c\u4ee4\u724c\u63a7\u5236\u5668\uff08Service Account & Token Controllers\uff09\uff1a\u4e3a\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u9ed8\u8ba4\u5e10\u6237\u548cAPI\u8bbf\u95ee\u4ee4\u724c\u3002 \u4e2a\u4eba\u7406\u89e3: etcd: \u5b98\u65b9\u89e3\u91ca: \u662fKubernetes \u63d0\u4f9b\u7684\u9ed8\u8ba4\u5b58\u50a8\uff0c\u6240\u6709\u96c6\u7fa4\u6570\u636e\u90fd\u4fdd\u5b58\u5728Etcd\u4e2d\uff0c\u4f7f\u7528\u65f6\u5efa\u8bae\u4e3aEtcd \u6570\u636e\u63d0\u4f9b\u5907\u4efd\u8ba1\u5212\uff1b \u4e2a\u4eba\u7406\u89e3: \u4e00\u5806\u63a7\u5236\u5668\uff08\u526f\u672c\u63a7\u5236\u5668\uff0c\u8282\u70b9\u63a7\u5236\u5668\uff0c\u547d\u4ee4\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u670d\u52a1\u5668\u8d26\u53f7\u63a7\u5236\u5668\u7b49\uff09\uff0c\u63a7\u5236\u5668\u505a\u4e3a\u96c6\u7fa4\u5185\u90e8\u7ba1\u7406\u63a7\u5236\u4e2d\u5fc3\uff0c\u8d1f\u8d23\u96c6\u7fa4\u5185\u7684node\u3001pod\u3001\u670d\u52a1\u7aef\u70b9\u3001\u547d\u540d\u7a7a\u95f4\u3001\u670d\u52a1\u5668\u8d26\u53f7\uff0c\u8d44\u6e90\u5b9a\u989d\u7684\u7ba1\u7406\uff0c\u5f53\u67d0\u4e2anode\u610f\u5916\u5b95\u673a\uff0cController-manager\u4f1a\u81ea\u52a8\u53d1\u73b0\u5e76\u6267\u884c\u81ea\u52a8\u6062\u590d\u6d41\u7a0b\u3002\u786e\u4fdd\u96c6\u7fa4\u4e2dpod\u7684\u526f\u672c\u59cb\u7ec8\u4fdd\u6301\u9884\u671f\u5de5\u4f5c\u7684\u72b6\u6001\u3002","title":"\u63a7\u5236\u5e73\u9762\u7ec4\u4ef6"},{"location":"k8s/base/k8s-overview/#node","text":"kubelet\uff1a \u5b98\u65b9\u89e3\u91ca: \u8d1f\u8d23Pod\u5bf9\u5e94\u5bb9\u5668\u7684\u521b\u5efa\u3001\u8d77\u505c\u7b49\u4efb\u52a1\uff0c\u540c\u65f6\u4e0eMaster \u8282\u70b9\u5bc6\u5207\u534f\u4f5c\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7ba1\u7406\u7684\u57fa\u672c\u529f\u80fd\u3002 \u4e2a\u4eba\u7406\u89e3: kubelet\u662f\u7ef4\u62a4node\u4e0apod\u7684\u72b6\u6001\uff0c\u5982\u679cpod\u6302\u4e86\uff0c\u4ed6\u4f1a\u5c06\u4e8b\u4ef6\u53d1\u9001\u7ed9api-server\uff0capi-server\u5c06\u4e8b\u4ef6\u5199\u5165etcd\uff0ckube-scheduler\u6216\u8005Controller-manager\u4f1a\u62ff\u5230api-server\u4e0a\u7684\u4e8b\u4ef6\uff0c\u5bf9pod\u8fdb\u884c\u91cd\u5efa\u7b49\u64cd\u4f5c\u3002 kube-proxy\uff1a \u5b98\u65b9\u89e3\u91ca: \u5b98\u7f51\u89e3\u91ca\uff1a\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u7f51\u7edc\u4ee3\u7406\uff0c \u5b9e\u73b0 Kubernetes \u670d\u52a1\uff08Service\uff09 \u6982\u5ff5\u7684\u4e00\u90e8\u5206 \u4e2a\u4eba\u7406\u89e3: \u4e2a\u4eba\u7406\u89e3\uff1a\u7ef4\u62a4\u5f53\u524d\u4e3b\u673a\u7684\u7f51\u7edc\u89c4\u5219","title":"Node \u7ec4\u4ef6"},{"location":"k8s/base/k8s-overview/#_3","text":"helm install mysql presslabs/mysql-operator \\ -f 1-config.yaml \\ -n infra helm install mysql-operator bitpoke/mysql-operator \\ -f 1-config.yaml \\ -n infra","title":"\u5176\u4ed6\u7ec4\u4ef6"},{"location":"k8s/controller/StatefulSet/","text":"","title":"StatefulSet"},{"location":"k8s/controller/deployment/","text":"","title":"Deployment"},{"location":"k8s/controller/rs/","text":"","title":"ReplicaSet"},{"location":"k8s/mysql-operator/1-install-mysql-operator/","text":"Mysql-operator \u5b89\u88c5\u90e8\u7f72 \u00b6 \u73af\u5883\u8981\u6c42 \u5b89\u88c5Helm\u5305\u7ba1\u7406\u5de5\u5177 \u5b89\u88c5rook-ceph\u4f5c\u4e3a\u540e\u7aef\u5b58\u50a8 \u5b89\u88c5operator \u5b89\u88c5mysql\u670d\u52a1 \u5b89\u88c5 \u00b6 Helm\u548crook-ceph\u8fd9\u91cc\u5c31\u4e0d\u8fdb\u884c\u6f14\u793a\u5b89\u88c5\u4e86\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e4b\u524d\u7684\u6587\u7ae0\u6765\u5b89\u88c5 \u5b89\u88c5operator \u00b6 github\u4e0a\u6709\u4e00\u4e2a\u9879\u76ee\u53ef\u4ee5\u5e2e\u6211\u4eec\u5feb\u901f\u7684\u6765\u5b89\u88c5 mysql \u4f18\u52bf: \u5feb\u901f\u90e8\u7f72Mysql\u670d\u52a1 \u89e3\u51b3\u4e86\u76d1\u63a7\u3001\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5907\u4efd\u95ee\u9898 \u901a\u8fc7storageClass\u6765\u89e3\u51b3\u5b58\u50a8\u95ee\u9898 \u5f00\u7bb1\u5373\u7528\u7684\u5907\u4efd\uff08\u8ba1\u5212\u548c\u6309\u9700\uff09\u548c\u65f6\u95f4\u70b9\u6062\u590d \u6dfb\u52a0chart\u5730\u5740 \u00b6 helm repo add bitpoke https://helm-charts.bitpoke.io helm repo update \u5b89\u88c5mysql-operator \u00b6 $ helm install mysql-operator bitpoke/mysql-operator \\ -f 1 -config.yaml \\ -n infra WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config NAME: mysql-operator LAST DEPLOYED: Thu Aug 25 10 :24:53 2022 NAMESPACE: infra STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat <<EOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF \u8fd9\u4e2a1-config.yaml\u6587\u4ef6\u662f\u7ed9operator\u6765\u4f7f\u7528\u7684\uff0c\u8fd9\u91cc\u5b9a\u4e49\u4e86storageClass\u548c\u5bb9\u91cf $ cat 1 -config.yaml orchestrator: persistence: enabled: true storageClass: \"rook-ceph-block\" accessMode: \"ReadWriteOnce\" size: 10Gi \u9a8c\u8bc1\uff1a $ k get pod NAME READY STATUS RESTARTS AGE mysql-operator-0 2 /2 Running 2 ( 6m46s ago ) 6m47s \u8fd9\u65f6\uff0cmysql-operator\u5c31\u5b89\u88c5\u597d\u4e86\uff0c\u7b2c\u4e8c\u6b65\u5c31\u5b89\u88c5Mysql \u5b89\u88c5mysql \u00b6 $ k apply -f 2 -openbayes-mysql.yaml secret/openbayes-db-secret created mysqlcluster.mysql.presslabs.org/openbayes created","title":"Mysql-operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql-operator","text":"\u73af\u5883\u8981\u6c42 \u5b89\u88c5Helm\u5305\u7ba1\u7406\u5de5\u5177 \u5b89\u88c5rook-ceph\u4f5c\u4e3a\u540e\u7aef\u5b58\u50a8 \u5b89\u88c5operator \u5b89\u88c5mysql\u670d\u52a1","title":"Mysql-operator \u5b89\u88c5\u90e8\u7f72"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#_1","text":"Helm\u548crook-ceph\u8fd9\u91cc\u5c31\u4e0d\u8fdb\u884c\u6f14\u793a\u5b89\u88c5\u4e86\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e4b\u524d\u7684\u6587\u7ae0\u6765\u5b89\u88c5","title":"\u5b89\u88c5"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#operator","text":"github\u4e0a\u6709\u4e00\u4e2a\u9879\u76ee\u53ef\u4ee5\u5e2e\u6211\u4eec\u5feb\u901f\u7684\u6765\u5b89\u88c5 mysql \u4f18\u52bf: \u5feb\u901f\u90e8\u7f72Mysql\u670d\u52a1 \u89e3\u51b3\u4e86\u76d1\u63a7\u3001\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5907\u4efd\u95ee\u9898 \u901a\u8fc7storageClass\u6765\u89e3\u51b3\u5b58\u50a8\u95ee\u9898 \u5f00\u7bb1\u5373\u7528\u7684\u5907\u4efd\uff08\u8ba1\u5212\u548c\u6309\u9700\uff09\u548c\u65f6\u95f4\u70b9\u6062\u590d","title":"\u5b89\u88c5operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#chart","text":"helm repo add bitpoke https://helm-charts.bitpoke.io helm repo update","title":"\u6dfb\u52a0chart\u5730\u5740"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql-operator_1","text":"$ helm install mysql-operator bitpoke/mysql-operator \\ -f 1 -config.yaml \\ -n infra WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config NAME: mysql-operator LAST DEPLOYED: Thu Aug 25 10 :24:53 2022 NAMESPACE: infra STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat <<EOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF \u8fd9\u4e2a1-config.yaml\u6587\u4ef6\u662f\u7ed9operator\u6765\u4f7f\u7528\u7684\uff0c\u8fd9\u91cc\u5b9a\u4e49\u4e86storageClass\u548c\u5bb9\u91cf $ cat 1 -config.yaml orchestrator: persistence: enabled: true storageClass: \"rook-ceph-block\" accessMode: \"ReadWriteOnce\" size: 10Gi \u9a8c\u8bc1\uff1a $ k get pod NAME READY STATUS RESTARTS AGE mysql-operator-0 2 /2 Running 2 ( 6m46s ago ) 6m47s \u8fd9\u65f6\uff0cmysql-operator\u5c31\u5b89\u88c5\u597d\u4e86\uff0c\u7b2c\u4e8c\u6b65\u5c31\u5b89\u88c5Mysql","title":"\u5b89\u88c5mysql-operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql","text":"$ k apply -f 2 -openbayes-mysql.yaml secret/openbayes-db-secret created mysqlcluster.mysql.presslabs.org/openbayes created","title":"\u5b89\u88c5mysql"},{"location":"k8s/yaml/yaml/","text":"\u8d44\u6e90\u6e05\u5355 \u00b6 YAML \u662f \"YAML Ain't a Markup Language\"\uff08YAML \u4e0d\u662f\u4e00\u79cd\u6807\u8bb0\u8bed\u8a00\uff09\u7684\u9012\u5f52\u7f29\u5199\u3002AML \u7684\u8bed\u6cd5\u548c\u5176\u4ed6\u9ad8\u7ea7\u8bed\u8a00\u7c7b\u4f3c\uff0c\u5e76\u4e14\u53ef\u4ee5\u7b80\u5355\u8868\u8fbe\u6e05\u5355\u3001\u6563\u5217\u8868\uff0c\u6807\u91cf\u7b49\u6570\u636e\u5f62\u6001\u3002\u5b83\u4f7f\u7528\u7a7a\u767d\u7b26\u53f7\u7f29\u8fdb\u548c\u5927\u91cf\u4f9d\u8d56\u5916\u89c2\u7684\u7279\u8272\uff0c\u7279\u522b\u9002\u5408\u7528\u6765\u8868\u8fbe\u6216\u7f16\u8f91\u6570\u636e\u7ed3\u6784\u3001\u5404\u79cd\u914d\u7f6e\u6587\u4ef6\u3001\u503e\u5370\u8c03\u8bd5\u5185\u5bb9\u3001\u6587\u4ef6\u5927\u7eb2\uff08\u4f8b\u5982\uff1a\u8bb8\u591a\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u683c\u5f0f\u548cYAML\u975e\u5e38\u63a5\u8fd1\uff09\u3002 YAML \u7684\u914d\u7f6e\u6587\u4ef6\u540e\u7f00\u4e3a .yml\uff0c\u5982\uff1arunoob.yml \u3002 \u57fa\u672c\u8bed\u6cd5\uff1a \u5927\u5c0f\u5199\u654f\u611f \u4f7f\u7528\u7f29\u8fdb\u8868\u793a\u5c42\u7ea7\u5173\u7cfb \u7f29\u8fdb\u4e0d\u5141\u8bb8\u4f7f\u7528tab\uff0c\u53ea\u5141\u8bb8\u7a7a\u683c \u7f29\u8fdb\u7684\u7a7a\u683c\u6570\u4e0d\u91cd\u8981\uff0c\u53ea\u8981\u76f8\u540c\u5c42\u7ea7\u7684\u5143\u7d20\u5de6\u5bf9\u9f50\u5373\u53ef '#'\u8868\u793a\u6ce8\u91ca \u8d44\u6e90\u6e05\u5355\u89e3\u8bfb \u00b6 apiVersion : v1 #\u7248\u672c\u53f7\uff0c\u4f8b\u5982v1 kind : Pod #\u8d44\u6e90\u7c7b\u578b\uff0c\u5982Pod metadata : #\u5143\u6570\u636e name : string # Pod\u540d\u5b57 namespace : string # Pod\u6240\u5c5e\u7684\u547d\u540d\u7a7a\u95f4 labels : #\u81ea\u5b9a\u4e49\u6807\u7b7e - name : string #\u81ea\u5b9a\u4e49\u6807\u7b7e\u540d\u5b57 annotations : #\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5217\u8868 - name : string spec : # Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 containers : # Pod\u4e2d\u5bb9\u5668\u5217\u8868 - name : string #\u5bb9\u5668\u540d\u79f0 image : string #\u5bb9\u5668\u7684\u955c\u50cf\u540d\u79f0 imagePullPolicy : [ Always | Never | IfNotPresent ] #\u83b7\u53d6\u955c\u50cf\u7684\u7b56\u7565 Alawys\u8868\u793a\u4e0b\u8f7d\u955c\u50cf IfnotPresent\u8868\u793a\u4f18\u5148\u4f7f\u7528\u672c\u5730\u955c\u50cf\uff0c\u5426\u5219\u4e0b\u8f7d\u955c\u50cf\uff0cNerver\u8868\u793a\u4ec5\u4f7f\u7528\u672c\u5730\u955c\u50cf command : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u5217\u8868\uff0c\u5982\u4e0d\u6307\u5b9a\uff0c\u4f7f\u7528\u6253\u5305\u65f6\u4f7f\u7528\u7684\u542f\u52a8\u547d\u4ee4 args : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u53c2\u6570\u5217\u8868 workingDir : string #\u5bb9\u5668\u7684\u5de5\u4f5c\u76ee\u5f55 volumeMounts : #\u6302\u8f7d\u5230\u5bb9\u5668\u5185\u90e8\u7684\u5b58\u50a8\u5377\u914d\u7f6e - name : string #\u5f15\u7528pod\u5b9a\u4e49\u7684\u5171\u4eab\u5b58\u50a8\u5377\u7684\u540d\u79f0\uff0c\u9700\u7528volumes[]\u90e8\u5206\u5b9a\u4e49\u7684\u7684\u5377\u540d mountPath : string #\u5b58\u50a8\u5377\u5728\u5bb9\u5668\u5185mount\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u5e94\u5c11\u4e8e512\u5b57\u7b26 readOnly : boolean #\u662f\u5426\u4e3a\u53ea\u8bfb\u6a21\u5f0f ports : # \u9700\u8981\u66b4\u9732\u7684\u7aef\u53e3\u5e93\u53f7 - name : string # \u7aef\u53e3\u53f7\u540d\u79f0 containerPort : int #\u5bb9\u5668\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7 hostPort : int #\u5bb9\u5668\u6240\u5728\u4e3b\u673a\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7\uff0c\u9ed8\u8ba4\u4e0eContainer\u76f8\u540c protocol : string #\u7aef\u53e3\u534f\u8bae\uff0c\u652f\u6301TCP\u548cUDP\uff0c\u9ed8\u8ba4TCP env : #\u5bb9\u5668\u8fd0\u884c\u524d\u9700\u8bbe\u7f6e\u7684\u73af\u5883\u53d8\u91cf\u5217\u8868 - name : string #\u73af\u5883\u53d8\u91cf\u540d\u79f0 value : string #\u73af\u5883\u53d8\u91cf\u7684\u503c resources : #\u8d44\u6e90\u9650\u5236\u548c\u8bf7\u6c42\u7684\u8bbe\u7f6e limits : #\u8d44\u6e90\u9650\u5236\u7684\u8bbe\u7f6e cpu : string #cpu\u7684\u9650\u5236\uff0c\u5355\u4f4d\u4e3acore\u6570 memory : string #\u5185\u5b58\u9650\u5236\uff0c\u5355\u4f4d\u53ef\u4ee5\u4e3aMib/Gib requests : #\u8d44\u6e90\u8bf7\u6c42\u7684\u8bbe\u7f6e cpu : string #cpu\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u6570\u91cf memory : string #\u5185\u5b58\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u5185\u5b58 livenessProbe : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u7684\u8bbe\u7f6e\uff0c\u5f53\u63a2\u6d4b\u65e0\u54cd\u5e94\u51e0\u6b21\u540e\u5c06\u81ea\u52a8\u91cd\u542f\u8be5\u5bb9\u5668\uff0c\u68c0\u67e5\u65b9\u6cd5\u6709exec\u3001httpGet\u548ctcpSocket\uff0c\u5bf9\u4e00\u4e2a\u5bb9\u5668\u53ea\u9700\u8bbe\u7f6e\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5373\u53ef exec : #\u5bf9Pod\u5bb9\u5668\u5185\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3aexec\u65b9\u5f0f command : [ string ] #exec\u65b9\u5f0f\u9700\u8981\u5236\u5b9a\u7684\u547d\u4ee4\u6216\u811a\u672c httpGet : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u6cd5\u8bbe\u7f6e\u4e3aHttpGet\uff0c\u9700\u8981\u5236\u5b9aPath\u3001port path : string port : number host : string scheme : string HttpHeaders : - name : string value : string tcpSocket : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3atcpSocket\u65b9\u5f0f port : number initialDelaySeconds : 0 #\u5bb9\u5668\u542f\u52a8\u5b8c\u6210\u540e\u9996\u6b21\u63a2\u6d4b\u7684\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2 timeoutSeconds : 0 #\u5bf9\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u63a2\u6d4b\u7b49\u5f85\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba41\u79d2 periodSeconds : 0 #\u5bf9\u5bb9\u5668\u76d1\u63a7\u68c0\u67e5\u7684\u5b9a\u671f\u63a2\u6d4b\u65f6\u95f4\u8bbe\u7f6e\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba410\u79d2\u4e00\u6b21 successThreshold : 0 failureThreshold : 0 securityContext : privileged:false restartPolicy : [ Always | Never | OnFailure ] #Pod\u7684\u91cd\u542f\u7b56\u7565\uff0cAlways\u8868\u793a\u4e00\u65e6\u4e0d\u7ba1\u4ee5\u4f55\u79cd\u65b9\u5f0f\u7ec8\u6b62\u8fd0\u884c\uff0ckubelet\u90fd\u5c06\u91cd\u542f\uff0cOnFailure\u8868\u793a\u53ea\u6709Pod\u4ee5\u975e0\u9000\u51fa\u7801\u9000\u51fa\u624d\u91cd\u542f\uff0cNerver\u8868\u793a\u4e0d\u518d\u91cd\u542f\u8be5Pod nodeSelector : obeject #\u8bbe\u7f6eNodeSelector\u8868\u793a\u5c06\u8be5Pod\u8c03\u5ea6\u5230\u5305\u542b\u8fd9\u4e2alabel\u7684node\u4e0a\uff0c\u4ee5key\uff1avalue\u7684\u683c\u5f0f\u6307\u5b9a imagePullSecrets : #Pull\u955c\u50cf\u65f6\u4f7f\u7528\u7684secret\u540d\u79f0\uff0c\u4ee5key\uff1asecretkey\u683c\u5f0f\u6307\u5b9a - name : string hostNetwork:false #\u662f\u5426\u4f7f\u7528\u4e3b\u673a\u7f51\u7edc\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3afalse\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3atrue\uff0c\u8868\u793a\u4f7f\u7528\u5bbf\u4e3b\u673a\u7f51\u7edc volumes : #\u5728\u8be5pod\u4e0a\u5b9a\u4e49\u5171\u4eab\u5b58\u50a8\u5377\u5217\u8868 - name : string #\u5171\u4eab\u5b58\u50a8\u5377\u540d\u79f0 \uff08volumes\u7c7b\u578b\u6709\u5f88\u591a\u79cd\uff09 emptyDir : {} #\u7c7b\u578b\u4e3aemtyDir\u7684\u5b58\u50a8\u5377\uff0c\u4e0ePod\u540c\u751f\u547d\u5468\u671f\u7684\u4e00\u4e2a\u4e34\u65f6\u76ee\u5f55\u3002\u4e3a\u7a7a\u503c hostPath : string #\u7c7b\u578b\u4e3ahostPath\u7684\u5b58\u50a8\u5377\uff0c\u8868\u793a\u6302\u8f7dPod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55 path : string #Pod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55\uff0c\u5c06\u88ab\u7528\u4e8e\u540c\u671f\u4e2dmount\u7684\u76ee\u5f55 secret : #\u7c7b\u578b\u4e3asecret\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u96c6\u7fa4\u4e0e\u5b9a\u4e49\u7684secre\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 scretname : string items : - key : string path : string configMap : #\u7c7b\u578b\u4e3aconfigMap\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u9884\u5b9a\u4e49\u7684configMap\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 name : string items : - key : string path : string API\u548ckubernetes\u7684\u5bf9\u5e94\u5173\u7cfb rancher\u751f\u4ea7\u6848\u4f8bYaml \u00b6 ####\u542f\u52a8docker#### eval PROJECT_ID='$'$k8s_group if [ $ms_group == 'center' ] then replicas=1 else replicas=1 fi /opt/rancher/rancher context switch $PROJECT_ID cat <<EOF | /opt/rancher/rancher kubectl apply -f - apiVersion : apps/v1 # \u8868\u793aapi\u7248\u672c\uff0cv1 kind : Deployment # kind\u8868\u793a\u8d44\u6e90\u7c7b\u578b\uff0c\u8fd9\u91cc\u662fDeployment metadata : # \u5143\u6570\u636e labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name name : $server_name # \u670d\u52a1\u540d namespace : $name_space # \u547d\u540d\u7a7a\u95f4 spec : replicas : $replicas Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 selector : matchLabels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name template : metadata : labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name spec : imagePullSecrets : - name : old-harbor # \u955c\u50cf \u4ed3\u5e93\u540d restartPolicy : Always containers : - image : $harbor_addr/$name_space/$image_name:$image_tag # \u5bb9\u5668\u7684\u955c\u50cf\u540d imagePullPolicy : Always readinessProbe : failureThreshold : 60 initialDelaySeconds : 5 periodSeconds : 5 successThreshold : 1 tcpSocket : port : $NodePort # \u7aef\u53e3\u66b4\u9732\u65b9\u5f0f timeoutSeconds : 1 livenessProbe : failureThreshold : 3 initialDelaySeconds : 300 periodSeconds : 2 successThreshold : 1 tcpSocket : port : $NodePort timeoutSeconds : 1 env : - name : CSProjFile value : $csproj_file - name : FOR_GODS_SAKE_PLEASE_REDEPLOY value : \"`date +%s`\" name : $server_name ports : - containerPort : $NodePort resources : requests : memory : $memory args : [ \"bash\" , \"-c\" , \"dotnet /opt/$csproj_file/$csproj_file.dll --serviceName $server_name \\ --webApiServiceAddress http://0.0.0.0:$NodePort --zkConfigServer $zk_configserver \\ --zkAppRole $zk_approle --runScope $run_scope --msGroup $ms_group \\ --KPversion 2 --psapp v2 --ser protobuf \\ --zkTimeOut 15000 --mcTimeOut 30000 --Cors *.xxxxx.net \\ --trace kafka --webApiHelp off $other_parameters\" ] # localtime volumeMounts : - mountPath : /etc/localtime name : localtime readOnly : true volumes : - hostPath : path : /etc/localtime type : \"\" name : localtime --- apiVersion : v1 kind : Service metadata : name : $server_name namespace : $name_space spec : type : NodePort ports : - name : default nodePort : $NodePort port : $NodePort protocol : TCP targetPort : $NodePort selector : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name EOF jenkins\u4f20\u6570 \u00b6 \u8fd9\u91ccJenkins\u4f5c\u4e3a\u4e0a\u7ea7\u9879\u76ee\uff0c\u5b9a\u4e49\u53d8\u91cf\u4e3a\u4e0b\u7ea7\u9879\u76ee\u4f20\u9012\u53c2\u6570 image_name = accountwebapiserver name_space = webapi server_name = account csproj_file = AccountWebAPIServer zk_approle = Common-AccountWebApi Controller = account run_scope = Core991 ms_group = $ms_group zk_configserver = w1.confandsa.zk.group.hex.com:2181,w2.confandsa.zk.group.hex.com:2181,w3.confandsa.zk.group.hex.com:2181 image_tag = $image_tag memory = 1Gi maxmemory = 2 .2Gi NodePort = 32037 k8s_group = $k8s_group other_parameters = --UrlPrefix tms-zw4 \u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u5bb9\u5668\u5316\u793a\u4f8b\uff1a \u00b6 $ cat nginx-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-yyds namespace : web spec : selector : matchLabels : app : nginx replicas : 2 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 \u8fd0\u884cYaml\u6587\u4ef6 k apply -f nginx-deploy.yaml \u67e5\u770bpod\u7684\u72b6\u6001 $ k get pod NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 47m nginx-yyds-585449566-vzwkw 1 /1 Running 0 47m \u5e38\u7528\u7684\u7ba1\u7406\u547d\u4ee4\uff1a \u00b6 \u67e5\u770b\u63a7\u5236\u5668 $ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-yyds 2 /2 2 2 51m \u67e5\u770bpod\u7684\u8be6\u7ec6\u4fe1\u606f $ k get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-yyds-585449566-76hgr 1 /1 Running 0 53m 10 .42.2.130 node1 <none> <none> nginx-yyds-585449566-vzwkw 1 /1 Running 0 53m 10 .42.1.127 node0 <none> <none \u901a\u8fc7\u6807\u7b7e\u6765\u67e5\u627epod $ k get pod -l app = nginx NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 56m nginx-yyds-585449566-vzwkw 1 /1 Running 0 56m $ k describe pod nginx-yyds-585449566-76hgr Name: nginx-yyds-585449566-76hgr Namespace: web Priority: 0 Node: node1/192.168.0.151 Start Time: Tue, 23 Aug 2022 15 :46:16 +0800 Labels: app = nginx pod-template-hash = 585449566 Annotations: <none> Status: Running IP: 10 .42.2.130 IPs: IP: 10 .42.2.130 Controlled By: ReplicaSet/nginx-yyds-585449566 Containers: nginx: Container ID: docker://7994248f600aa93444272eff8092938c866a5648db1126545d28635d41251b51 Image: nginx:latest Image ID: docker-pullable://nginx@sha256:dc29f133a33a1d6311807f3b88134000ce67318a40517b1060b929b84b0bbea0 Port: 80 /TCP Host Port: 0 /TCP State: Running Started: Tue, 23 Aug 2022 15 :48:42 +0800 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rf7j8 ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-rf7j8: Type: Projected ( a volume that contains injected data from multiple sources ) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op = Exists for 300s node.kubernetes.io/unreachable:NoExecute op = Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 58m default-scheduler Successfully assigned web/nginx-yyds-585449566-76hgr to node1 Normal Pulling 58m kubelet Pulling image \"nginx:latest\" Normal Pulled 55m kubelet Successfully pulled image \"nginx:latest\" in 2m24.557367751s Normal Created 55m kubelet Created container nginx Normal Started 55m kubelet Started container nginx \u901a\u8fc7explain\u6765\u67e5\u770bYaml\u6587\u4ef6\u5199\u6cd5 $ k explain","title":"\u8d44\u6e90\u6e05\u5355"},{"location":"k8s/yaml/yaml/#_1","text":"YAML \u662f \"YAML Ain't a Markup Language\"\uff08YAML \u4e0d\u662f\u4e00\u79cd\u6807\u8bb0\u8bed\u8a00\uff09\u7684\u9012\u5f52\u7f29\u5199\u3002AML \u7684\u8bed\u6cd5\u548c\u5176\u4ed6\u9ad8\u7ea7\u8bed\u8a00\u7c7b\u4f3c\uff0c\u5e76\u4e14\u53ef\u4ee5\u7b80\u5355\u8868\u8fbe\u6e05\u5355\u3001\u6563\u5217\u8868\uff0c\u6807\u91cf\u7b49\u6570\u636e\u5f62\u6001\u3002\u5b83\u4f7f\u7528\u7a7a\u767d\u7b26\u53f7\u7f29\u8fdb\u548c\u5927\u91cf\u4f9d\u8d56\u5916\u89c2\u7684\u7279\u8272\uff0c\u7279\u522b\u9002\u5408\u7528\u6765\u8868\u8fbe\u6216\u7f16\u8f91\u6570\u636e\u7ed3\u6784\u3001\u5404\u79cd\u914d\u7f6e\u6587\u4ef6\u3001\u503e\u5370\u8c03\u8bd5\u5185\u5bb9\u3001\u6587\u4ef6\u5927\u7eb2\uff08\u4f8b\u5982\uff1a\u8bb8\u591a\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u683c\u5f0f\u548cYAML\u975e\u5e38\u63a5\u8fd1\uff09\u3002 YAML \u7684\u914d\u7f6e\u6587\u4ef6\u540e\u7f00\u4e3a .yml\uff0c\u5982\uff1arunoob.yml \u3002 \u57fa\u672c\u8bed\u6cd5\uff1a \u5927\u5c0f\u5199\u654f\u611f \u4f7f\u7528\u7f29\u8fdb\u8868\u793a\u5c42\u7ea7\u5173\u7cfb \u7f29\u8fdb\u4e0d\u5141\u8bb8\u4f7f\u7528tab\uff0c\u53ea\u5141\u8bb8\u7a7a\u683c \u7f29\u8fdb\u7684\u7a7a\u683c\u6570\u4e0d\u91cd\u8981\uff0c\u53ea\u8981\u76f8\u540c\u5c42\u7ea7\u7684\u5143\u7d20\u5de6\u5bf9\u9f50\u5373\u53ef '#'\u8868\u793a\u6ce8\u91ca","title":"\u8d44\u6e90\u6e05\u5355"},{"location":"k8s/yaml/yaml/#_2","text":"apiVersion : v1 #\u7248\u672c\u53f7\uff0c\u4f8b\u5982v1 kind : Pod #\u8d44\u6e90\u7c7b\u578b\uff0c\u5982Pod metadata : #\u5143\u6570\u636e name : string # Pod\u540d\u5b57 namespace : string # Pod\u6240\u5c5e\u7684\u547d\u540d\u7a7a\u95f4 labels : #\u81ea\u5b9a\u4e49\u6807\u7b7e - name : string #\u81ea\u5b9a\u4e49\u6807\u7b7e\u540d\u5b57 annotations : #\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5217\u8868 - name : string spec : # Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 containers : # Pod\u4e2d\u5bb9\u5668\u5217\u8868 - name : string #\u5bb9\u5668\u540d\u79f0 image : string #\u5bb9\u5668\u7684\u955c\u50cf\u540d\u79f0 imagePullPolicy : [ Always | Never | IfNotPresent ] #\u83b7\u53d6\u955c\u50cf\u7684\u7b56\u7565 Alawys\u8868\u793a\u4e0b\u8f7d\u955c\u50cf IfnotPresent\u8868\u793a\u4f18\u5148\u4f7f\u7528\u672c\u5730\u955c\u50cf\uff0c\u5426\u5219\u4e0b\u8f7d\u955c\u50cf\uff0cNerver\u8868\u793a\u4ec5\u4f7f\u7528\u672c\u5730\u955c\u50cf command : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u5217\u8868\uff0c\u5982\u4e0d\u6307\u5b9a\uff0c\u4f7f\u7528\u6253\u5305\u65f6\u4f7f\u7528\u7684\u542f\u52a8\u547d\u4ee4 args : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u53c2\u6570\u5217\u8868 workingDir : string #\u5bb9\u5668\u7684\u5de5\u4f5c\u76ee\u5f55 volumeMounts : #\u6302\u8f7d\u5230\u5bb9\u5668\u5185\u90e8\u7684\u5b58\u50a8\u5377\u914d\u7f6e - name : string #\u5f15\u7528pod\u5b9a\u4e49\u7684\u5171\u4eab\u5b58\u50a8\u5377\u7684\u540d\u79f0\uff0c\u9700\u7528volumes[]\u90e8\u5206\u5b9a\u4e49\u7684\u7684\u5377\u540d mountPath : string #\u5b58\u50a8\u5377\u5728\u5bb9\u5668\u5185mount\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u5e94\u5c11\u4e8e512\u5b57\u7b26 readOnly : boolean #\u662f\u5426\u4e3a\u53ea\u8bfb\u6a21\u5f0f ports : # \u9700\u8981\u66b4\u9732\u7684\u7aef\u53e3\u5e93\u53f7 - name : string # \u7aef\u53e3\u53f7\u540d\u79f0 containerPort : int #\u5bb9\u5668\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7 hostPort : int #\u5bb9\u5668\u6240\u5728\u4e3b\u673a\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7\uff0c\u9ed8\u8ba4\u4e0eContainer\u76f8\u540c protocol : string #\u7aef\u53e3\u534f\u8bae\uff0c\u652f\u6301TCP\u548cUDP\uff0c\u9ed8\u8ba4TCP env : #\u5bb9\u5668\u8fd0\u884c\u524d\u9700\u8bbe\u7f6e\u7684\u73af\u5883\u53d8\u91cf\u5217\u8868 - name : string #\u73af\u5883\u53d8\u91cf\u540d\u79f0 value : string #\u73af\u5883\u53d8\u91cf\u7684\u503c resources : #\u8d44\u6e90\u9650\u5236\u548c\u8bf7\u6c42\u7684\u8bbe\u7f6e limits : #\u8d44\u6e90\u9650\u5236\u7684\u8bbe\u7f6e cpu : string #cpu\u7684\u9650\u5236\uff0c\u5355\u4f4d\u4e3acore\u6570 memory : string #\u5185\u5b58\u9650\u5236\uff0c\u5355\u4f4d\u53ef\u4ee5\u4e3aMib/Gib requests : #\u8d44\u6e90\u8bf7\u6c42\u7684\u8bbe\u7f6e cpu : string #cpu\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u6570\u91cf memory : string #\u5185\u5b58\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u5185\u5b58 livenessProbe : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u7684\u8bbe\u7f6e\uff0c\u5f53\u63a2\u6d4b\u65e0\u54cd\u5e94\u51e0\u6b21\u540e\u5c06\u81ea\u52a8\u91cd\u542f\u8be5\u5bb9\u5668\uff0c\u68c0\u67e5\u65b9\u6cd5\u6709exec\u3001httpGet\u548ctcpSocket\uff0c\u5bf9\u4e00\u4e2a\u5bb9\u5668\u53ea\u9700\u8bbe\u7f6e\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5373\u53ef exec : #\u5bf9Pod\u5bb9\u5668\u5185\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3aexec\u65b9\u5f0f command : [ string ] #exec\u65b9\u5f0f\u9700\u8981\u5236\u5b9a\u7684\u547d\u4ee4\u6216\u811a\u672c httpGet : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u6cd5\u8bbe\u7f6e\u4e3aHttpGet\uff0c\u9700\u8981\u5236\u5b9aPath\u3001port path : string port : number host : string scheme : string HttpHeaders : - name : string value : string tcpSocket : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3atcpSocket\u65b9\u5f0f port : number initialDelaySeconds : 0 #\u5bb9\u5668\u542f\u52a8\u5b8c\u6210\u540e\u9996\u6b21\u63a2\u6d4b\u7684\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2 timeoutSeconds : 0 #\u5bf9\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u63a2\u6d4b\u7b49\u5f85\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba41\u79d2 periodSeconds : 0 #\u5bf9\u5bb9\u5668\u76d1\u63a7\u68c0\u67e5\u7684\u5b9a\u671f\u63a2\u6d4b\u65f6\u95f4\u8bbe\u7f6e\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba410\u79d2\u4e00\u6b21 successThreshold : 0 failureThreshold : 0 securityContext : privileged:false restartPolicy : [ Always | Never | OnFailure ] #Pod\u7684\u91cd\u542f\u7b56\u7565\uff0cAlways\u8868\u793a\u4e00\u65e6\u4e0d\u7ba1\u4ee5\u4f55\u79cd\u65b9\u5f0f\u7ec8\u6b62\u8fd0\u884c\uff0ckubelet\u90fd\u5c06\u91cd\u542f\uff0cOnFailure\u8868\u793a\u53ea\u6709Pod\u4ee5\u975e0\u9000\u51fa\u7801\u9000\u51fa\u624d\u91cd\u542f\uff0cNerver\u8868\u793a\u4e0d\u518d\u91cd\u542f\u8be5Pod nodeSelector : obeject #\u8bbe\u7f6eNodeSelector\u8868\u793a\u5c06\u8be5Pod\u8c03\u5ea6\u5230\u5305\u542b\u8fd9\u4e2alabel\u7684node\u4e0a\uff0c\u4ee5key\uff1avalue\u7684\u683c\u5f0f\u6307\u5b9a imagePullSecrets : #Pull\u955c\u50cf\u65f6\u4f7f\u7528\u7684secret\u540d\u79f0\uff0c\u4ee5key\uff1asecretkey\u683c\u5f0f\u6307\u5b9a - name : string hostNetwork:false #\u662f\u5426\u4f7f\u7528\u4e3b\u673a\u7f51\u7edc\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3afalse\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3atrue\uff0c\u8868\u793a\u4f7f\u7528\u5bbf\u4e3b\u673a\u7f51\u7edc volumes : #\u5728\u8be5pod\u4e0a\u5b9a\u4e49\u5171\u4eab\u5b58\u50a8\u5377\u5217\u8868 - name : string #\u5171\u4eab\u5b58\u50a8\u5377\u540d\u79f0 \uff08volumes\u7c7b\u578b\u6709\u5f88\u591a\u79cd\uff09 emptyDir : {} #\u7c7b\u578b\u4e3aemtyDir\u7684\u5b58\u50a8\u5377\uff0c\u4e0ePod\u540c\u751f\u547d\u5468\u671f\u7684\u4e00\u4e2a\u4e34\u65f6\u76ee\u5f55\u3002\u4e3a\u7a7a\u503c hostPath : string #\u7c7b\u578b\u4e3ahostPath\u7684\u5b58\u50a8\u5377\uff0c\u8868\u793a\u6302\u8f7dPod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55 path : string #Pod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55\uff0c\u5c06\u88ab\u7528\u4e8e\u540c\u671f\u4e2dmount\u7684\u76ee\u5f55 secret : #\u7c7b\u578b\u4e3asecret\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u96c6\u7fa4\u4e0e\u5b9a\u4e49\u7684secre\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 scretname : string items : - key : string path : string configMap : #\u7c7b\u578b\u4e3aconfigMap\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u9884\u5b9a\u4e49\u7684configMap\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 name : string items : - key : string path : string API\u548ckubernetes\u7684\u5bf9\u5e94\u5173\u7cfb","title":"\u8d44\u6e90\u6e05\u5355\u89e3\u8bfb"},{"location":"k8s/yaml/yaml/#rancheryaml","text":"####\u542f\u52a8docker#### eval PROJECT_ID='$'$k8s_group if [ $ms_group == 'center' ] then replicas=1 else replicas=1 fi /opt/rancher/rancher context switch $PROJECT_ID cat <<EOF | /opt/rancher/rancher kubectl apply -f - apiVersion : apps/v1 # \u8868\u793aapi\u7248\u672c\uff0cv1 kind : Deployment # kind\u8868\u793a\u8d44\u6e90\u7c7b\u578b\uff0c\u8fd9\u91cc\u662fDeployment metadata : # \u5143\u6570\u636e labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name name : $server_name # \u670d\u52a1\u540d namespace : $name_space # \u547d\u540d\u7a7a\u95f4 spec : replicas : $replicas Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 selector : matchLabels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name template : metadata : labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name spec : imagePullSecrets : - name : old-harbor # \u955c\u50cf \u4ed3\u5e93\u540d restartPolicy : Always containers : - image : $harbor_addr/$name_space/$image_name:$image_tag # \u5bb9\u5668\u7684\u955c\u50cf\u540d imagePullPolicy : Always readinessProbe : failureThreshold : 60 initialDelaySeconds : 5 periodSeconds : 5 successThreshold : 1 tcpSocket : port : $NodePort # \u7aef\u53e3\u66b4\u9732\u65b9\u5f0f timeoutSeconds : 1 livenessProbe : failureThreshold : 3 initialDelaySeconds : 300 periodSeconds : 2 successThreshold : 1 tcpSocket : port : $NodePort timeoutSeconds : 1 env : - name : CSProjFile value : $csproj_file - name : FOR_GODS_SAKE_PLEASE_REDEPLOY value : \"`date +%s`\" name : $server_name ports : - containerPort : $NodePort resources : requests : memory : $memory args : [ \"bash\" , \"-c\" , \"dotnet /opt/$csproj_file/$csproj_file.dll --serviceName $server_name \\ --webApiServiceAddress http://0.0.0.0:$NodePort --zkConfigServer $zk_configserver \\ --zkAppRole $zk_approle --runScope $run_scope --msGroup $ms_group \\ --KPversion 2 --psapp v2 --ser protobuf \\ --zkTimeOut 15000 --mcTimeOut 30000 --Cors *.xxxxx.net \\ --trace kafka --webApiHelp off $other_parameters\" ] # localtime volumeMounts : - mountPath : /etc/localtime name : localtime readOnly : true volumes : - hostPath : path : /etc/localtime type : \"\" name : localtime --- apiVersion : v1 kind : Service metadata : name : $server_name namespace : $name_space spec : type : NodePort ports : - name : default nodePort : $NodePort port : $NodePort protocol : TCP targetPort : $NodePort selector : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name EOF","title":"rancher\u751f\u4ea7\u6848\u4f8bYaml"},{"location":"k8s/yaml/yaml/#jenkins","text":"\u8fd9\u91ccJenkins\u4f5c\u4e3a\u4e0a\u7ea7\u9879\u76ee\uff0c\u5b9a\u4e49\u53d8\u91cf\u4e3a\u4e0b\u7ea7\u9879\u76ee\u4f20\u9012\u53c2\u6570 image_name = accountwebapiserver name_space = webapi server_name = account csproj_file = AccountWebAPIServer zk_approle = Common-AccountWebApi Controller = account run_scope = Core991 ms_group = $ms_group zk_configserver = w1.confandsa.zk.group.hex.com:2181,w2.confandsa.zk.group.hex.com:2181,w3.confandsa.zk.group.hex.com:2181 image_tag = $image_tag memory = 1Gi maxmemory = 2 .2Gi NodePort = 32037 k8s_group = $k8s_group other_parameters = --UrlPrefix tms-zw4","title":"jenkins\u4f20\u6570"},{"location":"k8s/yaml/yaml/#_3","text":"$ cat nginx-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-yyds namespace : web spec : selector : matchLabels : app : nginx replicas : 2 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 \u8fd0\u884cYaml\u6587\u4ef6 k apply -f nginx-deploy.yaml \u67e5\u770bpod\u7684\u72b6\u6001 $ k get pod NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 47m nginx-yyds-585449566-vzwkw 1 /1 Running 0 47m","title":"\u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u5bb9\u5668\u5316\u793a\u4f8b\uff1a"},{"location":"k8s/yaml/yaml/#_4","text":"\u67e5\u770b\u63a7\u5236\u5668 $ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-yyds 2 /2 2 2 51m \u67e5\u770bpod\u7684\u8be6\u7ec6\u4fe1\u606f $ k get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-yyds-585449566-76hgr 1 /1 Running 0 53m 10 .42.2.130 node1 <none> <none> nginx-yyds-585449566-vzwkw 1 /1 Running 0 53m 10 .42.1.127 node0 <none> <none \u901a\u8fc7\u6807\u7b7e\u6765\u67e5\u627epod $ k get pod -l app = nginx NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 56m nginx-yyds-585449566-vzwkw 1 /1 Running 0 56m $ k describe pod nginx-yyds-585449566-76hgr Name: nginx-yyds-585449566-76hgr Namespace: web Priority: 0 Node: node1/192.168.0.151 Start Time: Tue, 23 Aug 2022 15 :46:16 +0800 Labels: app = nginx pod-template-hash = 585449566 Annotations: <none> Status: Running IP: 10 .42.2.130 IPs: IP: 10 .42.2.130 Controlled By: ReplicaSet/nginx-yyds-585449566 Containers: nginx: Container ID: docker://7994248f600aa93444272eff8092938c866a5648db1126545d28635d41251b51 Image: nginx:latest Image ID: docker-pullable://nginx@sha256:dc29f133a33a1d6311807f3b88134000ce67318a40517b1060b929b84b0bbea0 Port: 80 /TCP Host Port: 0 /TCP State: Running Started: Tue, 23 Aug 2022 15 :48:42 +0800 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rf7j8 ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-rf7j8: Type: Projected ( a volume that contains injected data from multiple sources ) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op = Exists for 300s node.kubernetes.io/unreachable:NoExecute op = Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 58m default-scheduler Successfully assigned web/nginx-yyds-585449566-76hgr to node1 Normal Pulling 58m kubelet Pulling image \"nginx:latest\" Normal Pulled 55m kubelet Successfully pulled image \"nginx:latest\" in 2m24.557367751s Normal Created 55m kubelet Created container nginx Normal Started 55m kubelet Started container nginx \u901a\u8fc7explain\u6765\u67e5\u770bYaml\u6587\u4ef6\u5199\u6cd5 $ k explain","title":"\u5e38\u7528\u7684\u7ba1\u7406\u547d\u4ee4\uff1a"},{"location":"log/rsyslog/","text":"","title":"rsyslog\u65e5\u5fd7\u670d\u52a1\u7ba1\u7406"},{"location":"monitor/prometheus/","text":"Prometheus \u76d1\u63a7 \u00b6 Unordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci \u53c2\u8003\u5730\u5740: Github\u5730\u5740: https://github.com/prometheus-community/helm-charts/","title":"prometheus \u76d1\u63a7"},{"location":"monitor/prometheus/#prometheus","text":"Unordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci Ordered list Sed sagittis eleifend rutrum Donec vitae suscipit est Nulla tempor lobortis orci \u53c2\u8003\u5730\u5740: Github\u5730\u5740: https://github.com/prometheus-community/helm-charts/","title":"Prometheus \u76d1\u63a7"},{"location":"other/network/","text":"\u5f02\u5730\u7ec4\u7f51\u65b9\u6848 \u00b6 https://www.zerotier.com/ \u5ba2\u6237\u7aef\u4e0b\u8f7d https://www.zerotier.com/download/","title":"\u5f02\u5730\u7ec4\u7f51\u65b9\u6848"},{"location":"other/network/#_1","text":"https://www.zerotier.com/ \u5ba2\u6237\u7aef\u4e0b\u8f7d https://www.zerotier.com/download/","title":"\u5f02\u5730\u7ec4\u7f51\u65b9\u6848"},{"location":"prometheus/1.%20prometheus/","text":"curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}%","title":"1. prometheus"},{"location":"redis-operator/redis-operator/","text":"kubectl apply -f deploy/crds/redis.kun_distributedredisclusters_crd.yaml kubectl apply -f deploy/crds/redis.kun_redisclusterbackups_crd.yaml k create ns redis-cluster kubectl apply -f deploy/service_account.yaml kubectl apply -f deploy/namespace/role.yaml kubectl apply -f deploy/namespace/role_binding.yaml kubectl apply -f deploy/namespace/operator.yaml helm repo add ucloud-operator https://ucloud.github.io/redis-cluster-operator/ helm repo update helm install --generate-name ucloud-operator/redis-cluster-operator kubectl apply -f deploy/example/redis.kun_v1alpha1_distributedrediscluster_cr.yaml","title":"Redis operator"},{"location":"runtime/k8s-update/","text":"\u4e0a\u6d77\u4ea4\u5927k8s\u5347\u7ea7\uff1a\uff08kubeadm \u5b89\u88c5\uff09 \u00b6 https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ \u901a\u8fc7\u547d\u4ee4\u6211\u4eec\u53d1\u73b0\u6b64\u65f6k8s\u7684\u7248\u672c\u4e3av1.19.16\uff0c\u6211\u6253\u7b97\u4ecev1.19.16\u5347\u7ea7\u5230v1.23.8\uff0c\uff08\u6700\u540e\u4e00\u4e2a\u652f\u6301docker\u7684\u7248\u672c\uff09\uff0c\u7531\u4e8e\u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c\u3002\u6240\u4ee5\u9700\u8981\u901a\u8fc7 v1.19-v1.20-v1.21-v1.22-1.23\u56db\u6b21\u5347\u7ea7\u5347\u5230\u6307\u5b9a\u7248\u672c\u3002 [sjtu] root@master:/home/lixie# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready ingress,master 713d v1.19.16 node1 Ready gpu,storage,user 713d v1.19.16 node2 Ready gpu,storage,user 713d v1.19.16 node3 Ready gpu,storage,user 713d v1.19.16 \u5347\u7ea7kubeadm\uff08\u6240\u6709\u8282\u70b9\uff09 \u00b6 apt update [ucloud] root@master0:~# apt-cache policy kubeadm | grep 1.20. 1.20.15-00 500 # \u6ce8\u610f\u8fd9\u8fb9\u5347\u7ea7\u7684\u8bdd\u9700\u8981\u5347\u5230\u6240\u5c5e\u7248\u672c\u7684\u6700\u9ad8\u5c0f\u7248\u672c apt-get install kubeadm=1.20.15-00 # \u6240\u6709\u8282\u70b9 \u5982\u679c\u8fd9\u8fb9\u8282\u70b9\u8fc7\u591a\u4e5f\u53ef\u4ee5\u4f7f\u7528ansible\u6279\u91cf\u6267\u884c tasks : # - name: apt kubeadm # shell: apt-get install kubeadm=1.23.8-00 \u6267\u884cansible-playbook\uff1a ansible-playbook -i inventory/sjtu sjtu.yaml \u67e5\u770b\u955c\u50cf\u7248\u672c \u00b6 kubeadm config images list --kubernetes-version v1.20.15 \u6240\u6709\u8282\u70b9\u90fd\u51c6\u5907\u597d\u4e4b\u540e\uff0c\u5148\u8fdb\u884c\u5347\u7ea7master\u8282\u70b9 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 kubeadm upgrade apply v1.20.9 \u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u53ef\u4ee5\u6dfb\u52a0 --etcd-upgrade=false \u5347\u7ea7kubelet\uff08\u6240\u6709\u8282\u70b9) \u00b6 \u5347\u7ea7\u5b8c\u6210\u53d1\u73b0\u7248\u672c\u6ca1\u6709\u53d8\u5316\uff0c\u9700\u8981\u5347\u7ea7kubelet apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 node\u8282\u70b9 \u00b6 \u5347\u7ea7kubelet\u7684\u914d\u7f6e \u00b6 sudo kubeadm upgrade node apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 \u6821\u9a8ckubelet\u7248\u672c \u00b6 [ucloud] root@node0:/home/lixie# kubelet --version Kubernetes v1.23.8","title":"\u4e0a\u6d77\u4ea4\u5927k8s\u5347\u7ea7\uff1a\uff08kubeadm \u5b89\u88c5\uff09"},{"location":"runtime/k8s-update/#k8skubeadm","text":"https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ \u901a\u8fc7\u547d\u4ee4\u6211\u4eec\u53d1\u73b0\u6b64\u65f6k8s\u7684\u7248\u672c\u4e3av1.19.16\uff0c\u6211\u6253\u7b97\u4ecev1.19.16\u5347\u7ea7\u5230v1.23.8\uff0c\uff08\u6700\u540e\u4e00\u4e2a\u652f\u6301docker\u7684\u7248\u672c\uff09\uff0c\u7531\u4e8e\u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c\u3002\u6240\u4ee5\u9700\u8981\u901a\u8fc7 v1.19-v1.20-v1.21-v1.22-1.23\u56db\u6b21\u5347\u7ea7\u5347\u5230\u6307\u5b9a\u7248\u672c\u3002 [sjtu] root@master:/home/lixie# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready ingress,master 713d v1.19.16 node1 Ready gpu,storage,user 713d v1.19.16 node2 Ready gpu,storage,user 713d v1.19.16 node3 Ready gpu,storage,user 713d v1.19.16","title":"\u4e0a\u6d77\u4ea4\u5927k8s\u5347\u7ea7\uff1a\uff08kubeadm \u5b89\u88c5\uff09"},{"location":"runtime/k8s-update/#kubeadm","text":"apt update [ucloud] root@master0:~# apt-cache policy kubeadm | grep 1.20. 1.20.15-00 500 # \u6ce8\u610f\u8fd9\u8fb9\u5347\u7ea7\u7684\u8bdd\u9700\u8981\u5347\u5230\u6240\u5c5e\u7248\u672c\u7684\u6700\u9ad8\u5c0f\u7248\u672c apt-get install kubeadm=1.20.15-00 # \u6240\u6709\u8282\u70b9 \u5982\u679c\u8fd9\u8fb9\u8282\u70b9\u8fc7\u591a\u4e5f\u53ef\u4ee5\u4f7f\u7528ansible\u6279\u91cf\u6267\u884c tasks : # - name: apt kubeadm # shell: apt-get install kubeadm=1.23.8-00 \u6267\u884cansible-playbook\uff1a ansible-playbook -i inventory/sjtu sjtu.yaml","title":"\u5347\u7ea7kubeadm\uff08\u6240\u6709\u8282\u70b9\uff09"},{"location":"runtime/k8s-update/#_1","text":"kubeadm config images list --kubernetes-version v1.20.15 \u6240\u6709\u8282\u70b9\u90fd\u51c6\u5907\u597d\u4e4b\u540e\uff0c\u5148\u8fdb\u884c\u5347\u7ea7master\u8282\u70b9 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 kubeadm upgrade apply v1.20.9 \u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u53ef\u4ee5\u6dfb\u52a0 --etcd-upgrade=false","title":"\u67e5\u770b\u955c\u50cf\u7248\u672c"},{"location":"runtime/k8s-update/#kubelet","text":"\u5347\u7ea7\u5b8c\u6210\u53d1\u73b0\u7248\u672c\u6ca1\u6709\u53d8\u5316\uff0c\u9700\u8981\u5347\u7ea7kubelet apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00","title":"\u5347\u7ea7kubelet\uff08\u6240\u6709\u8282\u70b9)"},{"location":"runtime/k8s-update/#node","text":"","title":"node\u8282\u70b9"},{"location":"runtime/k8s-update/#kubelet_1","text":"sudo kubeadm upgrade node apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00","title":"\u5347\u7ea7kubelet\u7684\u914d\u7f6e"},{"location":"runtime/k8s-update/#kubelet_2","text":"[ucloud] root@node0:/home/lixie# kubelet --version Kubernetes v1.23.8","title":"\u6821\u9a8ckubelet\u7248\u672c"},{"location":"runtime/overview/","text":"\u5bb9\u5668\u57fa\u7840 \u00b6 \u5b89\u88c5 \u00b6 \u57fa\u7840\u547d\u4ee4 \u00b6","title":"\u5bb9\u5668\u57fa\u7840"},{"location":"runtime/overview/#_1","text":"","title":"\u5bb9\u5668\u57fa\u7840"},{"location":"runtime/overview/#_2","text":"","title":"\u5b89\u88c5"},{"location":"runtime/overview/#_3","text":"","title":"\u57fa\u7840\u547d\u4ee4"},{"location":"ubuntu/ubuntu-bmc/","text":"\u7269\u7406\u670d\u52a1\u5668\u5e26\u5916\u7ba1\u7406 \u00b6 1.\u8ba4\u8bc6BMC \u00b6 \u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668 \u652f\u6301IPMI\uff08\u667a\u80fd\u5e73\u53f0\u7ba1\u7406\u63a5\u53e3\uff09 \u7528\u6237\u53ef\u4ee5\u5229\u7528IPMI\u6765\u76d1\u89c6\u670d\u52a1\u5668\u7684\u7269\u7406\u5065\u5eb7\u7279\u5f81\uff0c\u5982\u98ce\u6247\uff0c\u7535\u6e90\uff0c\u5185\u5b58\uff0c\u78c1\u76d8\u7b49 \u597d\u5904\uff1a \u5de5\u7a0b\u5e08\u53ef\u4ee5\u8fdc\u7a0b\u5728\u529e\u516c\u5ba4\u4e2d\u5bf9\u670d\u52a1\u5668\u5f00\u673a \u5173\u673a \u91cd\u88c5\u7cfb\u7edf \u67e5\u770b\u786c\u4ef6\u72b6\u6001 \u8fd9\u6837\u5c31\u53ef\u4ee5\u51cf\u5c11\u53bb\u673a\u623f\u7684\u6b21\u6570\uff0c\u6765\u5b8c\u6210\u6211\u4eec\u7684\u6d3b \u5e26\u5916\u7ba1\u7406\u53e3\u89c6\u56fe: \u00b6 2. \u914d\u7f6eBMC\u5e26\u5916\u7ba1\u7406\u5730\u5740 \u00b6 apt-get install ipmitool ipmitool lan print # \u67e5\u770bBMC\u7684\u5730\u5740 ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr 192 .168.2.21 ipmitool lan set 1 netmask 255 .255.255.0 ipmitool lan set 1 defgw ipaddr 192 .168.2.1 \u8fd9\u6837\u8bbf\u95ee\u8fd9\u4e2a\u5730\u5740\u7684443\u7aef\u53e3\uff0c\u5c31\u53ef\u4ee5\u8bbf\u95ee\u5230\u5e26\u5916\u7ba1\u7406\u4e86\u3002","title":"\u670d\u52a1\u5668BMC\u5e26\u5916\u7ba1\u7406"},{"location":"ubuntu/ubuntu-bmc/#_1","text":"","title":"\u7269\u7406\u670d\u52a1\u5668\u5e26\u5916\u7ba1\u7406"},{"location":"ubuntu/ubuntu-bmc/#1bmc","text":"\u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668 \u652f\u6301IPMI\uff08\u667a\u80fd\u5e73\u53f0\u7ba1\u7406\u63a5\u53e3\uff09 \u7528\u6237\u53ef\u4ee5\u5229\u7528IPMI\u6765\u76d1\u89c6\u670d\u52a1\u5668\u7684\u7269\u7406\u5065\u5eb7\u7279\u5f81\uff0c\u5982\u98ce\u6247\uff0c\u7535\u6e90\uff0c\u5185\u5b58\uff0c\u78c1\u76d8\u7b49 \u597d\u5904\uff1a \u5de5\u7a0b\u5e08\u53ef\u4ee5\u8fdc\u7a0b\u5728\u529e\u516c\u5ba4\u4e2d\u5bf9\u670d\u52a1\u5668\u5f00\u673a \u5173\u673a \u91cd\u88c5\u7cfb\u7edf \u67e5\u770b\u786c\u4ef6\u72b6\u6001 \u8fd9\u6837\u5c31\u53ef\u4ee5\u51cf\u5c11\u53bb\u673a\u623f\u7684\u6b21\u6570\uff0c\u6765\u5b8c\u6210\u6211\u4eec\u7684\u6d3b","title":"1.\u8ba4\u8bc6BMC"},{"location":"ubuntu/ubuntu-bmc/#_2","text":"","title":"\u5e26\u5916\u7ba1\u7406\u53e3\u89c6\u56fe:"},{"location":"ubuntu/ubuntu-bmc/#2-bmc","text":"apt-get install ipmitool ipmitool lan print # \u67e5\u770bBMC\u7684\u5730\u5740 ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr 192 .168.2.21 ipmitool lan set 1 netmask 255 .255.255.0 ipmitool lan set 1 defgw ipaddr 192 .168.2.1 \u8fd9\u6837\u8bbf\u95ee\u8fd9\u4e2a\u5730\u5740\u7684443\u7aef\u53e3\uff0c\u5c31\u53ef\u4ee5\u8bbf\u95ee\u5230\u5e26\u5916\u7ba1\u7406\u4e86\u3002","title":"2. \u914d\u7f6eBMC\u5e26\u5916\u7ba1\u7406\u5730\u5740"},{"location":"ubuntu/ubuntu-system/","text":"ubuntu\u7cfb\u7edf\u521d\u59cb\u5316 \u00b6 \u66f4\u6362\u56fd\u5185\u6e90 deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse \u5141\u8bb8root\u767b\u9646 \u76f4\u63a5\u4f7f\u7528root\uff0c\u5728\u521d\u59cb\u5316\u53ef\u4ee5\u4f7f\u7528\uff0c\u4e4b\u540e\u5e94\u8be5\u5173\u95ed sed -i '100aPermitRootLogin yes' /etc/ssh/sshd_config && systemctl restart sshd SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/ \u7cfb\u7edf\u5b89\u5168 \u00b6 \u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002 \u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09 \u00b6 \u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e \u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668 \u00b6 \u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 )","title":"Ubuntu\u64cd\u4f5c\u7cfb\u7edf"},{"location":"ubuntu/ubuntu-system/#ubuntu","text":"\u66f4\u6362\u56fd\u5185\u6e90 deb http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse deb-src http://mirrors.aliyun.com/ubuntu/ focal-backports main restricted universe multiverse \u5141\u8bb8root\u767b\u9646 \u76f4\u63a5\u4f7f\u7528root\uff0c\u5728\u521d\u59cb\u5316\u53ef\u4ee5\u4f7f\u7528\uff0c\u4e4b\u540e\u5e94\u8be5\u5173\u95ed sed -i '100aPermitRootLogin yes' /etc/ssh/sshd_config && systemctl restart sshd SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/","title":"ubuntu\u7cfb\u7edf\u521d\u59cb\u5316"},{"location":"ubuntu/ubuntu-system/#_1","text":"\u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002","title":"\u7cfb\u7edf\u5b89\u5168"},{"location":"ubuntu/ubuntu-system/#root-centosubuntu","text":"\u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e","title":"\u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09"},{"location":"ubuntu/ubuntu-system/#_2","text":"\u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 )","title":"\u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668"},{"location":"vm/promox/","text":"","title":"Promox"}]}