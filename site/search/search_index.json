{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes \u8fdb\u9636\u8bad\u7ec3\u8425 \u00b6 Hello\uff0c\u5927\u5bb6\u597d\uff01 \u6211\u53eb\u963f\u52f0\uff0c\u5317\u6f02\u4e00\u65cf\u3002\u6c5f\u6e56\u4eba\u79f0\u52f0\u54e5\uff08xie\uff09\uff0c\u53ef\u80fd\u5927\u90e8\u5206\u4eba\u90fd\u4e0d\u8ba4\u8bc6\u8fd9\u4e2a\u5b57\uff5e\uff0c\u5728\u5f04\u8fd9\u4e2a\u7f51\u7ad9\u7684\u521d\u8877\u4e00\u65b9\u9762\u4e3a\u5f00\u6e90\u505a\u8d21\u732e\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u4e86\u8bb0\u5f55\u81ea\u5df1\u5de5\u4f5c\u548c\u751f\u6d3b\u7684\u5185\u5bb9\u3002\u4ee5\u540e\u4e5f\u5c06\u6301\u7eed\u66f4\u65b0\uff5e \u4e3b\u8981\u8bb0\u5f55\u5de5\u4f5c\u4ee5\u53ca\u751f\u6d3b\u5185\u5bb9 \u628a\u5de5\u4f5c\u548c\u751f\u6d3b\u7684\u4e8b\u60c5\u5217\u6e05\u695a\uff0c\u4e3a\u4e86\u4ee5\u540e\u505a\u590d\u76d8 \u90ae\u7bb1: lixie0215@163.com \u8054\u7cfb\u65b9\u5f0f: \u6709\u5174\u8da3\u53ef\u4ee5\u52a0\u6211\uff0c\u4e00\u8d77\u8fdb\u8fd0\u7ef4\u4ea4\u6d41\u7fa4\uff0c\u4e00\u8d77\u4ea4\u6d41\u5b66\u4e60\uff5e \u6211\u76842022\u76ee\u6807: \u751f\u6d3b/\u8fdb\u5ea6: \u8df3\u69fd\uff084\u6708\u672b\u5c3e\u5df2\u5b8c\u6210\uff09 \u722c\u957f\u57ce\uff08\u5df2\u5b8c\u6210\uff09 \u4f53\u9a8c\u767e\u5ea6\u65e0\u4eba\u9a7e\u9a76\uff08\u5df2\u5b8c\u6210\uff09 \u7ef4\u62a4\u4e00\u4e2a\u81ea\u5df1\u7684\u535a\u5ba2\u5e73\u53f0\uff08\u8fdb\u884c\u4e2d\uff09 \u6bcf\u5929\u575a\u6301\u5199\u7b14\u8bb0\uff08\u8fdb\u884c\u4e2d\uff09 \u56fd\u5e86\u56de\u5bb6\uff08\u8fdb\u884c\u4e2d\uff09 \u8003\u5b8c\u9a7e\u7167\uff08\u8fdb\u884c\u4e2d\uff09 \u5de5\u4f5c/\u8fdb\u5ea6: \u53ef\u4ee5\u6210\u529f\u8f6c\u6b63 \u5de5\u4f5c\u5b89\u6392\uff1a 8.22--8.26 \u7814\u7a76juicefs\u5b58\u50a8 juicefs\u4e0ecephfs\u6027\u80fd\u5bf9\u6bd4 \u5907\u4efdenflame\u96c6\u7fa4\u7684\u6570\u636e \u73af\u5883\u5347\u7ea7\uff088-29--9.13\uff09 8.29--9.2 \u673a\u5668\u4e0b\u67b6 \u65b0\u673a\u5668\u4e0a\u67b6 \u66f4\u6362sn2700 100g \u8c03\u8bd5Ansible-playbook 9.5--9.9 \u66f4\u65b0\u4e0a\u6d77\u4e34\u6e2f\u96c6\u7fa4\u8bc1\u4e66 \u8c03\u8bd5\u516c\u53f8\u6cf0\u5766\u673a\u5668dns\u95ee\u9898 \u6d4b\u8bd5\u65b0\u589e\u673a\u5668\u4e0eceph\u7684mon\u5176\u4e2d\u4e24\u4e2a\u4e0d\u540c\u95ee\u9898 \u6dfb\u52a0stor3\uff08ansible-playbook\uff09\u5230produce\u96c6\u7fa4\u4e2d \u6dfb\u52a0\u65b0ssd\u56fa\u6001\u78c1\u76d8\u5230T40\uff0c\u7528\u6765\u7ed9docker\u6570\u636e\u76ee\u5f55\u505a\u6570\u636e\u76d8 a100-1 \u6362\u4e24\u6839 100g \u7f51\u7ebf \u7edf\u8ba1\u6b63\u5f0f\u673a\u5668Mac\u5730\u5740\u8868 9.13--9.16 \u8c03\u8bd5\u516c\u53f8\u7684\u7f51\u7edc\u8bbe\u5907 \u6d4b\u8bd5\u65b0\u589e3090\u7f51\u7edc\u662f\u5426\u53ef\u7528\uff08\u5df2\u5b8c\u6210\uff09 \u6dfb\u52a0A100-1\uff08ansible-playbook\uff09\u5230produce\u96c6\u7fa4\u4e2d \u8c03\u8bd5\u65b0\u589e\u673a\u5668lxcfs\uff08ubunt22.04 \u53ea\u5f00\u542fcgroup v2\u5bfc\u81f4\u7684\uff09 \u53bb\u6570\u636e\u4e2d\u5fc3\u5b89\u88c5stor4\u65b0\u7cfb\u7edf\uff0c\u5e76\u67e5\u770bstor3\u7f51\u7ebf\u95ee\u9898 \u6dfb\u52a0ssd osd \u5230produce\u96c6\u7fa4\u4e2d 9.19--9.23 \u6dfb\u52a0ssd osd \u5230produce\u96c6\u7fa4\u4e2d \u6dfb\u52a0\u65b0\u589e\u673a\u5668\u7f51\u5361\u914d\u7f6e 9.26--9.30 \u642d\u5efa\u65b0\u73af\u5883\uff0c\u7ed9\u7814\u53d1\u4eba\u5458\u4f7f\u7528 produce \u73af\u5883\u8c03\u8bd5\u95ee\u9898 ansible-playbook g2 \u91cd\u65b0\u5b89\u88c5a100-1 \u91cd\u65b0\u5b89\u88c5g1 10.8--10.14 \u91cd\u65b0\u5b89\u88c5a100-2 \u5317\u4eac\u529e\u516c\u5ba4office\u91cd\u65b0\u90e8\u7f72 \uff08k8s\u65b9\u5f0f\uff09 \u71e7\u539f\u73af\u5883\u91cd\u65b0\u90e8\u7f72\uff08k3s\u65b9\u5f0f\uff09 \u90e8\u7f72\u5b8c\u6210\uff0c\u603b\u7ed3\u6587\u7ae0\uff0c\u63d0\u4ea4github \uff5e 11.28--12.2 \u5f00\u542f\u8d85\u7ebf\u7a0b\uff0c\u5c06adm\u91cd\u65b0\u52a0\u5165c1\u96c6\u7fa4 \u7ed9dev\u73af\u5883\u589e\u52a0\u4e00\u53f0node\u8282\u70b9 \u89e3\u51b3ucloud\u673a\u5668\u91cd\u542fIP\u4e22\u5931\u7684\u95ee\u9898 traefik2\u6ce8\u89e3\u5bf9traefik1\u7684\u652f\u6301 \u770b\u4e00\u4e9b\u7f51\u7edc\u76f8\u5173\u7684\u6587\u7ae0\uff0c\u53ca\u7f51\u7edc\u63d2\u4ef6\uff0c\u5982caclio \u5e76\u6c47\u603b 12.5--12.9 \u5b66\u4e60 kubernetes \u7f51\u7edc\u7ec4\u4ef6 \u8ddf\u8fdb hpc \u955c\u50cf\u4e8b\u5b9c(\u5b89\u88c5apptainer) \u89e3\u51b3 dev \u73af\u5883\u5e94\u78c1\u76d8\u51fa\u95ee\u9898\u5bfc\u81f4\u96c6\u7fa4\u4e0d\u53ef\u7528\u7684\u95ee\u9898 \u8e22\u51fa osd-6 \u78c1\u76d8 \u6dfb\u52a0\u4e09\u5757\u78c1\u76d8 \u7ed9 dev \u73af\u5883\u6dfb\u52a0 node3 \u603b\u7ed3dev \u73af\u5883\u4fee\u590d\u6587\u6863 \u89e3\u51b3 images-builder \u4ee3\u7801\u95ee\u9898 \u5b66\u4e60python (\u8fdb\u884c\u4e2d) \u8003\u5b8cCKA\u548cCKS\u4e24\u4e2a\u8bc1\u4e66\uff08\u8fdb\u884c\u4e2d\uff09 CKA \u6240\u6709\u8003\u9898\u7ec3\u4e60\u5b8c\u6210","title":"\u9996\u9875"},{"location":"#kubernetes","text":"Hello\uff0c\u5927\u5bb6\u597d\uff01 \u6211\u53eb\u963f\u52f0\uff0c\u5317\u6f02\u4e00\u65cf\u3002\u6c5f\u6e56\u4eba\u79f0\u52f0\u54e5\uff08xie\uff09\uff0c\u53ef\u80fd\u5927\u90e8\u5206\u4eba\u90fd\u4e0d\u8ba4\u8bc6\u8fd9\u4e2a\u5b57\uff5e\uff0c\u5728\u5f04\u8fd9\u4e2a\u7f51\u7ad9\u7684\u521d\u8877\u4e00\u65b9\u9762\u4e3a\u5f00\u6e90\u505a\u8d21\u732e\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u4e86\u8bb0\u5f55\u81ea\u5df1\u5de5\u4f5c\u548c\u751f\u6d3b\u7684\u5185\u5bb9\u3002\u4ee5\u540e\u4e5f\u5c06\u6301\u7eed\u66f4\u65b0\uff5e \u4e3b\u8981\u8bb0\u5f55\u5de5\u4f5c\u4ee5\u53ca\u751f\u6d3b\u5185\u5bb9 \u628a\u5de5\u4f5c\u548c\u751f\u6d3b\u7684\u4e8b\u60c5\u5217\u6e05\u695a\uff0c\u4e3a\u4e86\u4ee5\u540e\u505a\u590d\u76d8 \u90ae\u7bb1: lixie0215@163.com \u8054\u7cfb\u65b9\u5f0f: \u6709\u5174\u8da3\u53ef\u4ee5\u52a0\u6211\uff0c\u4e00\u8d77\u8fdb\u8fd0\u7ef4\u4ea4\u6d41\u7fa4\uff0c\u4e00\u8d77\u4ea4\u6d41\u5b66\u4e60\uff5e \u6211\u76842022\u76ee\u6807: \u751f\u6d3b/\u8fdb\u5ea6: \u8df3\u69fd\uff084\u6708\u672b\u5c3e\u5df2\u5b8c\u6210\uff09 \u722c\u957f\u57ce\uff08\u5df2\u5b8c\u6210\uff09 \u4f53\u9a8c\u767e\u5ea6\u65e0\u4eba\u9a7e\u9a76\uff08\u5df2\u5b8c\u6210\uff09 \u7ef4\u62a4\u4e00\u4e2a\u81ea\u5df1\u7684\u535a\u5ba2\u5e73\u53f0\uff08\u8fdb\u884c\u4e2d\uff09 \u6bcf\u5929\u575a\u6301\u5199\u7b14\u8bb0\uff08\u8fdb\u884c\u4e2d\uff09 \u56fd\u5e86\u56de\u5bb6\uff08\u8fdb\u884c\u4e2d\uff09 \u8003\u5b8c\u9a7e\u7167\uff08\u8fdb\u884c\u4e2d\uff09 \u5de5\u4f5c/\u8fdb\u5ea6: \u53ef\u4ee5\u6210\u529f\u8f6c\u6b63 \u5de5\u4f5c\u5b89\u6392\uff1a 8.22--8.26 \u7814\u7a76juicefs\u5b58\u50a8 juicefs\u4e0ecephfs\u6027\u80fd\u5bf9\u6bd4 \u5907\u4efdenflame\u96c6\u7fa4\u7684\u6570\u636e \u73af\u5883\u5347\u7ea7\uff088-29--9.13\uff09 8.29--9.2 \u673a\u5668\u4e0b\u67b6 \u65b0\u673a\u5668\u4e0a\u67b6 \u66f4\u6362sn2700 100g \u8c03\u8bd5Ansible-playbook 9.5--9.9 \u66f4\u65b0\u4e0a\u6d77\u4e34\u6e2f\u96c6\u7fa4\u8bc1\u4e66 \u8c03\u8bd5\u516c\u53f8\u6cf0\u5766\u673a\u5668dns\u95ee\u9898 \u6d4b\u8bd5\u65b0\u589e\u673a\u5668\u4e0eceph\u7684mon\u5176\u4e2d\u4e24\u4e2a\u4e0d\u540c\u95ee\u9898 \u6dfb\u52a0stor3\uff08ansible-playbook\uff09\u5230produce\u96c6\u7fa4\u4e2d \u6dfb\u52a0\u65b0ssd\u56fa\u6001\u78c1\u76d8\u5230T40\uff0c\u7528\u6765\u7ed9docker\u6570\u636e\u76ee\u5f55\u505a\u6570\u636e\u76d8 a100-1 \u6362\u4e24\u6839 100g \u7f51\u7ebf \u7edf\u8ba1\u6b63\u5f0f\u673a\u5668Mac\u5730\u5740\u8868 9.13--9.16 \u8c03\u8bd5\u516c\u53f8\u7684\u7f51\u7edc\u8bbe\u5907 \u6d4b\u8bd5\u65b0\u589e3090\u7f51\u7edc\u662f\u5426\u53ef\u7528\uff08\u5df2\u5b8c\u6210\uff09 \u6dfb\u52a0A100-1\uff08ansible-playbook\uff09\u5230produce\u96c6\u7fa4\u4e2d \u8c03\u8bd5\u65b0\u589e\u673a\u5668lxcfs\uff08ubunt22.04 \u53ea\u5f00\u542fcgroup v2\u5bfc\u81f4\u7684\uff09 \u53bb\u6570\u636e\u4e2d\u5fc3\u5b89\u88c5stor4\u65b0\u7cfb\u7edf\uff0c\u5e76\u67e5\u770bstor3\u7f51\u7ebf\u95ee\u9898 \u6dfb\u52a0ssd osd \u5230produce\u96c6\u7fa4\u4e2d 9.19--9.23 \u6dfb\u52a0ssd osd \u5230produce\u96c6\u7fa4\u4e2d \u6dfb\u52a0\u65b0\u589e\u673a\u5668\u7f51\u5361\u914d\u7f6e 9.26--9.30 \u642d\u5efa\u65b0\u73af\u5883\uff0c\u7ed9\u7814\u53d1\u4eba\u5458\u4f7f\u7528 produce \u73af\u5883\u8c03\u8bd5\u95ee\u9898 ansible-playbook g2 \u91cd\u65b0\u5b89\u88c5a100-1 \u91cd\u65b0\u5b89\u88c5g1 10.8--10.14 \u91cd\u65b0\u5b89\u88c5a100-2 \u5317\u4eac\u529e\u516c\u5ba4office\u91cd\u65b0\u90e8\u7f72 \uff08k8s\u65b9\u5f0f\uff09 \u71e7\u539f\u73af\u5883\u91cd\u65b0\u90e8\u7f72\uff08k3s\u65b9\u5f0f\uff09 \u90e8\u7f72\u5b8c\u6210\uff0c\u603b\u7ed3\u6587\u7ae0\uff0c\u63d0\u4ea4github \uff5e 11.28--12.2 \u5f00\u542f\u8d85\u7ebf\u7a0b\uff0c\u5c06adm\u91cd\u65b0\u52a0\u5165c1\u96c6\u7fa4 \u7ed9dev\u73af\u5883\u589e\u52a0\u4e00\u53f0node\u8282\u70b9 \u89e3\u51b3ucloud\u673a\u5668\u91cd\u542fIP\u4e22\u5931\u7684\u95ee\u9898 traefik2\u6ce8\u89e3\u5bf9traefik1\u7684\u652f\u6301 \u770b\u4e00\u4e9b\u7f51\u7edc\u76f8\u5173\u7684\u6587\u7ae0\uff0c\u53ca\u7f51\u7edc\u63d2\u4ef6\uff0c\u5982caclio \u5e76\u6c47\u603b 12.5--12.9 \u5b66\u4e60 kubernetes \u7f51\u7edc\u7ec4\u4ef6 \u8ddf\u8fdb hpc \u955c\u50cf\u4e8b\u5b9c(\u5b89\u88c5apptainer) \u89e3\u51b3 dev \u73af\u5883\u5e94\u78c1\u76d8\u51fa\u95ee\u9898\u5bfc\u81f4\u96c6\u7fa4\u4e0d\u53ef\u7528\u7684\u95ee\u9898 \u8e22\u51fa osd-6 \u78c1\u76d8 \u6dfb\u52a0\u4e09\u5757\u78c1\u76d8 \u7ed9 dev \u73af\u5883\u6dfb\u52a0 node3 \u603b\u7ed3dev \u73af\u5883\u4fee\u590d\u6587\u6863 \u89e3\u51b3 images-builder \u4ee3\u7801\u95ee\u9898 \u5b66\u4e60python (\u8fdb\u884c\u4e2d) \u8003\u5b8cCKA\u548cCKS\u4e24\u4e2a\u8bc1\u4e66\uff08\u8fdb\u884c\u4e2d\uff09 CKA \u6240\u6709\u8003\u9898\u7ec3\u4e60\u5b8c\u6210","title":"Kubernetes \u8fdb\u9636\u8bad\u7ec3\u8425"},{"location":"Devops/3-github-docs/","text":"\u7ffb\u5899\u5de5\u5177: \u00b6 \u7ecf\u5e38\u4f7f\u7528google\u6d4f\u89c8\u5668\u7684\u5c0f\u4f19\u4f34\u662f\u4e0d\u662f\u7ecf\u5e38\u60f3\u7ffb\u5899\u627e\u4e00\u4e9b\u5b66\u4e60\u8d44\u6e90\uff0c\u56e0\u4e3a\u8981\u79d1\u5b66\u4e0a\u7f51\uff0c\u4ecb\u7ecd\u4e00\u6b3e\u5de5\u5177\u53ef\u4ee5\u514d\u8d39\u7ffb\u5899\uff0c\u901f\u5ea6\u57281M\u4ee5\u5185\u6ee1\u8db3\u65e5\u5e38\u5076\u800c\u60f3\u7ffb\u5899\u770b\u770b\u7684\u670b\u53cb\u3002 \u5b98\u7f51: \u767d\u9cb8\u52a0\u901f\u5668\uff08\u50bb\u74dc\u5f0f\u5b89\u88c5\uff09 \u4e0b\u8f7d\u5730\u5740: https://www.bjch110.com/?mid=3013 \u4f18\u70b9: \u591a\u5e73\u53f0\u90fd\u53ef\u4ee5\u4f7f\u7528 \u514d\u8d39\u4f7f\u7528\uff08\u767d\u5ad6\u515a\u798f\u5229\uff09 \u4f7f\u7528\u7b80\u5355 \u4e0d\u80fd\u7ffb\u5899\u7684\u670b\u53cb\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u94fe\u63a5\u8bbf\u95ee\u6211\u7684\u4e91\u76d8 \u767d\u9cb8\u52a0\u901f\u5668 : https://www.aliyundrive.com/s/HcZDipB1dzM \u5f53\u7136\u5982\u679c\u4f60\u4f1a\u8ba1\u7b97\u673a\u4e13\u4e1a\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e0b\u8fd9\u4e2a\u9879\u76ee: https://github.com/Alvin9999/new-pac/wiki \u6cb9\u7334\u811a\u672c \u00b6 \u6cb9\u7334\u811a\u672c\u53c8\u53ebTampermonkey\uff0c\u8fd9\u4e2a\u662fGoogle\u4e0a\u7684\u4e00\u4e2a\u6269\u5c55\uff0c\u5982\u679c\u4f60\u4f1a\u7528\u4e00\u5b9a\u7ed9\u4f60\u7684\u5de5\u4f5c\u548c\u751f\u6d3b\u6709\u4e00\u5b9a\u7684\u5347\u534e\u3002 \u9996\u5148\u4f60\u9700\u8981\u5b89\u88c5\u4e00\u4e0b\u8fd9\u4e2a\u6269\u5c55\u5de5\u5177\uff0c\u8fd9\u91cc\u4e0d\u505a\u8fc7\u591a\u4ecb\u7ecd\uff0c\u7f51\u4e0a\u7684\u6559\u7a0b\u4e00\u5927\u5806\u3002\u4e0b\u9762\u8bf4\u4e00\u4e0b\u6211\u7528\u7684\u51e0\u4e2a\u811a\u672c\u3002 \u811a\u672c\u4e00: AC-baidu-\u91cd\u5b9a\u5411\u4f18\u5316\u767e\u5ea6\u641c\u72d7\u8c37\u6b4c\u5fc5\u5e94\u641c\u7d22_favicon_\u53cc\u5217 \u8fd9\u4e2a\u662f\u6ca1\u6709\u5f00\u542f\u8fd9\u4e2a\u811a\u672c\u4f7f\u7528Google: \u8fd9\u4e2a\u662f\u5f00\u542f\u811a\u672c\u4f7f\u7528Google,\u660e\u663e\u53ef\u4ee5\u770b\u5230\u542f\u7528\u811a\u672c\u4e4b\u540e\u4f60\u7684\u663e\u793a\u6210\u4e3a\u4e86\u4e24\u884c\u3002\u4e5f\u66f4\u52a0\u76f4\u89c2\u548c\u7f8e\u89c2\u3002 \u811a\u672c\u4e8c: Clean csdn blog \u6e05\u723d\u9605\u8bfb\u535a\u5ba2 CSDN \u4e00\u822c\u56fd\u5185\u7a0b\u5e8f\u5458\u4f1a\u8fdb\u884c\u4e00\u4e9b\u641c\u7d22\u6587\u7ae0\uff0c\u4f46\u662f\u6700\u8fd1\u51e0\u5e74\u8fd9\u4e2a\u4e0a\u9762\u57fa\u672c\u90fd\u662f\u96f7\u540c\uff0c\u5783\u573e\u6587\u7ae0\u592a\u591a\uff0c\u6536\u8d39\u7b49\u7b49\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u811a\u672c\u6765\u89c4\u907f\u8fd9\u4e9b\u95ee\u9898\uff08\u4e0d\u80fd\u4fdd\u8bc1\u6240\u6709\uff09 \u811a\u672c\u4e09: B\u7ad9\u89c6\u5c4f\u4e0b\u8f7d\u811a\u672c\u6216\u8005\u4e0b\u8f7d\u6269\u5c55","title":"Google \u6d4f\u89c8\u5668\u6269\u5c55"},{"location":"Devops/3-github-docs/#_1","text":"\u7ecf\u5e38\u4f7f\u7528google\u6d4f\u89c8\u5668\u7684\u5c0f\u4f19\u4f34\u662f\u4e0d\u662f\u7ecf\u5e38\u60f3\u7ffb\u5899\u627e\u4e00\u4e9b\u5b66\u4e60\u8d44\u6e90\uff0c\u56e0\u4e3a\u8981\u79d1\u5b66\u4e0a\u7f51\uff0c\u4ecb\u7ecd\u4e00\u6b3e\u5de5\u5177\u53ef\u4ee5\u514d\u8d39\u7ffb\u5899\uff0c\u901f\u5ea6\u57281M\u4ee5\u5185\u6ee1\u8db3\u65e5\u5e38\u5076\u800c\u60f3\u7ffb\u5899\u770b\u770b\u7684\u670b\u53cb\u3002 \u5b98\u7f51: \u767d\u9cb8\u52a0\u901f\u5668\uff08\u50bb\u74dc\u5f0f\u5b89\u88c5\uff09 \u4e0b\u8f7d\u5730\u5740: https://www.bjch110.com/?mid=3013 \u4f18\u70b9: \u591a\u5e73\u53f0\u90fd\u53ef\u4ee5\u4f7f\u7528 \u514d\u8d39\u4f7f\u7528\uff08\u767d\u5ad6\u515a\u798f\u5229\uff09 \u4f7f\u7528\u7b80\u5355 \u4e0d\u80fd\u7ffb\u5899\u7684\u670b\u53cb\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u94fe\u63a5\u8bbf\u95ee\u6211\u7684\u4e91\u76d8 \u767d\u9cb8\u52a0\u901f\u5668 : https://www.aliyundrive.com/s/HcZDipB1dzM \u5f53\u7136\u5982\u679c\u4f60\u4f1a\u8ba1\u7b97\u673a\u4e13\u4e1a\u4e5f\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e0b\u8fd9\u4e2a\u9879\u76ee: https://github.com/Alvin9999/new-pac/wiki","title":"\u7ffb\u5899\u5de5\u5177:"},{"location":"Devops/3-github-docs/#_2","text":"\u6cb9\u7334\u811a\u672c\u53c8\u53ebTampermonkey\uff0c\u8fd9\u4e2a\u662fGoogle\u4e0a\u7684\u4e00\u4e2a\u6269\u5c55\uff0c\u5982\u679c\u4f60\u4f1a\u7528\u4e00\u5b9a\u7ed9\u4f60\u7684\u5de5\u4f5c\u548c\u751f\u6d3b\u6709\u4e00\u5b9a\u7684\u5347\u534e\u3002 \u9996\u5148\u4f60\u9700\u8981\u5b89\u88c5\u4e00\u4e0b\u8fd9\u4e2a\u6269\u5c55\u5de5\u5177\uff0c\u8fd9\u91cc\u4e0d\u505a\u8fc7\u591a\u4ecb\u7ecd\uff0c\u7f51\u4e0a\u7684\u6559\u7a0b\u4e00\u5927\u5806\u3002\u4e0b\u9762\u8bf4\u4e00\u4e0b\u6211\u7528\u7684\u51e0\u4e2a\u811a\u672c\u3002 \u811a\u672c\u4e00: AC-baidu-\u91cd\u5b9a\u5411\u4f18\u5316\u767e\u5ea6\u641c\u72d7\u8c37\u6b4c\u5fc5\u5e94\u641c\u7d22_favicon_\u53cc\u5217 \u8fd9\u4e2a\u662f\u6ca1\u6709\u5f00\u542f\u8fd9\u4e2a\u811a\u672c\u4f7f\u7528Google: \u8fd9\u4e2a\u662f\u5f00\u542f\u811a\u672c\u4f7f\u7528Google,\u660e\u663e\u53ef\u4ee5\u770b\u5230\u542f\u7528\u811a\u672c\u4e4b\u540e\u4f60\u7684\u663e\u793a\u6210\u4e3a\u4e86\u4e24\u884c\u3002\u4e5f\u66f4\u52a0\u76f4\u89c2\u548c\u7f8e\u89c2\u3002 \u811a\u672c\u4e8c: Clean csdn blog \u6e05\u723d\u9605\u8bfb\u535a\u5ba2 CSDN \u4e00\u822c\u56fd\u5185\u7a0b\u5e8f\u5458\u4f1a\u8fdb\u884c\u4e00\u4e9b\u641c\u7d22\u6587\u7ae0\uff0c\u4f46\u662f\u6700\u8fd1\u51e0\u5e74\u8fd9\u4e2a\u4e0a\u9762\u57fa\u672c\u90fd\u662f\u96f7\u540c\uff0c\u5783\u573e\u6587\u7ae0\u592a\u591a\uff0c\u6536\u8d39\u7b49\u7b49\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u811a\u672c\u6765\u89c4\u907f\u8fd9\u4e9b\u95ee\u9898\uff08\u4e0d\u80fd\u4fdd\u8bc1\u6240\u6709\uff09 \u811a\u672c\u4e09: B\u7ad9\u89c6\u5c4f\u4e0b\u8f7d\u811a\u672c\u6216\u8005\u4e0b\u8f7d\u6269\u5c55","title":"\u6cb9\u7334\u811a\u672c"},{"location":"Devops/github/","text":"\u8ba4\u8bc6 Github: \u00b6 Github\u987e\u540d\u601d\u4e49\u662f\u4e00\u4e2aGit\u7248\u672c\u5e93\u7684\u6258\u7ba1\u670d\u52a1\uff0c\u662f\u76ee\u524d\u5168\u7403\u6700\u5927\u7684\u8f6f\u4ef6\u4ed3\u5e93\uff0c\u62e5\u6709\u4e0a\u767e\u4e07\u7684\u5f00\u53d1\u8005\u7528\u6237\uff0c\u4e5f\u662f\u8f6f\u4ef6\u5f00\u53d1\u548c\u5bfb\u627e\u8d44\u6e90\u7684\u6700\u4f73\u9014\u5f84\uff0cGithub\u4e0d\u4ec5\u53ef\u4ee5\u6258\u7ba1\u5404\u79cdGit\u7248\u672c\u4ed3\u5e93\uff0c\u8fd8\u62e5\u6709\u4e86\u66f4\u7f8e\u89c2\u7684Web\u754c\u9762\uff0c\u60a8\u7684\u4ee3\u7801\u6587\u4ef6\u53ef\u4ee5\u88ab\u4efb\u4f55\u4eba\u514b\u9686\uff0c\u4f7f\u5f97\u5f00\u53d1\u8005\u4e3a\u5f00\u6e90\u9879\u8d21\u732e\u4ee3\u7801\u53d8\u5f97\u66f4\u52a0\u5bb9\u6613\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u4ed8\u8d39\u8d2d\u4e70\u79c1\u6709\u5e93\uff0c\u8fd9\u6837\u9ad8\u6027\u4ef7\u6bd4\u7684\u79c1\u6709\u5e93\u771f\u7684\u662f\u5e2e\u52a9\u5230\u4e86\u5f88\u591a\u56e2\u961f\u548c\u4f01\u4e1a\u3002 \u4e00\u3001\u6ce8\u518cGithub\u5e10\u53f7 \u00b6 1.1. \u8fdb\u5165github\u7684\u5b98\u7f51: https://github.com/ 1.2. \u70b9\u51fb\u6ce8\u518c\u8d26\u53f7 \u6e29\u99a8\u63d0\u793a \u7528\u6237\u6635\u79f0\uff0c\u5efa\u8bae\u7528\u6709\u7279\u8272\u81ea\u5df1\u7684\uff0c\u800c\u4e14\u4e0d\u8981\u592a\u957f\uff0c\u8981\u597d\u8bb0\uff0c\u4ee5\u540e\u5bf9\u521b\u5efa\u4ed3\u5e93\u6709\u5f88\u5927\u7684\u5e2e\u52a9\u3002 \u7535\u5b50\u90ae\u7bb1\u5730\u5740\uff0c\u586b\u5199\u4f60\u5e38\u7528\u7684\uff0c\u4e0d\u8981\u4e71\u586b\uff0c\u4e00\u4f1a\u662f\u8981\u53d1\u6fc0\u6d3b\u94fe\u63a5\u5230\u4f60\u90ae\u7bb1\u7684\u3002 \u7528\u6237\u5bc6\u7801\uff0c\u786e\u4fdd\u81f3\u5c1115\u4e2a\u5b57\u7b26\u6216\u81f3\u5c118\u4e2a\u5b57\u7b26\uff08 \u5305\u62ec\u6570\u5b57 \u548c\u5c0f\u5199\u5b57\u6bcd\uff09 \u8f93\u5165\u5b8c\u4f1a\u6709\u4e00\u4e2a\u4eba\u673a\u9a8c\u8bc1\uff0c\u4f60\u53ea\u9700\u628a\u56fe\u7247\u77eb\u6b63\u5373\u53ef\u3002 1.3. \u9009\u62e9\u4e2a\u4eba\u7248 1.4. \u9009\u62e9\u5174\u8da3 1.5.\u9a8c\u8bc1\u90ae\u7bb1 \u5230\u8fd9\u91cc\u57fa\u672c\u5c31\u5b8c\u6210\u4e86\u5bf9github\u5e10\u53f7\u7684\u6ce8\u518c\u3002 \u4e8c\u3001\u521b\u5efa\u4ed3\u5e93 \u00b6 2.1. \u65b0\u5efa\u4e00\u4e2a\u516c\u5171\u4ed3\u5e93 2.2. \u521d\u59cb\u5316\u4ed3\u5e93 \u4e09\u3001\u63d0\u4ea4\u4ee3\u7801 \u00b6 Github \u63d0\u4ea4 pr \u4ee3\u7801commit\u89c4\u7ea6: https://www.conventionalcommits.org/zh-hans/v1.0.0/ git checkout -b lixie //\u5207\u6362\u5230lixie\u5206\u652f git add . git status // \u67e5\u770b\u6682\u5b58\u533a\u63d0\u4ea4\u7684\u5185\u5bb9 git commit -m \"feat(ssh): add lixie for sjtu\" git branch git remote origin git branch //\u67e5\u770b\u5206\u652f git push origin lixie //\u5c06\u4ee3\u7801\u4e0a\u4f20\u8fdc\u7a0blixie\u5206\u652f git rebase origin/master git push origin lixie git push origin lixie -f \u5f3a\u5236\u63d0\u4ea4\u5230lixie\u5206\u533a \u56db\u3001\u914d\u7f6e\u516c\u94a5 \u00b6 4.1. \u751f\u6210\u5bc6\u94a5\u5bf9 root@a100-1:/home/lixie# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:a92LUGI9FIkRl7wUlOVKqh3Xtsigo2XkG8iau0JKHUE root@a100-1 The key's randomart image is: +---[RSA 3072]----+ | .E o*=*. | | . ..*o | | . .o.. | | . =.o | | . . . S * o | | o o + = X = . | |+ o O = + o | |o o + = . . . | | .=o. . . . | +----[SHA256]-----+ // \u5bb6\u76ee\u5f55\u4e0b\u5c31\u4f1a\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@a100-1:~# ls .ssh/ authorized_keys id_rsa id_rsa.pub 4.2. \u5c06\u516c\u94a5\u4e0a\u4f20\u5230github \u9700\u8981\u7ed9\u516c\u94a5\u8d77\u4e00\u4e2a\u540d\u79f0\uff0c\u590d\u5236\u81ea\u5df1\u521a\u521a\u521b\u5efa\u7684\u516c\u94a5 \u4e0a\u4f20\u5b8c\u6210\u4e4b\u540e\uff0c\u5c31\u4e0d\u9700\u8981\u63d0\u4ea4\u7684\u65f6\u5019\u518d\u8f93\u5165\u5bc6\u7801\u9a8c\u8bc1\u4e86\u3002windows\u4e5f\u7c7b\u4f3c\u53ef\u4ee5\u81ea\u884c\u767e\u5ea6\u3002 windows \u4f7f\u7528git \u7f51\u7ad9\u5730\u5740: https://git-for-windows.github.io \u4e0b\u8f7d\u5730\u5740: https://github.com/git-for-windows/git/releases/download/v2.22.0.windows.1/Git-2.22.0-64-bit.exe","title":"Github"},{"location":"Devops/github/#github","text":"Github\u987e\u540d\u601d\u4e49\u662f\u4e00\u4e2aGit\u7248\u672c\u5e93\u7684\u6258\u7ba1\u670d\u52a1\uff0c\u662f\u76ee\u524d\u5168\u7403\u6700\u5927\u7684\u8f6f\u4ef6\u4ed3\u5e93\uff0c\u62e5\u6709\u4e0a\u767e\u4e07\u7684\u5f00\u53d1\u8005\u7528\u6237\uff0c\u4e5f\u662f\u8f6f\u4ef6\u5f00\u53d1\u548c\u5bfb\u627e\u8d44\u6e90\u7684\u6700\u4f73\u9014\u5f84\uff0cGithub\u4e0d\u4ec5\u53ef\u4ee5\u6258\u7ba1\u5404\u79cdGit\u7248\u672c\u4ed3\u5e93\uff0c\u8fd8\u62e5\u6709\u4e86\u66f4\u7f8e\u89c2\u7684Web\u754c\u9762\uff0c\u60a8\u7684\u4ee3\u7801\u6587\u4ef6\u53ef\u4ee5\u88ab\u4efb\u4f55\u4eba\u514b\u9686\uff0c\u4f7f\u5f97\u5f00\u53d1\u8005\u4e3a\u5f00\u6e90\u9879\u8d21\u732e\u4ee3\u7801\u53d8\u5f97\u66f4\u52a0\u5bb9\u6613\uff0c\u5f53\u7136\u4e5f\u53ef\u4ee5\u4ed8\u8d39\u8d2d\u4e70\u79c1\u6709\u5e93\uff0c\u8fd9\u6837\u9ad8\u6027\u4ef7\u6bd4\u7684\u79c1\u6709\u5e93\u771f\u7684\u662f\u5e2e\u52a9\u5230\u4e86\u5f88\u591a\u56e2\u961f\u548c\u4f01\u4e1a\u3002","title":"\u8ba4\u8bc6 Github:"},{"location":"Devops/github/#github_1","text":"1.1. \u8fdb\u5165github\u7684\u5b98\u7f51: https://github.com/ 1.2. \u70b9\u51fb\u6ce8\u518c\u8d26\u53f7 \u6e29\u99a8\u63d0\u793a \u7528\u6237\u6635\u79f0\uff0c\u5efa\u8bae\u7528\u6709\u7279\u8272\u81ea\u5df1\u7684\uff0c\u800c\u4e14\u4e0d\u8981\u592a\u957f\uff0c\u8981\u597d\u8bb0\uff0c\u4ee5\u540e\u5bf9\u521b\u5efa\u4ed3\u5e93\u6709\u5f88\u5927\u7684\u5e2e\u52a9\u3002 \u7535\u5b50\u90ae\u7bb1\u5730\u5740\uff0c\u586b\u5199\u4f60\u5e38\u7528\u7684\uff0c\u4e0d\u8981\u4e71\u586b\uff0c\u4e00\u4f1a\u662f\u8981\u53d1\u6fc0\u6d3b\u94fe\u63a5\u5230\u4f60\u90ae\u7bb1\u7684\u3002 \u7528\u6237\u5bc6\u7801\uff0c\u786e\u4fdd\u81f3\u5c1115\u4e2a\u5b57\u7b26\u6216\u81f3\u5c118\u4e2a\u5b57\u7b26\uff08 \u5305\u62ec\u6570\u5b57 \u548c\u5c0f\u5199\u5b57\u6bcd\uff09 \u8f93\u5165\u5b8c\u4f1a\u6709\u4e00\u4e2a\u4eba\u673a\u9a8c\u8bc1\uff0c\u4f60\u53ea\u9700\u628a\u56fe\u7247\u77eb\u6b63\u5373\u53ef\u3002 1.3. \u9009\u62e9\u4e2a\u4eba\u7248 1.4. \u9009\u62e9\u5174\u8da3 1.5.\u9a8c\u8bc1\u90ae\u7bb1 \u5230\u8fd9\u91cc\u57fa\u672c\u5c31\u5b8c\u6210\u4e86\u5bf9github\u5e10\u53f7\u7684\u6ce8\u518c\u3002","title":"\u4e00\u3001\u6ce8\u518cGithub\u5e10\u53f7"},{"location":"Devops/github/#_1","text":"2.1. \u65b0\u5efa\u4e00\u4e2a\u516c\u5171\u4ed3\u5e93 2.2. \u521d\u59cb\u5316\u4ed3\u5e93","title":"\u4e8c\u3001\u521b\u5efa\u4ed3\u5e93"},{"location":"Devops/github/#_2","text":"Github \u63d0\u4ea4 pr \u4ee3\u7801commit\u89c4\u7ea6: https://www.conventionalcommits.org/zh-hans/v1.0.0/ git checkout -b lixie //\u5207\u6362\u5230lixie\u5206\u652f git add . git status // \u67e5\u770b\u6682\u5b58\u533a\u63d0\u4ea4\u7684\u5185\u5bb9 git commit -m \"feat(ssh): add lixie for sjtu\" git branch git remote origin git branch //\u67e5\u770b\u5206\u652f git push origin lixie //\u5c06\u4ee3\u7801\u4e0a\u4f20\u8fdc\u7a0blixie\u5206\u652f git rebase origin/master git push origin lixie git push origin lixie -f \u5f3a\u5236\u63d0\u4ea4\u5230lixie\u5206\u533a","title":"\u4e09\u3001\u63d0\u4ea4\u4ee3\u7801"},{"location":"Devops/github/#_3","text":"4.1. \u751f\u6210\u5bc6\u94a5\u5bf9 root@a100-1:/home/lixie# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:a92LUGI9FIkRl7wUlOVKqh3Xtsigo2XkG8iau0JKHUE root@a100-1 The key's randomart image is: +---[RSA 3072]----+ | .E o*=*. | | . ..*o | | . .o.. | | . =.o | | . . . S * o | | o o + = X = . | |+ o O = + o | |o o + = . . . | | .=o. . . . | +----[SHA256]-----+ // \u5bb6\u76ee\u5f55\u4e0b\u5c31\u4f1a\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@a100-1:~# ls .ssh/ authorized_keys id_rsa id_rsa.pub 4.2. \u5c06\u516c\u94a5\u4e0a\u4f20\u5230github \u9700\u8981\u7ed9\u516c\u94a5\u8d77\u4e00\u4e2a\u540d\u79f0\uff0c\u590d\u5236\u81ea\u5df1\u521a\u521a\u521b\u5efa\u7684\u516c\u94a5 \u4e0a\u4f20\u5b8c\u6210\u4e4b\u540e\uff0c\u5c31\u4e0d\u9700\u8981\u63d0\u4ea4\u7684\u65f6\u5019\u518d\u8f93\u5165\u5bc6\u7801\u9a8c\u8bc1\u4e86\u3002windows\u4e5f\u7c7b\u4f3c\u53ef\u4ee5\u81ea\u884c\u767e\u5ea6\u3002 windows \u4f7f\u7528git \u7f51\u7ad9\u5730\u5740: https://git-for-windows.github.io \u4e0b\u8f7d\u5730\u5740: https://github.com/git-for-windows/git/releases/download/v2.22.0.windows.1/Git-2.22.0-64-bit.exe","title":"\u56db\u3001\u914d\u7f6e\u516c\u94a5"},{"location":"Helm/helm-install/","text":"Helm \u00b6 Helm \u7b80\u4ecb \u00b6 Helm \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u7ba1\u7406 Kubernetes \u5e94\u7528\u7a0b\u5e8f - Helm Charts \u53ef\u4ee5\u5b9a\u4e49\u3001\u5b89\u88c5\u548c\u5347\u7ea7\u590d\u6742\u7684 Kubernetes \u5e94\u7528\u7a0b\u5e8f\uff0cCharts \u5305\u5f88\u5bb9\u6613\u521b\u5efa\u3001\u7248\u672c\u7ba1\u7406\u3001\u5206\u4eab\u548c\u5206\u5e03. \u7b80\u5355\u53ef\u4ee5\u7406\u89e3Linux\u4e2dyum\u7684\u7684\u611f\u89c9 Helm \u5b89\u88c5 \u00b6 \u83b7\u53d6\u8f6f\u4ef6 \u5b98\u7f51\u5730\u5740\uff1ahttps://github.com/helm/helm/releases \u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u540e\uff0c\u5c06 helm \u4e8c\u8fdb\u5236\u5305\u6587\u4ef6\u79fb\u52a8\u5230\u4efb\u610f\u7684 PATH \u8def\u5f84\u4e0b $ helm version version.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.18.2\"} Linux \u4e0b\u5b89\u88c5 root@k8s-master:/opt# wget https://get.helm.sh/helm-v3.8.1-linux-amd64.tar.gz cni containerd helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt# tar -xf helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt/linux-amd64# mv helm /usr/bin/ \u7ba1\u7406\u914d\u7f6e \u00b6 Chart\u56fd\u5185\u4ed3\u5e93\u914d\u7f6e \u00b6 \u4ed3\u5e93\u914d\u7f6e \u5fae\u8f6f\u7684\u6e90\uff1ahttp://mirror.azure.cn/kubernetes/charts/ \u963f\u91cc\u7684\u6e90\uff1ahttps://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \u5b98\u65b9\u7684\u6e90\uff1ahttps://hub.kubeapps.com/charts/incubator \u6dfb\u52a0chart\u5b58\u50a8\u5e93 \u00b6 helm repo add stable http://mirror.azure.cn/kubernetes/charts helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts root@k8s-master:/opt/linux-amd64# helm repo update # \u66f4\u65b0\u6e90 \u67e5\u770b\u5b58\u50a8\u5e93 \u00b6 $ helm repo list NAME URL stable http://mirror.azure.cn/kubernetes/charts/ openbayes https://dev.openbayes.com/charts \u5220\u9664\u5b58\u50a8\u5e93 \u00b6 root@k8s-master:/opt/linux-amd64# helm repo remove aliyun Helm \u90e8\u7f72\u5e94\u7528: \u00b6 Helm \u90e8\u7f72traefix \u00b6 \u90e8\u7f72traefik \u6587\u7ae0\u53c2\u8003: https://github.com/traefik/traefik-helm-chart git clone https://github.com/traefik/traefik-helm-chart.git \u5b9a\u4e49values\u6587\u4ef6 $ cat sjtu-traefik.yaml image : name : traefik tag : \"2.7\" # values-prod.yaml # Create an IngressRoute for the dashboard ingressRoute : dashboard : enabled : false # \u7981\u7528helm\u4e2d\u6e32\u67d3\u7684dashboard\uff0c\u6211\u4eec\u81ea\u5df1\u624b\u52a8\u521b\u5efa # Configure ports ports : web : port : 8000 hostPort : 80 # \u4f7f\u7528 hostport \u6a21\u5f0f # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure : port : 8443 hostPort : 443 # \u4f7f\u7528 hostport \u6a21\u5f0f # Options for the main traefik service, where the entrypoints traffic comes # from. service : # \u4f7f\u7528 hostport \u6a21\u5f0f\u5c31\u4e0d\u9700\u8981Service\u4e86 enabled : false # Logs # https://docs.traefik.io/observability/logs/ #logs: # general: # level: DEBUG tolerations : # kubeadm \u5b89\u88c5\u7684\u96c6\u7fa4\u9ed8\u8ba4\u60c5\u51b5\u4e0bmaster\u662f\u6709\u6c61\u70b9\uff0c\u9700\u8981\u5bb9\u5fcd\u8fd9\u4e2a\u6c61\u70b9\u624d\u53ef\u4ee5\u90e8\u7f72 - key : \"node-role.kubernetes.io/master\" operator : \"Equal\" effect : \"NoSchedule\" nodeSelector : # \u56fa\u5b9a\u5230master1\u8282\u70b9\uff08\u8be5\u8282\u70b9\u624d\u53ef\u4ee5\u8bbf\u95ee\u5916\u7f51\uff09 kubernetes.io/hostname : \"master\" \u90e8\u7f72traefik helm upgrade --install traefik traefik/traefik -f ./traefik/values/sjtu-traefik.yaml --namespace kube-system \u67e5\u770b\u8fd9\u6b21\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u662f\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\u7684\uff0c-A \u53ef\u4ee5\u67e5\u770b\u6240\u6709helm release $ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik kube-system 5 2022-07-08 19:08:15.393244 +0800 CST deployed traefik-10.24.0 2.8.0 Helm \u5347\u7ea7\u56de\u6eda \u00b6 \u5347\u7ea7 \u00b6 \u4e3e\u4f8b\u8bf4\u660e \u4f8b\u5982\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6765\u5347\u7ea7grafana\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7--set \u5728\u540e\u9762\u4f20\u53c2\u6570 helm upgrade --install grafana grafana/grafana \u6ce8\u610f\u4e8b\u9879 \u5728\u5347\u7ea7\u5e94\u7528\u7a0b\u5e8f\u4e4b\u524d\u53ef\u4ee5\u901a\u8fc7diff\u7684\u65b9\u5f0f\u6765\u67e5\u770b\u4e24\u4e2a\u7248\u672c\u7684\u533a\u522b\uff0c\u786e\u5b9a\u6ca1\u95ee\u9898\uff0c\u518d\u5347\u7ea7 $ helm diff upgrade --install grafana grafana/grafana -f ./grafana.yaml -n infra \u56de\u6eda \u00b6 helm rollback version-id \u65e0\u8bba\u662f\u5347\u7ea7\u8fd8\u662f\u56de\u6eda\u90fd\u662f\u6709\u4e00\u4e2a\u7684\u98ce\u9669\u7684\uff0c\u6ce8\u610f\u9632\u6b62\u8bef\u64cd\u4f5c\u3002","title":"Helm \u7b80\u4ecb\u4e0e\u5b89\u88c5"},{"location":"Helm/helm-install/#helm","text":"","title":"Helm"},{"location":"Helm/helm-install/#helm_1","text":"Helm \u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u7ba1\u7406 Kubernetes \u5e94\u7528\u7a0b\u5e8f - Helm Charts \u53ef\u4ee5\u5b9a\u4e49\u3001\u5b89\u88c5\u548c\u5347\u7ea7\u590d\u6742\u7684 Kubernetes \u5e94\u7528\u7a0b\u5e8f\uff0cCharts \u5305\u5f88\u5bb9\u6613\u521b\u5efa\u3001\u7248\u672c\u7ba1\u7406\u3001\u5206\u4eab\u548c\u5206\u5e03. \u7b80\u5355\u53ef\u4ee5\u7406\u89e3Linux\u4e2dyum\u7684\u7684\u611f\u89c9","title":"Helm \u7b80\u4ecb"},{"location":"Helm/helm-install/#helm_2","text":"\u83b7\u53d6\u8f6f\u4ef6 \u5b98\u7f51\u5730\u5740\uff1ahttps://github.com/helm/helm/releases \u4e0b\u8f7d\u5230\u672c\u5730\u89e3\u538b\u540e\uff0c\u5c06 helm \u4e8c\u8fdb\u5236\u5305\u6587\u4ef6\u79fb\u52a8\u5230\u4efb\u610f\u7684 PATH \u8def\u5f84\u4e0b $ helm version version.BuildInfo{Version:\"v3.9.0\", GitCommit:\"7ceeda6c585217a19a1131663d8cd1f7d641b2a7\", GitTreeState:\"clean\", GoVersion:\"go1.18.2\"} Linux \u4e0b\u5b89\u88c5 root@k8s-master:/opt# wget https://get.helm.sh/helm-v3.8.1-linux-amd64.tar.gz cni containerd helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt# tar -xf helm-v3.8.1-linux-amd64.tar.gz root@k8s-master:/opt/linux-amd64# mv helm /usr/bin/","title":"Helm \u5b89\u88c5"},{"location":"Helm/helm-install/#_1","text":"","title":"\u7ba1\u7406\u914d\u7f6e"},{"location":"Helm/helm-install/#chart","text":"\u4ed3\u5e93\u914d\u7f6e \u5fae\u8f6f\u7684\u6e90\uff1ahttp://mirror.azure.cn/kubernetes/charts/ \u963f\u91cc\u7684\u6e90\uff1ahttps://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \u5b98\u65b9\u7684\u6e90\uff1ahttps://hub.kubeapps.com/charts/incubator","title":"Chart\u56fd\u5185\u4ed3\u5e93\u914d\u7f6e"},{"location":"Helm/helm-install/#chart_1","text":"helm repo add stable http://mirror.azure.cn/kubernetes/charts helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts root@k8s-master:/opt/linux-amd64# helm repo update # \u66f4\u65b0\u6e90","title":"\u6dfb\u52a0chart\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#_2","text":"$ helm repo list NAME URL stable http://mirror.azure.cn/kubernetes/charts/ openbayes https://dev.openbayes.com/charts","title":"\u67e5\u770b\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#_3","text":"root@k8s-master:/opt/linux-amd64# helm repo remove aliyun","title":"\u5220\u9664\u5b58\u50a8\u5e93"},{"location":"Helm/helm-install/#helm_3","text":"","title":"Helm \u90e8\u7f72\u5e94\u7528:"},{"location":"Helm/helm-install/#helm-traefix","text":"\u90e8\u7f72traefik \u6587\u7ae0\u53c2\u8003: https://github.com/traefik/traefik-helm-chart git clone https://github.com/traefik/traefik-helm-chart.git \u5b9a\u4e49values\u6587\u4ef6 $ cat sjtu-traefik.yaml image : name : traefik tag : \"2.7\" # values-prod.yaml # Create an IngressRoute for the dashboard ingressRoute : dashboard : enabled : false # \u7981\u7528helm\u4e2d\u6e32\u67d3\u7684dashboard\uff0c\u6211\u4eec\u81ea\u5df1\u624b\u52a8\u521b\u5efa # Configure ports ports : web : port : 8000 hostPort : 80 # \u4f7f\u7528 hostport \u6a21\u5f0f # Use nodeport if set. This is useful if you have configured Traefik in a # LoadBalancer # nodePort: 32080 # Port Redirections # Added in 2.2, you can make permanent redirects via entrypoints. # https://docs.traefik.io/routing/entrypoints/#redirection # redirectTo: websecure websecure : port : 8443 hostPort : 443 # \u4f7f\u7528 hostport \u6a21\u5f0f # Options for the main traefik service, where the entrypoints traffic comes # from. service : # \u4f7f\u7528 hostport \u6a21\u5f0f\u5c31\u4e0d\u9700\u8981Service\u4e86 enabled : false # Logs # https://docs.traefik.io/observability/logs/ #logs: # general: # level: DEBUG tolerations : # kubeadm \u5b89\u88c5\u7684\u96c6\u7fa4\u9ed8\u8ba4\u60c5\u51b5\u4e0bmaster\u662f\u6709\u6c61\u70b9\uff0c\u9700\u8981\u5bb9\u5fcd\u8fd9\u4e2a\u6c61\u70b9\u624d\u53ef\u4ee5\u90e8\u7f72 - key : \"node-role.kubernetes.io/master\" operator : \"Equal\" effect : \"NoSchedule\" nodeSelector : # \u56fa\u5b9a\u5230master1\u8282\u70b9\uff08\u8be5\u8282\u70b9\u624d\u53ef\u4ee5\u8bbf\u95ee\u5916\u7f51\uff09 kubernetes.io/hostname : \"master\" \u90e8\u7f72traefik helm upgrade --install traefik traefik/traefik -f ./traefik/values/sjtu-traefik.yaml --namespace kube-system \u67e5\u770b\u8fd9\u6b21\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u662f\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236\u7684\uff0c-A \u53ef\u4ee5\u67e5\u770b\u6240\u6709helm release $ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION traefik kube-system 5 2022-07-08 19:08:15.393244 +0800 CST deployed traefik-10.24.0 2.8.0","title":"Helm \u90e8\u7f72traefix"},{"location":"Helm/helm-install/#helm_4","text":"","title":"Helm \u5347\u7ea7\u56de\u6eda"},{"location":"Helm/helm-install/#_4","text":"\u4e3e\u4f8b\u8bf4\u660e \u4f8b\u5982\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6765\u5347\u7ea7grafana\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7--set \u5728\u540e\u9762\u4f20\u53c2\u6570 helm upgrade --install grafana grafana/grafana \u6ce8\u610f\u4e8b\u9879 \u5728\u5347\u7ea7\u5e94\u7528\u7a0b\u5e8f\u4e4b\u524d\u53ef\u4ee5\u901a\u8fc7diff\u7684\u65b9\u5f0f\u6765\u67e5\u770b\u4e24\u4e2a\u7248\u672c\u7684\u533a\u522b\uff0c\u786e\u5b9a\u6ca1\u95ee\u9898\uff0c\u518d\u5347\u7ea7 $ helm diff upgrade --install grafana grafana/grafana -f ./grafana.yaml -n infra","title":"\u5347\u7ea7"},{"location":"Helm/helm-install/#_5","text":"helm rollback version-id \u65e0\u8bba\u662f\u5347\u7ea7\u8fd8\u662f\u56de\u6eda\u90fd\u662f\u6709\u4e00\u4e2a\u7684\u98ce\u9669\u7684\uff0c\u6ce8\u610f\u9632\u6b62\u8bef\u64cd\u4f5c\u3002","title":"\u56de\u6eda"},{"location":"Helm/helm/","text":"Helm \u914d\u7f6egrafana \u00b6 \u83b7\u53d6Token \u00b6 \u53c2\u8003: \u5b98\u7f51 curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}% \u6253\u5f00dashboardProviders \u00b6 \u6ce8\u610f \u6ce8\u610f\u53bb\u6389dashboardProviders\u540e\u9762\u7684{} dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default # \u6dfb\u52a0\u5b98\u7f51dashboard dashboards : default : ceph-cluster : gnetId : 2842 revision : 14 datasource : Prometheus ceph-osd : gnetId : 5336 revision : 5 datasource : Prometheus ceph-pools : gnetId : 5342 revision : 5 datasource : Prometheus token : 'eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ=='","title":"Helm \u914d\u7f6egrafana"},{"location":"Helm/helm/#helm-grafana","text":"","title":"Helm \u914d\u7f6egrafana"},{"location":"Helm/helm/#token","text":"\u53c2\u8003: \u5b98\u7f51 curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}%","title":"\u83b7\u53d6Token"},{"location":"Helm/helm/#dashboardproviders","text":"\u6ce8\u610f \u6ce8\u610f\u53bb\u6389dashboardProviders\u540e\u9762\u7684{} dashboardProviders : dashboardproviders.yaml : apiVersion : 1 providers : - name : 'default' orgId : 1 folder : '' type : file disableDeletion : false editable : true options : path : /var/lib/grafana/dashboards/default # \u6dfb\u52a0\u5b98\u7f51dashboard dashboards : default : ceph-cluster : gnetId : 2842 revision : 14 datasource : Prometheus ceph-osd : gnetId : 5336 revision : 5 datasource : Prometheus ceph-pools : gnetId : 5342 revision : 5 datasource : Prometheus token : 'eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ=='","title":"\u6253\u5f00dashboardProviders"},{"location":"Macuse/mac-easy/","text":"Mac \u73af\u5883\u914d\u7f6e \u00b6 \u80cc\u666f \u00b6 \u5728\u7ef4\u62a4\u548c\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4\u65f6\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u90fd\u6709\u81ea\u5df1\u5bf9\u5e94\u7684config\u6587\u4ef6\uff0c\u90a3\u4e48\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u5728 ~/.kube \u76ee\u5f55\u4e0b\u5c31\u4f1a\u6709\u4e00\u5927\u5806\u5404\u79cd\u73af\u5883\u7684 yaml\uff0c\u5bf9\u4e8e\u7ba1\u7406\u6765\u8bf4\u4e0d\u662f\u7279\u522b\u7684\u53cb\u597d\u3002 \u66f4\u6709\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u96c6\u7fa4\u5207\u6765\u5207\u53bb\uff0c\u9020\u6210\u8fd0\u7ef4\u4e8b\u6545\u3002 kubecm \u7528\u6cd5 \u89e3\u51b3\u7684\u75db\u70b9\u95ee\u9898\uff1a \u7edf\u4e00\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4 \u6765\u56de\u5207\u6362\u6307\u5b9anamespace\u7e41\u7410 \u5f88\u591a\u65f6\u5019\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u54ea\u4e2a\u96c6\u7fa4\u4e0b \u5b89\u88c5 \u00b6 Mac\u5b89\u88c5\u5730\u5740\uff1a kubecm https://formulae.brew.sh/formula/kubecm \u4e0b\u8f7d\u597d\u4e86\uff0c\u5982\u4f55\u4f7f\u7528\uff1f \u4f7f\u7528\u8bf4\u660e\uff1a kubecm github\u5730\u5740 kubecm \u6848\u4f8b \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u6211\u6700\u5e38\u7528\u7684\u5c31\u662f\u6dfb\u52a0\uff0c\u5220\u9664\uff0c\u5207\u6362\u96c6\u7fa4 Manage your kubeconfig more easily. \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 Tips Find more information at: https://kubecm.cloud Usage: kubecm [ command ] Available Commands: add Add KubeConfig to $HOME /.kube/config # \u6dfb\u52a0 alias Generate alias for all contexts clear Clear lapsed context, cluster and user cloud manage kubeconfig from cloud completion Generate completion script create Create new KubeConfig ( experiment ) delete Delete the specified context from the kubeconfig # \u5220\u9664 help Help about any command list List KubeConfig merge Merge multiple kubeconfig files into one namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively # \u5207\u6362 version Print version info Flags: --config string path of kubeconfig ( default \" $HOME /.kube/config\" ) -h, --help help for kubecm --ui-size int number of list items to show in menu at once ( default 4 ) Use \"kubecm [command] --help\" for more information about a command. \u9996\u5148\u6211\u4eec\u5148\u56de\u5230\uff1a\u5bb6\u76ee\u5f55\u4e0b\u7684 .kube \u76ee\u5f55 $ pwd /Users/beiyiwangdejiyi/.kube \u6dfb\u52a0\u4e00\u4e2a\u96c6\u7fa4 \u00b6 kubecm add -f enflame.yaml \u5220\u9664\u4e00\u4e2a\u96c6\u7fa4 \u00b6 $ kubecm delete # \u9009\u62e9\u5220\u9664\u7684\u96c6\u7fa4\uff0c\u9009\u62e9True Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select The Delete Kube Context \ud83d\ude3c ucloud-k3s ( * ) enflame produce \u2193 pve \u5207\u6362\u4e00\u4e2a\u96c6\u7fa4 \u00b6 $ kubecm switch # \u53ef\u4ee5\u4e0a\u4e0b\u5207\u6362\uff0c\u9009\u62e9\u96c6\u7fa4\u73af\u5883 Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select Kube Context ucloud-k3s ( * ) bj \ud83d\ude3c dev \u2193 produce \u4ee5\u4e0a\u5c31\u662fkubecm \u7684\u4e00\u4e9b\u5b89\u88c5\u548c\u4f7f\u7528\u65b9\u6cd5\uff0c\u5176\u4ed6\u7684\u5982\u679c\u611f\u5174\u8da3\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u8bd5\u3002 kubens kubectx \u6587\u7ae0\u5730\u5740\uff1a kubenc kubectx\u5b89\u88c5 \u00b6 \u5982\u679c\u4f60\u4f7f\u7528Homebrew\uff0c\u4f60\u53ef\u4ee5\u50cf\u8fd9\u6837\u5b89\u88c5\uff1a brew install kubectx \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6765\u8fd9\u6837\u4f7f\u7528kubens\u5c31\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u5207\u6362namespace\u3002 fzf\u5b89\u88c5 \u00b6 fzf \u5b89\u88c5 fzf\u5b98\u7f51 \u8fd9\u91cc\u9762\u6bd4\u8f83\u597d\u73a9\u7684\u8fd8\u6709\u4e00\u4e2a\u5e26\u6709\u6a21\u7cca\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u83dc\u5355\uff0c\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u518d\u4f7f\u7528kubens\u5c31\u9999\u7684\u5f88\u554a brew install fzf $( brew --prefix ) /opt/fzf/install \u6e29\u99a8\u63d0\u793a \u5982\u679c\u4e0d\u80fd\u4f7f\u7528\uff0c\u9700\u8981\u5173\u95ed\u7ec8\u7aef\uff0c\u91cd\u65b0\u6253\u5f00 fzf\u9664\u4e86\u8fd9\u4e9b\uff0c\u8fd8\u6709\u5f88\u591a\u7684\u9a9a\u64cd\u4f5c\uff0cshell\u547d\u4ee4\u8865\u5168\uff0c\u53e6\u5916fzf \u91cd\u5199\u4e86 ctrl+r \u641c\u7d22\u5386\u53f2\u547d\u4ee4 word\u6587\u4ef6\u4fee\u590d \u00b6 \u53c2\u8003\u5730\u5740 https://www.51cto.com/article/708448.html","title":"Mac \u5c0f\u529f\u80fd"},{"location":"Macuse/mac-easy/#mac","text":"","title":"Mac \u73af\u5883\u914d\u7f6e"},{"location":"Macuse/mac-easy/#_1","text":"\u5728\u7ef4\u62a4\u548c\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4\u65f6\uff0c\u6bcf\u4e2a\u96c6\u7fa4\u90fd\u6709\u81ea\u5df1\u5bf9\u5e94\u7684config\u6587\u4ef6\uff0c\u90a3\u4e48\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u5728 ~/.kube \u76ee\u5f55\u4e0b\u5c31\u4f1a\u6709\u4e00\u5927\u5806\u5404\u79cd\u73af\u5883\u7684 yaml\uff0c\u5bf9\u4e8e\u7ba1\u7406\u6765\u8bf4\u4e0d\u662f\u7279\u522b\u7684\u53cb\u597d\u3002 \u66f4\u6709\u53ef\u80fd\u5728\u4e0d\u540c\u7684\u96c6\u7fa4\u5207\u6765\u5207\u53bb\uff0c\u9020\u6210\u8fd0\u7ef4\u4e8b\u6545\u3002 kubecm \u7528\u6cd5 \u89e3\u51b3\u7684\u75db\u70b9\u95ee\u9898\uff1a \u7edf\u4e00\u7ba1\u7406\u591a\u4e2ak8s\u96c6\u7fa4 \u6765\u56de\u5207\u6362\u6307\u5b9anamespace\u7e41\u7410 \u5f88\u591a\u65f6\u5019\u4e0d\u77e5\u9053\u81ea\u5df1\u5728\u54ea\u4e2a\u96c6\u7fa4\u4e0b","title":"\u80cc\u666f"},{"location":"Macuse/mac-easy/#_2","text":"Mac\u5b89\u88c5\u5730\u5740\uff1a kubecm https://formulae.brew.sh/formula/kubecm \u4e0b\u8f7d\u597d\u4e86\uff0c\u5982\u4f55\u4f7f\u7528\uff1f \u4f7f\u7528\u8bf4\u660e\uff1a kubecm github\u5730\u5740","title":"\u5b89\u88c5"},{"location":"Macuse/mac-easy/#kubecm","text":"\u53c2\u6570\u8bf4\u660e\uff1a \u6211\u6700\u5e38\u7528\u7684\u5c31\u662f\u6dfb\u52a0\uff0c\u5220\u9664\uff0c\u5207\u6362\u96c6\u7fa4 Manage your kubeconfig more easily. \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588\u2588\u2588\u2588\u2588 \u2588\u2588 \u2588\u2588 Tips Find more information at: https://kubecm.cloud Usage: kubecm [ command ] Available Commands: add Add KubeConfig to $HOME /.kube/config # \u6dfb\u52a0 alias Generate alias for all contexts clear Clear lapsed context, cluster and user cloud manage kubeconfig from cloud completion Generate completion script create Create new KubeConfig ( experiment ) delete Delete the specified context from the kubeconfig # \u5220\u9664 help Help about any command list List KubeConfig merge Merge multiple kubeconfig files into one namespace Switch or change namespace interactively rename Rename the contexts of kubeconfig switch Switch Kube Context interactively # \u5207\u6362 version Print version info Flags: --config string path of kubeconfig ( default \" $HOME /.kube/config\" ) -h, --help help for kubecm --ui-size int number of list items to show in menu at once ( default 4 ) Use \"kubecm [command] --help\" for more information about a command. \u9996\u5148\u6211\u4eec\u5148\u56de\u5230\uff1a\u5bb6\u76ee\u5f55\u4e0b\u7684 .kube \u76ee\u5f55 $ pwd /Users/beiyiwangdejiyi/.kube","title":"kubecm \u6848\u4f8b"},{"location":"Macuse/mac-easy/#_3","text":"kubecm add -f enflame.yaml","title":"\u6dfb\u52a0\u4e00\u4e2a\u96c6\u7fa4"},{"location":"Macuse/mac-easy/#_4","text":"$ kubecm delete # \u9009\u62e9\u5220\u9664\u7684\u96c6\u7fa4\uff0c\u9009\u62e9True Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select The Delete Kube Context \ud83d\ude3c ucloud-k3s ( * ) enflame produce \u2193 pve","title":"\u5220\u9664\u4e00\u4e2a\u96c6\u7fa4"},{"location":"Macuse/mac-easy/#_5","text":"$ kubecm switch # \u53ef\u4ee5\u4e0a\u4e0b\u5207\u6362\uff0c\u9009\u62e9\u96c6\u7fa4\u73af\u5883 Use the arrow keys to navigate: \u2193 \u2191 \u2192 \u2190 and / toggles search Select Kube Context ucloud-k3s ( * ) bj \ud83d\ude3c dev \u2193 produce \u4ee5\u4e0a\u5c31\u662fkubecm \u7684\u4e00\u4e9b\u5b89\u88c5\u548c\u4f7f\u7528\u65b9\u6cd5\uff0c\u5176\u4ed6\u7684\u5982\u679c\u611f\u5174\u8da3\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u8bd5\u3002 kubens kubectx \u6587\u7ae0\u5730\u5740\uff1a kubenc","title":"\u5207\u6362\u4e00\u4e2a\u96c6\u7fa4"},{"location":"Macuse/mac-easy/#kubectx","text":"\u5982\u679c\u4f60\u4f7f\u7528Homebrew\uff0c\u4f60\u53ef\u4ee5\u50cf\u8fd9\u6837\u5b89\u88c5\uff1a brew install kubectx \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6765\u8fd9\u6837\u4f7f\u7528kubens\u5c31\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u5207\u6362namespace\u3002","title":"kubectx\u5b89\u88c5"},{"location":"Macuse/mac-easy/#fzf","text":"fzf \u5b89\u88c5 fzf\u5b98\u7f51 \u8fd9\u91cc\u9762\u6bd4\u8f83\u597d\u73a9\u7684\u8fd8\u6709\u4e00\u4e2a\u5e26\u6709\u6a21\u7cca\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u83dc\u5355\uff0c\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u518d\u4f7f\u7528kubens\u5c31\u9999\u7684\u5f88\u554a brew install fzf $( brew --prefix ) /opt/fzf/install \u6e29\u99a8\u63d0\u793a \u5982\u679c\u4e0d\u80fd\u4f7f\u7528\uff0c\u9700\u8981\u5173\u95ed\u7ec8\u7aef\uff0c\u91cd\u65b0\u6253\u5f00 fzf\u9664\u4e86\u8fd9\u4e9b\uff0c\u8fd8\u6709\u5f88\u591a\u7684\u9a9a\u64cd\u4f5c\uff0cshell\u547d\u4ee4\u8865\u5168\uff0c\u53e6\u5916fzf \u91cd\u5199\u4e86 ctrl+r \u641c\u7d22\u5386\u53f2\u547d\u4ee4","title":"fzf\u5b89\u88c5"},{"location":"Macuse/mac-easy/#word","text":"\u53c2\u8003\u5730\u5740 https://www.51cto.com/article/708448.html","title":"word\u6587\u4ef6\u4fee\u590d"},{"location":"Macuse/mac-usb-system/","text":"Mac \u5236\u4f5cLinux\u7cfb\u7edf\u76d8\uff08\u5176\u4ed6\u53d1\u884c\u7248\u90fd\u53ef\u4ee5\uff09 \u00b6 1.1. Convert ISO file to RAW img file\" $ hdiutil convert -format UDRW -o target.img ~/Downloads/CentOS-7-x86_64-Minimal-2009.iso \u6b63\u5728\u8bfb\u53d6Master Boot Record\uff08MBR\uff1a0\uff09\u2026 \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a1\uff09\u2026 \u6b63\u5728\u8bfb\u53d6\uff08Type EF\uff1a2\uff09\u2026 . \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a3\uff09\u2026 ............................................................................... \u5df2\u8017\u65f6\uff1a 1 .235s \u901f\u5ea6\uff1a787.3MB/\u79d2 \u8282\u7701\uff1a0.0% created: /Users/bougou/target.img.dmg 1.2. Get USB dev id $ diskutil list /dev/disk0 ( internal ) : #: TYPE NAME SIZE IDENTIFIER 0 : GUID_partition_scheme 1 .0 TB disk0 1 : Apple_APFS_ISC 524 .3 MB disk0s1 2 : Apple_APFS Container disk3 994 .7 GB disk0s2 3 : Apple_APFS_Recovery 5 .4 GB disk0s3 /dev/disk3 ( synthesized ) : #: TYPE NAME SIZE IDENTIFIER 0 : APFS Container Scheme - +994.7 GB disk3 Physical Store disk0s2 1 : APFS Volume Macintosh HD 15 .2 GB disk3s1 2 : APFS Snapshot com.apple.os.update-... 15 .2 GB disk3s1s1 3 : APFS Volume Preboot 614 .7 MB disk3s2 4 : APFS Volume Recovery 1 .6 GB disk3s3 5 : APFS Volume Data 120 .9 GB disk3s5 6 : APFS Volume VM 20 .5 KB disk3s6 /dev/disk4 ( external, physical ) : #: TYPE NAME SIZE IDENTIFIER 0 : FDisk_partition_scheme *15.5 GB disk4 1 : 0xEF 9 .0 MB disk4s2 1.3. unmountDisk $ diskutil umountDisk /dev/disk4 Unmount of all volumes on disk4 was successful 1.4. dd and eject USB $ sudo dd if = ~/target.img.dmg of = /dev/disk4 bs = 1m Password: 972 +1 records in 972 +1 records out 1019942912 bytes transferred in 47 .017375 secs ( 21692894 bytes/sec )","title":"Mac \u5236\u4f5cubuntu\u7cfb\u7edf\u76d8"},{"location":"Macuse/mac-usb-system/#mac-linux","text":"1.1. Convert ISO file to RAW img file\" $ hdiutil convert -format UDRW -o target.img ~/Downloads/CentOS-7-x86_64-Minimal-2009.iso \u6b63\u5728\u8bfb\u53d6Master Boot Record\uff08MBR\uff1a0\uff09\u2026 \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a1\uff09\u2026 \u6b63\u5728\u8bfb\u53d6\uff08Type EF\uff1a2\uff09\u2026 . \u6b63\u5728\u8bfb\u53d6CentOS 7 x86_64 \uff08Apple_ISO\uff1a3\uff09\u2026 ............................................................................... \u5df2\u8017\u65f6\uff1a 1 .235s \u901f\u5ea6\uff1a787.3MB/\u79d2 \u8282\u7701\uff1a0.0% created: /Users/bougou/target.img.dmg 1.2. Get USB dev id $ diskutil list /dev/disk0 ( internal ) : #: TYPE NAME SIZE IDENTIFIER 0 : GUID_partition_scheme 1 .0 TB disk0 1 : Apple_APFS_ISC 524 .3 MB disk0s1 2 : Apple_APFS Container disk3 994 .7 GB disk0s2 3 : Apple_APFS_Recovery 5 .4 GB disk0s3 /dev/disk3 ( synthesized ) : #: TYPE NAME SIZE IDENTIFIER 0 : APFS Container Scheme - +994.7 GB disk3 Physical Store disk0s2 1 : APFS Volume Macintosh HD 15 .2 GB disk3s1 2 : APFS Snapshot com.apple.os.update-... 15 .2 GB disk3s1s1 3 : APFS Volume Preboot 614 .7 MB disk3s2 4 : APFS Volume Recovery 1 .6 GB disk3s3 5 : APFS Volume Data 120 .9 GB disk3s5 6 : APFS Volume VM 20 .5 KB disk3s6 /dev/disk4 ( external, physical ) : #: TYPE NAME SIZE IDENTIFIER 0 : FDisk_partition_scheme *15.5 GB disk4 1 : 0xEF 9 .0 MB disk4s2 1.3. unmountDisk $ diskutil umountDisk /dev/disk4 Unmount of all volumes on disk4 was successful 1.4. dd and eject USB $ sudo dd if = ~/target.img.dmg of = /dev/disk4 bs = 1m Password: 972 +1 records in 972 +1 records out 1019942912 bytes transferred in 47 .017375 secs ( 21692894 bytes/sec )","title":"Mac \u5236\u4f5cLinux\u7cfb\u7edf\u76d8\uff08\u5176\u4ed6\u53d1\u884c\u7248\u90fd\u53ef\u4ee5\uff09"},{"location":"Macuse/mac-vscode/","text":"mac \u4f7f\u7528code\u547d\u4ee4\u6253\u5f00VSCode \u00b6 \u89e3\u51b3VSCode\u5feb\u6377\u6307\u4ee4code\u91cd\u542f\u5931\u6548 \u6587\u7ae0\u53c2\u8003: - https://juejin.cn/post/6844903872989757447","title":"Mac vscode\u4f7f\u7528\u6280\u5de7"},{"location":"Macuse/mac-vscode/#mac-codevscode","text":"\u89e3\u51b3VSCode\u5feb\u6377\u6307\u4ee4code\u91cd\u542f\u5931\u6548 \u6587\u7ae0\u53c2\u8003: - https://juejin.cn/post/6844903872989757447","title":"mac \u4f7f\u7528code\u547d\u4ee4\u6253\u5f00VSCode"},{"location":"Storage/5-dev-docs/","text":"\u95ee\u9898\u63cf\u8ff0 \u00b6 dev \u73af\u5883\u56e0\u4e3a\u78c1\u76d8\u51fa\u73b0\u95ee\u9898\u5bfc\u81f4\u5b58\u50a8\u4e0d\u53ef\u7528,\u63d0\u51fa\u4e0d\u53ef\u7528\u78c1\u76d8,\u52a0\u5165\u65b0\u78c1\u76d8\u65e0\u6cd5\u52a0\u5165 \u95ee\u9898\u539f\u56e0 \u00b6 \u78c1\u76d8\u574f\u6389\u5177\u4f53\u4e0d\u6e05\u695a\u539f\u56e0,\u65b0\u52a0\u5165 osd \u65e0\u6cd5\u52a0\u5165 rook-ceph \u4e3b\u8981\u539f\u56e0\u6709\u4ee5\u4e0b\u51e0\u4e2a: \u4f7f\u7528 ceph \u539f\u751f\u65b9\u5f0f\u624b\u52a8\u8e22\u51fa osd,\u6709\u4e9b\u4fe1\u606f\u6b8b\u7559. \u62a5\u9519\u663e\u793a: stderr: Error EEXIST: entity osd.6 exists but key does not match operator \u65e0\u6cd5\u52a0\u5165 osd,\u4f46\u662f\u88f8\u8bbe\u5907\u5df2\u7ecf\u5b58\u5728 ceph \u76f8\u5173\u7684\u4fe1\u606f. \u4ece\u56fe\u53ef\u4ee5\u770b\u5230vdd\u5df2\u7ecf\u5b58\u5728 ceph \u76f8\u5173\u7684\u4fe1\u606f,\u4f46\u662f vgs \u5374\u6ca1\u6709\u4ed6\u7684\u8bb0\u5f55,\u4e5f\u6ca1\u6709 osd \u7684 deployment rook-ceph \u62a5\u7a7a\u95f4\u4e0d\u8db3 pg 25 unknown \u89e3\u51b3\u65b9\u6cd5 \u00b6 \u5c06\u6709\u95ee\u9898\u7684 osd-6 \u8e22\u51fa\u53bb rook-ceph \u96c6\u7fa4\u4e2d. \u65b9\u5f0f\u4e00 : ( \u5229\u7528 rook \u63d0\u4f9b\u7684\u811a\u672c\u6765\u5220\u9664,\u63a8\u8350! ) \u811a\u672c\u5730\u5740: https://github.com/rook/rook/blob/v1.6.11/cluster/examples/kubernetes/ceph/osd-purge.yaml \u65b9\u5f0f\u4e8c : \u4f7f\u7528 ceph \u539f\u751f\u65b9\u5f0f\u5220\u9664 [ root@node-1 ceph ] # ceph osd out osd.6 [ root@node-1 ceph ] # ceph osd purge 6 [ root@node-1 ceph ] # ceph osd tree //\u786e\u8ba4\u662f\u5426\u5df2\u7ecf\u5220\u9664 [ root@node-1 ceph ] # ceph auth del osd.6 //\u6ce8\u610f\u53ef\u80fd\u5c31\u662f\u8fd9\u6b65\u9aa4\u6ca1\u6709\u505a\u4ece\u800c\u5bfc\u81f4\u96c6\u7fa4\u52a0\u4e0d\u8fdb\u53bb\u65b0 osd [ root@node-1 ceph ] # kubectl delete deployments.apps rook-ceph-osd-6 //\u5220\u9664 osd \u7684 deploy \u6e05\u7406\u78c1\u76d8\u5df2\u7ecf\u88ab ceph \u6807\u8bb0\u4e09\u4e2a osd \u56e0\u4e3a\u914d\u7f6e Ceph \u5b58\u50a8,\u9700\u8981\u88f8\u8bbe\u5907\u6216\u8005\u6ca1\u6709\u6587\u4ef6\u7cfb\u7edf\u7684\u8bbe\u5907,\u5df2\u7ecf\u88ab ceph \u6807\u8bb0\u4e5f\u53ef\u80fd operator \u4f1a\u52a0\u5165 osd \u5931\u8d25,\u6240\u4ee5\u9700\u8981\u6e05\u7406 [ root@node-1 ceph ] # dmsetup ls //\u7528\u8fd9\u6761\u547d\u4ee4\u67e5\u51fa\u88ab ceph \u6807\u8bb0\u7684\u8bbe\u5907 [ root@node-1 ceph ] # dmsetup remove vg--test-vg--lv [ root@node-1 ceph ] # sgdisk -Z /dev/vdd roo-ceph \u7a7a\u95f4\u4e0d\u8db3 \u5c06\u540d\u79f0\u4e3a rook \u8fd9\u4e2a pool \u5220\u9664\u6765\u89e3\u51b3\u7684\u8fd9\u4e2a\u95ee\u9898(\u8fd9\u4e2a pool \u4e0d\u5728\u4f7f\u7528) ceph osd pool delete rook rook --yes-i-really-really-mean-it //\u5220\u9664\u9700\u8c28\u614e \u5982\u679c\u4e0d\u6e05\u695a\u662f\u5177\u4f53\u90a3\u5757\u8bbe\u5907\u51fa\u73b0\u4e86\u95ee\u9898,\u53ef\u4ee5\u901a\u8fc7\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6765\u8fc7\u6ee4\u51fa\u6765 # Get OSD Pods # This uses the example/default cluster name \"rook\" OSD_PODS = $( kubectl get pods --all-namespaces -l \\ app = rook-ceph-osd,rook_cluster = rook-ceph -o jsonpath = '{.items[*].metadata.name}' ) # Find node and drive associations from OSD pods for pod in $( echo ${ OSD_PODS } ) do echo \"Pod: ${ pod } \" echo \"Node: $( kubectl -n rook-ceph get pod ${ pod } -o jsonpath = '{.spec.nodeName}' ) \" kubectl -n rook-ceph exec ${ pod } -- sh -c '\\ for i in /var/lib/ceph/osd/ceph-*; do [ -f ${i}/ready ] || continue echo -ne \"-$(basename ${i}) \" echo $(lsblk -n -o NAME,SIZE ${i}/block 2> /dev/null || \\ findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type) done | sort -V echo' done \u4fee\u590dpg unknown PG # \u5217\u51fa\u72b6\u6001\u4e3a unkown \u7684 PG, \u7b2c\u4e00\u5217\u4e3a pgid $ ceph pg dump | grep unknown # \u5f3a\u5236\u91cd\u5efa PG\uff0c\u6ce8\uff1a\u4f1a\u4e22\u5931\u6570\u636e\uff0c\u614e\u91cd\u4f7f\u7528 $ ceph osd force-create-pg <pgid> \u6587\u7ae0\u53c2\u8003: https://www.linuxcool.com/dmsetup","title":"ceph \u5b58\u50a8\u4fee\u590d"},{"location":"Storage/5-dev-docs/#_1","text":"dev \u73af\u5883\u56e0\u4e3a\u78c1\u76d8\u51fa\u73b0\u95ee\u9898\u5bfc\u81f4\u5b58\u50a8\u4e0d\u53ef\u7528,\u63d0\u51fa\u4e0d\u53ef\u7528\u78c1\u76d8,\u52a0\u5165\u65b0\u78c1\u76d8\u65e0\u6cd5\u52a0\u5165","title":"\u95ee\u9898\u63cf\u8ff0"},{"location":"Storage/5-dev-docs/#_2","text":"\u78c1\u76d8\u574f\u6389\u5177\u4f53\u4e0d\u6e05\u695a\u539f\u56e0,\u65b0\u52a0\u5165 osd \u65e0\u6cd5\u52a0\u5165 rook-ceph \u4e3b\u8981\u539f\u56e0\u6709\u4ee5\u4e0b\u51e0\u4e2a: \u4f7f\u7528 ceph \u539f\u751f\u65b9\u5f0f\u624b\u52a8\u8e22\u51fa osd,\u6709\u4e9b\u4fe1\u606f\u6b8b\u7559. \u62a5\u9519\u663e\u793a: stderr: Error EEXIST: entity osd.6 exists but key does not match operator \u65e0\u6cd5\u52a0\u5165 osd,\u4f46\u662f\u88f8\u8bbe\u5907\u5df2\u7ecf\u5b58\u5728 ceph \u76f8\u5173\u7684\u4fe1\u606f. \u4ece\u56fe\u53ef\u4ee5\u770b\u5230vdd\u5df2\u7ecf\u5b58\u5728 ceph \u76f8\u5173\u7684\u4fe1\u606f,\u4f46\u662f vgs \u5374\u6ca1\u6709\u4ed6\u7684\u8bb0\u5f55,\u4e5f\u6ca1\u6709 osd \u7684 deployment rook-ceph \u62a5\u7a7a\u95f4\u4e0d\u8db3 pg 25 unknown","title":"\u95ee\u9898\u539f\u56e0"},{"location":"Storage/5-dev-docs/#_3","text":"\u5c06\u6709\u95ee\u9898\u7684 osd-6 \u8e22\u51fa\u53bb rook-ceph \u96c6\u7fa4\u4e2d. \u65b9\u5f0f\u4e00 : ( \u5229\u7528 rook \u63d0\u4f9b\u7684\u811a\u672c\u6765\u5220\u9664,\u63a8\u8350! ) \u811a\u672c\u5730\u5740: https://github.com/rook/rook/blob/v1.6.11/cluster/examples/kubernetes/ceph/osd-purge.yaml \u65b9\u5f0f\u4e8c : \u4f7f\u7528 ceph \u539f\u751f\u65b9\u5f0f\u5220\u9664 [ root@node-1 ceph ] # ceph osd out osd.6 [ root@node-1 ceph ] # ceph osd purge 6 [ root@node-1 ceph ] # ceph osd tree //\u786e\u8ba4\u662f\u5426\u5df2\u7ecf\u5220\u9664 [ root@node-1 ceph ] # ceph auth del osd.6 //\u6ce8\u610f\u53ef\u80fd\u5c31\u662f\u8fd9\u6b65\u9aa4\u6ca1\u6709\u505a\u4ece\u800c\u5bfc\u81f4\u96c6\u7fa4\u52a0\u4e0d\u8fdb\u53bb\u65b0 osd [ root@node-1 ceph ] # kubectl delete deployments.apps rook-ceph-osd-6 //\u5220\u9664 osd \u7684 deploy \u6e05\u7406\u78c1\u76d8\u5df2\u7ecf\u88ab ceph \u6807\u8bb0\u4e09\u4e2a osd \u56e0\u4e3a\u914d\u7f6e Ceph \u5b58\u50a8,\u9700\u8981\u88f8\u8bbe\u5907\u6216\u8005\u6ca1\u6709\u6587\u4ef6\u7cfb\u7edf\u7684\u8bbe\u5907,\u5df2\u7ecf\u88ab ceph \u6807\u8bb0\u4e5f\u53ef\u80fd operator \u4f1a\u52a0\u5165 osd \u5931\u8d25,\u6240\u4ee5\u9700\u8981\u6e05\u7406 [ root@node-1 ceph ] # dmsetup ls //\u7528\u8fd9\u6761\u547d\u4ee4\u67e5\u51fa\u88ab ceph \u6807\u8bb0\u7684\u8bbe\u5907 [ root@node-1 ceph ] # dmsetup remove vg--test-vg--lv [ root@node-1 ceph ] # sgdisk -Z /dev/vdd roo-ceph \u7a7a\u95f4\u4e0d\u8db3 \u5c06\u540d\u79f0\u4e3a rook \u8fd9\u4e2a pool \u5220\u9664\u6765\u89e3\u51b3\u7684\u8fd9\u4e2a\u95ee\u9898(\u8fd9\u4e2a pool \u4e0d\u5728\u4f7f\u7528) ceph osd pool delete rook rook --yes-i-really-really-mean-it //\u5220\u9664\u9700\u8c28\u614e \u5982\u679c\u4e0d\u6e05\u695a\u662f\u5177\u4f53\u90a3\u5757\u8bbe\u5907\u51fa\u73b0\u4e86\u95ee\u9898,\u53ef\u4ee5\u901a\u8fc7\u5b98\u65b9\u63d0\u4f9b\u7684\u811a\u672c\u6765\u8fc7\u6ee4\u51fa\u6765 # Get OSD Pods # This uses the example/default cluster name \"rook\" OSD_PODS = $( kubectl get pods --all-namespaces -l \\ app = rook-ceph-osd,rook_cluster = rook-ceph -o jsonpath = '{.items[*].metadata.name}' ) # Find node and drive associations from OSD pods for pod in $( echo ${ OSD_PODS } ) do echo \"Pod: ${ pod } \" echo \"Node: $( kubectl -n rook-ceph get pod ${ pod } -o jsonpath = '{.spec.nodeName}' ) \" kubectl -n rook-ceph exec ${ pod } -- sh -c '\\ for i in /var/lib/ceph/osd/ceph-*; do [ -f ${i}/ready ] || continue echo -ne \"-$(basename ${i}) \" echo $(lsblk -n -o NAME,SIZE ${i}/block 2> /dev/null || \\ findmnt -n -v -o SOURCE,SIZE -T ${i}) $(cat ${i}/type) done | sort -V echo' done \u4fee\u590dpg unknown PG # \u5217\u51fa\u72b6\u6001\u4e3a unkown \u7684 PG, \u7b2c\u4e00\u5217\u4e3a pgid $ ceph pg dump | grep unknown # \u5f3a\u5236\u91cd\u5efa PG\uff0c\u6ce8\uff1a\u4f1a\u4e22\u5931\u6570\u636e\uff0c\u614e\u91cd\u4f7f\u7528 $ ceph osd force-create-pg <pgid> \u6587\u7ae0\u53c2\u8003: https://www.linuxcool.com/dmsetup","title":"\u89e3\u51b3\u65b9\u6cd5"},{"location":"Storage/Juicefs/","text":"Juicefs\u673a\u5668\u5b66\u4e60\u5b58\u50a8\u65b9\u6848 \u00b6 \u73af\u5883\u8981\u6c42 docker\u73af\u5883 k3s \u73af\u5883\u5b89\u88c5 redis\u6570\u636e\u5e93 rook-ceph\u5b58\u50a8 juicefs \u73af\u5883\u90e8\u7f72 \u00b6 \u53c2\u8003\u6587\u7ae0: Juicefs\u5b98\u7f51 \u8fdb\u884c\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u5982\u679c\u662f\u751f\u4ea7\u73af\u5883\u9700\u8981\u8003\u8651redis\u7684\u9ad8\u53ef\u7528\uff0c\u4ee5\u53ca\u539f\u6570\u636e\u7684\u5907\u4efd\u3002 Cephfs\u548cjuicefs\u6027\u80fd\u5bf9\u6bd4 \u00b6 \u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4 \u00b6 cephfs \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 816 : Mon Aug 15 09 :48:26 2022 read: IOPS = 33 .5k, BW = 131MiB/s ( 137MB/s )( 200MiB/1527msec ) clat ( nsec ) : min = 570 , max = 248033k, avg = 29131 .89, stdev = 2094821 .44 lat ( nsec ) : min = 605 , max = 248033k, avg = 29167 .63, stdev = 2094821 .43 clat percentiles ( nsec ) : | 1 .00th =[ 644 ] , 5 .00th =[ 708 ] , 10 .00th =[ 732 ] , | 20 .00th =[ 748 ] , 30 .00th =[ 756 ] , 40 .00th =[ 780 ] , | 50 .00th =[ 796 ] , 60 .00th =[ 828 ] , 70 .00th =[ 892 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1160 ] , | 99 .00th =[ 1416 ] , 99 .50th =[ 1752 ] , 99 .90th =[ 23168 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 93847552 ] bw ( KiB/s ) : min = 73728 , max = 196608 , per = 21 .37%, avg = 135168 .00, stdev = 86889 .28, samples = 2 iops : min = 18432 , max = 49152 , avg = 33792 .00, stdev = 21722 .32, samples = 2 lat ( nsec ) : 750 = 23 .25%, 1000 = 63 .18% lat ( usec ) : 2 = 13 .14%, 4 = 0 .15%, 10 = 0 .12%, 20 = 0 .04%, 50 = 0 .02% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01% cpu : usr = 0 .72%, sys = 6 .55%, ctx = 81 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 823 : Mon Aug 15 09 :48:31 2022 read: IOPS = 41 .1k, BW = 161MiB/s ( 168MB/s )( 200MiB/1246msec ) clat ( nsec ) : min = 620 , max = 346202k, avg = 23787 .02, stdev = 1805527 .05 lat ( nsec ) : min = 654 , max = 346202k, avg = 23822 .73, stdev = 1805527 .05 clat percentiles ( nsec ) : | 1 .00th =[ 692 ] , 5 .00th =[ 740 ] , 10 .00th =[ 748 ] , | 20 .00th =[ 764 ] , 30 .00th =[ 780 ] , 40 .00th =[ 804 ] , | 50 .00th =[ 828 ] , 60 .00th =[ 876 ] , 70 .00th =[ 924 ] , | 80 .00th =[ 980 ] , 90 .00th =[ 1128 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 1464 ] , 99 .50th =[ 1672 ] , 99 .90th =[ 11712 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 63700992 ] bw ( KiB/s ) : min = 159960 , max = 224614 , per = 26 .87%, avg = 192287 .00, stdev = 45717 .28, samples = 2 iops : min = 39990 , max = 56153 , avg = 48071 .50, stdev = 11428 .97, samples = 2 lat ( nsec ) : 750 = 9 .15%, 1000 = 72 .77% lat ( usec ) : 2 = 17 .78%, 4 = 0 .09%, 10 = 0 .10%, 20 = 0 .04%, 50 = 0 .01% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 500 = 0 .01% cpu : usr = 0 .88%, sys = 8 .19%, ctx = 88 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 824 : Mon Aug 15 09 :48:31 2022 jufice \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) Jobs: 5 (f=5): [R(5)][66.7%][r=304MiB/s][r=77.8k IOPS][eta 00m:02s] big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=831: Mon Aug 15 09:50:58 2022 read: IOPS=14.9k, BW=58.0MiB/s (60.9MB/s)(200MiB/3446msec) clat (nsec): min=373, max=533198k, avg=67006.12, stdev=4299023.59 lat (nsec): min=406, max=533198k, avg=67043.41, stdev=4299023.63 clat percentiles (nsec): | 1.00th=[ 390], 5.00th=[ 418], 10.00th=[ 462], | 20.00th=[ 532], 30.00th=[ 548], 40.00th=[ 564], | 50.00th=[ 572], 60.00th=[ 580], 70.00th=[ 596], | 80.00th=[ 620], 90.00th=[ 692], 95.00th=[ 860], | 99.00th=[ 58624], 99.50th=[ 86528], 99.90th=[ 220160], | 99.95th=[ 3031040], 99.99th=[248512512] bw ( KiB/s): min=24576, max=90112, per=22.61%, avg=64140.67, stdev=22219.00, samples=6 iops : min= 6144, max=22528, avg=16035.17, stdev=5554.75, samples=6 lat (nsec) : 500=14.08%, 750=78.30%, 1000=4.37% lat (usec) : 2=0.36%, 4=0.65%, 10=0.30%, 20=0.09%, 50=0.28% lat (usec) : 100=1.21%, 250=0.26%, 500=0.03%, 750=0.01%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 20=0.01%, 50=0.01%, 100=0.01% lat (msec) : 250=0.01%, 500=0.01%, 750=0.01% cpu : usr=0.70%, sys=1.92%, ctx=874, majf=0, minf=16 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=838: Mon Aug 15 09:51:22 2022 read: IOPS=157k, BW=613MiB/s (643MB/s)(200MiB/326msec) clat (nsec): min=379, max=7481.0k, avg=5697.24, stdev=75385.63 lat (nsec): min=411, max=7481.1k, avg=5733.65, stdev=75387.68 clat percentiles (nsec): | 1.00th=[ 458], 5.00th=[ 498], 10.00th=[ 524], | 20.00th=[ 556], 30.00th=[ 572], 40.00th=[ 580], | 50.00th=[ 596], 60.00th=[ 612], 70.00th=[ 636], | 80.00th=[ 684], 90.00th=[ 852], 95.00th=[ 964], | 99.00th=[ 102912], 99.50th=[ 280576], 99.90th=[ 962560], | 99.95th=[1302528], 99.99th=[2539520] lat (nsec) : 500=5.63%, 750=79.66%, 1000=10.50% lat (usec) : 2=1.80%, 4=0.02%, 10=0.21%, 20=0.06%, 50=0.26% lat (usec) : 100=0.84%, 250=0.45%, 500=0.38%, 750=0.07%, 1000=0.03% lat (msec) : 2=0.08%, 4=0.01%, 10=0.01% cpu : usr=5.85%, sys=27.08%, ctx=1348, majf=0, minf=17 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=839: Mon Aug 15 09:51:22 2022","title":"Juicefs\u5b58\u50a8"},{"location":"Storage/Juicefs/#juicefs","text":"\u73af\u5883\u8981\u6c42 docker\u73af\u5883 k3s \u73af\u5883\u5b89\u88c5 redis\u6570\u636e\u5e93 rook-ceph\u5b58\u50a8","title":"Juicefs\u673a\u5668\u5b66\u4e60\u5b58\u50a8\u65b9\u6848"},{"location":"Storage/Juicefs/#juicefs_1","text":"\u53c2\u8003\u6587\u7ae0: Juicefs\u5b98\u7f51 \u8fdb\u884c\u90e8\u7f72 \u6e29\u99a8\u63d0\u793a \u5982\u679c\u662f\u751f\u4ea7\u73af\u5883\u9700\u8981\u8003\u8651redis\u7684\u9ad8\u53ef\u7528\uff0c\u4ee5\u53ca\u539f\u6570\u636e\u7684\u5907\u4efd\u3002","title":"juicefs \u73af\u5883\u90e8\u7f72"},{"location":"Storage/Juicefs/#cephfsjuicefs","text":"","title":"Cephfs\u548cjuicefs\u6027\u80fd\u5bf9\u6bd4"},{"location":"Storage/Juicefs/#_1","text":"cephfs \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 816 : Mon Aug 15 09 :48:26 2022 read: IOPS = 33 .5k, BW = 131MiB/s ( 137MB/s )( 200MiB/1527msec ) clat ( nsec ) : min = 570 , max = 248033k, avg = 29131 .89, stdev = 2094821 .44 lat ( nsec ) : min = 605 , max = 248033k, avg = 29167 .63, stdev = 2094821 .43 clat percentiles ( nsec ) : | 1 .00th =[ 644 ] , 5 .00th =[ 708 ] , 10 .00th =[ 732 ] , | 20 .00th =[ 748 ] , 30 .00th =[ 756 ] , 40 .00th =[ 780 ] , | 50 .00th =[ 796 ] , 60 .00th =[ 828 ] , 70 .00th =[ 892 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1160 ] , | 99 .00th =[ 1416 ] , 99 .50th =[ 1752 ] , 99 .90th =[ 23168 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 93847552 ] bw ( KiB/s ) : min = 73728 , max = 196608 , per = 21 .37%, avg = 135168 .00, stdev = 86889 .28, samples = 2 iops : min = 18432 , max = 49152 , avg = 33792 .00, stdev = 21722 .32, samples = 2 lat ( nsec ) : 750 = 23 .25%, 1000 = 63 .18% lat ( usec ) : 2 = 13 .14%, 4 = 0 .15%, 10 = 0 .12%, 20 = 0 .04%, 50 = 0 .02% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01% cpu : usr = 0 .72%, sys = 6 .55%, ctx = 81 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 root@nginx-run-7877759d45-484kx:/data# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 5 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 5 processes Jobs: 5 ( f = 5 ) big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 823 : Mon Aug 15 09 :48:31 2022 read: IOPS = 41 .1k, BW = 161MiB/s ( 168MB/s )( 200MiB/1246msec ) clat ( nsec ) : min = 620 , max = 346202k, avg = 23787 .02, stdev = 1805527 .05 lat ( nsec ) : min = 654 , max = 346202k, avg = 23822 .73, stdev = 1805527 .05 clat percentiles ( nsec ) : | 1 .00th =[ 692 ] , 5 .00th =[ 740 ] , 10 .00th =[ 748 ] , | 20 .00th =[ 764 ] , 30 .00th =[ 780 ] , 40 .00th =[ 804 ] , | 50 .00th =[ 828 ] , 60 .00th =[ 876 ] , 70 .00th =[ 924 ] , | 80 .00th =[ 980 ] , 90 .00th =[ 1128 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 1464 ] , 99 .50th =[ 1672 ] , 99 .90th =[ 11712 ] , | 99 .95th =[ 2899968 ] , 99 .99th =[ 63700992 ] bw ( KiB/s ) : min = 159960 , max = 224614 , per = 26 .87%, avg = 192287 .00, stdev = 45717 .28, samples = 2 iops : min = 39990 , max = 56153 , avg = 48071 .50, stdev = 11428 .97, samples = 2 lat ( nsec ) : 750 = 9 .15%, 1000 = 72 .77% lat ( usec ) : 2 = 17 .78%, 4 = 0 .09%, 10 = 0 .10%, 20 = 0 .04%, 50 = 0 .01% lat ( usec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01% lat ( msec ) : 2 = 0 .01%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 500 = 0 .01% cpu : usr = 0 .88%, sys = 8 .19%, ctx = 88 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 824 : Mon Aug 15 09 :48:31 2022 jufice \u57fa\u51c6\u6d4b\u8bd5 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) big-file-multi-read: Laying out IO file (1 file / 200MiB) Jobs: 5 (f=5): [R(5)][66.7%][r=304MiB/s][r=77.8k IOPS][eta 00m:02s] big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=831: Mon Aug 15 09:50:58 2022 read: IOPS=14.9k, BW=58.0MiB/s (60.9MB/s)(200MiB/3446msec) clat (nsec): min=373, max=533198k, avg=67006.12, stdev=4299023.59 lat (nsec): min=406, max=533198k, avg=67043.41, stdev=4299023.63 clat percentiles (nsec): | 1.00th=[ 390], 5.00th=[ 418], 10.00th=[ 462], | 20.00th=[ 532], 30.00th=[ 548], 40.00th=[ 564], | 50.00th=[ 572], 60.00th=[ 580], 70.00th=[ 596], | 80.00th=[ 620], 90.00th=[ 692], 95.00th=[ 860], | 99.00th=[ 58624], 99.50th=[ 86528], 99.90th=[ 220160], | 99.95th=[ 3031040], 99.99th=[248512512] bw ( KiB/s): min=24576, max=90112, per=22.61%, avg=64140.67, stdev=22219.00, samples=6 iops : min= 6144, max=22528, avg=16035.17, stdev=5554.75, samples=6 lat (nsec) : 500=14.08%, 750=78.30%, 1000=4.37% lat (usec) : 2=0.36%, 4=0.65%, 10=0.30%, 20=0.09%, 50=0.28% lat (usec) : 100=1.21%, 250=0.26%, 500=0.03%, 750=0.01%, 1000=0.01% lat (msec) : 2=0.01%, 4=0.01%, 20=0.01%, 50=0.01%, 100=0.01% lat (msec) : 250=0.01%, 500=0.01%, 750=0.01% cpu : usr=0.70%, sys=1.92%, ctx=874, majf=0, minf=16 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 root@nginx-run-7877759d45-484kx:/config# fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=5 big-file-multi-read: (g=0): rw=read, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=1 ... fio-3.25 Starting 5 processes big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=838: Mon Aug 15 09:51:22 2022 read: IOPS=157k, BW=613MiB/s (643MB/s)(200MiB/326msec) clat (nsec): min=379, max=7481.0k, avg=5697.24, stdev=75385.63 lat (nsec): min=411, max=7481.1k, avg=5733.65, stdev=75387.68 clat percentiles (nsec): | 1.00th=[ 458], 5.00th=[ 498], 10.00th=[ 524], | 20.00th=[ 556], 30.00th=[ 572], 40.00th=[ 580], | 50.00th=[ 596], 60.00th=[ 612], 70.00th=[ 636], | 80.00th=[ 684], 90.00th=[ 852], 95.00th=[ 964], | 99.00th=[ 102912], 99.50th=[ 280576], 99.90th=[ 962560], | 99.95th=[1302528], 99.99th=[2539520] lat (nsec) : 500=5.63%, 750=79.66%, 1000=10.50% lat (usec) : 2=1.80%, 4=0.02%, 10=0.21%, 20=0.06%, 50=0.26% lat (usec) : 100=0.84%, 250=0.45%, 500=0.38%, 750=0.07%, 1000=0.03% lat (msec) : 2=0.08%, 4=0.01%, 10=0.01% cpu : usr=5.85%, sys=27.08%, ctx=1348, majf=0, minf=17 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0% issued rwts: total=51200,0,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 big-file-multi-read: (groupid=0, jobs=1): err= 0: pid=839: Mon Aug 15 09:51:22 2022","title":"\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4"},{"location":"Storage/osd/","text":"osd \u6027\u80fd\u6d4b\u8bd5 \u00b6 \u73af\u5883\u8981\u6c42 \u51c6\u5907\u4e00\u4e2ak8s\u96c6\u7fa4 \u51c6\u5907\u4e00\u4e2aceph\u96c6\u7fa4 \u673a\u68b0\u78c1\u76d8\u6d4b\u8bd5 \u00b6 \u80cc\u666f \u6d4b\u8bd5\u673a\u68b0\u78c1\u76d8\u66f4\u6362\u56fa\u6001\u78c1\u76d8\u540e\uff0cosd\u7684\u8bfb\u5199\u901f\u5ea6 \u65b9\u6cd5\u4e00\uff1aFio \u538b\u529b\u6d4b\u8bd5 \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u00b6 fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest filename : \u538b\u6d4b\u7684\u6587\u4ef6\uff08\u6302\u5728ceph\u7684\u76ee\u5f55\u4e0b\uff09 -iodepth : \u961f\u5217\u6df1\u5ea6 -size : \u6307\u5b9a\u5199\u591a\u5927\u7684\u6570\u636e rw : I/O\u6a21\u5f0f\uff0c\u968f\u673a\u8bfb\u5199\uff0c\u987a\u5e8f\u8bfb\u5199\u7b49\u7b49 bs : I/O block\u5927\u5c0f \u793a\u4f8b\uff1a \u00b6 4K\u968f\u673a\u5199-iops fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randwrite, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ w ( 6 )][ 100 .0% ][ w = 15 .0MiB/s ][ w = 4089 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 96930 : Mon Aug 1 14 :34:37 2022 write: IOPS = 4104 , BW = 16 .0MiB/s ( 16 .8MB/s )( 963MiB/60054msec ) ; 0 zone resets slat ( nsec ) : min = 1755 , max = 48814k, avg = 10925 .38, stdev = 316419 .53 clat ( msec ) : min = 5 , max = 191 , avg = 46 .76, stdev = 13 .55 lat ( msec ) : min = 5 , max = 191 , avg = 46 .77, stdev = 13 .54 clat percentiles ( msec ) : | 1 .00th =[ 25 ] , 5 .00th =[ 29 ] , 10 .00th =[ 31 ] , 20 .00th =[ 33 ] , | 30 .00th =[ 37 ] , 40 .00th =[ 44 ] , 50 .00th =[ 49 ] , 60 .00th =[ 53 ] , | 70 .00th =[ 55 ] , 80 .00th =[ 58 ] , 90 .00th =[ 61 ] , 95 .00th =[ 64 ] , | 99 .00th =[ 85 ] , 99 .50th =[ 96 ] , 99 .90th =[ 130 ] , 99 .95th =[ 142 ] , | 99 .99th =[ 163 ] bw ( KiB/s ) : min = 14912 , max = 19106 , per = 100 .00%, avg = 16419 .87, stdev = 80 .74, samples = 720 iops : min = 3728 , max = 4776 , avg = 4104 .83, stdev = 20 .18, samples = 720 lat ( msec ) : 10 = 0 .04%, 20 = 0 .38%, 50 = 53 .46%, 100 = 45 .67%, 250 = 0 .45% cpu : usr = 0 .19%, sys = 0 .71%, ctx = 182573 , majf = 1 , minf = 11 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,246515,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 16 .0MiB/s ( 16 .8MB/s ) , 16 .0MiB/s-16.0MiB/s ( 16 .8MB/s-16.8MB/s ) , io = 963MiB ( 1010MB ) , run = 60054 -60054msec Disk stats ( read/write ) : rbd0: ios = 4 /245332, merge = 0 /637, ticks = 81 /11412705, in_queue = 10919116 , util = 57 .39% 4k\u968f\u673a\u8bfb-iops [ ucloud ] root@master0:~# fio -filename = /data/fio2.img -direct = 1 -iodepth 32 -thread -rw = randread -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randread, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ r ( 6 )][ 88 .9% ][ r = 200MiB/s ][ r = 51 .2k IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 101153 : Mon Aug 1 14 :40:36 2022 read: IOPS = 36 .1k, BW = 141MiB/s ( 148MB/s )( 1200MiB/8508msec ) slat ( nsec ) : min = 1151 , max = 14537k, avg = 19572 .33, stdev = 101622 .30 clat ( nsec ) : min = 455 , max = 79415k, avg = 5280191 .11, stdev = 7133988 .93 lat ( usec ) : min = 64 , max = 79418 , avg = 5299 .94, stdev = 7130 .13 clat percentiles ( usec ) : | 1 .00th =[ 363 ] , 5 .00th =[ 865 ] , 10 .00th =[ 1237 ] , 20 .00th =[ 1778 ] , | 30 .00th =[ 2245 ] , 40 .00th =[ 2704 ] , 50 .00th =[ 3163 ] , 60 .00th =[ 3720 ] , | 70 .00th =[ 4490 ] , 80 .00th =[ 5866 ] , 90 .00th =[ 10290 ] , 95 .00th =[ 19268 ] , | 99 .00th =[ 40109 ] , 99 .50th =[ 44303 ] , 99 .90th =[ 52167 ] , 99 .95th =[ 56361 ] , | 99 .99th =[ 65274 ] bw ( KiB/s ) : min = 45752 , max = 234048 , per = 98 .88%, avg = 142805 .42, stdev = 12299 .59, samples = 98 iops : min = 11438 , max = 58512 , avg = 35701 .11, stdev = 3074 .86, samples = 98 lat ( nsec ) : 500 = 0 .01% lat ( usec ) : 20 = 0 .01%, 50 = 0 .01%, 100 = 0 .03%, 250 = 0 .38%, 500 = 1 .39% lat ( usec ) : 750 = 2 .05%, 1000 = 2 .80% lat ( msec ) : 2 = 18 .03%, 4 = 39 .21%, 10 = 25 .81%, 20 = 5 .53%, 50 = 4 .61% lat ( msec ) : 100 = 0 .14% cpu : usr = 1 .03%, sys = 3 .66%, ctx = 239175 , majf = 0 , minf = 198 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 307200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 141MiB/s ( 148MB/s ) , 141MiB/s-141MiB/s ( 148MB/s-148MB/s ) , io = 1200MiB ( 1258MB ) , run = 8508 -8508msec Disk stats ( read/write ) : rbd0: ios = 298435 /0, merge = 1141 /0, ticks = 1470086 /0, in_queue = 887248 , util = 98 .83% 4k\u968f\u673a\u8bfb\u5199-iops [ ucloud ] root@master0:~# fio -filename = /data/fio3.img -direct = 1 -iodepth 32 -thread -rw = randrw -rwmixread = 70 -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randrw, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 6 ( f = 6 ) : [ m ( 6 )][ 100 .0% ][ r = 26 .7MiB/s,w = 11 .6MiB/s ][ r = 6835 ,w = 2976 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 104592 : Mon Aug 1 14 :45:40 2022 read: IOPS = 6434 , BW = 25 .1MiB/s ( 26 .4MB/s )( 838MiB/33355msec ) slat ( nsec ) : min = 1268 , max = 512155 , avg = 5128 .49, stdev = 4908 .50 clat ( usec ) : min = 108 , max = 237284 , avg = 17218 .47, stdev = 13454 .22 lat ( usec ) : min = 112 , max = 237286 , avg = 17223 .74, stdev = 13454 .22 clat percentiles ( usec ) : | 1 .00th =[ 857 ] , 5 .00th =[ 1696 ] , 10 .00th =[ 2507 ] , 20 .00th =[ 4293 ] , | 30 .00th =[ 6980 ] , 40 .00th =[ 12256 ] , 50 .00th =[ 17695 ] , 60 .00th =[ 20579 ] , | 70 .00th =[ 22938 ] , 80 .00th =[ 25560 ] , 90 .00th =[ 31589 ] , 95 .00th =[ 41157 ] , | 99 .00th =[ 55313 ] , 99 .50th =[ 64226 ] , 99 .90th =[ 94897 ] , 99 .95th =[ 154141 ] , | 99 .99th =[ 227541 ] bw ( KiB/s ) : min = 18128 , max = 31360 , per = 99 .94%, avg = 25724 .41, stdev = 445 .55, samples = 396 iops : min = 4532 , max = 7840 , avg = 6430 .97, stdev = 111 .38, samples = 396 write: IOPS = 2775 , BW = 10 .8MiB/s ( 11 .4MB/s )( 362MiB/33355msec ) ; 0 zone resets slat ( nsec ) : min = 1519 , max = 586139 , avg = 5581 .99, stdev = 5697 .03 clat ( usec ) : min = 735 , max = 234047 , avg = 29153 .24, stdev = 13547 .74 lat ( usec ) : min = 740 , max = 234050 , avg = 29158 .97, stdev = 13547 .75 clat percentiles ( msec ) : | 1 .00th =[ 5 ] , 5 .00th =[ 14 ] , 10 .00th =[ 19 ] , 20 .00th =[ 21 ] , | 30 .00th =[ 23 ] , 40 .00th =[ 24 ] , 50 .00th =[ 26 ] , 60 .00th =[ 28 ] , | 70 .00th =[ 33 ] , 80 .00th =[ 40 ] , 90 .00th =[ 46 ] , 95 .00th =[ 52 ] , | 99 .00th =[ 67 ] , 99 .50th =[ 77 ] , 99 .90th =[ 153 ] , 99 .95th =[ 215 ] , | 99 .99th =[ 230 ] bw ( KiB/s ) : min = 7264 , max = 13640 , per = 99 .93%, avg = 11092 .62, stdev = 204 .64, samples = 396 iops : min = 1816 , max = 3410 , avg = 2773 .11, stdev = 51 .15, samples = 396 lat ( usec ) : 250 = 0 .01%, 500 = 0 .12%, 750 = 0 .35%, 1000 = 0 .56% lat ( msec ) : 2 = 3 .77%, 4 = 8 .37%, 10 = 13 .38%, 20 = 18 .51%, 50 = 52 .05% lat ( msec ) : 100 = 2 .75%, 250 = 0 .11% cpu : usr = 0 .42%, sys = 1 .37%, ctx = 262129 , majf = 1 , minf = 6 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 214635 ,92565,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 25 .1MiB/s ( 26 .4MB/s ) , 25 .1MiB/s-25.1MiB/s ( 26 .4MB/s-26.4MB/s ) , io = 838MiB ( 879MB ) , run = 33355 -33355msec WRITE: bw = 10 .8MiB/s ( 11 .4MB/s ) , 10 .8MiB/s-10.8MiB/s ( 11 .4MB/s-11.4MB/s ) , io = 362MiB ( 379MB ) , run = 33355 -33355msec Disk stats ( read/write ) : rbd0: ios = 212275 /91760, merge = 1000 /200, ticks = 3650146 /2659754, in_queue = 5701392 , util = 86 .01% 1M\u987a\u5e8f\u5199-\u541e\u5410 [ ucloud ] root@master0:~# fio -filename = /data/fio4.img -direct = 1 -iodepth 32 -thread -rw = write -ioengine = libaio -bs = 1M -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = write, bs =( R ) 1024KiB-1024KiB, ( W ) 1024KiB-1024KiB, ( T ) 1024KiB-1024KiB, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 3 ( f = 3 ) : [ _ ( 2 ) ,W ( 2 ) ,_ ( 1 ) ,W ( 1 )][ 90 .0% ][ w = 177MiB/s ][ w = 177 IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 107996 : Mon Aug 1 14 :51:00 2022 write: IOPS = 131 , BW = 131MiB/s ( 138MB/s )( 1200MiB/9131msec ) ; 0 zone resets slat ( usec ) : min = 41 , max = 13194 , avg = 208 .41, stdev = 756 .44 clat ( msec ) : min = 20 , max = 3855 , avg = 1392 .05, stdev = 816 .66 lat ( msec ) : min = 20 , max = 3855 , avg = 1392 .26, stdev = 816 .65 clat percentiles ( msec ) : | 1 .00th =[ 284 ] , 5 .00th =[ 351 ] , 10 .00th =[ 435 ] , 20 .00th =[ 625 ] , | 30 .00th =[ 885 ] , 40 .00th =[ 1099 ] , 50 .00th =[ 1234 ] , 60 .00th =[ 1418 ] , | 70 .00th =[ 1620 ] , 80 .00th =[ 1938 ] , 90 .00th =[ 2769 ] , 95 .00th =[ 2903 ] , | 99 .00th =[ 3608 ] , 99 .50th =[ 3675 ] , 99 .90th =[ 3809 ] , 99 .95th =[ 3842 ] , | 99 .99th =[ 3842 ] bw ( KiB/s ) : min = 24554 , max = 273468 , per = 99 .58%, avg = 134013 .71, stdev = 11587 .87, samples = 93 iops : min = 22 , max = 267 , avg = 130 .28, stdev = 11 .35, samples = 93 lat ( msec ) : 50 = 0 .25%, 250 = 0 .33%, 500 = 13 .33%, 750 = 11 .50%, 1000 = 10 .08% lat ( msec ) : 2000 = 44 .92%, > = 2000 = 19 .58% cpu : usr = 0 .18%, sys = 0 .09%, ctx = 792 , majf = 1 , minf = 6 IO depths : 1 = 0 .5%, 2 = 1 .0%, 4 = 2 .0%, 8 = 4 .0%, 16 = 8 .0%, 32 = 84 .5%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 99 .4%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .6%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,1200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 1200MiB ( 1258MB ) , run = 9131 -9131msec Disk stats ( read/write ) : rbd0: ios = 0 /887, merge = 0 /280, ticks = 0 /1112233, in_queue = 1110460 , util = 16 .67% \u65b9\u6cd5\u4e8c\uff1aRBD bench \u538b\u529b\u6d4b\u8bd5 \u00b6 \u53c2\u6570\u8bf4\u660e\uff1a \u00b6 [ ucloud ] root@master0:~# rbd help bench usage : rbd bench [--pool <pool>] [--namespace <namespace>] [--image <image>] [--io-size <io-size>] [--io-threads <io-threads>] [--io-total <io-total>] [--io-pattern <io-pattern>] [--rw-mix-read <rw-mix-read>] --io-type <io-type> <image-spec> Simple benchmark. Positional arguments <image-spec> image specification (example : [ <pool-name>/ [ <namespace>/ ]] <image-name>) Optional arguments -p [ --pool ] arg pool name # \u6307\u5b9apool\u7684\u540d\u79f0 --namespace arg namespace name # \u6307\u5b9anamespace --image arg image name --io-size arg IO size (in B/K/M/G/T) [default : 4K] # \u6307\u5b9aIO\u5927\u5c0f --io-threads arg ios in flight [default : 16] # \u6307\u5b9a\u5e76\u53d1 --io-total arg total size for IO (in B/K/M/G/T) [default : 1G] # \u6570\u636e\u7684\u5927\u5c0f --io-pattern arg IO pattern (rand, seq, or full-seq) [default : seq] # iops\uff08rand\u4e3a\u968f\u673a\uff0cseq\u987a\u5e8f\uff09 --rw-mix-read arg read proportion in readwrite (<= 100) [default : 50] # --rw-mix-read \u6df7\u5408\u8bfb\u5199\u8bfb\u7684\u5360\u6bd4 --io-type arg IO type (read, write, or readwrite(rw)) # \u7c7b\u578b\uff0c\u8981\u4ee5\u4ec0\u4e48\u65b9\u5f0f\u538b\u6d4b\uff0c\u8bfb\u6216\u8005\u5199 \u793a\u4f8b\uff1a \u00b6 \u6e29\u99a8\u63d0\u793a \u538b\u6d4b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7iostat -x 1 \u5bf9\u78c1\u76d8\u8fdb\u884c\u76d1\u63a7 - ceph osd perf \u53ef\u4ee5\u4f7f\u7528\u8be5\u547d\u4ee4\u67e5\u770bosd\u5ef6\u8fdf\u60c5\u51b5 4K\u968f\u673a\u5199 rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type write bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 5536 5574 .28 22 MiB/s 2 9664 4849 .68 19 MiB/s 3 13776 4603 .46 18 MiB/s 4 17968 4500 .49 18 MiB/s ...... 64 261440 3935 .99 15 MiB/s elapsed: 64 ops: 262144 ops/sec: 4064 .99 bytes/sec: 16 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a4064.99 4K\u968f\u673a\u8bfb [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type read bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 26816 26939 .7 105 MiB/s 2 53728 26925 .8 105 MiB/s 3 81648 27257 .6 106 MiB/s 4 90896 22569 .9 88 MiB/s 5 115328 23087 .2 90 MiB/s 6 142416 23119 .9 90 MiB/s 7 170048 23263 .9 91 MiB/s 8 197104 23091 .1 90 MiB/s 9 225264 27046 .6 106 MiB/s 10 253360 27606 .3 108 MiB/s elapsed: 10 ops: 262144 ops/sec: 25391 .6 bytes/sec: 99 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a25391.6 4K\u968f\u673a\u6df7\u5408\u8bfb\u5199 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type readwrite --rw-mix-read 70 bench type readwrite read:write = 70 :30 io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 12144 12208 .8 48 MiB/s 2 23488 11775 .5 46 MiB/s 3 34624 11562 45 MiB/s 4 45552 11403 .4 45 MiB/s 5 56528 11317 .8 44 MiB/s 6 67664 11104 43 MiB/s 7 78976 11097 .6 43 MiB/s 8 89872 11049 .6 43 MiB/s 9 101216 11132 .8 43 MiB/s 10 112608 11216 44 MiB/s ..... elapsed: 23 ops: 262144 ops/sec: 11012 .6 bytes/sec: 43 MiB/s read_ops: 183730 read_ops/sec: 7718 .43 read_bytes/sec: 30 MiB/s write_ops: 78414 write_ops/sec: 3294 .14 write_bytes/sec: 13 MiB/s 1M\u987a\u5e8f\u5199\uff08\u6d4b\u541e\u5410\u91cf\uff09 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 1M --io-threads 16 --io-total 200M --io-pattern seq --io-type write bench type write io_size 1048576 io_threads 16 bytes 209715200 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 160 175 .298 175 MiB/s elapsed: 1 ops: 200 ops/sec: 136 .986 bytes/sec: 137 MiB/s \u56fa\u6001\u78c1\u76d8\u6d4b\u8bd5","title":"osd \u6027\u80fd\u6d4b\u8bd5"},{"location":"Storage/osd/#osd","text":"\u73af\u5883\u8981\u6c42 \u51c6\u5907\u4e00\u4e2ak8s\u96c6\u7fa4 \u51c6\u5907\u4e00\u4e2aceph\u96c6\u7fa4","title":"osd \u6027\u80fd\u6d4b\u8bd5"},{"location":"Storage/osd/#_1","text":"\u80cc\u666f \u6d4b\u8bd5\u673a\u68b0\u78c1\u76d8\u66f4\u6362\u56fa\u6001\u78c1\u76d8\u540e\uff0cosd\u7684\u8bfb\u5199\u901f\u5ea6","title":"\u673a\u68b0\u78c1\u76d8\u6d4b\u8bd5"},{"location":"Storage/osd/#fio","text":"","title":"\u65b9\u6cd5\u4e00\uff1aFio \u538b\u529b\u6d4b\u8bd5"},{"location":"Storage/osd/#_2","text":"fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest filename : \u538b\u6d4b\u7684\u6587\u4ef6\uff08\u6302\u5728ceph\u7684\u76ee\u5f55\u4e0b\uff09 -iodepth : \u961f\u5217\u6df1\u5ea6 -size : \u6307\u5b9a\u5199\u591a\u5927\u7684\u6570\u636e rw : I/O\u6a21\u5f0f\uff0c\u968f\u673a\u8bfb\u5199\uff0c\u987a\u5e8f\u8bfb\u5199\u7b49\u7b49 bs : I/O block\u5927\u5c0f","title":"\u53c2\u6570\u8bf4\u660e\uff1a"},{"location":"Storage/osd/#_3","text":"4K\u968f\u673a\u5199-iops fio -filename = /data/fio.img -direct = 1 -iodepth 32 -thread -rw = randwrite -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randwrite, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ w ( 6 )][ 100 .0% ][ w = 15 .0MiB/s ][ w = 4089 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 96930 : Mon Aug 1 14 :34:37 2022 write: IOPS = 4104 , BW = 16 .0MiB/s ( 16 .8MB/s )( 963MiB/60054msec ) ; 0 zone resets slat ( nsec ) : min = 1755 , max = 48814k, avg = 10925 .38, stdev = 316419 .53 clat ( msec ) : min = 5 , max = 191 , avg = 46 .76, stdev = 13 .55 lat ( msec ) : min = 5 , max = 191 , avg = 46 .77, stdev = 13 .54 clat percentiles ( msec ) : | 1 .00th =[ 25 ] , 5 .00th =[ 29 ] , 10 .00th =[ 31 ] , 20 .00th =[ 33 ] , | 30 .00th =[ 37 ] , 40 .00th =[ 44 ] , 50 .00th =[ 49 ] , 60 .00th =[ 53 ] , | 70 .00th =[ 55 ] , 80 .00th =[ 58 ] , 90 .00th =[ 61 ] , 95 .00th =[ 64 ] , | 99 .00th =[ 85 ] , 99 .50th =[ 96 ] , 99 .90th =[ 130 ] , 99 .95th =[ 142 ] , | 99 .99th =[ 163 ] bw ( KiB/s ) : min = 14912 , max = 19106 , per = 100 .00%, avg = 16419 .87, stdev = 80 .74, samples = 720 iops : min = 3728 , max = 4776 , avg = 4104 .83, stdev = 20 .18, samples = 720 lat ( msec ) : 10 = 0 .04%, 20 = 0 .38%, 50 = 53 .46%, 100 = 45 .67%, 250 = 0 .45% cpu : usr = 0 .19%, sys = 0 .71%, ctx = 182573 , majf = 1 , minf = 11 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,246515,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 16 .0MiB/s ( 16 .8MB/s ) , 16 .0MiB/s-16.0MiB/s ( 16 .8MB/s-16.8MB/s ) , io = 963MiB ( 1010MB ) , run = 60054 -60054msec Disk stats ( read/write ) : rbd0: ios = 4 /245332, merge = 0 /637, ticks = 81 /11412705, in_queue = 10919116 , util = 57 .39% 4k\u968f\u673a\u8bfb-iops [ ucloud ] root@master0:~# fio -filename = /data/fio2.img -direct = 1 -iodepth 32 -thread -rw = randread -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randread, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads mytest: Laying out IO file ( 1 file / 200MiB ) Jobs: 6 ( f = 6 ) : [ r ( 6 )][ 88 .9% ][ r = 200MiB/s ][ r = 51 .2k IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 101153 : Mon Aug 1 14 :40:36 2022 read: IOPS = 36 .1k, BW = 141MiB/s ( 148MB/s )( 1200MiB/8508msec ) slat ( nsec ) : min = 1151 , max = 14537k, avg = 19572 .33, stdev = 101622 .30 clat ( nsec ) : min = 455 , max = 79415k, avg = 5280191 .11, stdev = 7133988 .93 lat ( usec ) : min = 64 , max = 79418 , avg = 5299 .94, stdev = 7130 .13 clat percentiles ( usec ) : | 1 .00th =[ 363 ] , 5 .00th =[ 865 ] , 10 .00th =[ 1237 ] , 20 .00th =[ 1778 ] , | 30 .00th =[ 2245 ] , 40 .00th =[ 2704 ] , 50 .00th =[ 3163 ] , 60 .00th =[ 3720 ] , | 70 .00th =[ 4490 ] , 80 .00th =[ 5866 ] , 90 .00th =[ 10290 ] , 95 .00th =[ 19268 ] , | 99 .00th =[ 40109 ] , 99 .50th =[ 44303 ] , 99 .90th =[ 52167 ] , 99 .95th =[ 56361 ] , | 99 .99th =[ 65274 ] bw ( KiB/s ) : min = 45752 , max = 234048 , per = 98 .88%, avg = 142805 .42, stdev = 12299 .59, samples = 98 iops : min = 11438 , max = 58512 , avg = 35701 .11, stdev = 3074 .86, samples = 98 lat ( nsec ) : 500 = 0 .01% lat ( usec ) : 20 = 0 .01%, 50 = 0 .01%, 100 = 0 .03%, 250 = 0 .38%, 500 = 1 .39% lat ( usec ) : 750 = 2 .05%, 1000 = 2 .80% lat ( msec ) : 2 = 18 .03%, 4 = 39 .21%, 10 = 25 .81%, 20 = 5 .53%, 50 = 4 .61% lat ( msec ) : 100 = 0 .14% cpu : usr = 1 .03%, sys = 3 .66%, ctx = 239175 , majf = 0 , minf = 198 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 307200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 141MiB/s ( 148MB/s ) , 141MiB/s-141MiB/s ( 148MB/s-148MB/s ) , io = 1200MiB ( 1258MB ) , run = 8508 -8508msec Disk stats ( read/write ) : rbd0: ios = 298435 /0, merge = 1141 /0, ticks = 1470086 /0, in_queue = 887248 , util = 98 .83% 4k\u968f\u673a\u8bfb\u5199-iops [ ucloud ] root@master0:~# fio -filename = /data/fio3.img -direct = 1 -iodepth 32 -thread -rw = randrw -rwmixread = 70 -ioengine = libaio -bs = 4k -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = randrw, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 6 ( f = 6 ) : [ m ( 6 )][ 100 .0% ][ r = 26 .7MiB/s,w = 11 .6MiB/s ][ r = 6835 ,w = 2976 IOPS ][ eta 00m:00s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 104592 : Mon Aug 1 14 :45:40 2022 read: IOPS = 6434 , BW = 25 .1MiB/s ( 26 .4MB/s )( 838MiB/33355msec ) slat ( nsec ) : min = 1268 , max = 512155 , avg = 5128 .49, stdev = 4908 .50 clat ( usec ) : min = 108 , max = 237284 , avg = 17218 .47, stdev = 13454 .22 lat ( usec ) : min = 112 , max = 237286 , avg = 17223 .74, stdev = 13454 .22 clat percentiles ( usec ) : | 1 .00th =[ 857 ] , 5 .00th =[ 1696 ] , 10 .00th =[ 2507 ] , 20 .00th =[ 4293 ] , | 30 .00th =[ 6980 ] , 40 .00th =[ 12256 ] , 50 .00th =[ 17695 ] , 60 .00th =[ 20579 ] , | 70 .00th =[ 22938 ] , 80 .00th =[ 25560 ] , 90 .00th =[ 31589 ] , 95 .00th =[ 41157 ] , | 99 .00th =[ 55313 ] , 99 .50th =[ 64226 ] , 99 .90th =[ 94897 ] , 99 .95th =[ 154141 ] , | 99 .99th =[ 227541 ] bw ( KiB/s ) : min = 18128 , max = 31360 , per = 99 .94%, avg = 25724 .41, stdev = 445 .55, samples = 396 iops : min = 4532 , max = 7840 , avg = 6430 .97, stdev = 111 .38, samples = 396 write: IOPS = 2775 , BW = 10 .8MiB/s ( 11 .4MB/s )( 362MiB/33355msec ) ; 0 zone resets slat ( nsec ) : min = 1519 , max = 586139 , avg = 5581 .99, stdev = 5697 .03 clat ( usec ) : min = 735 , max = 234047 , avg = 29153 .24, stdev = 13547 .74 lat ( usec ) : min = 740 , max = 234050 , avg = 29158 .97, stdev = 13547 .75 clat percentiles ( msec ) : | 1 .00th =[ 5 ] , 5 .00th =[ 14 ] , 10 .00th =[ 19 ] , 20 .00th =[ 21 ] , | 30 .00th =[ 23 ] , 40 .00th =[ 24 ] , 50 .00th =[ 26 ] , 60 .00th =[ 28 ] , | 70 .00th =[ 33 ] , 80 .00th =[ 40 ] , 90 .00th =[ 46 ] , 95 .00th =[ 52 ] , | 99 .00th =[ 67 ] , 99 .50th =[ 77 ] , 99 .90th =[ 153 ] , 99 .95th =[ 215 ] , | 99 .99th =[ 230 ] bw ( KiB/s ) : min = 7264 , max = 13640 , per = 99 .93%, avg = 11092 .62, stdev = 204 .64, samples = 396 iops : min = 1816 , max = 3410 , avg = 2773 .11, stdev = 51 .15, samples = 396 lat ( usec ) : 250 = 0 .01%, 500 = 0 .12%, 750 = 0 .35%, 1000 = 0 .56% lat ( msec ) : 2 = 3 .77%, 4 = 8 .37%, 10 = 13 .38%, 20 = 18 .51%, 50 = 52 .05% lat ( msec ) : 100 = 2 .75%, 250 = 0 .11% cpu : usr = 0 .42%, sys = 1 .37%, ctx = 262129 , majf = 1 , minf = 6 IO depths : 1 = 0 .1%, 2 = 0 .1%, 4 = 0 .1%, 8 = 0 .1%, 16 = 0 .1%, 32 = 99 .9%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .1%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 214635 ,92565,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : READ: bw = 25 .1MiB/s ( 26 .4MB/s ) , 25 .1MiB/s-25.1MiB/s ( 26 .4MB/s-26.4MB/s ) , io = 838MiB ( 879MB ) , run = 33355 -33355msec WRITE: bw = 10 .8MiB/s ( 11 .4MB/s ) , 10 .8MiB/s-10.8MiB/s ( 11 .4MB/s-11.4MB/s ) , io = 362MiB ( 379MB ) , run = 33355 -33355msec Disk stats ( read/write ) : rbd0: ios = 212275 /91760, merge = 1000 /200, ticks = 3650146 /2659754, in_queue = 5701392 , util = 86 .01% 1M\u987a\u5e8f\u5199-\u541e\u5410 [ ucloud ] root@master0:~# fio -filename = /data/fio4.img -direct = 1 -iodepth 32 -thread -rw = write -ioengine = libaio -bs = 1M -size = 200m -numjobs = 6 -runtime = 60 -group_reporting -name = mytest mytest: ( g = 0 ) : rw = write, bs =( R ) 1024KiB-1024KiB, ( W ) 1024KiB-1024KiB, ( T ) 1024KiB-1024KiB, ioengine = libaio, iodepth = 32 ... fio-3.16 Starting 6 threads Jobs: 3 ( f = 3 ) : [ _ ( 2 ) ,W ( 2 ) ,_ ( 1 ) ,W ( 1 )][ 90 .0% ][ w = 177MiB/s ][ w = 177 IOPS ][ eta 00m:01s ] mytest: ( groupid = 0 , jobs = 6 ) : err = 0 : pid = 107996 : Mon Aug 1 14 :51:00 2022 write: IOPS = 131 , BW = 131MiB/s ( 138MB/s )( 1200MiB/9131msec ) ; 0 zone resets slat ( usec ) : min = 41 , max = 13194 , avg = 208 .41, stdev = 756 .44 clat ( msec ) : min = 20 , max = 3855 , avg = 1392 .05, stdev = 816 .66 lat ( msec ) : min = 20 , max = 3855 , avg = 1392 .26, stdev = 816 .65 clat percentiles ( msec ) : | 1 .00th =[ 284 ] , 5 .00th =[ 351 ] , 10 .00th =[ 435 ] , 20 .00th =[ 625 ] , | 30 .00th =[ 885 ] , 40 .00th =[ 1099 ] , 50 .00th =[ 1234 ] , 60 .00th =[ 1418 ] , | 70 .00th =[ 1620 ] , 80 .00th =[ 1938 ] , 90 .00th =[ 2769 ] , 95 .00th =[ 2903 ] , | 99 .00th =[ 3608 ] , 99 .50th =[ 3675 ] , 99 .90th =[ 3809 ] , 99 .95th =[ 3842 ] , | 99 .99th =[ 3842 ] bw ( KiB/s ) : min = 24554 , max = 273468 , per = 99 .58%, avg = 134013 .71, stdev = 11587 .87, samples = 93 iops : min = 22 , max = 267 , avg = 130 .28, stdev = 11 .35, samples = 93 lat ( msec ) : 50 = 0 .25%, 250 = 0 .33%, 500 = 13 .33%, 750 = 11 .50%, 1000 = 10 .08% lat ( msec ) : 2000 = 44 .92%, > = 2000 = 19 .58% cpu : usr = 0 .18%, sys = 0 .09%, ctx = 792 , majf = 1 , minf = 6 IO depths : 1 = 0 .5%, 2 = 1 .0%, 4 = 2 .0%, 8 = 4 .0%, 16 = 8 .0%, 32 = 84 .5%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 99 .4%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .6%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,1200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 32 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 1200MiB ( 1258MB ) , run = 9131 -9131msec Disk stats ( read/write ) : rbd0: ios = 0 /887, merge = 0 /280, ticks = 0 /1112233, in_queue = 1110460 , util = 16 .67%","title":"\u793a\u4f8b\uff1a"},{"location":"Storage/osd/#rbd-bench","text":"","title":"\u65b9\u6cd5\u4e8c\uff1aRBD bench \u538b\u529b\u6d4b\u8bd5"},{"location":"Storage/osd/#_4","text":"[ ucloud ] root@master0:~# rbd help bench usage : rbd bench [--pool <pool>] [--namespace <namespace>] [--image <image>] [--io-size <io-size>] [--io-threads <io-threads>] [--io-total <io-total>] [--io-pattern <io-pattern>] [--rw-mix-read <rw-mix-read>] --io-type <io-type> <image-spec> Simple benchmark. Positional arguments <image-spec> image specification (example : [ <pool-name>/ [ <namespace>/ ]] <image-name>) Optional arguments -p [ --pool ] arg pool name # \u6307\u5b9apool\u7684\u540d\u79f0 --namespace arg namespace name # \u6307\u5b9anamespace --image arg image name --io-size arg IO size (in B/K/M/G/T) [default : 4K] # \u6307\u5b9aIO\u5927\u5c0f --io-threads arg ios in flight [default : 16] # \u6307\u5b9a\u5e76\u53d1 --io-total arg total size for IO (in B/K/M/G/T) [default : 1G] # \u6570\u636e\u7684\u5927\u5c0f --io-pattern arg IO pattern (rand, seq, or full-seq) [default : seq] # iops\uff08rand\u4e3a\u968f\u673a\uff0cseq\u987a\u5e8f\uff09 --rw-mix-read arg read proportion in readwrite (<= 100) [default : 50] # --rw-mix-read \u6df7\u5408\u8bfb\u5199\u8bfb\u7684\u5360\u6bd4 --io-type arg IO type (read, write, or readwrite(rw)) # \u7c7b\u578b\uff0c\u8981\u4ee5\u4ec0\u4e48\u65b9\u5f0f\u538b\u6d4b\uff0c\u8bfb\u6216\u8005\u5199","title":"\u53c2\u6570\u8bf4\u660e\uff1a"},{"location":"Storage/osd/#_5","text":"\u6e29\u99a8\u63d0\u793a \u538b\u6d4b\u65f6\uff0c\u53ef\u4ee5\u901a\u8fc7iostat -x 1 \u5bf9\u78c1\u76d8\u8fdb\u884c\u76d1\u63a7 - ceph osd perf \u53ef\u4ee5\u4f7f\u7528\u8be5\u547d\u4ee4\u67e5\u770bosd\u5ef6\u8fdf\u60c5\u51b5 4K\u968f\u673a\u5199 rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type write bench type write io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 5536 5574 .28 22 MiB/s 2 9664 4849 .68 19 MiB/s 3 13776 4603 .46 18 MiB/s 4 17968 4500 .49 18 MiB/s ...... 64 261440 3935 .99 15 MiB/s elapsed: 64 ops: 262144 ops/sec: 4064 .99 bytes/sec: 16 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a4064.99 4K\u968f\u673a\u8bfb [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type read bench type read io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 26816 26939 .7 105 MiB/s 2 53728 26925 .8 105 MiB/s 3 81648 27257 .6 106 MiB/s 4 90896 22569 .9 88 MiB/s 5 115328 23087 .2 90 MiB/s 6 142416 23119 .9 90 MiB/s 7 170048 23263 .9 91 MiB/s 8 197104 23091 .1 90 MiB/s 9 225264 27046 .6 106 MiB/s 10 253360 27606 .3 108 MiB/s elapsed: 10 ops: 262144 ops/sec: 25391 .6 bytes/sec: 99 MiB/s # \u6d4b\u8bd5\u51fa\u5f53\u524d\u7684iops\u4e3a25391.6 4K\u968f\u673a\u6df7\u5408\u8bfb\u5199 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 4K --io-threads 16 --io-total 1G --io-pattern rand --io-type readwrite --rw-mix-read 70 bench type readwrite read:write = 70 :30 io_size 4096 io_threads 16 bytes 1073741824 pattern random SEC OPS OPS/SEC BYTES/SEC 1 12144 12208 .8 48 MiB/s 2 23488 11775 .5 46 MiB/s 3 34624 11562 45 MiB/s 4 45552 11403 .4 45 MiB/s 5 56528 11317 .8 44 MiB/s 6 67664 11104 43 MiB/s 7 78976 11097 .6 43 MiB/s 8 89872 11049 .6 43 MiB/s 9 101216 11132 .8 43 MiB/s 10 112608 11216 44 MiB/s ..... elapsed: 23 ops: 262144 ops/sec: 11012 .6 bytes/sec: 43 MiB/s read_ops: 183730 read_ops/sec: 7718 .43 read_bytes/sec: 30 MiB/s write_ops: 78414 write_ops/sec: 3294 .14 write_bytes/sec: 13 MiB/s 1M\u987a\u5e8f\u5199\uff08\u6d4b\u541e\u5410\u91cf\uff09 [ ucloud ] root@master0:~# rbd bench rook/rook-rbd.img --io-size 1M --io-threads 16 --io-total 200M --io-pattern seq --io-type write bench type write io_size 1048576 io_threads 16 bytes 209715200 pattern sequential SEC OPS OPS/SEC BYTES/SEC 1 160 175 .298 175 MiB/s elapsed: 1 ops: 200 ops/sec: 136 .986 bytes/sec: 137 MiB/s \u56fa\u6001\u78c1\u76d8\u6d4b\u8bd5","title":"\u793a\u4f8b\uff1a"},{"location":"Storage/pv-rep/","text":"k8s pv \u548c pvc \u00b6 \u76ee\u524dPV\u7684\u63d0\u4f9b\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a\u9759\u6001\u6216\u52a8\u6001\u3002(pv \u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236) \u9759\u6001PV\u7531\u7ba1\u7406\u5458\u63d0\u524d\u521b\u5efa\uff0c\u52a8\u6001PV\u65e0\u9700\u63d0\u524d\u521b\u5efa\uff0c\u53ea\u9700\u6307\u5b9aPVC\u7684StorageClasse\u5373\u53ef \u56de\u6536\u7b56\u7565 \u00b6 Retain\uff1a\u4fdd\u7559\uff0c\u8be5\u7b56\u7565\u5141\u8bb8\u624b\u52a8\u56de\u6536\u8d44\u6e90\uff0c\u5f53\u5220\u9664PVC\u65f6\uff0cPV\u4ecd\u7136\u5b58\u5728\uff0cPV\u88ab\u89c6\u4e3a\u5df2\u91ca\u653e\uff0c\u7ba1\u7406\u5458\u53ef\u4ee5\u624b\u52a8\u56de\u6536\u5377\u3002 Recycle\uff1a\u56de\u6536\uff0c\u5982\u679cVolume\u63d2\u4ef6\u652f\u6301\uff0cRecycle\u7b56\u7565\u4f1a\u5bf9\u5377\u6267\u884crm -rf\u6e05\u7406\u8be5PV\uff0c\u5e76\u4f7f\u5176\u53ef\u7528\u4e8e\u4e0b\u4e00\u4e2a\u65b0\u7684PVC\uff0c\u4f46\u662f\u672c\u7b56\u7565\u5c06\u6765\u4f1a\u88ab\u5f03\u7528\uff0c\u76ee\u524d\u53ea\u6709NFS\u548cHostPath\u652f\u6301\u8be5\u7b56\u7565\u3002\uff08\u4e0d\u63a8\u8350\u4f7f\u7528\uff09 Delete\uff1a\u5220\u9664\uff0c\u5982\u679cVolume\u63d2\u4ef6\u652f\u6301\uff0c\u5220\u9664PVC\u65f6\u4f1a\u540c\u65f6\u5220\u9664PV\uff0c\u52a8\u6001\u5377\u9ed8\u8ba4\u4e3aDelete\uff0c\u76ee\u524d\u652f\u6301Delete\u7684\u5b58\u50a8\u540e\u7aef\u5305\u62ecAWS EBS, GCE PD, Azure Disk, or OpenStack Cinder\u7b49\u3002 \u53ef\u4ee5\u901a\u8fc7persistentVolumeReclaimPolicy: Recycle\u5b57\u6bb5\u914d\u7f6e \u5b98\u7f51\u6587\u732e: pv \u56de\u6536\u7b56\u7565 pv \u7684\u72b6\u6001 \u00b6 Available\uff1a\u53ef\u7528\uff0c\u6ca1\u6709\u88abPVC\u7ed1\u5b9a\u7684\u7a7a\u95f2\u8d44\u6e90\u3002 Bound\uff1a\u5df2\u7ed1\u5b9a\uff0c\u5df2\u7ecf\u88abPVC\u7ed1\u5b9a\u3002 Released\uff1a\u5df2\u91ca\u653e\uff0cPVC\u88ab\u5220\u9664\uff0c\u4f46\u662f\u8d44\u6e90\u8fd8\u672a\u88ab\u91cd\u65b0\u4f7f\u7528\u3002 Failed\uff1a\u5931\u8d25\uff0c\u81ea\u52a8\u56de\u6536\u5931\u8d25\u3002 pv/pvc \u7684\u521b\u5efa \u00b6 \u521b\u5efaPV apiVersion : v1 kind : PersistentVolume # \u8d44\u6e90\u5bf9\u8c61\u7684\u540d\u79f0\u4e3a pv metadata : name : pv-nfs # pv \u7684\u540d\u79f0\u4e3a\u8bbe\u7f6e\u4e3a pv-nfs spec : capacity : # capacity\uff1a\u5bb9\u91cf\u914d\u7f6e storage : 5Gi volumeMode : Filesystem # \u5377\u7684\u6a21\u5f0f\uff0c\u76ee\u524d\u652f\u6301Filesystem\uff08\u6587\u4ef6\u7cfb\u7edf\uff09 \u548c Block\uff08\u5757\uff09\uff0c\u5176\u4e2dBlock\u7c7b\u578b\u9700\u8981\u540e\u7aef\u5b58\u50a8\u652f\u6301\uff0c\u9ed8\u8ba4\u4e3a\u6587\u4ef6\u7cfb\u7edf accessModes : # \u8be5PV\u7684\u8bbf\u95ee\u6a21\u5f0f(\u8fd9\u91cc\u53ef\u4ee5\u5199\u591a\u4e2a\u6a21\u5f0f) - ReadWriteMany persistentVolumeReclaimPolicy : Recycle # \u56de\u6536\u7b56\u7565 storageClassName : nfs-slow # pvc \u60f3\u7ed1\u5b9a pv,\u9700\u8981\u6307\u5b9astorageClassName\u8fd9\u4e2a\u540d\u79f0 nfs : path : /nfs/share server : 192.168.159.201 \u521b\u5efaPVC root@ubuntu:/opt/k8s-data/pv-pvc# cat 3-pvc.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : nfs # pvc\u7684\u540d\u79f0 spec : storageClassName : nfs-slow # pv \u7684storageClassName ,\u5fc5\u987b\u4e00\u81f4 accessModes : - ReadWriteMany # \u5fc5\u987b\u4e00\u81f4 resources : requests : storage : 100Gi \u8bbf\u95ee\u6a21\u5f0f: ReadWriteOnce:\u53ef\u8bfb\u53ef\u5199\uff0c\u4f46\u53ea\u652f\u6301\u88ab\u5355\u4e2anode\u6302\u8f7d\u3002\u8be5\u8bbf\u95ee\u6a21\u5f0f\u7ea6\u675f\u4ec5\u6709\u4e00\u4e2anode\u8282\u70b9\u53ef\u4ee5\u8bbf\u95eepvc\u3002\u6362\u53e5\u8bdd\u6765\u8bf4\uff0c\u540c\u4e00node\u8282\u70b9\u7684\u4e0d\u540cpod\u662f\u53ef\u4ee5\u5bf9\u540c\u4e00pvc\u8fdb\u884c\u8bfb\u5199\u7684 ReadOnlyMany:\u53ef\u4ee5\u4ee5\u53ea\u8bfb\u7684\u65b9\u5f0f\u88ab\u591a\u4e2anode\u6302\u8f7d ReadWriteMany:\u53ef\u4ee5\u4ee5\u8bfb\u5199\u7684\u65b9\u5f0f\u88ab\u591a\u4e2anode\u6302\u8f7d ReadWriteOncePod\uff1a\u4ec5\u6709\u4e00\u4e2aPod\u53ef\u4ee5\u8bbf\u95ee\u4f7f\u7528\u8be5pvc\uff08Kubernetes >= v1.22\uff09\u5f53\u4f60\u521b\u5efa\u4e00\u4e2a\u5e26\u6709pvc\u8bbf\u95ee\u6a21\u5f0f\u4e3aReadWriteOncePod\u7684Pod A\u65f6\uff0cKubernetes\u786e\u4fdd\u6574\u4e2a\u96c6\u7fa4\u5185\u53ea\u6709\u4e00\u4e2aPod\u53ef\u4ee5\u8bfb\u5199\u8be5PVC\u3002\u6b64\u65f6\u5982\u679c\u4f60\u521b\u5efaPod B\u5e76\u5f15\u7528\u4e86\u4e0ePod A\u76f8\u540c\u7684PVC(ReadWriteOncePod)\u65f6\uff0c\u90a3\u4e48Pod B\u4f1a\u7531\u4e8e\u8be5pvc\u88abPod A\u5f15\u7528\u800c\u542f\u52a8\u5931\u8d25\u3002 \u4e0a\u6d77\u4ea4\u5927\u4fee\u590dpv\u6b65\u9aa4 \u00b6 \u8fd9\u91cc\u4e3b\u8981\u662fkubeadm\u90e8\u7f72\u7684k8s \u73b0\u8c61 \u00b6 PV\u4e0d\u77e5\u9053\u56e0\u4e3a\u4ec0\u4e48\u539f\u56e0\u5904\u4e8eTerminating. \u4fee\u590dPV \u00b6 \u6587\u7ae0\u53c2\u8003\uff1ahttps://github.com/jianz/k8s-reset-terminating-pv \u7aef\u53e3\u8f6c\u53d1\u5230\u672c\u5730 \u00b6 kubectl port-forward pods/etcd-master 2379 :2379 -n kube-system \u4fee\u590dpv \u00b6 ./resetpv-linux-x86-64 --k8s-key-prefix registry pvc-bd426570-7fc2-4270-bd41-9512262b0790 --etcd-ca = /etc/kubernetes/pki/etcd/ca.crt --etcd-cert = /etc/kubernetes/pki/etcd/server.crt --etcd-key = /etc/kubernetes/pki/etcd/server.key Warning pvc-bd426570-7fc2-4270-bd41-9512262b0790 \u662f\u5904\u4e8eTerminating\u7684pv\uff0c\u4fee\u590d\u5b8c\u6210pv\u5904\u4e8eBound\u72b6\u6001 k8s\u5347\u7ea7\u5bfc\u81f4sc\u65e0\u6cd5\u6b63\u5e38work \u00b6 \u53c2\u8003\u5730\u5740: sc\u65e0\u6cd5\u6b63\u5e38work Bug Using Kubernetes v1.20.0, getting \"unexpected error getting claim reference: selfLink was empty, can't make reference \u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u7f16\u8f91 /etc/kubernetes/manifests/kube-apiserver.yaml \u6dfb\u52a0\u8fd9\u4e00\u884c\uff1a ---feature-gates = RemoveSelfLink = false \u4eb2\u548c\u6027\u5bfc\u81f4rook-ceph\u65e0\u6cd5\u6b63\u5e38\u4f7f\u7528 \u00b6 Bug unable to get monitor info from DNS SRV with service name: ceph-mon \u53d6\u6d88\u4eb2\u548c\u6027\u7684\u914d\u7f6e","title":"pv/pvc\u7b80\u4ecb"},{"location":"Storage/pv-rep/#k8s-pv-pvc","text":"\u76ee\u524dPV\u7684\u63d0\u4f9b\u65b9\u5f0f\u6709\u4e24\u79cd\uff1a\u9759\u6001\u6216\u52a8\u6001\u3002(pv \u6ca1\u6709\u547d\u540d\u7a7a\u95f4\u9650\u5236) \u9759\u6001PV\u7531\u7ba1\u7406\u5458\u63d0\u524d\u521b\u5efa\uff0c\u52a8\u6001PV\u65e0\u9700\u63d0\u524d\u521b\u5efa\uff0c\u53ea\u9700\u6307\u5b9aPVC\u7684StorageClasse\u5373\u53ef","title":"k8s pv \u548c pvc"},{"location":"Storage/pv-rep/#_1","text":"Retain\uff1a\u4fdd\u7559\uff0c\u8be5\u7b56\u7565\u5141\u8bb8\u624b\u52a8\u56de\u6536\u8d44\u6e90\uff0c\u5f53\u5220\u9664PVC\u65f6\uff0cPV\u4ecd\u7136\u5b58\u5728\uff0cPV\u88ab\u89c6\u4e3a\u5df2\u91ca\u653e\uff0c\u7ba1\u7406\u5458\u53ef\u4ee5\u624b\u52a8\u56de\u6536\u5377\u3002 Recycle\uff1a\u56de\u6536\uff0c\u5982\u679cVolume\u63d2\u4ef6\u652f\u6301\uff0cRecycle\u7b56\u7565\u4f1a\u5bf9\u5377\u6267\u884crm -rf\u6e05\u7406\u8be5PV\uff0c\u5e76\u4f7f\u5176\u53ef\u7528\u4e8e\u4e0b\u4e00\u4e2a\u65b0\u7684PVC\uff0c\u4f46\u662f\u672c\u7b56\u7565\u5c06\u6765\u4f1a\u88ab\u5f03\u7528\uff0c\u76ee\u524d\u53ea\u6709NFS\u548cHostPath\u652f\u6301\u8be5\u7b56\u7565\u3002\uff08\u4e0d\u63a8\u8350\u4f7f\u7528\uff09 Delete\uff1a\u5220\u9664\uff0c\u5982\u679cVolume\u63d2\u4ef6\u652f\u6301\uff0c\u5220\u9664PVC\u65f6\u4f1a\u540c\u65f6\u5220\u9664PV\uff0c\u52a8\u6001\u5377\u9ed8\u8ba4\u4e3aDelete\uff0c\u76ee\u524d\u652f\u6301Delete\u7684\u5b58\u50a8\u540e\u7aef\u5305\u62ecAWS EBS, GCE PD, Azure Disk, or OpenStack Cinder\u7b49\u3002 \u53ef\u4ee5\u901a\u8fc7persistentVolumeReclaimPolicy: Recycle\u5b57\u6bb5\u914d\u7f6e \u5b98\u7f51\u6587\u732e: pv \u56de\u6536\u7b56\u7565","title":"\u56de\u6536\u7b56\u7565"},{"location":"Storage/pv-rep/#pv","text":"Available\uff1a\u53ef\u7528\uff0c\u6ca1\u6709\u88abPVC\u7ed1\u5b9a\u7684\u7a7a\u95f2\u8d44\u6e90\u3002 Bound\uff1a\u5df2\u7ed1\u5b9a\uff0c\u5df2\u7ecf\u88abPVC\u7ed1\u5b9a\u3002 Released\uff1a\u5df2\u91ca\u653e\uff0cPVC\u88ab\u5220\u9664\uff0c\u4f46\u662f\u8d44\u6e90\u8fd8\u672a\u88ab\u91cd\u65b0\u4f7f\u7528\u3002 Failed\uff1a\u5931\u8d25\uff0c\u81ea\u52a8\u56de\u6536\u5931\u8d25\u3002","title":"pv \u7684\u72b6\u6001"},{"location":"Storage/pv-rep/#pvpvc","text":"\u521b\u5efaPV apiVersion : v1 kind : PersistentVolume # \u8d44\u6e90\u5bf9\u8c61\u7684\u540d\u79f0\u4e3a pv metadata : name : pv-nfs # pv \u7684\u540d\u79f0\u4e3a\u8bbe\u7f6e\u4e3a pv-nfs spec : capacity : # capacity\uff1a\u5bb9\u91cf\u914d\u7f6e storage : 5Gi volumeMode : Filesystem # \u5377\u7684\u6a21\u5f0f\uff0c\u76ee\u524d\u652f\u6301Filesystem\uff08\u6587\u4ef6\u7cfb\u7edf\uff09 \u548c Block\uff08\u5757\uff09\uff0c\u5176\u4e2dBlock\u7c7b\u578b\u9700\u8981\u540e\u7aef\u5b58\u50a8\u652f\u6301\uff0c\u9ed8\u8ba4\u4e3a\u6587\u4ef6\u7cfb\u7edf accessModes : # \u8be5PV\u7684\u8bbf\u95ee\u6a21\u5f0f(\u8fd9\u91cc\u53ef\u4ee5\u5199\u591a\u4e2a\u6a21\u5f0f) - ReadWriteMany persistentVolumeReclaimPolicy : Recycle # \u56de\u6536\u7b56\u7565 storageClassName : nfs-slow # pvc \u60f3\u7ed1\u5b9a pv,\u9700\u8981\u6307\u5b9astorageClassName\u8fd9\u4e2a\u540d\u79f0 nfs : path : /nfs/share server : 192.168.159.201 \u521b\u5efaPVC root@ubuntu:/opt/k8s-data/pv-pvc# cat 3-pvc.yaml kind : PersistentVolumeClaim apiVersion : v1 metadata : name : nfs # pvc\u7684\u540d\u79f0 spec : storageClassName : nfs-slow # pv \u7684storageClassName ,\u5fc5\u987b\u4e00\u81f4 accessModes : - ReadWriteMany # \u5fc5\u987b\u4e00\u81f4 resources : requests : storage : 100Gi \u8bbf\u95ee\u6a21\u5f0f: ReadWriteOnce:\u53ef\u8bfb\u53ef\u5199\uff0c\u4f46\u53ea\u652f\u6301\u88ab\u5355\u4e2anode\u6302\u8f7d\u3002\u8be5\u8bbf\u95ee\u6a21\u5f0f\u7ea6\u675f\u4ec5\u6709\u4e00\u4e2anode\u8282\u70b9\u53ef\u4ee5\u8bbf\u95eepvc\u3002\u6362\u53e5\u8bdd\u6765\u8bf4\uff0c\u540c\u4e00node\u8282\u70b9\u7684\u4e0d\u540cpod\u662f\u53ef\u4ee5\u5bf9\u540c\u4e00pvc\u8fdb\u884c\u8bfb\u5199\u7684 ReadOnlyMany:\u53ef\u4ee5\u4ee5\u53ea\u8bfb\u7684\u65b9\u5f0f\u88ab\u591a\u4e2anode\u6302\u8f7d ReadWriteMany:\u53ef\u4ee5\u4ee5\u8bfb\u5199\u7684\u65b9\u5f0f\u88ab\u591a\u4e2anode\u6302\u8f7d ReadWriteOncePod\uff1a\u4ec5\u6709\u4e00\u4e2aPod\u53ef\u4ee5\u8bbf\u95ee\u4f7f\u7528\u8be5pvc\uff08Kubernetes >= v1.22\uff09\u5f53\u4f60\u521b\u5efa\u4e00\u4e2a\u5e26\u6709pvc\u8bbf\u95ee\u6a21\u5f0f\u4e3aReadWriteOncePod\u7684Pod A\u65f6\uff0cKubernetes\u786e\u4fdd\u6574\u4e2a\u96c6\u7fa4\u5185\u53ea\u6709\u4e00\u4e2aPod\u53ef\u4ee5\u8bfb\u5199\u8be5PVC\u3002\u6b64\u65f6\u5982\u679c\u4f60\u521b\u5efaPod B\u5e76\u5f15\u7528\u4e86\u4e0ePod A\u76f8\u540c\u7684PVC(ReadWriteOncePod)\u65f6\uff0c\u90a3\u4e48Pod B\u4f1a\u7531\u4e8e\u8be5pvc\u88abPod A\u5f15\u7528\u800c\u542f\u52a8\u5931\u8d25\u3002","title":"pv/pvc \u7684\u521b\u5efa"},{"location":"Storage/pv-rep/#pv_1","text":"\u8fd9\u91cc\u4e3b\u8981\u662fkubeadm\u90e8\u7f72\u7684k8s","title":"\u4e0a\u6d77\u4ea4\u5927\u4fee\u590dpv\u6b65\u9aa4"},{"location":"Storage/pv-rep/#_2","text":"PV\u4e0d\u77e5\u9053\u56e0\u4e3a\u4ec0\u4e48\u539f\u56e0\u5904\u4e8eTerminating.","title":"\u73b0\u8c61"},{"location":"Storage/pv-rep/#pv_2","text":"\u6587\u7ae0\u53c2\u8003\uff1ahttps://github.com/jianz/k8s-reset-terminating-pv","title":"\u4fee\u590dPV"},{"location":"Storage/pv-rep/#_3","text":"kubectl port-forward pods/etcd-master 2379 :2379 -n kube-system","title":"\u7aef\u53e3\u8f6c\u53d1\u5230\u672c\u5730"},{"location":"Storage/pv-rep/#pv_3","text":"./resetpv-linux-x86-64 --k8s-key-prefix registry pvc-bd426570-7fc2-4270-bd41-9512262b0790 --etcd-ca = /etc/kubernetes/pki/etcd/ca.crt --etcd-cert = /etc/kubernetes/pki/etcd/server.crt --etcd-key = /etc/kubernetes/pki/etcd/server.key Warning pvc-bd426570-7fc2-4270-bd41-9512262b0790 \u662f\u5904\u4e8eTerminating\u7684pv\uff0c\u4fee\u590d\u5b8c\u6210pv\u5904\u4e8eBound\u72b6\u6001","title":"\u4fee\u590dpv"},{"location":"Storage/pv-rep/#k8sscwork","text":"\u53c2\u8003\u5730\u5740: sc\u65e0\u6cd5\u6b63\u5e38work Bug Using Kubernetes v1.20.0, getting \"unexpected error getting claim reference: selfLink was empty, can't make reference \u5f53\u524d\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u7f16\u8f91 /etc/kubernetes/manifests/kube-apiserver.yaml \u6dfb\u52a0\u8fd9\u4e00\u884c\uff1a ---feature-gates = RemoveSelfLink = false","title":"k8s\u5347\u7ea7\u5bfc\u81f4sc\u65e0\u6cd5\u6b63\u5e38work"},{"location":"Storage/pv-rep/#rook-ceph","text":"Bug unable to get monitor info from DNS SRV with service name: ceph-mon \u53d6\u6d88\u4eb2\u548c\u6027\u7684\u914d\u7f6e","title":"\u4eb2\u548c\u6027\u5bfc\u81f4rook-ceph\u65e0\u6cd5\u6b63\u5e38\u4f7f\u7528"},{"location":"Storage/rook-ceph-update/","text":"rook-ceph \u7248\u672c\u5347\u7ea7 \u00b6 https://blog.51cto.com/foxhound/2553979","title":"rook-ceph \u7248\u672c\u5347\u7ea7"},{"location":"Storage/rook-ceph-update/#rook-ceph","text":"https://blog.51cto.com/foxhound/2553979","title":"rook-ceph \u7248\u672c\u5347\u7ea7"},{"location":"Storage/rook-ceph/","text":"rook-ceph\u7b80\u4ecb\u548c\u90e8\u7f72 \u00b6 rook-ceph\u7b80\u4ecb \u00b6 \u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u8c61\u5b58\u50a8\uff0c\u5757\u8bbe\u5907\uff0c\u6587\u4ef6\u7cfb\u7edf \u5757\u5b58\u50a8 CephFs \u5bf9\u8c61\u5b58\u50a8 ceph \u7684\u7248\u672c\u5386\u53f2 \u00b6 x.0.z - \u5f00\u53d1\u7248 x.1.z - \u5019\u9009\u7248 x.2.z - \u7a33\u5b9a\uff0c\u4fee\u6b63\u7248 ceph\u96c6\u7fa4\u89d2\u8272\u5b9a\u4e49 \u00b6 \u6ce8\u610f ceph\u96c6\u7fa4\u7684osd\u8282\u70b9\u4e00\u822c\u4fdd\u8bc1>=3\u4e2a\uff0c\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u9ad8\u53ef\u7528\u6027\u3002 mon : ceph \u76d1\u89c6\u5668,\u5728\u4e00\u4e2a\u4e3b\u673a\u4e0a\u8fd0\u884c\u7684\u4e00\u4e2a\u5b88\u62a4\u8fdb\u7a0b\uff0c\u7528\u4e8e\u7ef4\u62a4\u96c6\u7fa4\u72b6\u6001\u6620\u5c04\u5173\u7cfb mgr : \u8d1f\u8d23\u8ddf\u8e2a\u8fd0\u884c\u65f6\u6307\u6807\u548cceph\u96c6\u7fa4\u7684\u5f53\u524d\u72b6\u6001 osd : \u78c1\u76d8\uff08\u771f\u6b63\u5b58\u50a8\u6570\u636e\u7684\u5730\u65b9\uff09 ceph \u9762\u8bd5\u9898 ceph rook-ceph\u90e8\u7f72 \u00b6 \u73af\u5883\u8981\u6c42 \u4e00\u4e2ak8s\u96c6\u7fa4\uff0cnode\u8282\u70b9\u6700\u5c11\u4e09\u4e2a\u8282\u70b9 mon: 8C 8G/200G 16C 16g/32-200G \u666e\u901a\u6d4b\u8bd5\u90e8\u7f72\uff1a \u00b6 \u90e8\u7f72crds\uff0ccommon\uff0coperator \u00b6 [ucloud] root@master0:~# git clone --single-branch --branch v1.5.5 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml \u955c\u50cf\u5217\u8868\uff1a \u00b6 # ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.4.0\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.3.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.3.0\" CSI\u83b7\u53d6\u955c\u50cf\u811a\u672c\uff1a \u00b6 [ ucloud ] root@node0:/home/lixie# cat <<EOF>> image-sci.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF \u5b9a\u5236\u5316\u90e8\u7f72\uff1a \u00b6 \u5b9a\u5236mon\u8c03\u5ea6\u53c2\u6570 \u00b6 \u80cc\u666f \u2f63\u4ea7\u73af\u5883\u6709\u2f00\u4e9b\u4e13\u2ed4\u7684\u8282\u70b9\u2f64\u4e8emon\u3001mgr\uff0c\u5b58\u50a8\u8282\u70b9\u8282\u70b9\u4f7f\u2f64\u5355\u72ec\u7684\u8282\u70b9\u627f\u62c5,\u5229\u2f64\u8c03\u5ea6\u673a\u5236\u5b9e \u73b0 placement : mon : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mon operator : In values : - enabled #\u8bbe\u7f6e\u78c1\u76d8\u7684\u53c2\u6570\uff0c\u8c03\u6574\u4e3afalse\uff0c\u2f45\u4fbf\u540e\u2faf\u5b9a\u5236 214 useAllNodes : false 215 useAllDevices : false \u5206\u522b\u7ed9\u8282\u70b9\u6253\u4e0a\u6807\u7b7e [ucloud] root@master0:~# kubectl label node node0 ceph-mon=enabled node/node0 labeled [ucloud] root@master0:~# kubectl label node node1 ceph-mon=enabled node/node1 labeled [ucloud] root@master0:~# kubectl label node node2 ceph-mon=enabled node/node2 labeled \u83b7\u53d6\u955c\u50cf\u811a\u672c $ cat image-rook-ceph-sci-v1.7.11.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} #docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF \u5b9a\u5236mgr\u8c03\u5ea6\u53c2\u6570 \u00b6 \u6e29\u99a8\u63d0\u793a \u4fee\u6539mgr\u7684\u8c03\u5ea6\u53c2\u6570,\u4fee\u6539\u5b8c\u4e4b\u540e\u91cd\u65b0apply cluster.yaml\u914d\u7f6e\u4f7f\u5176\u52a0\u8f7d\u5230\u96c6\u7fa4\u4e2d mgr : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mgr operator : In values : - enabled \u6b64\u65f6\u8c03\u5ea6\u4f1a\u5931\u8d25\uff0c\u7ed9node-1\u548cnode-2\u6253\u4e0a ceph-mgr=enabled \u7684\u6807\u7b7e $ kubectl label nodes node0 ceph-mgr=enabled node/node0 labeled $ kubectl label nodes node1 ceph-mgr=enabled node/node1 labeled \u5b9a\u5236msd\u8c03\u5ea6\u53c2\u6570 \u00b6 \u8bbe\u7f6eosd\u7684\u8c03\u5ea6\u53c2\u6570 osd : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-osd operator : In values : - enabled \u5b9a\u5236osd\u7684\u78c1\u76d8\u53c2\u6570 nodes : - name : \"node0\" devices : # specific devices to use for storage can be specified for each node - name : \"vdb\" - name : \"node1\" devices : # specific devices to use for storage can be specified for each node - name : \"vdc\" toolbox\u5ba2\u6237\u7aef \u00b6 apply\u4ee5\u4e0b\u4e2d\u4e24\u4e2a\u6587\u4ef6\u7684\u4e00\u4e2a\u5c31\u53ef\u4ee5\uff0c\u4e00\u822c\u9009\u62e9toolbox.yaml $ ll tool* -rw-r--r-- 1 beiyiwangdejiyi staff 1.7K 7 19 17:26 toolbox-job.yaml # \u4e00\u6b21\u6027\u4efb\u52a1 -rw-r--r-- 1 beiyiwangdejiyi staff 1.4K 7 19 17:26 toolbox.yaml kubectl apply -f toolbox.yaml k8s\u8bbf\u95eeceph \u00b6 centos\u7cfb\u7edf\uff1a 1. \u914d\u7f6eCeph yum\u6e90 \u00b6 [root@node-1 ~]# cat /etc/yum.repos.d/ceph.repo [ceph] name=ceph baseurl=https://mirrors.aliyun.com/ceph/rpm-octopus/el8/x86_64/ enabled=1 gpgcheck=0 2.\u5b89\u88c5ceph-common \u00b6 [root@node-1 ~]# yum -y install ceph-common 3. \u521b\u5efaceph\u914d\u7f6e\u6587\u4ef6 \u00b6 [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/ceph.conf # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [global] mon_host = 10.43.248.216:6789,10.43.174.200:6789,10.43.9.21:6789 [client.admin] keyring = /etc/ceph/keyring [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/keyring # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [client.admin] key = AQCRS+NijUeiIxAAhFtv6je2FmMEAVHAJJqPwg== ubuntu\u7cfb\u7edf\uff1a apt install ceph-common \u8bbf\u95eeRBD\u5757\u5b58\u50a8 \u00b6 1.\u521b\u5efa\u4e00\u4e2apool \u00b6 [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # ceph osd pool create rook 16 16 pool 'rook' created [root@rook-ceph-tools-65c94d77bb-xg6xs /]# ceph osd lspools # \u67e5\u770bpools 1 device_health_metrics 2 replicapool 3 rook 2. \u5728pool\u4e0a\u521b\u5efa\u5757\u8bbe\u5907 \u00b6 [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd create -p rook --image rook-rbd.img --size 10G [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd ls -p rook rook-rbd.img [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd info rook/rook-rbd.img # \u67e5\u770b\u8be6\u7ec6\u4fe1\u606f rbd image 'rook-rbd.img' : size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count : 0 id : 50a7fcf85890 block_name_prefix : rbd_data.50a7fcf85890 format : 2 features : layering op_features : flags : create_timestamp : Fri Jul 29 05:40:05 2022 access_timestamp : Fri Jul 29 05:40:05 2022 modify_timestamp : Fri Jul 29 05:40:05 2022 3\u3001\u5ba2\u6237\u6302\u8f7dRBD\u5757 \u00b6 [ ucloud ] root@master0:/home/lixie# rbd map rook/rook-rbd.img /dev/rbd0 [ ucloud ] root@master0:/home/lixie# rbd showmapped id pool namespace image snap device 0 rook rook-rbd.img - /dev/rbd0 [ ucloud ] root@master0:/home/lixie# mkfs.xfs /dev/rbd0 meta-data = /dev/rbd1 isize = 512 agcount = 16 , agsize = 163840 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 1 , sparse = 1 , rmapbt = 0 = reflink = 1 data = bsize = 4096 blocks = 2621440 , imaxpct = 25 = sunit = 16 swidth = 16 blks naming = version 2 bsize = 4096 ascii-ci = 0 , ftype = 1 log = internal log bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 16 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 \u95ee\u9898\u4e00\uff1a\u52a0\u8f7d rbd \u5185\u6838\u6a21\u5757\u5931\u8d25 [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd map rook/rook-rbd.img modinfo: ERROR: Module alias rbd not found. modprobe: FATAL: Module rbd not found in directory /lib/modules/5.4.0-48-generic rbd: failed to load rbd kernel module (1) rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (2) No such file or directory \u89e3\u51b3\u65b9\u6cd5\uff1a [ucloud] root@node0:/home/lixie# modprobe rbd [ucloud] root@node0:/home/lixie# lsmod |grep rbd rbd 106496 0 libceph 327680 1 rbd \u95ee\u9898\u4e8c\uff1amap rdb [root@rook-ceph-tools-65c94d77bb-b9b2h /]# rbd map rook/rook-rbd.img rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". \u89e3\u51b3\u65b9\u6cd5\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\u6267\u884c\uff0c\u8be5\u547d\u4ee4\u3002 \u95ee\u9898\u4e09: \u5185\u6838\u6a21\u5757\u4e0d\u652f\u6301\u8fd9\u4e48\u591a\u7684\u7279\u6027 [dev] root@master0:/home/lixie# rbd map rook/rook-rbd1.img rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \"rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten\". In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (6) No such device or address \u89e3\u51b3\u65b9\u6cd5\uff1a rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten # \u6309\u7167\u4ed6\u7684\u63d0\u793a\uff0c\u5148\u7981\u6b62\u8fd9\u4e9b\u7279\u6027\u518dmap Dashbaard \u56fe\u5f62\u7ba1\u7406 \u00b6 \u6e29\u99a8\u63d0\u793a \u6ce8\u610f\u9700\u8981\u5c06\u4e3b\u673a\u66b4\u6f0f\u7aef\u53e3\u7684\u5b89\u5168\u7ec4\u6253\u5f00\uff0c\u5b89\u5168\u7ec4\u6253\u5f00 31926 \u7aef\u53e3 \u542f\u2f64\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230rook-ceph-mgr-dashboard-external-http\u7684service\uff0c\u5176\u7c7b\u578b\u662fNodePort\uff0c /Users/beiyiwangdejiyi/k8s-data/rook-v1.6.11/cluster/examples/kubernetes/ceph k apply -f dashboard-external-http.yaml $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 579d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 579d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 579d rook-ceph-mgr-dashboard ClusterIP 10.97.4.98 <none> 7000/TCP 579d rook-ceph-mgr-dashboard-external-http NodePort 10.97.10.172 <none> 7000:31926/TCP 8m18s \u9ed8\u8ba4mgr\u521b\u5efa\u4e86\u2f00\u4e2aadmin\u7684\u2f64\u6237\uff0c\u5176\u5bc6\u7801\u5b58\u653e\u5728rook-ceph-dashboard-password\u7684secrets\u5bf9\u8c61\u4e2d\uff0c\u901a\u8fc7\u5982\u4e0b\u2f45\u5f0f\u53ef\u4ee5\u83b7\u53d6\u5230 kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -oyaml apiVersion : v1 data : password : XTBndS0iREN1bE9UMGpQY2JQSSE= # \u91c7\u7528base64\u52a0\u5bc6 kind : Secret metadata : creationTimestamp : \"2020-12-16T18:01:16Z\" name : rook-ceph-dashboard-password namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"103972533\" uid : 7082d4ad-47bb-46e2-b439-624016cc5f81 type : kubernetes.io/rook base64 \u89e3\u5bc6 $ echo XTBndS0iREN1bE9UMGpQY2JQSSE= | base64 -d ]0gu-\"DCulOT0jPcbPI!% \u6d4b\u8bd5\u767b\u9646 URL: http://106.75.119.241:31926/ \u5e10\u53f7: admin \u5bc6\u7801: ]0gu-\"DCulOT0jPcbPI! \u8fdb\u5165\u5c31\u53ef\u4ee5\u770b\u5230\u4e00\u4e0b\u754c\u9762 Dashboard \u76d1\u63a7ceph \u00b6 $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 580d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 580d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 580d # \u7ed9prometheus\u4f5c\u4e3a\u5ba2\u6237\u7aef\u4f7f\u7528\u7684 ...... \u90e8\u7f72Prometheus Operator \u00b6 kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml \u786e\u8ba4prometheus-operator\u5904\u4e8erun\u72b6\u6001 $ k get pod prometheus-operator-7ccf6dfc8-d9dmm 1/1 Running 0 142 \u90e8\u7f72Prometheus Instances \u00b6 $ git clone --single-branch --branch v1.6.11 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph/monitoring \u521b\u5efa\u670d\u52a1\u76d1\u89c6\u5668\u4ee5\u53ca Prometheus \u670d\u52a1\u5668 pod \u548c\u670d\u52a1 kubectl create -f service-monitor.yaml kubectl create -f prometheus.yaml kubectl create -f prometheus-service.yaml \u672c\u5730\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/prometheus-rook-prometheus-0 -n rook-ceph 9090 9090 \u8bbf\u95ee\uff1ahttp://localhost:9090/ grafana\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/grafana-cc568dbd8-4nvlq -n infra 3000 80 \u8bbf\u95ee\uff1ahttp://localhost:3000/ \u5e10\u53f7\uff1aadmin \u5bc6\u7801\uff1astrongpassword \u76d1\u63a7\u5c55\u677f Ceph - Cluster\uff1ahttps://grafana.com/grafana/dashboards/2842 Ceph - OSD \uff1a https://grafana.com/grafana/dashboards/5336 Ceph - Pools\uff1a https://grafana.com/grafana/dashboards/5342 \u5bf9\u8c61\u5b58\u50a8 \u00b6 \u90e8\u7f72RGW\u5bf9\u8c61\u5b58\u50a8 \u00b6 $ k apply -f object.yaml [ ucloud ] root@master0:~/.kube# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-rbdplugin-metrics ClusterIP 10 .43.40.164 <none> 8080 /TCP,8081/TCP 7d4h csi-cephfsplugin-metrics ClusterIP 10 .43.170.151 <none> 8080 /TCP,8081/TCP 7d4h rook-ceph-mon-a ClusterIP 10 .43.248.216 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-b ClusterIP 10 .43.174.200 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-c ClusterIP 10 .43.9.21 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mgr-dashboard ClusterIP 10 .43.193.31 <none> 8443 /TCP 7d4h rook-ceph-mgr ClusterIP 10 .43.114.15 <none> 9283 /TCP 7d4h wordpress-mysql ClusterIP None <none> 3306 /TCP 7d3h rook-prometheus NodePort 10 .43.128.1 <none> 9090 :30900/TCP 3d prometheus-operated ClusterIP None <none> 9090 /TCP 3d rook-ceph-rgw-my-store ClusterIP 10 .43.73.235 <none> 80 /TCP 18m [ ucloud ] root@master0:~/.kube# curl http://10.43.73.235 <?xml version = \"1.0\" encoding = \"UTF-8\" ?><ListAllMyBucketsResult xmlns = \"http://s3.amazonaws.com/doc/2006-03-01/\" ><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult> [ ucloud ] root@master0:~/.kube# RGW\u9ad8\u53ef\u7528 \u00b6 $ vim object.yaml 54 instances: 2 \u521b\u5efaBucket \u00b6 \u521b\u5efastorageclass $ k apply -f storageclass-bucket-delete.yaml storageclass.storage.k8s.io/rook-ceph-delete-bucket created \u521b\u5efabucket $ k apply -f object-bucket-claim-delete.yaml objectbucketclaim.objectbucket.io/ceph-delete-bucket created \u5bb9\u5668\u8bbf\u95ee\u5bf9\u8c61\u5b58\u50a8 \u00b6 \u83b7\u53d6ceph-rgw\u7684\u8bbf\u95ee\u5730\u5740 $ k get cm ceph-delete-bucket -o yaml apiVersion : v1 data : BUCKET_HOST : rook-ceph-rgw-my-store.rook-ceph.svc BUCKET_NAME : ceph-bkt-148b1fa5-7868-42e5-8135-383d357c41cd BUCKET_PORT : \"80\" BUCKET_REGION : us-east-1 BUCKET_SUBREGION : \"\" kind : ConfigMap metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282724\" uid : 26786f4b-9371-46fa-8ca9-f470e5e92cb2 \u62ff\u5230secrets $ k get secrets ceph-delete-bucket -o yaml apiVersion : v1 data : AWS_ACCESS_KEY_ID : Rk9JSjBJNlg0NDVNUVVMVkpGMzc= AWS_SECRET_ACCESS_KEY : SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA== kind : Secret metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282723\" uid : 1b0af759-7c7d-4fe4-9a80-ea2615fa8fef type : Opaque base64 \u89e3\u5bc6 $ echo Rk9JSjBJNlg0NDVNUVVMVkpGMzc = | base64 -d FOIJ0I6X445MQULVJF37% # beiyiwangdejiyi @ beiyiwangdejiyideMacBook-Pro in ~/note-work/hugo on git:main x [14:28:13] $ echo SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA == | base64 -d IWchIZyWTm3F6DrgoTqD4GX39Yas4xKVfXLDDsXx% fio --name=sequential-read --directory=/config --rw=read --refill_buffers --bs=4K --size=200M root@nginx-run-685fdf6467-mdl9v:/# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: Laying out IO file ( 1 file / 200MiB ) sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 723 : Wed Aug 10 06 :28:37 2022 read: IOPS = 98 .8k, BW = 386MiB/s ( 405MB/s )( 200MiB/518msec ) clat ( nsec ) : min = 378 , max = 86956k, avg = 9833 .73, stdev = 534902 .03 lat ( nsec ) : min = 411 , max = 86956k, avg = 9868 .39, stdev = 534902 .42 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 486 ] , 10 .00th =[ 532 ] , | 20 .00th =[ 556 ] , 30 .00th =[ 580 ] , 40 .00th =[ 604 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 708 ] , 90 .00th =[ 804 ] , 95 .00th =[ 932 ] , | 99 .00th =[ 70144 ] , 99 .50th =[ 102912 ] , 99 .90th =[ 456704 ] , | 99 .95th =[ 1253376 ] , 99 .99th =[ 26083328 ] bw ( KiB/s ) : min = 401376 , max = 401376 , per = 100 .00%, avg = 401376 .00, stdev = 0 .00, samples = 1 iops : min = 100344 , max = 100344 , avg = 100344 .00, stdev = 0 .00, samples = 1 lat ( nsec ) : 500 = 5 .72%, 750 = 80 .66%, 1000 = 9 .78% lat ( usec ) : 2 = 1 .74%, 4 = 0 .06%, 10 = 0 .17%, 20 = 0 .06%, 50 = 0 .11% lat ( usec ) : 100 = 1 .17%, 250 = 0 .37%, 500 = 0 .07%, 750 = 0 .02%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01% cpu : usr = 3 .29%, sys = 17 .02%, ctx = 571 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 386MiB/s ( 405MB/s ) , 386MiB/s-386MiB/s ( 405MB/s-405MB/s ) , io = 200MiB ( 210MB ) , run = 518 -518msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 729 : Wed Aug 10 06 :30:48 2022 read: IOPS = 344k, BW = 1342MiB/s ( 1407MB/s )( 200MiB/149msec ) clat ( nsec ) : min = 375 , max = 7097 .8k, avg = 2339 .11, stdev = 39079 .52 lat ( nsec ) : min = 406 , max = 7097 .8k, avg = 2372 .97, stdev = 39080 .14 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 588 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 700 ] , 90 .00th =[ 748 ] , 95 .00th =[ 868 ] , | 99 .00th =[ 67072 ] , 99 .50th =[ 73216 ] , 99 .90th =[ 114176 ] , | 99 .95th =[ 156672 ] , 99 .99th =[ 1122304 ] lat ( nsec ) : 500 = 2 .29%, 750 = 87 .79%, 1000 = 7 .05% lat ( usec ) : 2 = 0 .96%, 4 = 0 .01%, 10 = 0 .15%, 20 = 0 .04%, 50 = 0 .11% lat ( usec ) : 100 = 1 .42%, 250 = 0 .15%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01% cpu : usr = 25 .68%, sys = 49 .32%, ctx = 335 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 1342MiB/s ( 1407MB/s ) , 1342MiB/s-1342MiB/s ( 1407MB/s-1407MB/s ) , io = 200MiB ( 210MB ) , run = 149 -149msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) Jobs: 2 ( f = 2 ) : [ _ ( 4 ) ,R ( 2 )][ 100 .0% ][ r = 264MiB/s ][ r = 67 .7k IOPS ][ eta 00m:00s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 732 : Wed Aug 10 06 :32:40 2022 read: IOPS = 11 .1k, BW = 43 .5MiB/s ( 45 .6MB/s )( 200MiB/4602msec ) clat ( nsec ) : min = 377 , max = 522247k, avg = 89494 .70, stdev = 4682750 .76 lat ( nsec ) : min = 408 , max = 522247k, avg = 89553 .19, stdev = 4682751 .34 clat percentiles ( nsec ) : | 1 .00th =[ 414 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 796 ] , | 80 .00th =[ 884 ] , 90 .00th =[ 1020 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 83456 ] , 99 .50th =[ 218112 ] , 99 .90th =[ 5144576 ] , | 99 .95th =[ 20578304 ] , 99 .99th =[ 240123904 ] bw ( KiB/s ) : min = 14080 , max = 90112 , per = 18 .59%, avg = 45625 .50, stdev = 27571 .34, samples = 8 iops : min = 3520 , max = 22528 , avg = 11406 .37, stdev = 6892 .84, samples = 8 lat ( nsec ) : 500 = 3 .47%, 750 = 60 .05%, 1000 = 25 .67% lat ( usec ) : 2 = 8 .49%, 4 = 0 .21%, 10 = 0 .15%, 20 = 0 .08%, 50 = 0 .07% lat ( usec ) : 100 = 1 .00%, 250 = 0 .33%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .05%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .59%, sys = 2 .00%, ctx = 671 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 733 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .7k, BW = 41 .9MiB/s ( 43 .9MB/s )( 200MiB/4776msec ) clat ( nsec ) : min = 376 , max = 734234k, avg = 92901 .06, stdev = 5934361 .60 lat ( nsec ) : min = 411 , max = 734234k, avg = 92951 .67, stdev = 5934362 .28 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 588 ] , 30 .00th =[ 636 ] , 40 .00th =[ 684 ] , | 50 .00th =[ 732 ] , 60 .00th =[ 788 ] , 70 .00th =[ 860 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1080 ] , 95 .00th =[ 1272 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 193536 ] , 99 .90th =[ 3981312 ] , | 99 .95th =[ 27131904 ] , 99 .99th =[ 233832448 ] bw ( KiB/s ) : min = 8192 , max = 98304 , per = 19 .42%, avg = 47655 .75, stdev = 31431 .05, samples = 8 iops : min = 2048 , max = 24576 , avg = 11913 .88, stdev = 7857 .79, samples = 8 lat ( nsec ) : 500 = 3 .87%, 750 = 50 .45%, 1000 = 31 .27% lat ( usec ) : 2 = 12 .03%, 4 = 0 .18%, 10 = 0 .15%, 20 = 0 .07%, 50 = 0 .06% lat ( usec ) : 100 = 1 .02%, 250 = 0 .48%, 500 = 0 .15%, 750 = 0 .05%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .05%, 4 = 0 .02%, 10 = 0 .03%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .44%, sys = 2 .07%, ctx = 845 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 734 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .9k, BW = 42 .6MiB/s ( 44 .7MB/s )( 200MiB/4696msec ) clat ( nsec ) : min = 379 , max = 607583k, avg = 91313 .18, stdev = 4982310 .93 lat ( nsec ) : min = 412 , max = 607583k, avg = 91361 .07, stdev = 4982311 .06 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 700 ] , 60 .00th =[ 740 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 130560 ] , 99 .90th =[ 4882432 ] , | 99 .95th =[ 14483456 ] , 99 .99th =[ 254803968 ] bw ( KiB/s ) : min = 7680 , max = 90112 , per = 17 .01%, avg = 41739 .89, stdev = 24407 .57, samples = 9 iops : min = 1920 , max = 22528 , avg = 10434 .89, stdev = 6101 .90, samples = 9 lat ( nsec ) : 500 = 3 .93%, 750 = 58 .07%, 1000 = 25 .64% lat ( usec ) : 2 = 10 .13%, 4 = 0 .22%, 10 = 0 .13%, 20 = 0 .04%, 50 = 0 .08% lat ( usec ) : 100 = 1 .10%, 250 = 0 .34%, 500 = 0 .08%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .04%, 20 = 0 .02%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .72%, sys = 1 .81%, ctx = 502 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 735 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .6k, BW = 41 .5MiB/s ( 43 .5MB/s )( 200MiB/4822msec ) clat ( nsec ) : min = 374 , max = 700599k, avg = 93757 .53, stdev = 5099889 .71 lat ( nsec ) : min = 413 , max = 700599k, avg = 93803 .83, stdev = 5099890 .00 clat percentiles ( nsec ) : | 1 .00th =[ 430 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1256 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 226304 ] , 99 .90th =[ 5931008 ] , | 99 .95th =[ 24248320 ] , 99 .99th =[ 235929600 ] bw ( KiB/s ) : min = 6520 , max = 65536 , per = 15 .07%, avg = 36989 .89, stdev = 18379 .13, samples = 9 iops : min = 1630 , max = 16384 , avg = 9247 .44, stdev = 4594 .77, samples = 9 lat ( nsec ) : 500 = 3 .74%, 750 = 59 .39%, 1000 = 24 .25% lat ( usec ) : 2 = 10 .19%, 4 = 0 .22%, 10 = 0 .18%, 20 = 0 .08%, 50 = 0 .08% lat ( usec ) : 100 = 0 .96%, 250 = 0 .43%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .06%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .21%, sys = 2 .24%, ctx = 874 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 736 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .2k, BW = 39 .9MiB/s ( 41 .9MB/s )( 200MiB/5008msec ) clat ( nsec ) : min = 377 , max = 806541k, avg = 97234 .58, stdev = 6320832 .41 lat ( nsec ) : min = 414 , max = 806541k, avg = 97345 .92, stdev = 6320844 .03 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 652 ] , | 50 .00th =[ 684 ] , 60 .00th =[ 724 ] , 70 .00th =[ 772 ] , | 80 .00th =[ 868 ] , 90 .00th =[ 1032 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 80384 ] , 99 .50th =[ 154624 ] , 99 .90th =[ 5275648 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 200278016 ] bw ( KiB/s ) : min = 8192 , max = 49152 , per = 12 .40%, avg = 30434 .00, stdev = 15445 .75, samples = 9 iops : min = 2048 , max = 12288 , avg = 7608 .44, stdev = 3861 .51, samples = 9 lat ( nsec ) : 500 = 3 .77%, 750 = 62 .98%, 1000 = 22 .07% lat ( usec ) : 2 = 8 .67%, 4 = 0 .18%, 10 = 0 .26%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .05%, 250 = 0 .40%, 500 = 0 .11%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .05%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .46%, sys = 1 .96%, ctx = 787 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 737 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .3k, BW = 40 .2MiB/s ( 42 .1MB/s )( 200MiB/4977msec ) clat ( nsec ) : min = 378 , max = 894860k, avg = 96613 .11, stdev = 5799729 .12 lat ( nsec ) : min = 413 , max = 894860k, avg = 96658 .78, stdev = 5799729 .19 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 506 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 644 ] , | 50 .00th =[ 676 ] , 60 .00th =[ 716 ] , 70 .00th =[ 764 ] , | 80 .00th =[ 852 ] , 90 .00th =[ 988 ] , 95 .00th =[ 1176 ] , | 99 .00th =[ 87552 ] , 99 .50th =[ 216064 ] , 99 .90th =[ 6848512 ] , | 99 .95th =[ 31064064 ] , 99 .99th =[ 231735296 ] bw ( KiB/s ) : min = 16929 , max = 69632 , per = 14 .42%, avg = 35383 .25, stdev = 17459 .47, samples = 8 iops : min = 4232 , max = 17408 , avg = 8845 .75, stdev = 4364 .90, samples = 8 lat ( nsec ) : 500 = 4 .66%, 750 = 63 .16%, 1000 = 22 .55% lat ( usec ) : 2 = 7 .10%, 4 = 0 .20%, 10 = 0 .27%, 20 = 0 .09%, 50 = 0 .11% lat ( usec ) : 100 = 0 .99%, 250 = 0 .44%, 500 = 0 .12%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .32%, sys = 2 .05%, ctx = 1006 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 240MiB/s ( 251MB/s ) , 39 .9MiB/s-43.5MiB/s ( 41 .9MB/s-45.6MB/s ) , io = 1200MiB ( 1258MB ) , run = 4602 -5008msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes Jobs: 3 ( f = 3 ) : [ _ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 )][ 80 .0% ][ r = 172MiB/s ][ r = 44 .1k IOPS ][ eta 00m:02s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 740 : Wed Aug 10 06 :34:00 2022 read: IOPS = 8276 , BW = 32 .3MiB/s ( 33 .9MB/s )( 200MiB/6186msec ) clat ( nsec ) : min = 378 , max = 805881k, avg = 120250 .60, stdev = 7449083 .91 lat ( nsec ) : min = 410 , max = 805881k, avg = 120301 .98, stdev = 7449084 .14 clat percentiles ( nsec ) : | 1 .00th =[ 422 ] , 5 .00th =[ 532 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 612 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 660 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 996 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 132096 ] , 99 .90th =[ 1318912 ] , | 99 .95th =[ 10289152 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8192 , max = 65536 , per = 23 .93%, avg = 35045 .82, stdev = 25507 .11, samples = 11 iops : min = 2048 , max = 16384 , avg = 8761 .45, stdev = 6376 .78, samples = 11 lat ( nsec ) : 500 = 2 .40%, 750 = 77 .33%, 1000 = 15 .36% lat ( usec ) : 2 = 2 .64%, 4 = 0 .06%, 10 = 0 .18%, 20 = 0 .11%, 50 = 0 .13% lat ( usec ) : 100 = 1 .11%, 250 = 0 .39%, 500 = 0 .09%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .03%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .34%, sys = 1 .62%, ctx = 845 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 741 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6102 , BW = 23 .8MiB/s ( 24 .0MB/s )( 200MiB/8390msec ) clat ( nsec ) : min = 379 , max = 1190 .5M, avg = 162758 .58, stdev = 11051920 .43 lat ( nsec ) : min = 413 , max = 1190 .5M, avg = 162807 .90, stdev = 11051920 .69 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 700 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1004 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1073152 ] , | 99 .95th =[ 5275648 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 512 , max = 73728 , per = 20 .69%, avg = 30307 .20, stdev = 21673 .33, samples = 10 iops : min = 128 , max = 18432 , avg = 7576 .80, stdev = 5418 .33, samples = 10 lat ( nsec ) : 500 = 2 .76%, 750 = 73 .76%, 1000 = 18 .35% lat ( usec ) : 2 = 2 .79%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .13%, 50 = 0 .14% lat ( usec ) : 100 = 1 .05%, 250 = 0 .42%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .02%, 20 = 0 .01%, 100 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .33%, sys = 1 .10%, ctx = 982 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 742 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6906 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7413msec ) clat ( nsec ) : min = 380 , max = 1034 .3M, avg = 144218 .03, stdev = 9148025 .05 lat ( nsec ) : min = 414 , max = 1034 .3M, avg = 144254 .80, stdev = 9148025 .00 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 596 ] , 40 .00th =[ 620 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 676 ] , 70 .00th =[ 708 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 988 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 146432 ] , 99 .90th =[ 1253376 ] , | 99 .95th =[ 4620288 ] , 99 .99th =[ 522190848 ] bw ( KiB/s ) : min = 16384 , max = 73728 , per = 26 .85%, avg = 39318 .40, stdev = 23491 .33, samples = 10 iops : min = 4096 , max = 18432 , avg = 9829 .60, stdev = 5872 .83, samples = 10 lat ( nsec ) : 500 = 3 .29%, 750 = 76 .66%, 1000 = 15 .22% lat ( usec ) : 2 = 2 .43%, 4 = 0 .10%, 10 = 0 .23%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .13%, 250 = 0 .37%, 500 = 0 .13%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .32%, sys = 1 .32%, ctx = 864 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 743 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6106 , BW = 23 .9MiB/s ( 25 .0MB/s )( 200MiB/8385msec ) clat ( nsec ) : min = 380 , max = 1507 .9M, avg = 162772 .24, stdev = 13174473 .96 lat ( nsec ) : min = 414 , max = 1507 .9M, avg = 162810 .08, stdev = 13174473 .94 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 510 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 692 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 79360 ] , 99 .50th =[ 136192 ] , 99 .90th =[ 897024 ] , | 99 .95th =[ 2506752 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8 , max = 81920 , per = 20 .70%, avg = 30310 .40, stdev = 22461 .38, samples = 10 iops : min = 2 , max = 20480 , avg = 7577 .60, stdev = 5615 .35, samples = 10 lat ( nsec ) : 500 = 4 .29%, 750 = 73 .14%, 1000 = 17 .30% lat ( usec ) : 2 = 2 .94%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .07%, 50 = 0 .19% lat ( usec ) : 100 = 1 .10%, 250 = 0 .44%, 500 = 0 .11%, 750 = 0 .05%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .03%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .05%, sys = 1 .40%, ctx = 993 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 744 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6911 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7408msec ) clat ( nsec ) : min = 381 , max = 1179 .3M, avg = 143994 .58, stdev = 11079777 .48 lat ( nsec ) : min = 413 , max = 1179 .3M, avg = 144029 .70, stdev = 11079777 .45 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 580 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 636 ] , 60 .00th =[ 668 ] , 70 .00th =[ 692 ] , | 80 .00th =[ 732 ] , 90 .00th =[ 836 ] , 95 .00th =[ 956 ] , | 99 .00th =[ 76288 ] , 99 .50th =[ 111104 ] , 99 .90th =[ 995328 ] , | 99 .95th =[ 3227648 ] , 99 .99th =[ 742391808 ] bw ( KiB/s ) : min = 5040 , max = 73728 , per = 22 .93%, avg = 33587 .20, stdev = 27492 .28, samples = 10 iops : min = 1260 , max = 18432 , avg = 8396 .80, stdev = 6873 .07, samples = 10 lat ( nsec ) : 500 = 3 .81%, 750 = 78 .93%, 1000 = 13 .08% lat ( usec ) : 2 = 2 .02%, 4 = 0 .02%, 10 = 0 .21%, 20 = 0 .07%, 50 = 0 .12% lat ( usec ) : 100 = 1 .15%, 250 = 0 .36%, 500 = 0 .07%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .35%, ctx = 724 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 745 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6238 , BW = 24 .4MiB/s ( 25 .6MB/s )( 200MiB/8207msec ) clat ( nsec ) : min = 379 , max = 1170 .8M, avg = 159639 .78, stdev = 10672683 .50 lat ( nsec ) : min = 413 , max = 1170 .8M, avg = 159675 .27, stdev = 10672683 .47 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 596 ] , 40 .00th =[ 628 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1810432 ] , | 99 .95th =[ 5865472 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 6400 , max = 74752 , per = 18 .50%, avg = 27096 .62, stdev = 22310 .47, samples = 13 iops : min = 1600 , max = 18688 , avg = 6774 .15, stdev = 5577 .62, samples = 13 lat ( nsec ) : 500 = 4 .00%, 750 = 74 .22%, 1000 = 16 .52% lat ( usec ) : 2 = 3 .01%, 4 = 0 .05%, 10 = 0 .19%, 20 = 0 .07%, 50 = 0 .11% lat ( usec ) : 100 = 1 .06%, 250 = 0 .40%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .03%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .22%, ctx = 949 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 143MiB/s ( 150MB/s ) , 23 .8MiB/s-32.3MiB/s ( 24 .0MB/s-33.9MB/s ) , io = 1200MiB ( 1258MB ) , run = 6186 -8390msec fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=6 fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-write: Laying out IO file ( 1 file / 200MiB ) Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 756 : Wed Aug 10 06 :39:40 2022 write: IOPS = 33 .6k, BW = 131MiB/s ( 138MB/s )( 200MiB/1525msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 7420 , avg = 27 .69, stdev = 125 .85 lat ( usec ) : min = 7 , max = 7420 , avg = 27 .76, stdev = 125 .85 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 12 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 28 ] , 95 .00th =[ 37 ] , | 99 .00th =[ 118 ] , 99 .50th =[ 285 ] , 99 .90th =[ 1860 ] , 99 .95th =[ 3097 ] , | 99 .99th =[ 4752 ] bw ( KiB/s ) : min = 132286 , max = 138088 , per = 100 .00%, avg = 135187 .00, stdev = 4102 .63, samples = 2 iops : min = 33071 , max = 34522 , avg = 33796 .50, stdev = 1026 .01, samples = 2 lat ( usec ) : 10 = 9 .54%, 20 = 24 .43%, 50 = 63 .08%, 100 = 1 .75%, 250 = 0 .66% lat ( usec ) : 500 = 0 .22%, 750 = 0 .09%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .10%, 4 = 0 .07%, 10 = 0 .03% cpu : usr = 9 .84%, sys = 31 .04%, ctx = 51905 , majf = 0 , minf = 12 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 200MiB ( 210MB ) , run = 1525 -1525msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 759 : Wed Aug 10 06 :41:20 2022 write: IOPS = 31 .3k, BW = 122MiB/s ( 128MB/s )( 200MiB/1637msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 9234 , avg = 30 .16, stdev = 137 .54 lat ( usec ) : min = 7 , max = 9234 , avg = 30 .21, stdev = 137 .54 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 18 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 29 ] , 95 .00th =[ 41 ] , | 99 .00th =[ 147 ] , 99 .50th =[ 379 ] , 99 .90th =[ 2311 ] , 99 .95th =[ 3064 ] , | 99 .99th =[ 4490 ] bw ( KiB/s ) : min = 119544 , max = 132640 , per = 100 .00%, avg = 128018 .67, stdev = 7349 .32, samples = 3 iops : min = 29886 , max = 33160 , avg = 32004 .67, stdev = 1837 .33, samples = 3 lat ( usec ) : 10 = 7 .32%, 20 = 25 .16%, 50 = 63 .95%, 100 = 2 .08%, 250 = 0 .85% lat ( usec ) : 500 = 0 .23%, 750 = 0 .09%, 1000 = 0 .08% lat ( msec ) : 2 = 0 .12%, 4 = 0 .11%, 10 = 0 .02% cpu : usr = 7 .21%, sys = 32 .64%, ctx = 51971 , majf = 0 , minf = 13 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 122MiB/s ( 128MB/s ) , 122MiB/s-122MiB/s ( 128MB/s-128MB/s ) , io = 200MiB ( 210MB ) , run = 1637 -1637msec fio --name=big-file-multi-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --numjobs=6 --end_fsync=1 fio -filename=/config/fio.img -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=mytest \u56fa\u6001\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes Jobs: 6 ( f = 6 ) : [ R ( 6 )][ 88 .9% ][ r = 130MiB/s ][ r = 33 .2k IOPS ][ eta 00m:01s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 225237 : Fri Aug 12 14 :09:04 2022 read: IOPS = 5925 , BW = 23 .1MiB/s ( 24 .3MB/s )( 200MiB/8641msec ) clat ( nsec ) : min = 434 , max = 21757k, avg = 168221 .11, stdev = 1662876 .10 lat ( nsec ) : min = 471 , max = 21757k, avg = 168260 .51, stdev = 1662876 .63 clat percentiles ( nsec ) : | 1 .00th =[ 524 ] , 5 .00th =[ 540 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , | 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , 70 .00th =[ 628 ] , | 80 .00th =[ 660 ] , 90 .00th =[ 732 ] , 95 .00th =[ 884 ] , | 99 .00th =[ 3948544 ] , 99 .50th =[ 19005440 ] , 99 .90th =[ 20054016 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 20054016 ] \u5185\u5b58\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 12095 : Fri Aug 12 15 :05:27 2022 read: IOPS = 966k, BW = 3774MiB/s ( 3957MB/s )( 200MiB/53msec ) clat ( nsec ) : min = 520 , max = 221610 , avg = 757 .38, stdev = 2561 .09 lat ( nsec ) : min = 553 , max = 221646 , avg = 792 .50, stdev = 2561 .18 clat percentiles ( nsec ) : | 1 .00th =[ 540 ] , 5 .00th =[ 556 ] , 10 .00th =[ 556 ] , 20 .00th =[ 564 ] , | 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , | 70 .00th =[ 644 ] , 80 .00th =[ 724 ] , 90 .00th =[ 908 ] , 95 .00th =[ 940 ] , | 99 .00th =[ 3344 ] , 99 .50th =[ 3728 ] , 99 .90th =[ 19072 ] , 99 .95th =[ 43264 ] , | 99 .99th =[ 115200 ] fio --name=small-file-multi-read \\ --directory=/config \\ --rw=read --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=500 \\ --numjobs=2","title":"rook-ceph\u7b80\u4ecb"},{"location":"Storage/rook-ceph/#rook-ceph","text":"","title":"rook-ceph\u7b80\u4ecb\u548c\u90e8\u7f72"},{"location":"Storage/rook-ceph/#rook-ceph_1","text":"\u5f00\u6e90\u7684\u5206\u5e03\u5f0f\u5b58\u50a8\u7cfb\u7edf\uff0c\u652f\u6301\u5bf9\u8c61\u5b58\u50a8\uff0c\u5757\u8bbe\u5907\uff0c\u6587\u4ef6\u7cfb\u7edf \u5757\u5b58\u50a8 CephFs \u5bf9\u8c61\u5b58\u50a8","title":"rook-ceph\u7b80\u4ecb"},{"location":"Storage/rook-ceph/#ceph","text":"x.0.z - \u5f00\u53d1\u7248 x.1.z - \u5019\u9009\u7248 x.2.z - \u7a33\u5b9a\uff0c\u4fee\u6b63\u7248","title":"ceph \u7684\u7248\u672c\u5386\u53f2"},{"location":"Storage/rook-ceph/#ceph_1","text":"\u6ce8\u610f ceph\u96c6\u7fa4\u7684osd\u8282\u70b9\u4e00\u822c\u4fdd\u8bc1>=3\u4e2a\uff0c\u6765\u4fdd\u8bc1\u6570\u636e\u7684\u9ad8\u53ef\u7528\u6027\u3002 mon : ceph \u76d1\u89c6\u5668,\u5728\u4e00\u4e2a\u4e3b\u673a\u4e0a\u8fd0\u884c\u7684\u4e00\u4e2a\u5b88\u62a4\u8fdb\u7a0b\uff0c\u7528\u4e8e\u7ef4\u62a4\u96c6\u7fa4\u72b6\u6001\u6620\u5c04\u5173\u7cfb mgr : \u8d1f\u8d23\u8ddf\u8e2a\u8fd0\u884c\u65f6\u6307\u6807\u548cceph\u96c6\u7fa4\u7684\u5f53\u524d\u72b6\u6001 osd : \u78c1\u76d8\uff08\u771f\u6b63\u5b58\u50a8\u6570\u636e\u7684\u5730\u65b9\uff09 ceph \u9762\u8bd5\u9898 ceph","title":"ceph\u96c6\u7fa4\u89d2\u8272\u5b9a\u4e49"},{"location":"Storage/rook-ceph/#rook-ceph_2","text":"\u73af\u5883\u8981\u6c42 \u4e00\u4e2ak8s\u96c6\u7fa4\uff0cnode\u8282\u70b9\u6700\u5c11\u4e09\u4e2a\u8282\u70b9 mon: 8C 8G/200G 16C 16g/32-200G","title":"rook-ceph\u90e8\u7f72"},{"location":"Storage/rook-ceph/#_1","text":"","title":"\u666e\u901a\u6d4b\u8bd5\u90e8\u7f72\uff1a"},{"location":"Storage/rook-ceph/#crdscommonoperator","text":"[ucloud] root@master0:~# git clone --single-branch --branch v1.5.5 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph kubectl create -f crds.yaml -f common.yaml -f operator.yaml kubectl create -f cluster.yaml","title":"\u90e8\u7f72crds\uff0ccommon\uff0coperator"},{"location":"Storage/rook-ceph/#_2","text":"# ROOK_CSI_CEPH_IMAGE: \"quay.io/cephcsi/cephcsi:v3.4.0\" # ROOK_CSI_REGISTRAR_IMAGE: \"k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.3.0\" # ROOK_CSI_RESIZER_IMAGE: \"k8s.gcr.io/sig-storage/csi-resizer:v1.3.0\" # ROOK_CSI_PROVISIONER_IMAGE: \"k8s.gcr.io/sig-storage/csi-provisioner:v3.0.0\" # ROOK_CSI_SNAPSHOTTER_IMAGE: \"k8s.gcr.io/sig-storage/csi-snapshotter:v4.2.0\" # ROOK_CSI_ATTACHER_IMAGE: \"k8s.gcr.io/sig-storage/csi-attacher:v3.3.0\"","title":"\u955c\u50cf\u5217\u8868\uff1a"},{"location":"Storage/rook-ceph/#csi","text":"[ ucloud ] root@node0:/home/lixie# cat <<EOF>> image-sci.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF","title":"CSI\u83b7\u53d6\u955c\u50cf\u811a\u672c\uff1a"},{"location":"Storage/rook-ceph/#_3","text":"","title":"\u5b9a\u5236\u5316\u90e8\u7f72\uff1a"},{"location":"Storage/rook-ceph/#mon","text":"\u80cc\u666f \u2f63\u4ea7\u73af\u5883\u6709\u2f00\u4e9b\u4e13\u2ed4\u7684\u8282\u70b9\u2f64\u4e8emon\u3001mgr\uff0c\u5b58\u50a8\u8282\u70b9\u8282\u70b9\u4f7f\u2f64\u5355\u72ec\u7684\u8282\u70b9\u627f\u62c5,\u5229\u2f64\u8c03\u5ea6\u673a\u5236\u5b9e \u73b0 placement : mon : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mon operator : In values : - enabled #\u8bbe\u7f6e\u78c1\u76d8\u7684\u53c2\u6570\uff0c\u8c03\u6574\u4e3afalse\uff0c\u2f45\u4fbf\u540e\u2faf\u5b9a\u5236 214 useAllNodes : false 215 useAllDevices : false \u5206\u522b\u7ed9\u8282\u70b9\u6253\u4e0a\u6807\u7b7e [ucloud] root@master0:~# kubectl label node node0 ceph-mon=enabled node/node0 labeled [ucloud] root@master0:~# kubectl label node node1 ceph-mon=enabled node/node1 labeled [ucloud] root@master0:~# kubectl label node node2 ceph-mon=enabled node/node2 labeled \u83b7\u53d6\u955c\u50cf\u811a\u672c $ cat image-rook-ceph-sci-v1.7.11.sh #!/bin/bash image_list=' csi-node-driver-registrar:v2.0.1 csi-attacher:v3.0.0 csi-snapshotter:v3.0.0 csi-resizer:v1.0.0 csi-provisioner:v2.0.0 ' aliyuncs=\"registry.aliyuncs.com/it00021hot\" google_gcr=\"k8s.gcr.io/sig-storage\" for image in $image_list do echo $image docker pull ${aliyuncs}/${image} docker tag ${aliyuncs}/${image} ${google_gcr}/${image} #docker rm ${aliyuncs}/${image} #echo \"${aliyuncs}/${image} ${google_gcr}/${image} downloaded.\" done EOF","title":"\u5b9a\u5236mon\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#mgr","text":"\u6e29\u99a8\u63d0\u793a \u4fee\u6539mgr\u7684\u8c03\u5ea6\u53c2\u6570,\u4fee\u6539\u5b8c\u4e4b\u540e\u91cd\u65b0apply cluster.yaml\u914d\u7f6e\u4f7f\u5176\u52a0\u8f7d\u5230\u96c6\u7fa4\u4e2d mgr : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-mgr operator : In values : - enabled \u6b64\u65f6\u8c03\u5ea6\u4f1a\u5931\u8d25\uff0c\u7ed9node-1\u548cnode-2\u6253\u4e0a ceph-mgr=enabled \u7684\u6807\u7b7e $ kubectl label nodes node0 ceph-mgr=enabled node/node0 labeled $ kubectl label nodes node1 ceph-mgr=enabled node/node1 labeled","title":"\u5b9a\u5236mgr\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#msd","text":"\u8bbe\u7f6eosd\u7684\u8c03\u5ea6\u53c2\u6570 osd : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : ceph-osd operator : In values : - enabled \u5b9a\u5236osd\u7684\u78c1\u76d8\u53c2\u6570 nodes : - name : \"node0\" devices : # specific devices to use for storage can be specified for each node - name : \"vdb\" - name : \"node1\" devices : # specific devices to use for storage can be specified for each node - name : \"vdc\"","title":"\u5b9a\u5236msd\u8c03\u5ea6\u53c2\u6570"},{"location":"Storage/rook-ceph/#toolbox","text":"apply\u4ee5\u4e0b\u4e2d\u4e24\u4e2a\u6587\u4ef6\u7684\u4e00\u4e2a\u5c31\u53ef\u4ee5\uff0c\u4e00\u822c\u9009\u62e9toolbox.yaml $ ll tool* -rw-r--r-- 1 beiyiwangdejiyi staff 1.7K 7 19 17:26 toolbox-job.yaml # \u4e00\u6b21\u6027\u4efb\u52a1 -rw-r--r-- 1 beiyiwangdejiyi staff 1.4K 7 19 17:26 toolbox.yaml kubectl apply -f toolbox.yaml","title":"toolbox\u5ba2\u6237\u7aef"},{"location":"Storage/rook-ceph/#k8sceph","text":"centos\u7cfb\u7edf\uff1a","title":"k8s\u8bbf\u95eeceph"},{"location":"Storage/rook-ceph/#1-ceph-yum","text":"[root@node-1 ~]# cat /etc/yum.repos.d/ceph.repo [ceph] name=ceph baseurl=https://mirrors.aliyun.com/ceph/rpm-octopus/el8/x86_64/ enabled=1 gpgcheck=0","title":"1. \u914d\u7f6eCeph yum\u6e90"},{"location":"Storage/rook-ceph/#2ceph-common","text":"[root@node-1 ~]# yum -y install ceph-common","title":"2.\u5b89\u88c5ceph-common"},{"location":"Storage/rook-ceph/#3-ceph","text":"[root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/ceph.conf # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [global] mon_host = 10.43.248.216:6789,10.43.174.200:6789,10.43.9.21:6789 [client.admin] keyring = /etc/ceph/keyring [root@rook-ceph-tools-65c94d77bb-b9b2h /]# cat /etc/ceph/keyring # \u67e5\u770b\u4e4b\u540e\u5bbf\u4e3b\u673a\u521b\u5efa [client.admin] key = AQCRS+NijUeiIxAAhFtv6je2FmMEAVHAJJqPwg== ubuntu\u7cfb\u7edf\uff1a apt install ceph-common","title":"3. \u521b\u5efaceph\u914d\u7f6e\u6587\u4ef6"},{"location":"Storage/rook-ceph/#rbd","text":"","title":"\u8bbf\u95eeRBD\u5757\u5b58\u50a8"},{"location":"Storage/rook-ceph/#1pool","text":"[ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # ceph osd pool create rook 16 16 pool 'rook' created [root@rook-ceph-tools-65c94d77bb-xg6xs /]# ceph osd lspools # \u67e5\u770bpools 1 device_health_metrics 2 replicapool 3 rook","title":"1.\u521b\u5efa\u4e00\u4e2apool"},{"location":"Storage/rook-ceph/#2-pool","text":"[ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd create -p rook --image rook-rbd.img --size 10G [ root@rook-ceph-tools-65c94d77bb-xg6xs / ] # rbd ls -p rook rook-rbd.img [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd info rook/rook-rbd.img # \u67e5\u770b\u8be6\u7ec6\u4fe1\u606f rbd image 'rook-rbd.img' : size 10 GiB in 2560 objects order 22 (4 MiB objects) snapshot_count : 0 id : 50a7fcf85890 block_name_prefix : rbd_data.50a7fcf85890 format : 2 features : layering op_features : flags : create_timestamp : Fri Jul 29 05:40:05 2022 access_timestamp : Fri Jul 29 05:40:05 2022 modify_timestamp : Fri Jul 29 05:40:05 2022","title":"2. \u5728pool\u4e0a\u521b\u5efa\u5757\u8bbe\u5907"},{"location":"Storage/rook-ceph/#3rbd","text":"[ ucloud ] root@master0:/home/lixie# rbd map rook/rook-rbd.img /dev/rbd0 [ ucloud ] root@master0:/home/lixie# rbd showmapped id pool namespace image snap device 0 rook rook-rbd.img - /dev/rbd0 [ ucloud ] root@master0:/home/lixie# mkfs.xfs /dev/rbd0 meta-data = /dev/rbd1 isize = 512 agcount = 16 , agsize = 163840 blks = sectsz = 512 attr = 2 , projid32bit = 1 = crc = 1 finobt = 1 , sparse = 1 , rmapbt = 0 = reflink = 1 data = bsize = 4096 blocks = 2621440 , imaxpct = 25 = sunit = 16 swidth = 16 blks naming = version 2 bsize = 4096 ascii-ci = 0 , ftype = 1 log = internal log bsize = 4096 blocks = 2560 , version = 2 = sectsz = 512 sunit = 16 blks, lazy-count = 1 realtime = none extsz = 4096 blocks = 0 , rtextents = 0 \u95ee\u9898\u4e00\uff1a\u52a0\u8f7d rbd \u5185\u6838\u6a21\u5757\u5931\u8d25 [root@rook-ceph-tools-65c94d77bb-xg6xs /]# rbd map rook/rook-rbd.img modinfo: ERROR: Module alias rbd not found. modprobe: FATAL: Module rbd not found in directory /lib/modules/5.4.0-48-generic rbd: failed to load rbd kernel module (1) rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (2) No such file or directory \u89e3\u51b3\u65b9\u6cd5\uff1a [ucloud] root@node0:/home/lixie# modprobe rbd [ucloud] root@node0:/home/lixie# lsmod |grep rbd rbd 106496 0 libceph 327680 1 rbd \u95ee\u9898\u4e8c\uff1amap rdb [root@rook-ceph-tools-65c94d77bb-b9b2h /]# rbd map rook/rook-rbd.img rbd: failed to set udev buffer size: (1) Operation not permitted rbd: sysfs write failed In some cases useful info is found in syslog - try \"dmesg | tail\". \u89e3\u51b3\u65b9\u6cd5\uff1a \u5728\u5bbf\u4e3b\u673a\u4e0a\u6267\u884c\uff0c\u8be5\u547d\u4ee4\u3002 \u95ee\u9898\u4e09: \u5185\u6838\u6a21\u5757\u4e0d\u652f\u6301\u8fd9\u4e48\u591a\u7684\u7279\u6027 [dev] root@master0:/home/lixie# rbd map rook/rook-rbd1.img rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with \"rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten\". In some cases useful info is found in syslog - try \"dmesg | tail\". rbd: map failed: (6) No such device or address \u89e3\u51b3\u65b9\u6cd5\uff1a rbd feature disable rook/rook-rbd1.img object-map fast-diff deep-flatten # \u6309\u7167\u4ed6\u7684\u63d0\u793a\uff0c\u5148\u7981\u6b62\u8fd9\u4e9b\u7279\u6027\u518dmap","title":"3\u3001\u5ba2\u6237\u6302\u8f7dRBD\u5757"},{"location":"Storage/rook-ceph/#dashbaard","text":"\u6e29\u99a8\u63d0\u793a \u6ce8\u610f\u9700\u8981\u5c06\u4e3b\u673a\u66b4\u6f0f\u7aef\u53e3\u7684\u5b89\u5168\u7ec4\u6253\u5f00\uff0c\u5b89\u5168\u7ec4\u6253\u5f00 31926 \u7aef\u53e3 \u542f\u2f64\u4e4b\u540e\uff0c\u53ef\u4ee5\u770b\u5230rook-ceph-mgr-dashboard-external-http\u7684service\uff0c\u5176\u7c7b\u578b\u662fNodePort\uff0c /Users/beiyiwangdejiyi/k8s-data/rook-v1.6.11/cluster/examples/kubernetes/ceph k apply -f dashboard-external-http.yaml $ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 579d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 579d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 579d rook-ceph-mgr-dashboard ClusterIP 10.97.4.98 <none> 7000/TCP 579d rook-ceph-mgr-dashboard-external-http NodePort 10.97.10.172 <none> 7000:31926/TCP 8m18s \u9ed8\u8ba4mgr\u521b\u5efa\u4e86\u2f00\u4e2aadmin\u7684\u2f64\u6237\uff0c\u5176\u5bc6\u7801\u5b58\u653e\u5728rook-ceph-dashboard-password\u7684secrets\u5bf9\u8c61\u4e2d\uff0c\u901a\u8fc7\u5982\u4e0b\u2f45\u5f0f\u53ef\u4ee5\u83b7\u53d6\u5230 kubectl get secrets -n rook-ceph rook-ceph-dashboard-password -oyaml apiVersion : v1 data : password : XTBndS0iREN1bE9UMGpQY2JQSSE= # \u91c7\u7528base64\u52a0\u5bc6 kind : Secret metadata : creationTimestamp : \"2020-12-16T18:01:16Z\" name : rook-ceph-dashboard-password namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"103972533\" uid : 7082d4ad-47bb-46e2-b439-624016cc5f81 type : kubernetes.io/rook base64 \u89e3\u5bc6 $ echo XTBndS0iREN1bE9UMGpQY2JQSSE= | base64 -d ]0gu-\"DCulOT0jPcbPI!% \u6d4b\u8bd5\u767b\u9646 URL: http://106.75.119.241:31926/ \u5e10\u53f7: admin \u5bc6\u7801: ]0gu-\"DCulOT0jPcbPI! \u8fdb\u5165\u5c31\u53ef\u4ee5\u770b\u5230\u4e00\u4e0b\u754c\u9762","title":"Dashbaard \u56fe\u5f62\u7ba1\u7406"},{"location":"Storage/rook-ceph/#dashboard-ceph","text":"$ k get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE csi-cephfsplugin-metrics ClusterIP 10.97.7.142 <none> 8080/TCP,8081/TCP 580d csi-rbdplugin-metrics ClusterIP 10.97.0.194 <none> 8080/TCP,8081/TCP 580d rook-ceph-mgr ClusterIP 10.97.15.77 <none> 9283/TCP 580d # \u7ed9prometheus\u4f5c\u4e3a\u5ba2\u6237\u7aef\u4f7f\u7528\u7684 ......","title":"Dashboard \u76d1\u63a7ceph"},{"location":"Storage/rook-ceph/#prometheus-operator","text":"kubectl apply -f https://raw.githubusercontent.com/coreos/prometheus-operator/v0.40.0/bundle.yaml \u786e\u8ba4prometheus-operator\u5904\u4e8erun\u72b6\u6001 $ k get pod prometheus-operator-7ccf6dfc8-d9dmm 1/1 Running 0 142","title":"\u90e8\u7f72Prometheus Operator"},{"location":"Storage/rook-ceph/#prometheus-instances","text":"$ git clone --single-branch --branch v1.6.11 https://github.com/rook/rook.git cd rook/cluster/examples/kubernetes/ceph/monitoring \u521b\u5efa\u670d\u52a1\u76d1\u89c6\u5668\u4ee5\u53ca Prometheus \u670d\u52a1\u5668 pod \u548c\u670d\u52a1 kubectl create -f service-monitor.yaml kubectl create -f prometheus.yaml kubectl create -f prometheus-service.yaml \u672c\u5730\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/prometheus-rook-prometheus-0 -n rook-ceph 9090 9090 \u8bbf\u95ee\uff1ahttp://localhost:9090/ grafana\u6d4b\u8bd5\u8bbf\u95ee\uff1a $ k port-forward pod/grafana-cc568dbd8-4nvlq -n infra 3000 80 \u8bbf\u95ee\uff1ahttp://localhost:3000/ \u5e10\u53f7\uff1aadmin \u5bc6\u7801\uff1astrongpassword \u76d1\u63a7\u5c55\u677f Ceph - Cluster\uff1ahttps://grafana.com/grafana/dashboards/2842 Ceph - OSD \uff1a https://grafana.com/grafana/dashboards/5336 Ceph - Pools\uff1a https://grafana.com/grafana/dashboards/5342","title":"\u90e8\u7f72Prometheus Instances"},{"location":"Storage/rook-ceph/#_4","text":"","title":"\u5bf9\u8c61\u5b58\u50a8"},{"location":"Storage/rook-ceph/#rgw","text":"$ k apply -f object.yaml [ ucloud ] root@master0:~/.kube# kubectl get svc -n rook-ceph NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE csi-rbdplugin-metrics ClusterIP 10 .43.40.164 <none> 8080 /TCP,8081/TCP 7d4h csi-cephfsplugin-metrics ClusterIP 10 .43.170.151 <none> 8080 /TCP,8081/TCP 7d4h rook-ceph-mon-a ClusterIP 10 .43.248.216 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-b ClusterIP 10 .43.174.200 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mon-c ClusterIP 10 .43.9.21 <none> 6789 /TCP,3300/TCP 7d4h rook-ceph-mgr-dashboard ClusterIP 10 .43.193.31 <none> 8443 /TCP 7d4h rook-ceph-mgr ClusterIP 10 .43.114.15 <none> 9283 /TCP 7d4h wordpress-mysql ClusterIP None <none> 3306 /TCP 7d3h rook-prometheus NodePort 10 .43.128.1 <none> 9090 :30900/TCP 3d prometheus-operated ClusterIP None <none> 9090 /TCP 3d rook-ceph-rgw-my-store ClusterIP 10 .43.73.235 <none> 80 /TCP 18m [ ucloud ] root@master0:~/.kube# curl http://10.43.73.235 <?xml version = \"1.0\" encoding = \"UTF-8\" ?><ListAllMyBucketsResult xmlns = \"http://s3.amazonaws.com/doc/2006-03-01/\" ><Owner><ID>anonymous</ID><DisplayName></DisplayName></Owner><Buckets></Buckets></ListAllMyBucketsResult> [ ucloud ] root@master0:~/.kube#","title":"\u90e8\u7f72RGW\u5bf9\u8c61\u5b58\u50a8"},{"location":"Storage/rook-ceph/#rgw_1","text":"$ vim object.yaml 54 instances: 2","title":"RGW\u9ad8\u53ef\u7528"},{"location":"Storage/rook-ceph/#bucket","text":"\u521b\u5efastorageclass $ k apply -f storageclass-bucket-delete.yaml storageclass.storage.k8s.io/rook-ceph-delete-bucket created \u521b\u5efabucket $ k apply -f object-bucket-claim-delete.yaml objectbucketclaim.objectbucket.io/ceph-delete-bucket created","title":"\u521b\u5efaBucket"},{"location":"Storage/rook-ceph/#_5","text":"\u83b7\u53d6ceph-rgw\u7684\u8bbf\u95ee\u5730\u5740 $ k get cm ceph-delete-bucket -o yaml apiVersion : v1 data : BUCKET_HOST : rook-ceph-rgw-my-store.rook-ceph.svc BUCKET_NAME : ceph-bkt-148b1fa5-7868-42e5-8135-383d357c41cd BUCKET_PORT : \"80\" BUCKET_REGION : us-east-1 BUCKET_SUBREGION : \"\" kind : ConfigMap metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282724\" uid : 26786f4b-9371-46fa-8ca9-f470e5e92cb2 \u62ff\u5230secrets $ k get secrets ceph-delete-bucket -o yaml apiVersion : v1 data : AWS_ACCESS_KEY_ID : Rk9JSjBJNlg0NDVNUVVMVkpGMzc= AWS_SECRET_ACCESS_KEY : SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA== kind : Secret metadata : creationTimestamp : \"2022-08-05T08:24:27Z\" finalizers : - objectbucket.io/finalizer labels : bucket-provisioner : rook-ceph.ceph.rook.io-bucket name : ceph-delete-bucket namespace : rook-ceph ownerReferences : - apiVersion : objectbucket.io/v1alpha1 blockOwnerDeletion : true controller : true kind : ObjectBucketClaim name : ceph-delete-bucket uid : a8c69ebc-1e4d-477c-97e7-479b532962b3 resourceVersion : \"1282723\" uid : 1b0af759-7c7d-4fe4-9a80-ea2615fa8fef type : Opaque base64 \u89e3\u5bc6 $ echo Rk9JSjBJNlg0NDVNUVVMVkpGMzc = | base64 -d FOIJ0I6X445MQULVJF37% # beiyiwangdejiyi @ beiyiwangdejiyideMacBook-Pro in ~/note-work/hugo on git:main x [14:28:13] $ echo SVdjaElaeVdUbTNGNkRyZ29UcUQ0R1gzOVlhczR4S1ZmWExERHNYeA == | base64 -d IWchIZyWTm3F6DrgoTqD4GX39Yas4xKVfXLDDsXx% fio --name=sequential-read --directory=/config --rw=read --refill_buffers --bs=4K --size=200M root@nginx-run-685fdf6467-mdl9v:/# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: Laying out IO file ( 1 file / 200MiB ) sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 723 : Wed Aug 10 06 :28:37 2022 read: IOPS = 98 .8k, BW = 386MiB/s ( 405MB/s )( 200MiB/518msec ) clat ( nsec ) : min = 378 , max = 86956k, avg = 9833 .73, stdev = 534902 .03 lat ( nsec ) : min = 411 , max = 86956k, avg = 9868 .39, stdev = 534902 .42 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 486 ] , 10 .00th =[ 532 ] , | 20 .00th =[ 556 ] , 30 .00th =[ 580 ] , 40 .00th =[ 604 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 708 ] , 90 .00th =[ 804 ] , 95 .00th =[ 932 ] , | 99 .00th =[ 70144 ] , 99 .50th =[ 102912 ] , 99 .90th =[ 456704 ] , | 99 .95th =[ 1253376 ] , 99 .99th =[ 26083328 ] bw ( KiB/s ) : min = 401376 , max = 401376 , per = 100 .00%, avg = 401376 .00, stdev = 0 .00, samples = 1 iops : min = 100344 , max = 100344 , avg = 100344 .00, stdev = 0 .00, samples = 1 lat ( nsec ) : 500 = 5 .72%, 750 = 80 .66%, 1000 = 9 .78% lat ( usec ) : 2 = 1 .74%, 4 = 0 .06%, 10 = 0 .17%, 20 = 0 .06%, 50 = 0 .11% lat ( usec ) : 100 = 1 .17%, 250 = 0 .37%, 500 = 0 .07%, 750 = 0 .02%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01% cpu : usr = 3 .29%, sys = 17 .02%, ctx = 571 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 386MiB/s ( 405MB/s ) , 386MiB/s-386MiB/s ( 405MB/s-405MB/s ) , io = 200MiB ( 210MB ) , run = 518 -518msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M sequential-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 729 : Wed Aug 10 06 :30:48 2022 read: IOPS = 344k, BW = 1342MiB/s ( 1407MB/s )( 200MiB/149msec ) clat ( nsec ) : min = 375 , max = 7097 .8k, avg = 2339 .11, stdev = 39079 .52 lat ( nsec ) : min = 406 , max = 7097 .8k, avg = 2372 .97, stdev = 39080 .14 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 588 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 628 ] , 60 .00th =[ 644 ] , 70 .00th =[ 668 ] , | 80 .00th =[ 700 ] , 90 .00th =[ 748 ] , 95 .00th =[ 868 ] , | 99 .00th =[ 67072 ] , 99 .50th =[ 73216 ] , 99 .90th =[ 114176 ] , | 99 .95th =[ 156672 ] , 99 .99th =[ 1122304 ] lat ( nsec ) : 500 = 2 .29%, 750 = 87 .79%, 1000 = 7 .05% lat ( usec ) : 2 = 0 .96%, 4 = 0 .01%, 10 = 0 .15%, 20 = 0 .04%, 50 = 0 .11% lat ( usec ) : 100 = 1 .42%, 250 = 0 .15%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2 = 0 .02%, 4 = 0 .01%, 10 = 0 .01% cpu : usr = 25 .68%, sys = 49 .32%, ctx = 335 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 1342MiB/s ( 1407MB/s ) , 1342MiB/s-1342MiB/s ( 1407MB/s-1407MB/s ) , io = 200MiB ( 210MB ) , run = 149 -149msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) big-file-multi-read: Laying out IO file ( 1 file / 200MiB ) Jobs: 2 ( f = 2 ) : [ _ ( 4 ) ,R ( 2 )][ 100 .0% ][ r = 264MiB/s ][ r = 67 .7k IOPS ][ eta 00m:00s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 732 : Wed Aug 10 06 :32:40 2022 read: IOPS = 11 .1k, BW = 43 .5MiB/s ( 45 .6MB/s )( 200MiB/4602msec ) clat ( nsec ) : min = 377 , max = 522247k, avg = 89494 .70, stdev = 4682750 .76 lat ( nsec ) : min = 408 , max = 522247k, avg = 89553 .19, stdev = 4682751 .34 clat percentiles ( nsec ) : | 1 .00th =[ 414 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 796 ] , | 80 .00th =[ 884 ] , 90 .00th =[ 1020 ] , 95 .00th =[ 1192 ] , | 99 .00th =[ 83456 ] , 99 .50th =[ 218112 ] , 99 .90th =[ 5144576 ] , | 99 .95th =[ 20578304 ] , 99 .99th =[ 240123904 ] bw ( KiB/s ) : min = 14080 , max = 90112 , per = 18 .59%, avg = 45625 .50, stdev = 27571 .34, samples = 8 iops : min = 3520 , max = 22528 , avg = 11406 .37, stdev = 6892 .84, samples = 8 lat ( nsec ) : 500 = 3 .47%, 750 = 60 .05%, 1000 = 25 .67% lat ( usec ) : 2 = 8 .49%, 4 = 0 .21%, 10 = 0 .15%, 20 = 0 .08%, 50 = 0 .07% lat ( usec ) : 100 = 1 .00%, 250 = 0 .33%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .05%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .59%, sys = 2 .00%, ctx = 671 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 733 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .7k, BW = 41 .9MiB/s ( 43 .9MB/s )( 200MiB/4776msec ) clat ( nsec ) : min = 376 , max = 734234k, avg = 92901 .06, stdev = 5934361 .60 lat ( nsec ) : min = 411 , max = 734234k, avg = 92951 .67, stdev = 5934362 .28 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 588 ] , 30 .00th =[ 636 ] , 40 .00th =[ 684 ] , | 50 .00th =[ 732 ] , 60 .00th =[ 788 ] , 70 .00th =[ 860 ] , | 80 .00th =[ 940 ] , 90 .00th =[ 1080 ] , 95 .00th =[ 1272 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 193536 ] , 99 .90th =[ 3981312 ] , | 99 .95th =[ 27131904 ] , 99 .99th =[ 233832448 ] bw ( KiB/s ) : min = 8192 , max = 98304 , per = 19 .42%, avg = 47655 .75, stdev = 31431 .05, samples = 8 iops : min = 2048 , max = 24576 , avg = 11913 .88, stdev = 7857 .79, samples = 8 lat ( nsec ) : 500 = 3 .87%, 750 = 50 .45%, 1000 = 31 .27% lat ( usec ) : 2 = 12 .03%, 4 = 0 .18%, 10 = 0 .15%, 20 = 0 .07%, 50 = 0 .06% lat ( usec ) : 100 = 1 .02%, 250 = 0 .48%, 500 = 0 .15%, 750 = 0 .05%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .05%, 4 = 0 .02%, 10 = 0 .03%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .02%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .44%, sys = 2 .07%, ctx = 845 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 734 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .9k, BW = 42 .6MiB/s ( 44 .7MB/s )( 200MiB/4696msec ) clat ( nsec ) : min = 379 , max = 607583k, avg = 91313 .18, stdev = 4982310 .93 lat ( nsec ) : min = 412 , max = 607583k, avg = 91361 .07, stdev = 4982311 .06 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 700 ] , 60 .00th =[ 740 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 130560 ] , 99 .90th =[ 4882432 ] , | 99 .95th =[ 14483456 ] , 99 .99th =[ 254803968 ] bw ( KiB/s ) : min = 7680 , max = 90112 , per = 17 .01%, avg = 41739 .89, stdev = 24407 .57, samples = 9 iops : min = 1920 , max = 22528 , avg = 10434 .89, stdev = 6101 .90, samples = 9 lat ( nsec ) : 500 = 3 .93%, 750 = 58 .07%, 1000 = 25 .64% lat ( usec ) : 2 = 10 .13%, 4 = 0 .22%, 10 = 0 .13%, 20 = 0 .04%, 50 = 0 .08% lat ( usec ) : 100 = 1 .10%, 250 = 0 .34%, 500 = 0 .08%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .04%, 20 = 0 .02%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .72%, sys = 1 .81%, ctx = 502 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 735 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .6k, BW = 41 .5MiB/s ( 43 .5MB/s )( 200MiB/4822msec ) clat ( nsec ) : min = 374 , max = 700599k, avg = 93757 .53, stdev = 5099889 .71 lat ( nsec ) : min = 413 , max = 700599k, avg = 93803 .83, stdev = 5099890 .00 clat percentiles ( nsec ) : | 1 .00th =[ 430 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 620 ] , 40 .00th =[ 660 ] , | 50 .00th =[ 692 ] , 60 .00th =[ 732 ] , 70 .00th =[ 804 ] , | 80 .00th =[ 900 ] , 90 .00th =[ 1048 ] , 95 .00th =[ 1256 ] , | 99 .00th =[ 91648 ] , 99 .50th =[ 226304 ] , 99 .90th =[ 5931008 ] , | 99 .95th =[ 24248320 ] , 99 .99th =[ 235929600 ] bw ( KiB/s ) : min = 6520 , max = 65536 , per = 15 .07%, avg = 36989 .89, stdev = 18379 .13, samples = 9 iops : min = 1630 , max = 16384 , avg = 9247 .44, stdev = 4594 .77, samples = 9 lat ( nsec ) : 500 = 3 .74%, 750 = 59 .39%, 1000 = 24 .25% lat ( usec ) : 2 = 10 .19%, 4 = 0 .22%, 10 = 0 .18%, 20 = 0 .08%, 50 = 0 .08% lat ( usec ) : 100 = 0 .96%, 250 = 0 .43%, 500 = 0 .14%, 750 = 0 .07%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .06%, 20 = 0 .02%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01% cpu : usr = 0 .21%, sys = 2 .24%, ctx = 874 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 736 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .2k, BW = 39 .9MiB/s ( 41 .9MB/s )( 200MiB/5008msec ) clat ( nsec ) : min = 377 , max = 806541k, avg = 97234 .58, stdev = 6320832 .41 lat ( nsec ) : min = 414 , max = 806541k, avg = 97345 .92, stdev = 6320844 .03 clat percentiles ( nsec ) : | 1 .00th =[ 402 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 652 ] , | 50 .00th =[ 684 ] , 60 .00th =[ 724 ] , 70 .00th =[ 772 ] , | 80 .00th =[ 868 ] , 90 .00th =[ 1032 ] , 95 .00th =[ 1240 ] , | 99 .00th =[ 80384 ] , 99 .50th =[ 154624 ] , 99 .90th =[ 5275648 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 200278016 ] bw ( KiB/s ) : min = 8192 , max = 49152 , per = 12 .40%, avg = 30434 .00, stdev = 15445 .75, samples = 9 iops : min = 2048 , max = 12288 , avg = 7608 .44, stdev = 3861 .51, samples = 9 lat ( nsec ) : 500 = 3 .77%, 750 = 62 .98%, 1000 = 22 .07% lat ( usec ) : 2 = 8 .67%, 4 = 0 .18%, 10 = 0 .26%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .05%, 250 = 0 .40%, 500 = 0 .11%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .05%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .02% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .46%, sys = 1 .96%, ctx = 787 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 737 : Wed Aug 10 06 :32:40 2022 read: IOPS = 10 .3k, BW = 40 .2MiB/s ( 42 .1MB/s )( 200MiB/4977msec ) clat ( nsec ) : min = 378 , max = 894860k, avg = 96613 .11, stdev = 5799729 .12 lat ( nsec ) : min = 413 , max = 894860k, avg = 96658 .78, stdev = 5799729 .19 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 506 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 612 ] , 40 .00th =[ 644 ] , | 50 .00th =[ 676 ] , 60 .00th =[ 716 ] , 70 .00th =[ 764 ] , | 80 .00th =[ 852 ] , 90 .00th =[ 988 ] , 95 .00th =[ 1176 ] , | 99 .00th =[ 87552 ] , 99 .50th =[ 216064 ] , 99 .90th =[ 6848512 ] , | 99 .95th =[ 31064064 ] , 99 .99th =[ 231735296 ] bw ( KiB/s ) : min = 16929 , max = 69632 , per = 14 .42%, avg = 35383 .25, stdev = 17459 .47, samples = 8 iops : min = 4232 , max = 17408 , avg = 8845 .75, stdev = 4364 .90, samples = 8 lat ( nsec ) : 500 = 4 .66%, 750 = 63 .16%, 1000 = 22 .55% lat ( usec ) : 2 = 7 .10%, 4 = 0 .20%, 10 = 0 .27%, 20 = 0 .09%, 50 = 0 .11% lat ( usec ) : 100 = 0 .99%, 250 = 0 .44%, 500 = 0 .12%, 750 = 0 .07%, 1000 = 0 .04% lat ( msec ) : 2 = 0 .06%, 4 = 0 .04%, 10 = 0 .03%, 20 = 0 .03%, 50 = 0 .01% lat ( msec ) : 100 = 0 .02%, 250 = 0 .02%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .32%, sys = 2 .05%, ctx = 1006 , majf = 0 , minf = 17 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 240MiB/s ( 251MB/s ) , 39 .9MiB/s-43.5MiB/s ( 41 .9MB/s-45.6MB/s ) , io = 1200MiB ( 1258MB ) , run = 4602 -5008msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = big-file-multi-read --directory = /config --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.25 Starting 6 processes Jobs: 3 ( f = 3 ) : [ _ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 ) ,_ ( 1 ) ,R ( 1 )][ 80 .0% ][ r = 172MiB/s ][ r = 44 .1k IOPS ][ eta 00m:02s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 740 : Wed Aug 10 06 :34:00 2022 read: IOPS = 8276 , BW = 32 .3MiB/s ( 33 .9MB/s )( 200MiB/6186msec ) clat ( nsec ) : min = 378 , max = 805881k, avg = 120250 .60, stdev = 7449083 .91 lat ( nsec ) : min = 410 , max = 805881k, avg = 120301 .98, stdev = 7449084 .14 clat percentiles ( nsec ) : | 1 .00th =[ 422 ] , 5 .00th =[ 532 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 580 ] , 30 .00th =[ 612 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 660 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 996 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 132096 ] , 99 .90th =[ 1318912 ] , | 99 .95th =[ 10289152 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8192 , max = 65536 , per = 23 .93%, avg = 35045 .82, stdev = 25507 .11, samples = 11 iops : min = 2048 , max = 16384 , avg = 8761 .45, stdev = 6376 .78, samples = 11 lat ( nsec ) : 500 = 2 .40%, 750 = 77 .33%, 1000 = 15 .36% lat ( usec ) : 2 = 2 .64%, 4 = 0 .06%, 10 = 0 .18%, 20 = 0 .11%, 50 = 0 .13% lat ( usec ) : 100 = 1 .11%, 250 = 0 .39%, 500 = 0 .09%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .03%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% cpu : usr = 0 .34%, sys = 1 .62%, ctx = 845 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 741 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6102 , BW = 23 .8MiB/s ( 24 .0MB/s )( 200MiB/8390msec ) clat ( nsec ) : min = 379 , max = 1190 .5M, avg = 162758 .58, stdev = 11051920 .43 lat ( nsec ) : min = 413 , max = 1190 .5M, avg = 162807 .90, stdev = 11051920 .69 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 700 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1004 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1073152 ] , | 99 .95th =[ 5275648 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 512 , max = 73728 , per = 20 .69%, avg = 30307 .20, stdev = 21673 .33, samples = 10 iops : min = 128 , max = 18432 , avg = 7576 .80, stdev = 5418 .33, samples = 10 lat ( nsec ) : 500 = 2 .76%, 750 = 73 .76%, 1000 = 18 .35% lat ( usec ) : 2 = 2 .79%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .13%, 50 = 0 .14% lat ( usec ) : 100 = 1 .05%, 250 = 0 .42%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .02%, 20 = 0 .01%, 100 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .33%, sys = 1 .10%, ctx = 982 , majf = 0 , minf = 16 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 742 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6906 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7413msec ) clat ( nsec ) : min = 380 , max = 1034 .3M, avg = 144218 .03, stdev = 9148025 .05 lat ( nsec ) : min = 414 , max = 1034 .3M, avg = 144254 .80, stdev = 9148025 .00 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 524 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 596 ] , 40 .00th =[ 620 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 676 ] , 70 .00th =[ 708 ] , | 80 .00th =[ 748 ] , 90 .00th =[ 860 ] , 95 .00th =[ 988 ] , | 99 .00th =[ 78336 ] , 99 .50th =[ 146432 ] , 99 .90th =[ 1253376 ] , | 99 .95th =[ 4620288 ] , 99 .99th =[ 522190848 ] bw ( KiB/s ) : min = 16384 , max = 73728 , per = 26 .85%, avg = 39318 .40, stdev = 23491 .33, samples = 10 iops : min = 4096 , max = 18432 , avg = 9829 .60, stdev = 5872 .83, samples = 10 lat ( nsec ) : 500 = 3 .29%, 750 = 76 .66%, 1000 = 15 .22% lat ( usec ) : 2 = 2 .43%, 4 = 0 .10%, 10 = 0 .23%, 20 = 0 .12%, 50 = 0 .12% lat ( usec ) : 100 = 1 .13%, 250 = 0 .37%, 500 = 0 .13%, 750 = 0 .05%, 1000 = 0 .03% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .32%, sys = 1 .32%, ctx = 864 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 743 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6106 , BW = 23 .9MiB/s ( 25 .0MB/s )( 200MiB/8385msec ) clat ( nsec ) : min = 380 , max = 1507 .9M, avg = 162772 .24, stdev = 13174473 .96 lat ( nsec ) : min = 414 , max = 1507 .9M, avg = 162810 .08, stdev = 13174473 .94 clat percentiles ( nsec ) : | 1 .00th =[ 398 ] , 5 .00th =[ 510 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 604 ] , 40 .00th =[ 636 ] , | 50 .00th =[ 668 ] , 60 .00th =[ 692 ] , 70 .00th =[ 724 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 79360 ] , 99 .50th =[ 136192 ] , 99 .90th =[ 897024 ] , | 99 .95th =[ 2506752 ] , 99 .99th =[ 434110464 ] bw ( KiB/s ) : min = 8 , max = 81920 , per = 20 .70%, avg = 30310 .40, stdev = 22461 .38, samples = 10 iops : min = 2 , max = 20480 , avg = 7577 .60, stdev = 5615 .35, samples = 10 lat ( nsec ) : 500 = 4 .29%, 750 = 73 .14%, 1000 = 17 .30% lat ( usec ) : 2 = 2 .94%, 4 = 0 .04%, 10 = 0 .22%, 20 = 0 .07%, 50 = 0 .19% lat ( usec ) : 100 = 1 .10%, 250 = 0 .44%, 500 = 0 .11%, 750 = 0 .05%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .03%, 4 = 0 .01%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .05%, sys = 1 .40%, ctx = 993 , majf = 0 , minf = 14 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 744 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6911 , BW = 26 .0MiB/s ( 28 .3MB/s )( 200MiB/7408msec ) clat ( nsec ) : min = 381 , max = 1179 .3M, avg = 143994 .58, stdev = 11079777 .48 lat ( nsec ) : min = 413 , max = 1179 .3M, avg = 144029 .70, stdev = 11079777 .45 clat percentiles ( nsec ) : | 1 .00th =[ 406 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 580 ] , 40 .00th =[ 612 ] , | 50 .00th =[ 636 ] , 60 .00th =[ 668 ] , 70 .00th =[ 692 ] , | 80 .00th =[ 732 ] , 90 .00th =[ 836 ] , 95 .00th =[ 956 ] , | 99 .00th =[ 76288 ] , 99 .50th =[ 111104 ] , 99 .90th =[ 995328 ] , | 99 .95th =[ 3227648 ] , 99 .99th =[ 742391808 ] bw ( KiB/s ) : min = 5040 , max = 73728 , per = 22 .93%, avg = 33587 .20, stdev = 27492 .28, samples = 10 iops : min = 1260 , max = 18432 , avg = 8396 .80, stdev = 6873 .07, samples = 10 lat ( nsec ) : 500 = 3 .81%, 750 = 78 .93%, 1000 = 13 .08% lat ( usec ) : 2 = 2 .02%, 4 = 0 .02%, 10 = 0 .21%, 20 = 0 .07%, 50 = 0 .12% lat ( usec ) : 100 = 1 .15%, 250 = 0 .36%, 500 = 0 .07%, 750 = 0 .04%, 1000 = 0 .02% lat ( msec ) : 2 = 0 .04%, 4 = 0 .02%, 10 = 0 .01%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01%, 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .35%, ctx = 724 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 745 : Wed Aug 10 06 :34:00 2022 read: IOPS = 6238 , BW = 24 .4MiB/s ( 25 .6MB/s )( 200MiB/8207msec ) clat ( nsec ) : min = 379 , max = 1170 .8M, avg = 159639 .78, stdev = 10672683 .50 lat ( nsec ) : min = 413 , max = 1170 .8M, avg = 159675 .27, stdev = 10672683 .47 clat percentiles ( nsec ) : | 1 .00th =[ 410 ] , 5 .00th =[ 516 ] , 10 .00th =[ 540 ] , | 20 .00th =[ 572 ] , 30 .00th =[ 596 ] , 40 .00th =[ 628 ] , | 50 .00th =[ 652 ] , 60 .00th =[ 684 ] , 70 .00th =[ 716 ] , | 80 .00th =[ 764 ] , 90 .00th =[ 876 ] , 95 .00th =[ 1012 ] , | 99 .00th =[ 77312 ] , 99 .50th =[ 156672 ] , 99 .90th =[ 1810432 ] , | 99 .95th =[ 5865472 ] , 99 .99th =[ 616562688 ] bw ( KiB/s ) : min = 6400 , max = 74752 , per = 18 .50%, avg = 27096 .62, stdev = 22310 .47, samples = 13 iops : min = 1600 , max = 18688 , avg = 6774 .15, stdev = 5577 .62, samples = 13 lat ( nsec ) : 500 = 4 .00%, 750 = 74 .22%, 1000 = 16 .52% lat ( usec ) : 2 = 3 .01%, 4 = 0 .05%, 10 = 0 .19%, 20 = 0 .07%, 50 = 0 .11% lat ( usec ) : 100 = 1 .06%, 250 = 0 .40%, 500 = 0 .14%, 750 = 0 .05%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .04%, 4 = 0 .03%, 10 = 0 .03%, 20 = 0 .01%, 50 = 0 .01% lat ( msec ) : 100 = 0 .01%, 250 = 0 .01%, 500 = 0 .01%, 750 = 0 .01%, 1000 = 0 .01% lat ( msec ) : 2000 = 0 .01% cpu : usr = 0 .26%, sys = 1 .22%, ctx = 949 , majf = 0 , minf = 15 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 51200 ,0,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : READ: bw = 143MiB/s ( 150MB/s ) , 23 .8MiB/s-32.3MiB/s ( 24 .0MB/s-33.9MB/s ) , io = 1200MiB ( 1258MB ) , run = 6186 -8390msec fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 fio --name=big-file-multi-read --directory=$PWD --rw=read --refill_buffers --bs=4K --size=200M --numjobs=6 fio --name=sequential-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --end_fsync=1 root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process sequential-write: Laying out IO file ( 1 file / 200MiB ) Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 756 : Wed Aug 10 06 :39:40 2022 write: IOPS = 33 .6k, BW = 131MiB/s ( 138MB/s )( 200MiB/1525msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 7420 , avg = 27 .69, stdev = 125 .85 lat ( usec ) : min = 7 , max = 7420 , avg = 27 .76, stdev = 125 .85 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 12 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 28 ] , 95 .00th =[ 37 ] , | 99 .00th =[ 118 ] , 99 .50th =[ 285 ] , 99 .90th =[ 1860 ] , 99 .95th =[ 3097 ] , | 99 .99th =[ 4752 ] bw ( KiB/s ) : min = 132286 , max = 138088 , per = 100 .00%, avg = 135187 .00, stdev = 4102 .63, samples = 2 iops : min = 33071 , max = 34522 , avg = 33796 .50, stdev = 1026 .01, samples = 2 lat ( usec ) : 10 = 9 .54%, 20 = 24 .43%, 50 = 63 .08%, 100 = 1 .75%, 250 = 0 .66% lat ( usec ) : 500 = 0 .22%, 750 = 0 .09%, 1000 = 0 .05% lat ( msec ) : 2 = 0 .10%, 4 = 0 .07%, 10 = 0 .03% cpu : usr = 9 .84%, sys = 31 .04%, ctx = 51905 , majf = 0 , minf = 12 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 131MiB/s ( 138MB/s ) , 131MiB/s-131MiB/s ( 138MB/s-138MB/s ) , io = 200MiB ( 210MB ) , run = 1525 -1525msec root@nginx-run-685fdf6467-mdl9v:/config# fio --name = sequential-write --directory = /config --rw = write --refill_buffers --bs = 4K --size = 200M --end_fsync = 1 sequential-write: ( g = 0 ) : rw = write, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 fio-3.25 Starting 1 process Jobs: 1 ( f = 1 ) sequential-write: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 759 : Wed Aug 10 06 :41:20 2022 write: IOPS = 31 .3k, BW = 122MiB/s ( 128MB/s )( 200MiB/1637msec ) ; 0 zone resets clat ( usec ) : min = 7 , max = 9234 , avg = 30 .16, stdev = 137 .54 lat ( usec ) : min = 7 , max = 9234 , avg = 30 .21, stdev = 137 .54 clat percentiles ( usec ) : | 1 .00th =[ 8 ] , 5 .00th =[ 9 ] , 10 .00th =[ 11 ] , 20 .00th =[ 18 ] , | 30 .00th =[ 20 ] , 40 .00th =[ 21 ] , 50 .00th =[ 22 ] , 60 .00th =[ 22 ] , | 70 .00th =[ 23 ] , 80 .00th =[ 24 ] , 90 .00th =[ 29 ] , 95 .00th =[ 41 ] , | 99 .00th =[ 147 ] , 99 .50th =[ 379 ] , 99 .90th =[ 2311 ] , 99 .95th =[ 3064 ] , | 99 .99th =[ 4490 ] bw ( KiB/s ) : min = 119544 , max = 132640 , per = 100 .00%, avg = 128018 .67, stdev = 7349 .32, samples = 3 iops : min = 29886 , max = 33160 , avg = 32004 .67, stdev = 1837 .33, samples = 3 lat ( usec ) : 10 = 7 .32%, 20 = 25 .16%, 50 = 63 .95%, 100 = 2 .08%, 250 = 0 .85% lat ( usec ) : 500 = 0 .23%, 750 = 0 .09%, 1000 = 0 .08% lat ( msec ) : 2 = 0 .12%, 4 = 0 .11%, 10 = 0 .02% cpu : usr = 7 .21%, sys = 32 .64%, ctx = 51971 , majf = 0 , minf = 13 IO depths : 1 = 100 .0%, 2 = 0 .0%, 4 = 0 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, > = 64 = 0 .0% submit : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% complete : 0 = 0 .0%, 4 = 100 .0%, 8 = 0 .0%, 16 = 0 .0%, 32 = 0 .0%, 64 = 0 .0%, > = 64 = 0 .0% issued rwts: total = 0 ,51200,0,0 short = 0 ,0,0,0 dropped = 0 ,0,0,0 latency : target = 0 , window = 0 , percentile = 100 .00%, depth = 1 Run status group 0 ( all jobs ) : WRITE: bw = 122MiB/s ( 128MB/s ) , 122MiB/s-122MiB/s ( 128MB/s-128MB/s ) , io = 200MiB ( 210MB ) , run = 1637 -1637msec fio --name=big-file-multi-write --directory=/config --rw=write --refill_buffers --bs=4K --size=200M --numjobs=6 --end_fsync=1 fio -filename=/config/fio.img -direct=1 -iodepth 32 -thread -rw=randread -ioengine=libaio -bs=4k -size=200m -numjobs=2 -runtime=60 -group_reporting -name=mytest \u56fa\u6001\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes Jobs: 6 ( f = 6 ) : [ R ( 6 )][ 88 .9% ][ r = 130MiB/s ][ r = 33 .2k IOPS ][ eta 00m:01s ] big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 225237 : Fri Aug 12 14 :09:04 2022 read: IOPS = 5925 , BW = 23 .1MiB/s ( 24 .3MB/s )( 200MiB/8641msec ) clat ( nsec ) : min = 434 , max = 21757k, avg = 168221 .11, stdev = 1662876 .10 lat ( nsec ) : min = 471 , max = 21757k, avg = 168260 .51, stdev = 1662876 .63 clat percentiles ( nsec ) : | 1 .00th =[ 524 ] , 5 .00th =[ 540 ] , 10 .00th =[ 548 ] , | 20 .00th =[ 564 ] , 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , | 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , 70 .00th =[ 628 ] , | 80 .00th =[ 660 ] , 90 .00th =[ 732 ] , 95 .00th =[ 884 ] , | 99 .00th =[ 3948544 ] , 99 .50th =[ 19005440 ] , 99 .90th =[ 20054016 ] , | 99 .95th =[ 20054016 ] , 99 .99th =[ 20054016 ] \u5185\u5b58\u5bbf\u4e3b\u673a\uff1a [ ucloud ] root@node1:/var/jfsCache# fio --name = big-file-multi-read --directory = $PWD --rw = read --refill_buffers --bs = 4K --size = 200M --numjobs = 6 big-file-multi-read: ( g = 0 ) : rw = read, bs =( R ) 4096B-4096B, ( W ) 4096B-4096B, ( T ) 4096B-4096B, ioengine = psync, iodepth = 1 ... fio-3.16 Starting 6 processes big-file-multi-read: ( groupid = 0 , jobs = 1 ) : err = 0 : pid = 12095 : Fri Aug 12 15 :05:27 2022 read: IOPS = 966k, BW = 3774MiB/s ( 3957MB/s )( 200MiB/53msec ) clat ( nsec ) : min = 520 , max = 221610 , avg = 757 .38, stdev = 2561 .09 lat ( nsec ) : min = 553 , max = 221646 , avg = 792 .50, stdev = 2561 .18 clat percentiles ( nsec ) : | 1 .00th =[ 540 ] , 5 .00th =[ 556 ] , 10 .00th =[ 556 ] , 20 .00th =[ 564 ] , | 30 .00th =[ 572 ] , 40 .00th =[ 580 ] , 50 .00th =[ 596 ] , 60 .00th =[ 612 ] , | 70 .00th =[ 644 ] , 80 .00th =[ 724 ] , 90 .00th =[ 908 ] , 95 .00th =[ 940 ] , | 99 .00th =[ 3344 ] , 99 .50th =[ 3728 ] , 99 .90th =[ 19072 ] , 99 .95th =[ 43264 ] , | 99 .99th =[ 115200 ] fio --name=small-file-multi-read \\ --directory=/config \\ --rw=read --file_service_type=sequential \\ --bs=4k --filesize=4k --nrfiles=500 \\ --numjobs=2","title":"\u5bb9\u5668\u8bbf\u95ee\u5bf9\u8c61\u5b58\u50a8"},{"location":"ansible/ansible-case1/","text":"Ansible \u6982\u8ff0 \u00b6 \u7531\u4e8e\u4e92\u8054\u7f51\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u4ea7\u54c1\u66f4\u65b0\u6362\u4ee3\u901f\u5ea6\u9010\u6b65\u589e\u957f\uff0c\u8fd0\u7ef4\u4eba\u5458\u6bcf\u5929\u90fd\u8981\u8fdb\u884c \u5927\u91cf\u7684\u7ef4\u62a4\u64cd\u4f5c\uff0c\u6309\u7167\u4f20\u7edf\u65b9\u5f0f\u8fdb\u884c\u7ef4\u62a4\u4f7f\u5f97\u5de5\u4f5c\u6548\u7387\u4f4e\u4e0b\u3002\u8fd9\u65f6\u90e8\u7f72\u81ea\u52a8\u5316\u8fd0\u7ef4\u5c31 \u53ef\u4ee5\u5c3d\u53ef\u80fd\u5b89\u5168\u3001\u9ad8\u6548\u7684\u5b8c\u6210\u8fd9\u4e9b\u5de5\u4f5c\u3002 Ansible \u662f\u57fa\u4e8e Python \u5f00\u53d1\uff0c\u96c6\u5408\u4e86\u4f17\u591a\u4f18\u79c0\u8fd0\u7ef4\u5de5\u5177\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u6279\u91cf\u8fd0\u884c \u547d\u4ee4\u3001\u90e8\u7f72\u7a0b\u5e8f\u3001\u914d\u7f6e\u7cfb\u7edf\u7b49\u529f\u80fd\u7684\u81ea\u52a8\u5316\u8fd0\u7ef4\u7ba1\u7406\u5de5\u5177\u3002\u9ed8\u8ba4\u901a\u8fc7 SSH \u534f\u8bae\u8fdb\u884c \u8fdc\u7a0b\u547d\u4ee4\u6267\u884c\u6216\u4e0b\u53d1\u914d\u7f6e\uff0c\u65e0\u9700\u90e8\u7f72\u4efb\u4f55\u5ba2\u6237\u7aef\u4ee3\u7406\u8f6f\u4ef6\uff0c\u4ece\u800c\u4f7f\u5f97\u81ea\u52a8\u5316\u73af\u5883\u90e8\u7f72 \u53d8\u5f97\u66f4\u52a0\u7b80\u5355\u3002\u53ef\u540c\u65f6\u652f\u6301\u591a\u53f0\u4e3b\u673a\u5e76\u884c\u7ba1\u7406\uff0c\u4f7f\u5f97\u7ba1\u7406\u4e3b\u673a\u66f4\u52a0\u4fbf\u6377\u3002 Ansible \u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u8fdb\u884c\u5de5\u4f5c\u7684\u6846\u67b6\u7ed3\u6784\uff0c\u6279\u91cf\u90e8\u7f72\u80fd\u529b\u5c31\u662f\u7531 Ansible \u6240\u8fd0\u884c\u7684\u6a21\u5757\u5b9e\u73b0\u7684\u3002\u7b80\u800c\u8a00\u4e4b Ansible \u662f\u57fa\u4e8e\u201c\u6a21\u5757\u201d\u5b8c\u6210\u5404\u79cd\u201c\u4efb\u52a1\u201d\u7684\u3002\u5176 \u57fa\u672c\u6846\u67b6\u7ed3\u6784\u5982\u56fe 3.1 \u6240\u793a\u3002 \u7531\u56fe 3.1 \u53ef\u4ee5\u5f97\u51fa Ansible \u7684\u57fa\u672c\u67b6\u6784\u7531\u516d\u5927\u4ef6\u6784\u6210\u3002 Ansible core \u6838\u5fc3\u5f15\u64ce\uff1a\u5373 Ansible \u672c\u8eab\uff1b Host Inventory \u4e3b\u673a\u6e05\u5355\uff1a\u7528\u6765\u5b9a\u4e49 Ansible \u6240\u7ba1\u7406\u4e3b\u673a\uff0c\u9ed8\u8ba4\u662f\u5728 Ansible \u7684 hosts \u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9a\u4e49\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u540c\u65f6\u4e5f\u652f\u6301\u81ea\u5b9a\u4e49\u52a8\u6001\u4e3b\u673a\u6e05\u5355\u548c\u6307\u5b9a\u5176\u5b83 \u914d\u7f6e\u6587\u4ef6\u7684\u4f4d\u7f6e\uff1b Connect plugin \u8fde\u63a5\u63d2\u4ef6\uff1a\u8d1f\u8d23\u548c\u88ab\u7ba1\u7406\u4e3b\u673a\u5b9e\u73b0\u901a\u4fe1\u3002\u9664\u652f\u6301\u4f7f\u7528 SSH \u8fde\u63a5 \u88ab\u7ba1\u7406\u4e3b\u673a\u5916\uff0cAnsible \u8fd8\u652f\u6301\u5176\u5b83\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u6240\u4ee5\u9700\u8981\u6709\u8fde\u63a5\u63d2\u4ef6\u5c06\u5404\u4e2a\u4e3b \u673a\u7528\u8fde\u63a5\u63d2\u4ef6\u8fde\u63a5\u5230 Ansible\uff1b Playbook\uff08yaml\uff0cjinjia2\uff09\u5267\u672c\uff1a\u7528\u6765\u96c6\u4e2d\u5b9a\u4e49 Ansible \u4efb\u52a1\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u5c06 \u591a\u4e2a\u4efb\u52a1\u5b9a\u4e49\u5728\u4e00\u4e2a\u5267\u672c\u4e2d\u7531 Ansible \u81ea\u52a8\u6267\u884c\uff0c\u53ef\u4ee5\u7531\u63a7\u5236\u4e3b\u673a\u9488\u5bf9\u591a\u53f0\u88ab\u7ba1 \u7406\u4e3b\u673a\u540c\u65f6\u8fd0\u884c\u591a\u4e2a\u4efb\u52a1\uff1b Core modules \u6838\u5fc3\u6a21\u5757\uff1a\u662f Ansible \u81ea\u5e26\u7684\u6a21\u5757\uff0c\u4f7f\u7528\u8fd9\u4e9b\u6a21\u5757\u5c06\u8d44\u6e90\u5206\u53d1\u5230\u88ab \u7ba1\u7406\u4e3b\u673a\u4f7f\u5176\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u6216\u5339\u914d\u7279\u5b9a\u7684\u72b6\u6001\uff1b Custom modules \u81ea\u5b9a\u4e49\u6a21\u5757\uff1a\u7528\u4e8e\u5b8c\u6210\u6a21\u5757\u529f\u80fd\u7684\u8865\u5145\uff0c\u53ef\u501f\u52a9\u76f8\u5173\u63d2\u4ef6\u5b8c\u6210 \u8bb0\u5f55\u65e5\u5fd7\u3001\u53d1\u9001\u90ae\u4ef6\u7b49\u529f\u80fd\u3002 \u5b89\u88c5\u90e8\u7f72 Ansible \u670d\u52a1 \u00b6 Ansible \u81ea\u52a8\u5316\u8fd0\u7ef4\u73af\u5883\u7531\u63a7\u5236\u4e3b\u673a\u4e0e\u88ab\u7ba1\u7406\u4e3b\u673a\u7ec4\u6210\u3002\u7531\u4e8e Ansible \u662f\u57fa\u4e8e SSH \u534f\u8bae \u8fdb\u884c\u901a\u4fe1\u7684\uff0c\u6240\u4ee5\u63a7\u5236\u4e3b\u673a\u5b89\u88c5 Ansible \u8f6f\u4ef6\u540e\u4e0d\u9700\u8981\u91cd\u542f\u6216\u8fd0\u884c\u4efb\u4f55\u7a0b\u5e8f\uff0c\u88ab\u7ba1\u7406\u4e3b\u673a\u4e5f \u4e0d\u9700\u8981\u5b89\u88c5\u548c\u8fd0\u884c\u4efb\u4f55\u4ee3\u7406\u7a0b\u5e8f\u3002 \u5b89\u88c5 Ansible \u00b6 Centos \u7cfb\u7edf Ansible \u53ef\u4ee5\u4f7f\u7528\u6e90\u7801\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u4e2d YUM \u8f6f\u4ef6\u5305\u7ba1\u7406\u5de5 \u5177\u8fdb\u884c\u5b89\u88c5\u3002 YUM \u65b9\u5f0f\u5b89\u88c5 Ansible\uff0c\u9700\u8981\u4f9d\u8d56\u7b2c\u4e09\u65b9\u7684 EPEL \u6e90\uff0c\u4e0b\u9762\u914d\u7f6e EPEL \u6e90 \u4f5c\u4e3a\u90e8\u7f72 Ansible \u7684 YUM \u6e90 [ root@ansible-node1 ~ ] # yum install -y epel-release [ root@ansible-node1 ~ ] # yum install -y ansible [ root@ansible-node1 ~ ] # ansible --version //\u67e5\u770bansible\u7248\u672c Mac \u7cfb\u7edf \u5b98\u65b9\u6587\u7ae0\u53c2\u8003\u5730\u5740 \u8fd9\u91cc\u4e00\u822c\u6211\u4f1a\u4f7f\u7528Mac\u7535\u8111\u4f5c\u4e3aansible-server \u6765\u6267\u884cansible\u811a\u672c brew install ansible \u914d\u7f6e\u4e3b\u673a\u6e05\u5355 \u00b6 \u51c6\u5907\u5de5\u4f5c \u914d\u7f6einventory/pve [ pve ] master.pve hostname = master ansible_host = 192 .168.1.11 ansible_ssh_user = root ansible_ssh_pass = \"pass@word1\" \u8fd9\u91cc\u9996\u5148\u6211\u4eec\u5148\u660e\u767d\uff0c\u6bcf\u4e00\u4e2a\u88ab\u63a7\u5236\u7684\u4e3b\u673a\u9ed8\u8ba4\u80af\u5b9a\u4f1a\u6709\u4e00\u4e2a\u5e10\u53f7\u548c\u5bc6\u7801\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u666e\u901a\u7528\u6237\u6765\u5bf9\u8be5\u4e3b\u673a\u8fdb\u884c\u4e00\u7cfb\u5217\u64cd\u4f5c\uff5e \u6e29\u99a8\u63d0\u793a \u53ef\u80fd\u51fa\u73b0sudo\u6743\u9650\u4e0d\u591f\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u5728inventory\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9 gtx3090.c1 hostname = ubuntu ansible_ssh_user = xxx ansible_sudo_pass = xxx ansible_ssh_pass = \"password\" \u8bbe\u7f6eSSH\u65e0\u5bc6\u7801\u767b\u9646 \u00b6 \u5982\u679c\u4e0a\u4e2a\u6b65\u9aa4\u914d\u7f6e\u597d\uff0c\u8fd9\u4e2a\u6b65\u9aa4\u53ef\u4ee5\u5ffd\u7565\uff5e \u5f53\u7136\u53ef\u4ee5\u7ed3\u5408\u4f7f\u7528\uff0c\u5177\u4f53\u573a\u666f\u8fd8\u9700\u8981\u6839\u636e\u81ea\u5df1\u60c5\u51b5\u6765\u914d\u7f6e \u4e3a\u4e86\u907f\u514d Ansible \u4e0b\u53d1\u6307\u4ee4\u65f6\u9700\u8981\u8f93\u5165\u88ab\u7ba1\u7406\u4e3b\u673a\u7684\u5bc6\u7801\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bc1\u4e66\u7b7e\u540d\u8fbe\u5230 SSH \u65e0\u5bc6\u7801\u767b\u5f55\u3002\u4f7f\u7528 ssh-keygen \u4ea7\u751f\u4e00\u5bf9\u5bc6\u94a5\uff0c\u5e76\u901a\u8fc7 ssh-copy-id \u547d\u4ee4\u6765\u53d1\u9001\u751f\u6210\u7684\u516c\u94a5\u3002 Ansible \u547d\u4ee4\u5e94\u7528\u57fa\u7840 \u00b6 Ansible \u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u8fdb\u884c\u81ea\u52a8\u5316\u7ba1\u7406\u3002\u547d\u4ee4\u7684\u57fa\u672c\u8bed\u6cd5\u5982\u4e0b\u6240\u793a\uff1a ansible <host-pattern> [-m module_name] [-a args] Ansible \u5e38\u7528\u9009\u9879 host-pattern\uff1a\u6307\u5b9a\u88ab\u7ba1\u7406\u4e3b\u673a [-m module_name]\uff1a\u6307\u5b9a\u6240\u4f7f\u7528\u7684\u6a21\u5757 [-a args]\uff1a\u8bbe\u7f6e\u6a21\u5757\u5bf9\u5e94\u7684\u53c2\u6570 -C //\u9884\u6267\u884c --list-hosts //\u5217\u51fa\u6b63\u5728\u8fd0\u884c\u4efb\u52a1\u7684\u4e3b\u673a --list-tasks //\u5217\u51fatasks --limit \u4e3b\u673a\u5217\u8868 //\u53ea\u9488\u5bf9\u7279\u5b9a\u4e3b\u673a\u6267\u884c Ansible \u7684\u547d\u4ee4\u884c\u7ba1\u7406\u5de5\u5177\u90fd\u662f\u7531\u4e00\u7cfb\u5217\u6a21\u5757\u3001\u53c2\u6570\u7ec4\u6210\u7684\uff0c\u4f7f\u7528\u67d0\u4e9b\u6a21\u5757\u6216\u53c2\u6570\u4e4b\u524d\uff0c \u53ef\u4ee5\u5728\u547d\u4ee4\u540e\u9762\u52a0\u4e0a-h \u6216--help \u6765\u83b7\u53d6\u5e2e\u52a9\u3002\u4f8b\u5982\uff0cansible-doc \u5de5\u5177\u53ef\u4ee5\u4f7f\u7528 ansible-doc -h \u6216\u8005 ansible-doc --help \u67e5\u770b\u5176\u5e2e\u52a9\u4fe1\u606f\u3002 ansible-doc \u5de5\u5177\u7528\u4e8e\u67e5\u770b\u6a21\u5757\u5e2e\u52a9\u4fe1\u606f\u3002\u4e3b\u8981\u9009\u9879\u5305\u62ec\uff1a -l \u7528\u6765\u5217\u51fa\u53ef\u4f7f\u7528\u7684\u6a21\u5757\uff1b -s \u7528\u6765\u5217\u51fa\u67d0\u4e2a\u6a21\u5757\u7684\u63cf\u8ff0\u4fe1\u606f\u548c\u4f7f\u7528\u793a\u5217\u3002 \u4f8b\u5982\uff1a\u4e0b\u9762\u64cd\u4f5c\u53ef\u4ee5\u5217\u51fa yum \u6a21\u5757\u7684\u63cf\u8ff0\u4fe1\u606f\u548c\u64cd\u4f5c\u52a8\u4f5c\u3002 [ root@ansible-node1 ~ ] # ansible-doc -s yum Ansible \u81ea\u5e26\u4e86\u5f88\u591a\u6a21\u5757\uff0c\u80fd\u591f\u4e0b\u53d1\u6267\u884c Ansible \u7684\u5404\u79cd\u7ba1\u7406\u4efb\u52a1\u3002\u9996\u5148\u6765\u4e86\u89e3\u4e0b Ansible \u5e38\u7528\u7684\u8fd9\u4e9b\u6838\u5fc3\u6a21\u5757\u3002 1. command \u6a21\u5757 \u00b6 Ansibale \u7ba1\u7406\u5de5\u5177\u4f7f\u7528-m \u9009\u9879\u6765\u6307\u5b9a\u6240\u4f7f\u7528\u6a21\u5757\uff0c\u9ed8\u8ba4\u4f7f\u7528 command \u6a21\u5757\uff0c\u5373 -m \u9009 \u9879\u7701\u7565\u65f6\u4f1a\u8fd0\u884c\u6b64\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002\u4f8b\u5982\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u6267\u884c date \u547d\u4ee4\uff0c\u663e\u793a\u88ab\u7ba1\u7406\u4e3b\u673a\u65f6\u95f4\u3002\u6709\u4e09\u79cd\u6267\u884c\u547d\u4ee4\u7684\u65b9\u5f0f\u6765\u7ba1\u7406\u5199\u5165\u4e3b\u673a\u6e05\u5355\u4e2d\u7684\u4e3b\u673a\u3002 \u793a\u4f8b\u4e00\uff1a\u4f7f\u7528 IP \u5730\u5740\u67e5\u770b\u88ab\u7ba1\u7406\u4e3b\u673a\u65e5\u671f $ ansible cka -m command -a 'date' --limit cka.master cka.master | CHANGED | rc = 0 >> Wed Oct 12 12 :24:17 AM CST 2022 \u6e29\u99a8\u63d0\u793a \u8fd9\u91cc\u6211\u4f7f\u7528\u7684mac\u7b14\u8bb0\u672c\uff0c\u4f7f\u7528\u7684ansible-playbook\uff0c\u5728\u4ed3\u5e93\u4e2d\u6709\u4e00\u4e2a\u5173\u4e8eansible.cfg\u7684\u914d\u7f6e\u6587\u4ef6\u3002 $ cat ansible.cfg [ defaults ] pipelining = True strategy = free host_key_checking = False inventory = inventory //\u8fd9\u91cc\u4e3b\u8981\u7684\u76ee\u5f55\uff0c\u4e3b\u673a\u6e05\u5355\u6587\u4ef6\u4f4d\u7f6e library = library # gathering = smart fact_caching = redis # redis on stg-master fact_caching_connection = 106 .75.63.184:3389:0:OpenBayesAnsibleCache fact_caching_timeout = 0 forks = 16 [ privilege_escalation ] become = True [ diff ] always = yes \u793a\u4f8b\u4e8c\uff1a\u4f7f\u7528\u7ba1\u63a7\u4e3b\u673a\u5206\u522b\u67e5\u770b\u88ab\u7ba1\u7406 cka-1 \u548c cka-2 \u7ec4\u91cc\u9762\u6240\u6709\u4e3b\u673a\u7684\u65e5\u671f \u9996\u5148\uff0c\u9700\u8981\u51c6\u5907\u4e3b\u673a\u6587\u4ef6 $ cat inventory/cka [ cka-1 ] cka.master hostname = master0 ansible_host = 172 .30.42.244 ansible_user = lixie [ cka-2 ] cka.node1 hostname = node1 ansible_host = 172 .30.192.106 ansible_user = lixie cka.node2 hostname = node2 ansible_host = 172 .30.226.223 ansible_user = lixie \u5206\u7ec4\u6267\u884cansible \u547d\u4ee4 $ ansible cka-1 -m command -a 'date' //\u7b2c\u4e00\u7ec4 cka.master | CHANGED | rc = 0 >> Wed Oct 12 12 :42:15 AM CST 2022 $ ansible cka-2 -m command -a 'date' //\u7b2c\u4e8c\u7ec4 cka.node1 | CHANGED | rc = 0 >> Wed Oct 12 12 :38:55 AM CST 2022 cka.node2 | CHANGED | rc = 0 >> Wed Oct 12 12 :38:55 AM CST 2022 \u5f53\u7136\u4e86\uff0c\u5982\u679c\u4e0a\u9762\u7684\u53ef\u4ee5\u6b63\u786e\u6267\u884c\uff0c\u90a3\u4e48\u5176\u5b9e\u547d\u4ee4\u4e5f\u5c31\u662f\u6709\u624b\u5c31\u884c \u793a\u4f8b: $ ansible cka-2 -m command -a 'df -h' \u793a\u4f8b\u4e09\uff1a\u67e5\u770b\u6240\u6709\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u7684\u65e5\u671f [ root@ansible-node1 ~ ] # ansible all -m command -a 'date 2. User \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 user \u6a21\u5757\u7528\u4e8e\u521b\u5efa\u65b0\u7528\u6237\u548c\u66f4\u6539\u3001\u5220\u9664\u5df2\u5b58\u5728\u7684\u7528\u6237\u3002\u5176\u4e2d name \u9009\u9879\u7528 \u4e8e\u6307\u660e\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u3002\u4e3b\u8981\u5305\u62ec\u4e24\u79cd\u72b6\u6001\uff08state\uff09\uff1a present \u8868\u793a\u6dfb\u52a0 (\u7701\u7565\u72b6\u6001\u65f6\u9ed8\u8ba4\u4f7f\u7528)\uff1b absent \u8868\u793a\u79fb\u9664 \u793a\u4f8b\u4e00\uff1a\u5728\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u4e2a user1 \u7528\u6237\u3002 $ ansible cka-1 -m user -a 'name=\"user1\"' \u793a\u4f8b\u4e8c\uff1a\u5220\u9664\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237 user1\u3002 $ ansible cka-1 -m user -a 'name=\"user1\" state=absent ' \u793a\u4f8b\u4e09: ansible\u521b\u5efa\u7528\u6237\u5e76\u6307\u5b9a\u5bc6\u7801 \u751f\u6210\u52a0\u5bc6\u5bc6\u7801 $ a = $( python3 -c 'import crypt,getpass;pw=\"pass@word\";print(crypt.crypt(pw))' ) $ echo $a 8e7klQ1BR7DIY \u66f4\u65b0\u5bc6\u7801 $ ansible note1 -m user -a 'name=test password=\"$a\" update_password=always' //\u53ef\u4ee5\u4f7f\u7528\u53d8\u91cf $ ansible cka-1 -m user -a 'name=\"user1\" password=8e7klQ1BR7DIY update_password=always' 3. cron \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 cron \u6a21\u5757\u7528\u4e8e\u5b9a\u4e49\u4efb\u52a1\u8ba1\u5212\u3002\u4e3b\u8981\u5305\u62ec\u4e24\u79cd\u72b6\u6001\uff08state\uff09\uff1a present \u8868\u793a\u6dfb\u52a0 (\u7701\u7565\u72b6\u6001\u65f6\u9ed8\u8ba4\u4f7f\u7528) absent \u8868\u793a\u79fb\u9664\u3002 4. group \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 group \u6a21\u5757\u7528\u4e8e\u5bf9\u7528\u6237\u7ec4\u8fdb\u884c\u7ba1\u7406\u3002 \u793a\u4f8b\u4e00\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u521b\u5efa mysql \u7ec4\uff0cgid \u4e3a 555\u3002 $ ansible cka-1 -m group -a 'name=mysql gid=555 system=yes' \u793a\u4f8b\u4e8c\uff1a\u5c06\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u7684 mysql \u7528\u6237\u6dfb\u52a0\u5230 mysql \u7ec4\u4e2d\u3002 ansible cka-1 -m user -a 'name=mysql uid=555 system=yes group=mysql' 5. copy \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 copy \u6a21\u5757\u7528\u4e8e\u5b9e\u73b0\u6587\u4ef6\u590d\u5236\u548c\u6279\u91cf\u4e0b\u53d1\u6587\u4ef6\u3002\u5176\u4e2d\u4f7f\u7528 src \u6765\u5b9a\u4e49\u672c\u5730\u6e90\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 dest \u5b9a\u4e49\u88ab\u7ba1\u7406\u4e3b\u673a\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 content \u5219\u662f\u4f7f\u7528\u6307\u5b9a\u4fe1\u606f\u5185\u5bb9\u751f\u6210 \u76ee\u6807\u6587\u4ef6\u3002 \u793a \u4f8b \u4e00 \uff1a \u5c06\u672c\u5730\u6587\u4ef6 ucloud.yaml \u590d\u5236\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u7684\u6240\u6709\u4e3b\u673a\u4e0a\u7684 /tmp/(\u5f53\u7136\u4e5f\u53ef\u4ee5\u91cd\u547d\u540d)\uff0c\u5e76\u5c06\u6240\u6709\u8005\u8bbe\u7f6e\u4e3a root\uff0c\u6743\u9650\u8bbe\u7f6e\u4e3a 640\u3002 $ ansible cka-1 -m copy -a 'src=ucloud.yaml dest=/tmp/ owner=root mode=640' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c\u3002 [ cka ] root@master0:/tmp# ll total 44 drwxrwxrwt 10 root root 4096 Oct 12 10 :37 ./ drwxr-xr-x 19 root root 4096 May 24 23 :27 ../ -rw-r----- 1 root root 1172 Oct 12 10 :33 ucloud.yaml \u6e29\u99a8\u63d0\u793a \u5982\u679c\u51fa\u73b0\u4ee5\u4e0b\u7684\u62a5\u9519\u4fe1\u606f\uff0c\u662f\u56e0\u4e3a\u88ab\u7ba1\u7406\u4e3b\u673a\u5f00\u542f\u4e86 SELinux\uff0c\u9700\u8981\u5728\u88ab\u7ba1\u7406\u673a\u4e0a\u5b89\u88c5 libselinux-python \u8f6f\u4ef6\u5305\uff0c\u624d\u53ef\u4ee5\u4f7f\u7528 Ansible \u4e2d\u4e0e copy\u3001file \u76f8\u5173\u7684\u51fd\u6570\u3002 \"msg\": \"Aborting, target uses selinux but python bindings (libselinux-python) aren't installed!\" \u793a\u4f8b\u4e8c\uff1a\u5c06\u201dHello Ansible Hi Ansible\u201d \u5199\u5165\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u7684/tmp/ucloud.yaml \u6587\u4ef6\u4e2d\u3002 \u5f53\u7136\u8fd9\u4e2a\u4f8b\u5b50\u6211\u89c9\u5f97\u5f88\u9e21\u808b\uff5e\uff0c\u6ce8\u610f\u8fd9\u5c06\u5b8c\u5168\u66ff\u6362ucloud.yaml\u91cc\u7684\u6587\u4ef6\u5185\u5bb9 $ ansible cka-1 -m copy -a 'content=\"Hello Ansible Hi Ansible\" dest=/tmp/ucloud.yaml' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a cka-1\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c\u3002 [ cka ] root@master0:/tmp# cat ucloud.yaml Hello Ansible Hi Ansible 6. file \u6a21\u5757 \u00b6 Ansible \u4e2d\u4f7f\u7528 file \u6a21\u5757\u6765\u8bbe\u7f6e\u6587\u4ef6\u5c5e\u6027\u3002\u5176\u4e2d\u4f7f\u7528 path \u6307\u5b9a\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 src \u5b9a\u4e49 \u6e90\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 name \u6216 dest \u6765\u66ff\u6362\u521b\u5efa\u6587\u4ef6\u7684\u7b26\u53f7\u94fe\u63a5\u3002 \u793a\u4f8b\u4e00\uff1a\u8bbe\u7f6e\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e2d/tmp/ucloud.yaml \u6587\u4ef6\u7684\u6240\u5c5e\u4e3b\u4e3a mysql\uff0c\u6240\u5c5e\u7ec4\u4e3a mysql\uff0c\u6743\u9650\u4e3a 644\u3002 $ ansible cka-1 -m file -a 'owner=mysql group=mysql mode=644 path=/tmp/ucloud.yaml' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c [ cka ] root@master0:/tmp# ll -rw-r--r-- 1 mysql mysql 24 Oct 12 10 :44 ucloud.yaml 7. ping \u6a21\u5757 \u00b6 Ansible \u4e2d\u4f7f\u7528 ping \u6a21\u5757\u6765\u68c0\u6d4b\u6307\u5b9a\u4e3b\u673a\u7684\u8fde\u901a\u6027\u3002 \u793a\u4f8b\uff1a\u68c0\u6d4b\u6240\u6709\u88ab\u7ba1\u7406\u4e3b\u673a\u7684\u8fde\u901a\u6027\u3002 $ ansible cka-1 -m ping cka.master | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" } cka.node2 | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" } cka.node1 | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" } 8. service \u6a21\u5757 \u00b6 Ansible \u4e2d\u4f7f\u7528 service \u6a21\u5757\u6765\u63a7\u5236\u7ba1\u7406\u670d\u52a1\u7684\u8fd0\u884c\u72b6\u6001\u3002\u5176\u4e2d\u4f7f\u7528 enabled \u8868\u793a\u662f\u5426 \u5f00\u673a\u81ea\u52a8\u542f\u52a8\uff0c\u53d6\u503c\u4e3a true \u6216\u8005 false\uff1b\u4f7f\u7528 name \u5b9a\u4e49\u670d\u52a1\u540d\u79f0\uff1b\u4f7f\u7528 state \u6307\u5b9a\u670d\u52a1\u72b6 \u6001\uff0c\u53d6\u503c\u6709 started\u3001stoped\u3001restarted\u3002 \u793a\u4f8b\u4e00\uff1a\u67e5\u770b\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a lxcfs \u670d\u52a1\u7684\u72b6\u6001\u3002 $ ansible cka-1 -a 'systemctl status lxcfs' \u793a\u4f8b\u4e8c\uff1a\u67e5\u770b\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u662f\u5426\u662f\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001\u3002 $ ansible cka-1 -a 'systemctl is-enabled lxcfs' cka.node2 | CHANGED | rc = 0 >> enabled cka.master | CHANGED | rc = 0 >> enabled cka.node1 | CHANGED | rc = 0 >> enabled \u542f\u52a8\u88ab\u7ba1\u7406\u7ec4\u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u5e76\u4e14\u8bbe\u7f6e\u4e3a\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001\u3002 ansible cka-1 -m service -a 'enabled=true name=lxcfs state=started' \u542f\u52a8\u88ab\u7ba1\u7406\u7ec4\u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u5e76\u4e14\u5173\u95ed\u4e3a\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001 $ ansible cka-1 -m service -a 'enabled=false name=lxcfs state=started' 9. shell \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 shell \u6a21\u5757\u53ef\u4ee5\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fd0\u884c\u547d\u4ee4\uff0c\u5e76\u652f\u6301\u50cf\u7ba1\u9053\u7b26\u7b49\u529f\u80fd\u7684\u590d\u6742\u547d\u4ee4\u3002 \u793a\u4f8b\u4e00\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u521b\u5efa\u7528\u6237 user2\uff0cuid\u548cgid \u90fd\u4e3a 1001\uff0c\u7528 \u6237\u5bb6\u76ee\u5f55\u4e3a/home/user1\uff0cshell \u4e3a/bin/bash\u3002 $ ansible cka-1 -m user -a 'name=user2' \u793a\u4f8b\u4e8c\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u7684\u6240\u6709\u4e3b\u673a\u4f7f\u7528\u65e0\u4ea4\u4e92\u6a21\u5f0f\u7ed9\u7528\u6237\u8bbe\u7f6e\u5bc6\u7801\u3002 $ ansible cka-1 -m shell -a 'echo redhat|passwd --stdin user2' $ ansible cka-1 -m shell -a \"echo 'user:tju_openbayes'|chpasswd\" \u793a\u4f8b\u4e09: \u53ef\u4ee5\u4f7f\u7528\u7ba1\u9053\u7b26 $ ansible cka-1 -m shell -a 'cat /etc/passwd |wc -l' 10. script \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 script \u6a21\u5757\u53ef\u4ee5\u5c06\u672c\u5730\u811a\u672c\u590d\u5236\u5230\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fdb\u884c\u8fd0\u884c\u3002\u9700\u8981\u6ce8 \u610f\u7684\u662f\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\u6307\u5b9a\u811a\u672c\u4f4d\u7f6e\u3002 \u793a\u4f8b\uff1a\u7f16\u8f91\u4e00\u4e2a\u672c\u5730\u811a\u672c test.sh\uff0c\u590d\u5236\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u8fd0\u884c\u3002 $ cat test.sh #!/bin/bash echo \"hello tom\" >> /tmp/1.txt //\u51c6\u5907\u4e00\u4e2a\u6d4b\u8bd5\u811a\u672c chmod +x test.sh $ ansible cka-1 -m script -a '../scripts/test.sh' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a,\u67e5\u770b\u6267\u884c\u7ed3\u679c [ cka ] root@master0:/tmp# cat 1 .txt hello tom 11. atp \u6a21\u5757 \u00b6 Ansible \u4e2d\u7684 apt\u6a21\u5757\u8d1f\u8d23\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u5b89\u88c5\u4e0e\u5378\u8f7d\u8f6f\u4ef6\u5305\uff0c\u4f46\u662f\u9700\u8981\u63d0\u524d\u5728\u6bcf\u4e2a\u8282\u70b9\u914d\u7f6e\u81ea\u5df1\u7684 apt \u4ed3\u5e93\u3002 \u4f7f\u7528 name \u6307\u5b9a\u8981\u5b89\u88c5\u7684\u8f6f\u4ef6\u5305\uff0c\u8fd8\u53ef\u4ee5\u5e26\u4e0a\u8f6f\u4ef6\u5305\u7684\u7248\u672c\u53f7\uff1b\u5426\u5219\u5b89\u88c5\u6700\u65b0\u7684\u8f6f\u4ef6\u5305 \u4f7f\u7528 state \u6307\u5b9a\u5b89\u88c5\u8f6f\u4ef6\u5305\u7684\u72b6\u6001\uff0cpresent\u3001latest \u7528\u6765\u8868\u793a\u5b89\u88c5\uff0cabsent \u8868\u793a\u5378\u8f7d\u3002 update_cache=yes \u5f53\u8fd9\u4e2a\u53c2\u6570\u4e3ayes\u7684\u65f6\u5019\u7b49\u4e8eapt-get update \u5b89\u88c5tree\u5305 $ ansible cka-1 -m apt -a 'name=tree update_cache=yes' \u5378\u8f7dtree\u5305 $ ansible cka-1 -m apt -a 'name=tree state=absent' 12. yum \u6a21\u5757 \u00b6 \u8fd9\u91cc\u88ab\u63a7\u4e3b\u673a\u6ca1\u6709centos\u7cfb\u7edf\uff0c\u5c31\u7565\uff5e 13. setup \u6a21\u5757 \u00b6 Ansible \u4e2d\u4f7f\u7528 setup \u6a21\u5757\u6536\u96c6\u3001\u67e5\u770b\u88ab\u7ba1\u7406\u4e3b\u673a\u7684 facts\uff08facts \u662f Ansible \u91c7\u96c6 \u88ab\u7ba1\u7406\u4e3b\u673a\u8bbe\u5907\u4fe1\u606f\u7684\u4e00\u4e2a\u529f\u80fd\uff09\u3002\u6bcf\u4e2a\u88ab\u7ba1\u7406\u4e3b\u673a\u5728\u63a5\u6536\u5e76\u8fd0\u884c\u7ba1\u7406\u547d\u4ee4\u4e4b\u524d\uff0c\u90fd \u4f1a\u5c06\u81ea\u5df1\u7684\u76f8\u5173\u4fe1\u606f\uff08\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u3001IP \u5730\u5740\u7b49\uff09\u53d1\u9001\u7ed9\u63a7\u5236\u4e3b\u673a\u3002 $ ansible cka-1 -m setup 14. hostname \u6a21\u5757 \u00b6 \u53ef\u4ee5\u5229\u7528hostname \u7ed9\u4e3b\u673a\u4fee\u6539\u4e3b\u673a\u540d $ ansible cka-1 -m hostname -a 'name=nginx01' Ansible \u6848\u4f8b \u00b6 \u6848\u4f8b\u4e00\uff1acreate_user.yaml \u00b6 Ansible \u6279\u91cf\u521b\u5efa\u7528\u6237 \u8fd9\u91cc\u8bbe\u7f6epassword\u9700\u8981\u4f7f\u7528python\u547d\u4ee4\u751f\u6210sha-512\u7b97\u6cd5,pw\u662f\u8981\u52a0\u5bc6\u7684\u5bc6\u7801 a=$(python3 -c 'import crypt,getpass;pw=\"123456\";print(crypt.crypt(pw))') $ echo $a QUbhMQ9BAGQBE \u793a\u4f8b\u4e00 \u7528\u6237\u5bc6\u7801 - hosts : pve tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - name : add user user : name={{ item }} groups=admin password=d6F1o2yfgTymU shell=/bin/bash with_items : - user1 - user2 - user3 \u793a\u4f8b\u4e8c: \u516c\u94a5\u5f62\u5f0f tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - user : name : \"{{ item }}\" groups : admin shell : /bin/bash with_items : - lixie - user : name : \"{{ item }}\" state : absent with_items : - user # - ubuntu - authorized_key : user : \"{{ item.u }}\" key : \"https://github.com/{{ item.k }}.keys\" # \u516c\u94a5\u4e0a\u4f20\u5230github with_items : - { u : lixie , k : lixie021 } \u6267\u884cansible-playbook ansible-playbook -i inventory/pve pve.yaml \u5b98\u65b9\u53c2\u8003\u6587\u732e: https://gist.github.com/alces/f7e3de25d98a19550a4e4f97cabc2cf4?from_wecom=1 ansible-playbook -i ucd-mysql, add-ssh-users.yml \u9644\u4ef6 \u00b6 ubuntu22.04 \u8fd0\u884cansible\u62a5\u9519 ubuntu22.04 \u6267\u884c\u62a5\u9519 TASK [ Gathering Facts ] ******************************************************************************************************* fatal: [ axis ] : FAILED! = > { \"ansible_facts\" : {} , \"changed\" : false, \"failed_modules\" : { \"ansible.legacy.setup\" : { \"failed\" : true, \"module_stderr\" : \"/bin/sh: 1: /usr/bin/python: not found\\n\" , \"module_stdout\" : \"\" , \"msg\" : \"The module failed to execute correctly, you probably need to set the interpreter.\\nSee stdout/stderr for the exact error\" , \"rc\" : 127 }} , \"msg\" : \"The following modules failed to execute: ansible.legacy.setup\\n\" }","title":"Ansible \u5e94\u7528\u57fa\u7840"},{"location":"ansible/ansible-case1/#ansible","text":"\u7531\u4e8e\u4e92\u8054\u7f51\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u4ea7\u54c1\u66f4\u65b0\u6362\u4ee3\u901f\u5ea6\u9010\u6b65\u589e\u957f\uff0c\u8fd0\u7ef4\u4eba\u5458\u6bcf\u5929\u90fd\u8981\u8fdb\u884c \u5927\u91cf\u7684\u7ef4\u62a4\u64cd\u4f5c\uff0c\u6309\u7167\u4f20\u7edf\u65b9\u5f0f\u8fdb\u884c\u7ef4\u62a4\u4f7f\u5f97\u5de5\u4f5c\u6548\u7387\u4f4e\u4e0b\u3002\u8fd9\u65f6\u90e8\u7f72\u81ea\u52a8\u5316\u8fd0\u7ef4\u5c31 \u53ef\u4ee5\u5c3d\u53ef\u80fd\u5b89\u5168\u3001\u9ad8\u6548\u7684\u5b8c\u6210\u8fd9\u4e9b\u5de5\u4f5c\u3002 Ansible \u662f\u57fa\u4e8e Python \u5f00\u53d1\uff0c\u96c6\u5408\u4e86\u4f17\u591a\u4f18\u79c0\u8fd0\u7ef4\u5de5\u5177\u7684\u4f18\u70b9\uff0c\u5b9e\u73b0\u4e86\u6279\u91cf\u8fd0\u884c \u547d\u4ee4\u3001\u90e8\u7f72\u7a0b\u5e8f\u3001\u914d\u7f6e\u7cfb\u7edf\u7b49\u529f\u80fd\u7684\u81ea\u52a8\u5316\u8fd0\u7ef4\u7ba1\u7406\u5de5\u5177\u3002\u9ed8\u8ba4\u901a\u8fc7 SSH \u534f\u8bae\u8fdb\u884c \u8fdc\u7a0b\u547d\u4ee4\u6267\u884c\u6216\u4e0b\u53d1\u914d\u7f6e\uff0c\u65e0\u9700\u90e8\u7f72\u4efb\u4f55\u5ba2\u6237\u7aef\u4ee3\u7406\u8f6f\u4ef6\uff0c\u4ece\u800c\u4f7f\u5f97\u81ea\u52a8\u5316\u73af\u5883\u90e8\u7f72 \u53d8\u5f97\u66f4\u52a0\u7b80\u5355\u3002\u53ef\u540c\u65f6\u652f\u6301\u591a\u53f0\u4e3b\u673a\u5e76\u884c\u7ba1\u7406\uff0c\u4f7f\u5f97\u7ba1\u7406\u4e3b\u673a\u66f4\u52a0\u4fbf\u6377\u3002 Ansible \u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u8fdb\u884c\u5de5\u4f5c\u7684\u6846\u67b6\u7ed3\u6784\uff0c\u6279\u91cf\u90e8\u7f72\u80fd\u529b\u5c31\u662f\u7531 Ansible \u6240\u8fd0\u884c\u7684\u6a21\u5757\u5b9e\u73b0\u7684\u3002\u7b80\u800c\u8a00\u4e4b Ansible \u662f\u57fa\u4e8e\u201c\u6a21\u5757\u201d\u5b8c\u6210\u5404\u79cd\u201c\u4efb\u52a1\u201d\u7684\u3002\u5176 \u57fa\u672c\u6846\u67b6\u7ed3\u6784\u5982\u56fe 3.1 \u6240\u793a\u3002 \u7531\u56fe 3.1 \u53ef\u4ee5\u5f97\u51fa Ansible \u7684\u57fa\u672c\u67b6\u6784\u7531\u516d\u5927\u4ef6\u6784\u6210\u3002 Ansible core \u6838\u5fc3\u5f15\u64ce\uff1a\u5373 Ansible \u672c\u8eab\uff1b Host Inventory \u4e3b\u673a\u6e05\u5355\uff1a\u7528\u6765\u5b9a\u4e49 Ansible \u6240\u7ba1\u7406\u4e3b\u673a\uff0c\u9ed8\u8ba4\u662f\u5728 Ansible \u7684 hosts \u914d\u7f6e\u6587\u4ef6\u4e2d\u5b9a\u4e49\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u540c\u65f6\u4e5f\u652f\u6301\u81ea\u5b9a\u4e49\u52a8\u6001\u4e3b\u673a\u6e05\u5355\u548c\u6307\u5b9a\u5176\u5b83 \u914d\u7f6e\u6587\u4ef6\u7684\u4f4d\u7f6e\uff1b Connect plugin \u8fde\u63a5\u63d2\u4ef6\uff1a\u8d1f\u8d23\u548c\u88ab\u7ba1\u7406\u4e3b\u673a\u5b9e\u73b0\u901a\u4fe1\u3002\u9664\u652f\u6301\u4f7f\u7528 SSH \u8fde\u63a5 \u88ab\u7ba1\u7406\u4e3b\u673a\u5916\uff0cAnsible \u8fd8\u652f\u6301\u5176\u5b83\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u6240\u4ee5\u9700\u8981\u6709\u8fde\u63a5\u63d2\u4ef6\u5c06\u5404\u4e2a\u4e3b \u673a\u7528\u8fde\u63a5\u63d2\u4ef6\u8fde\u63a5\u5230 Ansible\uff1b Playbook\uff08yaml\uff0cjinjia2\uff09\u5267\u672c\uff1a\u7528\u6765\u96c6\u4e2d\u5b9a\u4e49 Ansible \u4efb\u52a1\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u5373\u5c06 \u591a\u4e2a\u4efb\u52a1\u5b9a\u4e49\u5728\u4e00\u4e2a\u5267\u672c\u4e2d\u7531 Ansible \u81ea\u52a8\u6267\u884c\uff0c\u53ef\u4ee5\u7531\u63a7\u5236\u4e3b\u673a\u9488\u5bf9\u591a\u53f0\u88ab\u7ba1 \u7406\u4e3b\u673a\u540c\u65f6\u8fd0\u884c\u591a\u4e2a\u4efb\u52a1\uff1b Core modules \u6838\u5fc3\u6a21\u5757\uff1a\u662f Ansible \u81ea\u5e26\u7684\u6a21\u5757\uff0c\u4f7f\u7528\u8fd9\u4e9b\u6a21\u5757\u5c06\u8d44\u6e90\u5206\u53d1\u5230\u88ab \u7ba1\u7406\u4e3b\u673a\u4f7f\u5176\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u6216\u5339\u914d\u7279\u5b9a\u7684\u72b6\u6001\uff1b Custom modules \u81ea\u5b9a\u4e49\u6a21\u5757\uff1a\u7528\u4e8e\u5b8c\u6210\u6a21\u5757\u529f\u80fd\u7684\u8865\u5145\uff0c\u53ef\u501f\u52a9\u76f8\u5173\u63d2\u4ef6\u5b8c\u6210 \u8bb0\u5f55\u65e5\u5fd7\u3001\u53d1\u9001\u90ae\u4ef6\u7b49\u529f\u80fd\u3002","title":"Ansible \u6982\u8ff0"},{"location":"ansible/ansible-case1/#ansible_1","text":"Ansible \u81ea\u52a8\u5316\u8fd0\u7ef4\u73af\u5883\u7531\u63a7\u5236\u4e3b\u673a\u4e0e\u88ab\u7ba1\u7406\u4e3b\u673a\u7ec4\u6210\u3002\u7531\u4e8e Ansible \u662f\u57fa\u4e8e SSH \u534f\u8bae \u8fdb\u884c\u901a\u4fe1\u7684\uff0c\u6240\u4ee5\u63a7\u5236\u4e3b\u673a\u5b89\u88c5 Ansible \u8f6f\u4ef6\u540e\u4e0d\u9700\u8981\u91cd\u542f\u6216\u8fd0\u884c\u4efb\u4f55\u7a0b\u5e8f\uff0c\u88ab\u7ba1\u7406\u4e3b\u673a\u4e5f \u4e0d\u9700\u8981\u5b89\u88c5\u548c\u8fd0\u884c\u4efb\u4f55\u4ee3\u7406\u7a0b\u5e8f\u3002","title":"\u5b89\u88c5\u90e8\u7f72 Ansible \u670d\u52a1"},{"location":"ansible/ansible-case1/#ansible_2","text":"Centos \u7cfb\u7edf Ansible \u53ef\u4ee5\u4f7f\u7528\u6e90\u7801\u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u4e2d YUM \u8f6f\u4ef6\u5305\u7ba1\u7406\u5de5 \u5177\u8fdb\u884c\u5b89\u88c5\u3002 YUM \u65b9\u5f0f\u5b89\u88c5 Ansible\uff0c\u9700\u8981\u4f9d\u8d56\u7b2c\u4e09\u65b9\u7684 EPEL \u6e90\uff0c\u4e0b\u9762\u914d\u7f6e EPEL \u6e90 \u4f5c\u4e3a\u90e8\u7f72 Ansible \u7684 YUM \u6e90 [ root@ansible-node1 ~ ] # yum install -y epel-release [ root@ansible-node1 ~ ] # yum install -y ansible [ root@ansible-node1 ~ ] # ansible --version //\u67e5\u770bansible\u7248\u672c Mac \u7cfb\u7edf \u5b98\u65b9\u6587\u7ae0\u53c2\u8003\u5730\u5740 \u8fd9\u91cc\u4e00\u822c\u6211\u4f1a\u4f7f\u7528Mac\u7535\u8111\u4f5c\u4e3aansible-server \u6765\u6267\u884cansible\u811a\u672c brew install ansible","title":"\u5b89\u88c5 Ansible"},{"location":"ansible/ansible-case1/#_1","text":"\u51c6\u5907\u5de5\u4f5c \u914d\u7f6einventory/pve [ pve ] master.pve hostname = master ansible_host = 192 .168.1.11 ansible_ssh_user = root ansible_ssh_pass = \"pass@word1\" \u8fd9\u91cc\u9996\u5148\u6211\u4eec\u5148\u660e\u767d\uff0c\u6bcf\u4e00\u4e2a\u88ab\u63a7\u5236\u7684\u4e3b\u673a\u9ed8\u8ba4\u80af\u5b9a\u4f1a\u6709\u4e00\u4e2a\u5e10\u53f7\u548c\u5bc6\u7801\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e2a\u666e\u901a\u7528\u6237\u6765\u5bf9\u8be5\u4e3b\u673a\u8fdb\u884c\u4e00\u7cfb\u5217\u64cd\u4f5c\uff5e \u6e29\u99a8\u63d0\u793a \u53ef\u80fd\u51fa\u73b0sudo\u6743\u9650\u4e0d\u591f\u7684\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u5728inventory\u6dfb\u52a0\u4ee5\u4e0b\u5185\u5bb9 gtx3090.c1 hostname = ubuntu ansible_ssh_user = xxx ansible_sudo_pass = xxx ansible_ssh_pass = \"password\"","title":"\u914d\u7f6e\u4e3b\u673a\u6e05\u5355"},{"location":"ansible/ansible-case1/#ssh","text":"\u5982\u679c\u4e0a\u4e2a\u6b65\u9aa4\u914d\u7f6e\u597d\uff0c\u8fd9\u4e2a\u6b65\u9aa4\u53ef\u4ee5\u5ffd\u7565\uff5e \u5f53\u7136\u53ef\u4ee5\u7ed3\u5408\u4f7f\u7528\uff0c\u5177\u4f53\u573a\u666f\u8fd8\u9700\u8981\u6839\u636e\u81ea\u5df1\u60c5\u51b5\u6765\u914d\u7f6e \u4e3a\u4e86\u907f\u514d Ansible \u4e0b\u53d1\u6307\u4ee4\u65f6\u9700\u8981\u8f93\u5165\u88ab\u7ba1\u7406\u4e3b\u673a\u7684\u5bc6\u7801\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bc1\u4e66\u7b7e\u540d\u8fbe\u5230 SSH \u65e0\u5bc6\u7801\u767b\u5f55\u3002\u4f7f\u7528 ssh-keygen \u4ea7\u751f\u4e00\u5bf9\u5bc6\u94a5\uff0c\u5e76\u901a\u8fc7 ssh-copy-id \u547d\u4ee4\u6765\u53d1\u9001\u751f\u6210\u7684\u516c\u94a5\u3002","title":"\u8bbe\u7f6eSSH\u65e0\u5bc6\u7801\u767b\u9646"},{"location":"ansible/ansible-case1/#ansible_3","text":"Ansible \u53ef\u4ee5\u4f7f\u7528\u547d\u4ee4\u884c\u7684\u65b9\u5f0f\u8fdb\u884c\u81ea\u52a8\u5316\u7ba1\u7406\u3002\u547d\u4ee4\u7684\u57fa\u672c\u8bed\u6cd5\u5982\u4e0b\u6240\u793a\uff1a ansible <host-pattern> [-m module_name] [-a args] Ansible \u5e38\u7528\u9009\u9879 host-pattern\uff1a\u6307\u5b9a\u88ab\u7ba1\u7406\u4e3b\u673a [-m module_name]\uff1a\u6307\u5b9a\u6240\u4f7f\u7528\u7684\u6a21\u5757 [-a args]\uff1a\u8bbe\u7f6e\u6a21\u5757\u5bf9\u5e94\u7684\u53c2\u6570 -C //\u9884\u6267\u884c --list-hosts //\u5217\u51fa\u6b63\u5728\u8fd0\u884c\u4efb\u52a1\u7684\u4e3b\u673a --list-tasks //\u5217\u51fatasks --limit \u4e3b\u673a\u5217\u8868 //\u53ea\u9488\u5bf9\u7279\u5b9a\u4e3b\u673a\u6267\u884c Ansible \u7684\u547d\u4ee4\u884c\u7ba1\u7406\u5de5\u5177\u90fd\u662f\u7531\u4e00\u7cfb\u5217\u6a21\u5757\u3001\u53c2\u6570\u7ec4\u6210\u7684\uff0c\u4f7f\u7528\u67d0\u4e9b\u6a21\u5757\u6216\u53c2\u6570\u4e4b\u524d\uff0c \u53ef\u4ee5\u5728\u547d\u4ee4\u540e\u9762\u52a0\u4e0a-h \u6216--help \u6765\u83b7\u53d6\u5e2e\u52a9\u3002\u4f8b\u5982\uff0cansible-doc \u5de5\u5177\u53ef\u4ee5\u4f7f\u7528 ansible-doc -h \u6216\u8005 ansible-doc --help \u67e5\u770b\u5176\u5e2e\u52a9\u4fe1\u606f\u3002 ansible-doc \u5de5\u5177\u7528\u4e8e\u67e5\u770b\u6a21\u5757\u5e2e\u52a9\u4fe1\u606f\u3002\u4e3b\u8981\u9009\u9879\u5305\u62ec\uff1a -l \u7528\u6765\u5217\u51fa\u53ef\u4f7f\u7528\u7684\u6a21\u5757\uff1b -s \u7528\u6765\u5217\u51fa\u67d0\u4e2a\u6a21\u5757\u7684\u63cf\u8ff0\u4fe1\u606f\u548c\u4f7f\u7528\u793a\u5217\u3002 \u4f8b\u5982\uff1a\u4e0b\u9762\u64cd\u4f5c\u53ef\u4ee5\u5217\u51fa yum \u6a21\u5757\u7684\u63cf\u8ff0\u4fe1\u606f\u548c\u64cd\u4f5c\u52a8\u4f5c\u3002 [ root@ansible-node1 ~ ] # ansible-doc -s yum Ansible \u81ea\u5e26\u4e86\u5f88\u591a\u6a21\u5757\uff0c\u80fd\u591f\u4e0b\u53d1\u6267\u884c Ansible \u7684\u5404\u79cd\u7ba1\u7406\u4efb\u52a1\u3002\u9996\u5148\u6765\u4e86\u89e3\u4e0b Ansible \u5e38\u7528\u7684\u8fd9\u4e9b\u6838\u5fc3\u6a21\u5757\u3002","title":"Ansible \u547d\u4ee4\u5e94\u7528\u57fa\u7840"},{"location":"ansible/ansible-case1/#1-command","text":"Ansibale \u7ba1\u7406\u5de5\u5177\u4f7f\u7528-m \u9009\u9879\u6765\u6307\u5b9a\u6240\u4f7f\u7528\u6a21\u5757\uff0c\u9ed8\u8ba4\u4f7f\u7528 command \u6a21\u5757\uff0c\u5373 -m \u9009 \u9879\u7701\u7565\u65f6\u4f1a\u8fd0\u884c\u6b64\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fd0\u884c\u547d\u4ee4\u3002\u4f8b\u5982\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u6267\u884c date \u547d\u4ee4\uff0c\u663e\u793a\u88ab\u7ba1\u7406\u4e3b\u673a\u65f6\u95f4\u3002\u6709\u4e09\u79cd\u6267\u884c\u547d\u4ee4\u7684\u65b9\u5f0f\u6765\u7ba1\u7406\u5199\u5165\u4e3b\u673a\u6e05\u5355\u4e2d\u7684\u4e3b\u673a\u3002 \u793a\u4f8b\u4e00\uff1a\u4f7f\u7528 IP \u5730\u5740\u67e5\u770b\u88ab\u7ba1\u7406\u4e3b\u673a\u65e5\u671f $ ansible cka -m command -a 'date' --limit cka.master cka.master | CHANGED | rc = 0 >> Wed Oct 12 12 :24:17 AM CST 2022 \u6e29\u99a8\u63d0\u793a \u8fd9\u91cc\u6211\u4f7f\u7528\u7684mac\u7b14\u8bb0\u672c\uff0c\u4f7f\u7528\u7684ansible-playbook\uff0c\u5728\u4ed3\u5e93\u4e2d\u6709\u4e00\u4e2a\u5173\u4e8eansible.cfg\u7684\u914d\u7f6e\u6587\u4ef6\u3002 $ cat ansible.cfg [ defaults ] pipelining = True strategy = free host_key_checking = False inventory = inventory //\u8fd9\u91cc\u4e3b\u8981\u7684\u76ee\u5f55\uff0c\u4e3b\u673a\u6e05\u5355\u6587\u4ef6\u4f4d\u7f6e library = library # gathering = smart fact_caching = redis # redis on stg-master fact_caching_connection = 106 .75.63.184:3389:0:OpenBayesAnsibleCache fact_caching_timeout = 0 forks = 16 [ privilege_escalation ] become = True [ diff ] always = yes \u793a\u4f8b\u4e8c\uff1a\u4f7f\u7528\u7ba1\u63a7\u4e3b\u673a\u5206\u522b\u67e5\u770b\u88ab\u7ba1\u7406 cka-1 \u548c cka-2 \u7ec4\u91cc\u9762\u6240\u6709\u4e3b\u673a\u7684\u65e5\u671f \u9996\u5148\uff0c\u9700\u8981\u51c6\u5907\u4e3b\u673a\u6587\u4ef6 $ cat inventory/cka [ cka-1 ] cka.master hostname = master0 ansible_host = 172 .30.42.244 ansible_user = lixie [ cka-2 ] cka.node1 hostname = node1 ansible_host = 172 .30.192.106 ansible_user = lixie cka.node2 hostname = node2 ansible_host = 172 .30.226.223 ansible_user = lixie \u5206\u7ec4\u6267\u884cansible \u547d\u4ee4 $ ansible cka-1 -m command -a 'date' //\u7b2c\u4e00\u7ec4 cka.master | CHANGED | rc = 0 >> Wed Oct 12 12 :42:15 AM CST 2022 $ ansible cka-2 -m command -a 'date' //\u7b2c\u4e8c\u7ec4 cka.node1 | CHANGED | rc = 0 >> Wed Oct 12 12 :38:55 AM CST 2022 cka.node2 | CHANGED | rc = 0 >> Wed Oct 12 12 :38:55 AM CST 2022 \u5f53\u7136\u4e86\uff0c\u5982\u679c\u4e0a\u9762\u7684\u53ef\u4ee5\u6b63\u786e\u6267\u884c\uff0c\u90a3\u4e48\u5176\u5b9e\u547d\u4ee4\u4e5f\u5c31\u662f\u6709\u624b\u5c31\u884c \u793a\u4f8b: $ ansible cka-2 -m command -a 'df -h' \u793a\u4f8b\u4e09\uff1a\u67e5\u770b\u6240\u6709\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u7684\u65e5\u671f [ root@ansible-node1 ~ ] # ansible all -m command -a 'date","title":"1. command \u6a21\u5757"},{"location":"ansible/ansible-case1/#2-user","text":"Ansible \u4e2d\u7684 user \u6a21\u5757\u7528\u4e8e\u521b\u5efa\u65b0\u7528\u6237\u548c\u66f4\u6539\u3001\u5220\u9664\u5df2\u5b58\u5728\u7684\u7528\u6237\u3002\u5176\u4e2d name \u9009\u9879\u7528 \u4e8e\u6307\u660e\u521b\u5efa\u7684\u7528\u6237\u540d\u79f0\u3002\u4e3b\u8981\u5305\u62ec\u4e24\u79cd\u72b6\u6001\uff08state\uff09\uff1a present \u8868\u793a\u6dfb\u52a0 (\u7701\u7565\u72b6\u6001\u65f6\u9ed8\u8ba4\u4f7f\u7528)\uff1b absent \u8868\u793a\u79fb\u9664 \u793a\u4f8b\u4e00\uff1a\u5728\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u521b\u5efa\u4e00\u4e2a user1 \u7528\u6237\u3002 $ ansible cka-1 -m user -a 'name=\"user1\"' \u793a\u4f8b\u4e8c\uff1a\u5220\u9664\u4e0a\u8ff0\u521b\u5efa\u7684\u7528\u6237 user1\u3002 $ ansible cka-1 -m user -a 'name=\"user1\" state=absent ' \u793a\u4f8b\u4e09: ansible\u521b\u5efa\u7528\u6237\u5e76\u6307\u5b9a\u5bc6\u7801 \u751f\u6210\u52a0\u5bc6\u5bc6\u7801 $ a = $( python3 -c 'import crypt,getpass;pw=\"pass@word\";print(crypt.crypt(pw))' ) $ echo $a 8e7klQ1BR7DIY \u66f4\u65b0\u5bc6\u7801 $ ansible note1 -m user -a 'name=test password=\"$a\" update_password=always' //\u53ef\u4ee5\u4f7f\u7528\u53d8\u91cf $ ansible cka-1 -m user -a 'name=\"user1\" password=8e7klQ1BR7DIY update_password=always'","title":"2. User \u6a21\u5757"},{"location":"ansible/ansible-case1/#3-cron","text":"Ansible \u4e2d\u7684 cron \u6a21\u5757\u7528\u4e8e\u5b9a\u4e49\u4efb\u52a1\u8ba1\u5212\u3002\u4e3b\u8981\u5305\u62ec\u4e24\u79cd\u72b6\u6001\uff08state\uff09\uff1a present \u8868\u793a\u6dfb\u52a0 (\u7701\u7565\u72b6\u6001\u65f6\u9ed8\u8ba4\u4f7f\u7528) absent \u8868\u793a\u79fb\u9664\u3002","title":"3. cron \u6a21\u5757"},{"location":"ansible/ansible-case1/#4-group","text":"Ansible \u4e2d\u7684 group \u6a21\u5757\u7528\u4e8e\u5bf9\u7528\u6237\u7ec4\u8fdb\u884c\u7ba1\u7406\u3002 \u793a\u4f8b\u4e00\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u521b\u5efa mysql \u7ec4\uff0cgid \u4e3a 555\u3002 $ ansible cka-1 -m group -a 'name=mysql gid=555 system=yes' \u793a\u4f8b\u4e8c\uff1a\u5c06\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u7684 mysql \u7528\u6237\u6dfb\u52a0\u5230 mysql \u7ec4\u4e2d\u3002 ansible cka-1 -m user -a 'name=mysql uid=555 system=yes group=mysql'","title":"4. group \u6a21\u5757"},{"location":"ansible/ansible-case1/#5-copy","text":"Ansible \u4e2d\u7684 copy \u6a21\u5757\u7528\u4e8e\u5b9e\u73b0\u6587\u4ef6\u590d\u5236\u548c\u6279\u91cf\u4e0b\u53d1\u6587\u4ef6\u3002\u5176\u4e2d\u4f7f\u7528 src \u6765\u5b9a\u4e49\u672c\u5730\u6e90\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 dest \u5b9a\u4e49\u88ab\u7ba1\u7406\u4e3b\u673a\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 content \u5219\u662f\u4f7f\u7528\u6307\u5b9a\u4fe1\u606f\u5185\u5bb9\u751f\u6210 \u76ee\u6807\u6587\u4ef6\u3002 \u793a \u4f8b \u4e00 \uff1a \u5c06\u672c\u5730\u6587\u4ef6 ucloud.yaml \u590d\u5236\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u7684\u6240\u6709\u4e3b\u673a\u4e0a\u7684 /tmp/(\u5f53\u7136\u4e5f\u53ef\u4ee5\u91cd\u547d\u540d)\uff0c\u5e76\u5c06\u6240\u6709\u8005\u8bbe\u7f6e\u4e3a root\uff0c\u6743\u9650\u8bbe\u7f6e\u4e3a 640\u3002 $ ansible cka-1 -m copy -a 'src=ucloud.yaml dest=/tmp/ owner=root mode=640' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c\u3002 [ cka ] root@master0:/tmp# ll total 44 drwxrwxrwt 10 root root 4096 Oct 12 10 :37 ./ drwxr-xr-x 19 root root 4096 May 24 23 :27 ../ -rw-r----- 1 root root 1172 Oct 12 10 :33 ucloud.yaml \u6e29\u99a8\u63d0\u793a \u5982\u679c\u51fa\u73b0\u4ee5\u4e0b\u7684\u62a5\u9519\u4fe1\u606f\uff0c\u662f\u56e0\u4e3a\u88ab\u7ba1\u7406\u4e3b\u673a\u5f00\u542f\u4e86 SELinux\uff0c\u9700\u8981\u5728\u88ab\u7ba1\u7406\u673a\u4e0a\u5b89\u88c5 libselinux-python \u8f6f\u4ef6\u5305\uff0c\u624d\u53ef\u4ee5\u4f7f\u7528 Ansible \u4e2d\u4e0e copy\u3001file \u76f8\u5173\u7684\u51fd\u6570\u3002 \"msg\": \"Aborting, target uses selinux but python bindings (libselinux-python) aren't installed!\" \u793a\u4f8b\u4e8c\uff1a\u5c06\u201dHello Ansible Hi Ansible\u201d \u5199\u5165\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u7684/tmp/ucloud.yaml \u6587\u4ef6\u4e2d\u3002 \u5f53\u7136\u8fd9\u4e2a\u4f8b\u5b50\u6211\u89c9\u5f97\u5f88\u9e21\u808b\uff5e\uff0c\u6ce8\u610f\u8fd9\u5c06\u5b8c\u5168\u66ff\u6362ucloud.yaml\u91cc\u7684\u6587\u4ef6\u5185\u5bb9 $ ansible cka-1 -m copy -a 'content=\"Hello Ansible Hi Ansible\" dest=/tmp/ucloud.yaml' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a cka-1\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c\u3002 [ cka ] root@master0:/tmp# cat ucloud.yaml Hello Ansible Hi Ansible","title":"5. copy \u6a21\u5757"},{"location":"ansible/ansible-case1/#6-file","text":"Ansible \u4e2d\u4f7f\u7528 file \u6a21\u5757\u6765\u8bbe\u7f6e\u6587\u4ef6\u5c5e\u6027\u3002\u5176\u4e2d\u4f7f\u7528 path \u6307\u5b9a\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 src \u5b9a\u4e49 \u6e90\u6587\u4ef6\u8def\u5f84\uff1b\u4f7f\u7528 name \u6216 dest \u6765\u66ff\u6362\u521b\u5efa\u6587\u4ef6\u7684\u7b26\u53f7\u94fe\u63a5\u3002 \u793a\u4f8b\u4e00\uff1a\u8bbe\u7f6e\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e2d/tmp/ucloud.yaml \u6587\u4ef6\u7684\u6240\u5c5e\u4e3b\u4e3a mysql\uff0c\u6240\u5c5e\u7ec4\u4e3a mysql\uff0c\u6743\u9650\u4e3a 644\u3002 $ ansible cka-1 -m file -a 'owner=mysql group=mysql mode=644 path=/tmp/ucloud.yaml' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a\uff0c\u9a8c\u8bc1\u4e0a\u8ff0\u547d\u4ee4\u6267\u884c\u7ed3\u679c [ cka ] root@master0:/tmp# ll -rw-r--r-- 1 mysql mysql 24 Oct 12 10 :44 ucloud.yaml","title":"6. file \u6a21\u5757"},{"location":"ansible/ansible-case1/#7-ping","text":"Ansible \u4e2d\u4f7f\u7528 ping \u6a21\u5757\u6765\u68c0\u6d4b\u6307\u5b9a\u4e3b\u673a\u7684\u8fde\u901a\u6027\u3002 \u793a\u4f8b\uff1a\u68c0\u6d4b\u6240\u6709\u88ab\u7ba1\u7406\u4e3b\u673a\u7684\u8fde\u901a\u6027\u3002 $ ansible cka-1 -m ping cka.master | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" } cka.node2 | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" } cka.node1 | SUCCESS = > { \"changed\" : false, \"ping\" : \"pong\" }","title":"7. ping \u6a21\u5757"},{"location":"ansible/ansible-case1/#8-service","text":"Ansible \u4e2d\u4f7f\u7528 service \u6a21\u5757\u6765\u63a7\u5236\u7ba1\u7406\u670d\u52a1\u7684\u8fd0\u884c\u72b6\u6001\u3002\u5176\u4e2d\u4f7f\u7528 enabled \u8868\u793a\u662f\u5426 \u5f00\u673a\u81ea\u52a8\u542f\u52a8\uff0c\u53d6\u503c\u4e3a true \u6216\u8005 false\uff1b\u4f7f\u7528 name \u5b9a\u4e49\u670d\u52a1\u540d\u79f0\uff1b\u4f7f\u7528 state \u6307\u5b9a\u670d\u52a1\u72b6 \u6001\uff0c\u53d6\u503c\u6709 started\u3001stoped\u3001restarted\u3002 \u793a\u4f8b\u4e00\uff1a\u67e5\u770b\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a lxcfs \u670d\u52a1\u7684\u72b6\u6001\u3002 $ ansible cka-1 -a 'systemctl status lxcfs' \u793a\u4f8b\u4e8c\uff1a\u67e5\u770b\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u662f\u5426\u662f\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001\u3002 $ ansible cka-1 -a 'systemctl is-enabled lxcfs' cka.node2 | CHANGED | rc = 0 >> enabled cka.master | CHANGED | rc = 0 >> enabled cka.node1 | CHANGED | rc = 0 >> enabled \u542f\u52a8\u88ab\u7ba1\u7406\u7ec4\u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u5e76\u4e14\u8bbe\u7f6e\u4e3a\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001\u3002 ansible cka-1 -m service -a 'enabled=true name=lxcfs state=started' \u542f\u52a8\u88ab\u7ba1\u7406\u7ec4\u91cc\u6240\u6709\u4e3b\u673a\u7684 lxcfs \u670d\u52a1\u5e76\u4e14\u5173\u95ed\u4e3a\u5f00\u673a\u81ea\u52a8\u542f\u52a8\u72b6\u6001 $ ansible cka-1 -m service -a 'enabled=false name=lxcfs state=started'","title":"8. service \u6a21\u5757"},{"location":"ansible/ansible-case1/#9-shell","text":"Ansible \u4e2d\u7684 shell \u6a21\u5757\u53ef\u4ee5\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fd0\u884c\u547d\u4ee4\uff0c\u5e76\u652f\u6301\u50cf\u7ba1\u9053\u7b26\u7b49\u529f\u80fd\u7684\u590d\u6742\u547d\u4ee4\u3002 \u793a\u4f8b\u4e00\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u521b\u5efa\u7528\u6237 user2\uff0cuid\u548cgid \u90fd\u4e3a 1001\uff0c\u7528 \u6237\u5bb6\u76ee\u5f55\u4e3a/home/user1\uff0cshell \u4e3a/bin/bash\u3002 $ ansible cka-1 -m user -a 'name=user2' \u793a\u4f8b\u4e8c\uff1a\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u7684\u6240\u6709\u4e3b\u673a\u4f7f\u7528\u65e0\u4ea4\u4e92\u6a21\u5f0f\u7ed9\u7528\u6237\u8bbe\u7f6e\u5bc6\u7801\u3002 $ ansible cka-1 -m shell -a 'echo redhat|passwd --stdin user2' $ ansible cka-1 -m shell -a \"echo 'user:tju_openbayes'|chpasswd\" \u793a\u4f8b\u4e09: \u53ef\u4ee5\u4f7f\u7528\u7ba1\u9053\u7b26 $ ansible cka-1 -m shell -a 'cat /etc/passwd |wc -l'","title":"9. shell \u6a21\u5757"},{"location":"ansible/ansible-case1/#10-script","text":"Ansible \u4e2d\u7684 script \u6a21\u5757\u53ef\u4ee5\u5c06\u672c\u5730\u811a\u672c\u590d\u5236\u5230\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u8fdb\u884c\u8fd0\u884c\u3002\u9700\u8981\u6ce8 \u610f\u7684\u662f\u4f7f\u7528\u76f8\u5bf9\u8def\u5f84\u6307\u5b9a\u811a\u672c\u4f4d\u7f6e\u3002 \u793a\u4f8b\uff1a\u7f16\u8f91\u4e00\u4e2a\u672c\u5730\u811a\u672c test.sh\uff0c\u590d\u5236\u5230\u88ab\u7ba1\u7406\u7ec4 cka-1 \u91cc\u6240\u6709\u4e3b\u673a\u4e0a\u8fd0\u884c\u3002 $ cat test.sh #!/bin/bash echo \"hello tom\" >> /tmp/1.txt //\u51c6\u5907\u4e00\u4e2a\u6d4b\u8bd5\u811a\u672c chmod +x test.sh $ ansible cka-1 -m script -a '../scripts/test.sh' \u767b\u5f55\u88ab\u7ba1\u7406\u4e3b\u673a,\u67e5\u770b\u6267\u884c\u7ed3\u679c [ cka ] root@master0:/tmp# cat 1 .txt hello tom","title":"10. script \u6a21\u5757"},{"location":"ansible/ansible-case1/#11-atp","text":"Ansible \u4e2d\u7684 apt\u6a21\u5757\u8d1f\u8d23\u5728\u88ab\u7ba1\u7406\u4e3b\u673a\u4e0a\u5b89\u88c5\u4e0e\u5378\u8f7d\u8f6f\u4ef6\u5305\uff0c\u4f46\u662f\u9700\u8981\u63d0\u524d\u5728\u6bcf\u4e2a\u8282\u70b9\u914d\u7f6e\u81ea\u5df1\u7684 apt \u4ed3\u5e93\u3002 \u4f7f\u7528 name \u6307\u5b9a\u8981\u5b89\u88c5\u7684\u8f6f\u4ef6\u5305\uff0c\u8fd8\u53ef\u4ee5\u5e26\u4e0a\u8f6f\u4ef6\u5305\u7684\u7248\u672c\u53f7\uff1b\u5426\u5219\u5b89\u88c5\u6700\u65b0\u7684\u8f6f\u4ef6\u5305 \u4f7f\u7528 state \u6307\u5b9a\u5b89\u88c5\u8f6f\u4ef6\u5305\u7684\u72b6\u6001\uff0cpresent\u3001latest \u7528\u6765\u8868\u793a\u5b89\u88c5\uff0cabsent \u8868\u793a\u5378\u8f7d\u3002 update_cache=yes \u5f53\u8fd9\u4e2a\u53c2\u6570\u4e3ayes\u7684\u65f6\u5019\u7b49\u4e8eapt-get update \u5b89\u88c5tree\u5305 $ ansible cka-1 -m apt -a 'name=tree update_cache=yes' \u5378\u8f7dtree\u5305 $ ansible cka-1 -m apt -a 'name=tree state=absent'","title":"11. atp \u6a21\u5757"},{"location":"ansible/ansible-case1/#12-yum","text":"\u8fd9\u91cc\u88ab\u63a7\u4e3b\u673a\u6ca1\u6709centos\u7cfb\u7edf\uff0c\u5c31\u7565\uff5e","title":"12. yum \u6a21\u5757"},{"location":"ansible/ansible-case1/#13-setup","text":"Ansible \u4e2d\u4f7f\u7528 setup \u6a21\u5757\u6536\u96c6\u3001\u67e5\u770b\u88ab\u7ba1\u7406\u4e3b\u673a\u7684 facts\uff08facts \u662f Ansible \u91c7\u96c6 \u88ab\u7ba1\u7406\u4e3b\u673a\u8bbe\u5907\u4fe1\u606f\u7684\u4e00\u4e2a\u529f\u80fd\uff09\u3002\u6bcf\u4e2a\u88ab\u7ba1\u7406\u4e3b\u673a\u5728\u63a5\u6536\u5e76\u8fd0\u884c\u7ba1\u7406\u547d\u4ee4\u4e4b\u524d\uff0c\u90fd \u4f1a\u5c06\u81ea\u5df1\u7684\u76f8\u5173\u4fe1\u606f\uff08\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u3001IP \u5730\u5740\u7b49\uff09\u53d1\u9001\u7ed9\u63a7\u5236\u4e3b\u673a\u3002 $ ansible cka-1 -m setup","title":"13. setup \u6a21\u5757"},{"location":"ansible/ansible-case1/#14-hostname","text":"\u53ef\u4ee5\u5229\u7528hostname \u7ed9\u4e3b\u673a\u4fee\u6539\u4e3b\u673a\u540d $ ansible cka-1 -m hostname -a 'name=nginx01'","title":"14. hostname \u6a21\u5757"},{"location":"ansible/ansible-case1/#ansible_4","text":"","title":"Ansible \u6848\u4f8b"},{"location":"ansible/ansible-case1/#create_useryaml","text":"Ansible \u6279\u91cf\u521b\u5efa\u7528\u6237 \u8fd9\u91cc\u8bbe\u7f6epassword\u9700\u8981\u4f7f\u7528python\u547d\u4ee4\u751f\u6210sha-512\u7b97\u6cd5,pw\u662f\u8981\u52a0\u5bc6\u7684\u5bc6\u7801 a=$(python3 -c 'import crypt,getpass;pw=\"123456\";print(crypt.crypt(pw))') $ echo $a QUbhMQ9BAGQBE \u793a\u4f8b\u4e00 \u7528\u6237\u5bc6\u7801 - hosts : pve tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - name : add user user : name={{ item }} groups=admin password=d6F1o2yfgTymU shell=/bin/bash with_items : - user1 - user2 - user3 \u793a\u4f8b\u4e8c: \u516c\u94a5\u5f62\u5f0f tasks : - name : ensure admin group group : name : admin state : present - name : ensure admin group nopasswd sudo copy : dest : /etc/sudoers.d/admin content : | %admin ALL=(ALL:ALL) NOPASSWD: ALL - user : name : \"{{ item }}\" groups : admin shell : /bin/bash with_items : - lixie - user : name : \"{{ item }}\" state : absent with_items : - user # - ubuntu - authorized_key : user : \"{{ item.u }}\" key : \"https://github.com/{{ item.k }}.keys\" # \u516c\u94a5\u4e0a\u4f20\u5230github with_items : - { u : lixie , k : lixie021 } \u6267\u884cansible-playbook ansible-playbook -i inventory/pve pve.yaml \u5b98\u65b9\u53c2\u8003\u6587\u732e: https://gist.github.com/alces/f7e3de25d98a19550a4e4f97cabc2cf4?from_wecom=1 ansible-playbook -i ucd-mysql, add-ssh-users.yml","title":"\u6848\u4f8b\u4e00\uff1acreate_user.yaml"},{"location":"ansible/ansible-case1/#_2","text":"ubuntu22.04 \u8fd0\u884cansible\u62a5\u9519 ubuntu22.04 \u6267\u884c\u62a5\u9519 TASK [ Gathering Facts ] ******************************************************************************************************* fatal: [ axis ] : FAILED! = > { \"ansible_facts\" : {} , \"changed\" : false, \"failed_modules\" : { \"ansible.legacy.setup\" : { \"failed\" : true, \"module_stderr\" : \"/bin/sh: 1: /usr/bin/python: not found\\n\" , \"module_stdout\" : \"\" , \"msg\" : \"The module failed to execute correctly, you probably need to set the interpreter.\\nSee stdout/stderr for the exact error\" , \"rc\" : 127 }} , \"msg\" : \"The following modules failed to execute: ansible.legacy.setup\\n\" }","title":"\u9644\u4ef6"},{"location":"ansible/ansible-docs2/","text":"YAML \u4ecb\u7ecd \u00b6 YAML \u662f\u4e00\u79cd\u7528\u6765\u8868\u8fbe\u8d44\u6599\u5e8f\u5217\u7684\u683c\u5f0f\uff0c\u53c2\u8003\u4e86\u5176\u4ed6\u591a\u79cd\u8bed\u8a00\u6240\u4ee5\u5177\u6709\u5f88\u9ad8\u7684\u53ef \u8bfb\u6027\u3002YAML \u662f YAML Ain\u2019t Markup Language \u7684\u7f29\u5199\uff0c\u5373 YAML \u4e0d\u662f XML\u3002\u4e0d\u8fc7\u5728 \u7814\u53d1\u8fd9\u79cd\u8bed\u8a00\u65f6\uff0cYAML \u7684\u610f\u601d\u5176\u5b9e\u662f\u201dYet Another Markup Language\u201d\uff08\u4ecd\u662f\u4e00\u79cd\u6807 \u8bb0\u8bed\u8a00\uff09\u3002\u5176\u7279\u6027\u5982\u4e0b\uff1a \u5177\u6709\u5f88\u597d\u7684\u53ef\u8bfb\u6027\uff0c\u6613\u4e8e\u5b9e\u73b0\uff1b \u8868\u8fbe\u80fd\u529b\u5f3a\uff0c\u6269\u5c55\u6027\u597d\uff1b \u548c\u811a\u672c\u8bed\u8a00\u7684\u4ea4\u4e92\u6027\u597d\uff1b \u6709\u4e00\u4e2a\u4e00\u81f4\u7684\u4fe1\u606f\u6a21\u578b\uff1b \u53ef\u4ee5\u57fa\u4e8e\u6d41\u6765\u5904\u7406\u3002 YAML \u8bed\u6cd5 \u00b6 YAML \u7684\u8bed\u6cd5\u548c\u5176\u5b83\u8bed\u8a00\u7c7b\u4f3c\uff0c\u4e5f\u53ef\u4ee5\u8868\u8fbe\u6563\u5217\u8868\u3001\u6807\u91cf\u7b49\u6570\u636e\u7ed3\u6784\u3002\u5176\u4e2d\u7ed3\u6784 \uff08structure\uff09\u901a\u8fc7\u7a7a\u683c\u6765\u5c55\u793a\uff1b\u5e8f\u5217\uff08sequence\uff09\u91cc\u7684\u9879\u7528\u201d-\u201d\u6765\u4ee3\u8868\uff1bMap \u91cc\u7684\u952e\u503c \u5bf9\u7528\u201d:\u201d\u5206\u9694\u3002YAML \u6587\u4ef6\u6269\u5c55\u540d\u901a\u5e38\u4e3a\uff1ayaml\uff0c\u5982\uff1aexample.yaml\u3002\u4e0b\u9762\u662f YAML \u7684\u4e00\u4e2a\u793a\u4f8b\u3002 YAML\u5e38\u7528\u7684\u6570\u636e\u7c7b\u578b \u00b6 YAML \u4e2d\u6709\u4e24\u79cd\u5e38\u7528\u7684\u6570\u636e\u7c7b\u578b\uff1alist \u548c dictionary\u3002 1.list \uff08\u5217\u8868\uff09 \u5217\u8868\uff08list\uff09\u7684\u6240\u6709\u5143\u7d20\u5747\u4f7f\u7528\u201d-\u201d\u5f00\u5934\uff0c\u4f8b\u5982\uff1a -Apple -Orange -Strawberry -Mango 2.dictionary \u5b57\u5178\uff08dictionary\uff09\u901a\u8fc7 key \u4e0e value \u8fdb\u884c\u6807\u8bc6\uff0c\u4f8b\u5982\uff1a name: Example Developer Job: Developer Skill: Elite \u4e5f\u53ef\u4ee5\u4f7f\u7528 key:value \u7684\u5f62\u5f0f\u653e\u7f6e\u4e8e{ }\u4e2d\u8fdb\u884c\u8868\u793a\uff0c\u4f8b\u5982\uff1a { name: Example Developer, Job: Developer, Skill: Elite} Ansible \u57fa\u7840\u5143\u7d20\u4ecb\u7ecd \u00b6 1. Inventory(\u4e3b\u673a\u6e05\u5355) \u00b6 Inventory \u6587\u4ef6\u4e2d\u4ee5\u4e2d\u62ec\u53f7\u4e2d\u7684\u5b57\u7b26\u6807\u8bc6\u4e3a\u7ec4\u540d\uff0c\u5c06\u4e3b\u673a\u5206\u7ec4\u7ba1\u7406\uff0c\u4e5f\u53ef\u4ee5\u5c06\u540c\u4e00\u4e3b\u673a\u540c\u65f6\u5212\u5206\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u7ec4\u4e2d\u3002\u5982\u679c\u88ab\u7ba1\u7406\u4e3b\u673a\u4f7f\u7528\u975e\u9ed8\u8ba4\u7684SSH\u7aef\u53e3\uff0c\u4ee5\u4e0b\u4f8b\u5b50\u4e3b\u8981\u5206\u4e86\u4e24\u4e2a\u4e3b\u673a\u7ec4cka-1\u548ccka-2\uff0cinventory \u4e0b\u6709\u5f88\u591a\u7684\u4e3b\u673a\u6587\u4ef6\uff0c\u6ce8\u610f\u8fd9\u91ccansible\u7684\u914d\u7f6e\u6587\u4ef6\u662f\u505a\u4e86\u4fee\u6539\u7684\u3002 $ cat inventory/cka [cka-1] cka.master hostname=master0 ansible_host=172.30.42.244 ansible_user=lixie [cka-2] cka.node1 hostname=node1 ansible_host=172.30.192.106 ansible_user=lixie cka.node2 hostname=node2 ansible_host=172.30.226.223 ansible_user=lixie Inventory \u53c2\u6570: \u00b6 Ansible \u57fa\u4e8e SSH \u8fde\u63a5 Inventory \u4e2d\u6307\u5b9a\u7684\u88ab\u7ba1\u7406\u4e3b\u673a\u65f6\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u6307\u5b9a \u4ea4\u4e92\u65b9\u5f0f\uff0c\u8fd9\u4e9b\u53c2\u6570\u5982\u8868 3-2 \u6240\u793a\u3002 ansible_ssh_host \u5c06\u8981\u8fde\u63a5\u7684\u8fdc\u7a0b\u4e3b\u673a\u540d.\u4e0e\u4f60\u60f3\u8981\u8bbe\u5b9a\u7684\u4e3b\u673a\u7684\u522b\u540d\u4e0d\u540c\u7684\u8bdd,\u53ef\u901a\u8fc7\u6b64\u53d8\u91cf\u8bbe\u7f6e. ansible_ssh_port ssh\u7aef\u53e3\u53f7.\u5982\u679c\u4e0d\u662f\u9ed8\u8ba4\u7684\u7aef\u53e3\u53f7,\u901a\u8fc7\u6b64\u53d8\u91cf\u8bbe\u7f6e. ansible_ssh_user \u9ed8\u8ba4\u7684 ssh \u7528\u6237\u540d ansible_ssh_pass ssh \u5bc6\u7801 ( \u8fd9\u79cd\u65b9\u5f0f\u5e76\u4e0d\u5b89\u5168,\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 --ask-pass \u6216 SSH \u5bc6\u94a5 ) ansible_sudo_pass sudo \u5bc6\u7801 ( \u8fd9\u79cd\u65b9\u5f0f\u5e76\u4e0d\u5b89\u5168,\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 --ask-sudo-pass ) ansible_sudo_exe ( new in version 1 .8 ) sudo \u547d\u4ee4\u8def\u5f84 ( \u9002\u7528\u4e8e1.8\u53ca\u4ee5\u4e0a\u7248\u672c ) ansible_connection \u4e0e\u4e3b\u673a\u7684\u8fde\u63a5\u7c7b\u578b.\u6bd4\u5982:local, ssh \u6216\u8005 paramiko. Ansible 1 .2 \u4ee5\u524d\u9ed8\u8ba4\u4f7f\u7528 paramiko.1.2 \u4ee5\u540e\u9ed8\u8ba4\u4f7f\u7528 'smart' , 'smart' \u65b9\u5f0f\u4f1a\u6839\u636e\u662f\u5426\u652f\u6301 ControlPersist, \u6765\u5224\u65ad 'ssh' \u65b9\u5f0f\u662f\u5426\u53ef\u884c. ansible_ssh_private_key_file ssh \u4f7f\u7528\u7684\u79c1\u94a5\u6587\u4ef6.\u9002\u7528\u4e8e\u6709\u591a\u4e2a\u5bc6\u94a5,\u800c\u4f60\u4e0d\u60f3\u4f7f\u7528 SSH \u4ee3\u7406\u7684\u60c5\u51b5. ansible_shell_type \u76ee\u6807\u7cfb\u7edf\u7684shell\u7c7b\u578b.\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u547d\u4ee4\u7684\u6267\u884c\u4f7f\u7528 'sh' \u8bed\u6cd5,\u53ef\u8bbe\u7f6e\u4e3a 'csh' \u6216 'fish' . ansible_python_interpreter \u76ee\u6807\u4e3b\u673a\u7684 python \u8def\u5f84.\u9002\u7528\u4e8e\u7684\u60c5\u51b5: \u7cfb\u7edf\u4e2d\u6709\u591a\u4e2a Python, \u6216\u8005\u547d\u4ee4\u8def\u5f84\u4e0d\u662f \"/usr/bin/python\" ,\u6bd4\u5982 \\* BSD, \u6216\u8005 /usr/bin/python \u4e0d\u662f 2 .X \u7248\u672c\u7684 Python.\u6211\u4eec\u4e0d\u4f7f\u7528 \"/usr/bin/env\" \u673a\u5236,\u56e0\u4e3a\u8fd9\u8981\u6c42\u8fdc\u7a0b\u7528\u6237\u7684\u8def\u5f84\u8bbe\u7f6e\u6b63\u786e,\u4e14\u8981\u6c42 \"python\" \u53ef\u6267\u884c\u7a0b\u5e8f\u540d\u4e0d\u53ef\u4e3a python\u4ee5\u5916\u7684\u540d\u5b57 ( \u5b9e\u9645\u6709\u53ef\u80fd\u540d\u4e3apython26 ) . \u4e0e ansible_python_interpreter \u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c,\u53ef\u8bbe\u5b9a\u5982 ruby \u6216 perl \u7684\u8def\u5f84....","title":"Ansible \u9ad8\u7ea7\u7528\u6cd5"},{"location":"ansible/ansible-docs2/#yaml","text":"YAML \u662f\u4e00\u79cd\u7528\u6765\u8868\u8fbe\u8d44\u6599\u5e8f\u5217\u7684\u683c\u5f0f\uff0c\u53c2\u8003\u4e86\u5176\u4ed6\u591a\u79cd\u8bed\u8a00\u6240\u4ee5\u5177\u6709\u5f88\u9ad8\u7684\u53ef \u8bfb\u6027\u3002YAML \u662f YAML Ain\u2019t Markup Language \u7684\u7f29\u5199\uff0c\u5373 YAML \u4e0d\u662f XML\u3002\u4e0d\u8fc7\u5728 \u7814\u53d1\u8fd9\u79cd\u8bed\u8a00\u65f6\uff0cYAML \u7684\u610f\u601d\u5176\u5b9e\u662f\u201dYet Another Markup Language\u201d\uff08\u4ecd\u662f\u4e00\u79cd\u6807 \u8bb0\u8bed\u8a00\uff09\u3002\u5176\u7279\u6027\u5982\u4e0b\uff1a \u5177\u6709\u5f88\u597d\u7684\u53ef\u8bfb\u6027\uff0c\u6613\u4e8e\u5b9e\u73b0\uff1b \u8868\u8fbe\u80fd\u529b\u5f3a\uff0c\u6269\u5c55\u6027\u597d\uff1b \u548c\u811a\u672c\u8bed\u8a00\u7684\u4ea4\u4e92\u6027\u597d\uff1b \u6709\u4e00\u4e2a\u4e00\u81f4\u7684\u4fe1\u606f\u6a21\u578b\uff1b \u53ef\u4ee5\u57fa\u4e8e\u6d41\u6765\u5904\u7406\u3002","title":"YAML \u4ecb\u7ecd"},{"location":"ansible/ansible-docs2/#yaml_1","text":"YAML \u7684\u8bed\u6cd5\u548c\u5176\u5b83\u8bed\u8a00\u7c7b\u4f3c\uff0c\u4e5f\u53ef\u4ee5\u8868\u8fbe\u6563\u5217\u8868\u3001\u6807\u91cf\u7b49\u6570\u636e\u7ed3\u6784\u3002\u5176\u4e2d\u7ed3\u6784 \uff08structure\uff09\u901a\u8fc7\u7a7a\u683c\u6765\u5c55\u793a\uff1b\u5e8f\u5217\uff08sequence\uff09\u91cc\u7684\u9879\u7528\u201d-\u201d\u6765\u4ee3\u8868\uff1bMap \u91cc\u7684\u952e\u503c \u5bf9\u7528\u201d:\u201d\u5206\u9694\u3002YAML \u6587\u4ef6\u6269\u5c55\u540d\u901a\u5e38\u4e3a\uff1ayaml\uff0c\u5982\uff1aexample.yaml\u3002\u4e0b\u9762\u662f YAML \u7684\u4e00\u4e2a\u793a\u4f8b\u3002","title":"YAML \u8bed\u6cd5"},{"location":"ansible/ansible-docs2/#yaml_2","text":"YAML \u4e2d\u6709\u4e24\u79cd\u5e38\u7528\u7684\u6570\u636e\u7c7b\u578b\uff1alist \u548c dictionary\u3002 1.list \uff08\u5217\u8868\uff09 \u5217\u8868\uff08list\uff09\u7684\u6240\u6709\u5143\u7d20\u5747\u4f7f\u7528\u201d-\u201d\u5f00\u5934\uff0c\u4f8b\u5982\uff1a -Apple -Orange -Strawberry -Mango 2.dictionary \u5b57\u5178\uff08dictionary\uff09\u901a\u8fc7 key \u4e0e value \u8fdb\u884c\u6807\u8bc6\uff0c\u4f8b\u5982\uff1a name: Example Developer Job: Developer Skill: Elite \u4e5f\u53ef\u4ee5\u4f7f\u7528 key:value \u7684\u5f62\u5f0f\u653e\u7f6e\u4e8e{ }\u4e2d\u8fdb\u884c\u8868\u793a\uff0c\u4f8b\u5982\uff1a { name: Example Developer, Job: Developer, Skill: Elite}","title":"YAML\u5e38\u7528\u7684\u6570\u636e\u7c7b\u578b"},{"location":"ansible/ansible-docs2/#ansible","text":"","title":"Ansible \u57fa\u7840\u5143\u7d20\u4ecb\u7ecd"},{"location":"ansible/ansible-docs2/#1-inventory","text":"Inventory \u6587\u4ef6\u4e2d\u4ee5\u4e2d\u62ec\u53f7\u4e2d\u7684\u5b57\u7b26\u6807\u8bc6\u4e3a\u7ec4\u540d\uff0c\u5c06\u4e3b\u673a\u5206\u7ec4\u7ba1\u7406\uff0c\u4e5f\u53ef\u4ee5\u5c06\u540c\u4e00\u4e3b\u673a\u540c\u65f6\u5212\u5206\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u7ec4\u4e2d\u3002\u5982\u679c\u88ab\u7ba1\u7406\u4e3b\u673a\u4f7f\u7528\u975e\u9ed8\u8ba4\u7684SSH\u7aef\u53e3\uff0c\u4ee5\u4e0b\u4f8b\u5b50\u4e3b\u8981\u5206\u4e86\u4e24\u4e2a\u4e3b\u673a\u7ec4cka-1\u548ccka-2\uff0cinventory \u4e0b\u6709\u5f88\u591a\u7684\u4e3b\u673a\u6587\u4ef6\uff0c\u6ce8\u610f\u8fd9\u91ccansible\u7684\u914d\u7f6e\u6587\u4ef6\u662f\u505a\u4e86\u4fee\u6539\u7684\u3002 $ cat inventory/cka [cka-1] cka.master hostname=master0 ansible_host=172.30.42.244 ansible_user=lixie [cka-2] cka.node1 hostname=node1 ansible_host=172.30.192.106 ansible_user=lixie cka.node2 hostname=node2 ansible_host=172.30.226.223 ansible_user=lixie","title":"1. Inventory(\u4e3b\u673a\u6e05\u5355)"},{"location":"ansible/ansible-docs2/#inventory","text":"Ansible \u57fa\u4e8e SSH \u8fde\u63a5 Inventory \u4e2d\u6307\u5b9a\u7684\u88ab\u7ba1\u7406\u4e3b\u673a\u65f6\uff0c\u8fd8\u53ef\u4ee5\u901a\u8fc7\u53c2\u6570\u6307\u5b9a \u4ea4\u4e92\u65b9\u5f0f\uff0c\u8fd9\u4e9b\u53c2\u6570\u5982\u8868 3-2 \u6240\u793a\u3002 ansible_ssh_host \u5c06\u8981\u8fde\u63a5\u7684\u8fdc\u7a0b\u4e3b\u673a\u540d.\u4e0e\u4f60\u60f3\u8981\u8bbe\u5b9a\u7684\u4e3b\u673a\u7684\u522b\u540d\u4e0d\u540c\u7684\u8bdd,\u53ef\u901a\u8fc7\u6b64\u53d8\u91cf\u8bbe\u7f6e. ansible_ssh_port ssh\u7aef\u53e3\u53f7.\u5982\u679c\u4e0d\u662f\u9ed8\u8ba4\u7684\u7aef\u53e3\u53f7,\u901a\u8fc7\u6b64\u53d8\u91cf\u8bbe\u7f6e. ansible_ssh_user \u9ed8\u8ba4\u7684 ssh \u7528\u6237\u540d ansible_ssh_pass ssh \u5bc6\u7801 ( \u8fd9\u79cd\u65b9\u5f0f\u5e76\u4e0d\u5b89\u5168,\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 --ask-pass \u6216 SSH \u5bc6\u94a5 ) ansible_sudo_pass sudo \u5bc6\u7801 ( \u8fd9\u79cd\u65b9\u5f0f\u5e76\u4e0d\u5b89\u5168,\u6211\u4eec\u5f3a\u70c8\u5efa\u8bae\u4f7f\u7528 --ask-sudo-pass ) ansible_sudo_exe ( new in version 1 .8 ) sudo \u547d\u4ee4\u8def\u5f84 ( \u9002\u7528\u4e8e1.8\u53ca\u4ee5\u4e0a\u7248\u672c ) ansible_connection \u4e0e\u4e3b\u673a\u7684\u8fde\u63a5\u7c7b\u578b.\u6bd4\u5982:local, ssh \u6216\u8005 paramiko. Ansible 1 .2 \u4ee5\u524d\u9ed8\u8ba4\u4f7f\u7528 paramiko.1.2 \u4ee5\u540e\u9ed8\u8ba4\u4f7f\u7528 'smart' , 'smart' \u65b9\u5f0f\u4f1a\u6839\u636e\u662f\u5426\u652f\u6301 ControlPersist, \u6765\u5224\u65ad 'ssh' \u65b9\u5f0f\u662f\u5426\u53ef\u884c. ansible_ssh_private_key_file ssh \u4f7f\u7528\u7684\u79c1\u94a5\u6587\u4ef6.\u9002\u7528\u4e8e\u6709\u591a\u4e2a\u5bc6\u94a5,\u800c\u4f60\u4e0d\u60f3\u4f7f\u7528 SSH \u4ee3\u7406\u7684\u60c5\u51b5. ansible_shell_type \u76ee\u6807\u7cfb\u7edf\u7684shell\u7c7b\u578b.\u9ed8\u8ba4\u60c5\u51b5\u4e0b,\u547d\u4ee4\u7684\u6267\u884c\u4f7f\u7528 'sh' \u8bed\u6cd5,\u53ef\u8bbe\u7f6e\u4e3a 'csh' \u6216 'fish' . ansible_python_interpreter \u76ee\u6807\u4e3b\u673a\u7684 python \u8def\u5f84.\u9002\u7528\u4e8e\u7684\u60c5\u51b5: \u7cfb\u7edf\u4e2d\u6709\u591a\u4e2a Python, \u6216\u8005\u547d\u4ee4\u8def\u5f84\u4e0d\u662f \"/usr/bin/python\" ,\u6bd4\u5982 \\* BSD, \u6216\u8005 /usr/bin/python \u4e0d\u662f 2 .X \u7248\u672c\u7684 Python.\u6211\u4eec\u4e0d\u4f7f\u7528 \"/usr/bin/env\" \u673a\u5236,\u56e0\u4e3a\u8fd9\u8981\u6c42\u8fdc\u7a0b\u7528\u6237\u7684\u8def\u5f84\u8bbe\u7f6e\u6b63\u786e,\u4e14\u8981\u6c42 \"python\" \u53ef\u6267\u884c\u7a0b\u5e8f\u540d\u4e0d\u53ef\u4e3a python\u4ee5\u5916\u7684\u540d\u5b57 ( \u5b9e\u9645\u6709\u53ef\u80fd\u540d\u4e3apython26 ) . \u4e0e ansible_python_interpreter \u7684\u5de5\u4f5c\u65b9\u5f0f\u76f8\u540c,\u53ef\u8bbe\u5b9a\u5982 ruby \u6216 perl \u7684\u8def\u5f84....","title":"Inventory \u53c2\u6570:"},{"location":"basic/3-k8s-systeminit-docs/","text":"k8s\u96c6\u7fa4\u90e8\u7f72 \u00b6 kubeadm \u521b\u5efa\u96c6\u7fa4 \u00b6 \u6ce8\u610f\u4ee5\u4e0b\u662fcentos\u642d\u5efa \u96c6\u7fa4\u521d\u59cb\u5316 \u00b6 # \u5c06 SELinux \u8bbe\u7f6e\u4e3a permissive \u6a21\u5f0f\uff08\u76f8\u5f53\u4e8e\u5c06\u5176\u7981\u7528\uff09 sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #\u5173\u95edswap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #\u5141\u8bb8 iptables \u68c0\u67e5\u6865\u63a5\u6d41\u91cf cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system \u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes = kubernetes # \u6240\u6709\u8282\u70b9 sudo systemctl enable --now kubelet \u6e29\u99a8\u63d0\u793a kubelet \u73b0\u5728\u6bcf\u9694\u51e0\u79d2\u5c31\u4f1a\u91cd\u542f\uff0c\u56e0\u4e3a\u5b83\u9677\u5165\u4e86\u4e00\u4e2a\u7b49\u5f85 kubeadm \u6307\u4ee4\u7684\u6b7b\u5faa\u73af \u67e5\u770bkubeadm\u7684\u7248\u672c\uff0c\u5e76\u62c9\u53d6\u8be5\u7248\u672c\u7684images kubeadm config images list --kubernetes-version v1.20.9 > k8s.images root@ubuntu:/home/ubuntu/script# cat k8s.images |awk -F'/' '{print $2}' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 \u811a\u672c\u4e0b\u8f7d\u955c\u50cf root@ubuntu:/home/ubuntu/script# cat ubuntu-k8s-images.sh #!/bin/bash images=' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 ' for i in $images ; do docker pull registry.aliyuncs.com/google_containers/$i done \u521d\u59cb\u5316master\u8282\u70b9\uff1a #\u6240\u6709\u673a\u5668\u6dfb\u52a0master\u57df\u540d\u6620\u5c04\uff0c\u4ee5\u4e0b\u9700\u8981\u4fee\u6539\u4e3a\u81ea\u5df1\u7684 echo \"192.168.8.70 cluster-endpoint\" >> /etc/hosts # \u4e3b\u8282\u70b9\u521d\u59cb\u5316 kubeadm init \\ --apiserver-advertise-address=192.168.159.201 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=172.16.0.0/16 #\u6240\u6709\u7f51\u7edc\u8303\u56f4\u4e0d\u91cd\u53e0 \u521d\u59cb\u5316\u53d1\u73b0\u6240\u6709\u72b6\u6001\u90fd\u662f NotReady \u5b89\u88c5\u7f51\u7edc\u7ec4\u4ef6: calico Node\u8282\u70b9\u52a0\u5165\u96c6\u7fa4 \u00b6 Warning \u65b0\u4ee4\u724c,\u9ed8\u8ba4\u7684\u4ee4\u724c24\u5c0f\u65f6\u5019\u5931\u6548 kubeadm token create --print-join-command Mac\u8fde\u63a5\u96c6\u7fa4\u62a5\u9519 \u00b6 Mac \u8fde\u63a5k8s\u96c6\u7fa4\u62a5\u9519 x509: certificate signed by unknown authority \u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\u6ca1\u6709\u6267\u884c\u5916\u7f51\u5730\u5740.\u5bfc\u81f4\u8bc1\u4e66\u4e0d\u80fd\u6b63\u5e38\u4f7f\u7528 \u5c06 master \u7684\u5916\u7f51\u5730\u5740\u548c\u4e3b\u673a\u540d\u89e3\u6790\u5230\u672c\u5730 hosts \u6e05\u7406k8s\u96c6\u7fa4 \u00b6 Success [root@k8s-master ~]# kubeadm reset [root@k8s-master ~]# kubectl delete node 192.168.200.112 [root@k8s-node01 ~]# docker rm -f $(docker ps -aq) [root@k8s-node01 ~]# systemctl stop kubelet [root@k8s-node01 ~]# rm -rf /etc/kubernetes/* [root@k8s-node01 ~]# rm -rf /var/lib/kubelet/* \u62a5\u9519\u89e3\u51b3 \u00b6 \u5728\u4f7f\u7528k8s\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7ecf\u5e38\u4f1a\u9047\u5230\u96c6\u7fa4\u51fa\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u67e5\u770bkubelet \u65e5\u5fd7\uff0c\u89e3\u51b3\u95ee\u9898\u3002 [ cka ] root@node1:/home/lixie# journalctl -u kubelet -f //\u7edd\u5927\u90e8\u5206\u7684\u9519\u8bef\u90fd\u9700\u8981\u770b\u65e5\u5fd7\u6765\u89e3\u51b3 \u5f3a\u5236\u5220\u9664namespace \u00b6 delete namespace \u6253\u5f00\u4e00\u4e2a\u65b0\u7a97\u53e3\uff1aroot@master30:~# kubectl proxy --port=8001 \u65b9\u6cd5\u4e8c $ NAMESPACE_NAME=rook-ceph cat <<EOF | curl -X PUT \\ 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE_NAME/finalize \\ -H \"Content-Type : application/json\" \\ --data-binary @- { \"kind\" : \"Namespace\" , \"apiVersion\" : \"v1\" , \"metadata\" : { \"name\" : \"$NAMESPACE_NAME\" }, \"spec\" : { \"finalizers\" : null } } EOF System OOM encountered \u4e24\u79cdOOM\uff08\u8fdb\u7a0bOOM\uff0c\u5bb9\u5668OOM\uff09\u53d1\u751f\u540e\uff0c\u90fd\u53ef\u80fd\u4f1a\u4f34\u968f\u4e00\u4e2a\u7cfb\u7edfOOM\u4e8b\u4ef6\uff0c\u8be5\u4e8b\u4ef6\u7684\u539f\u56e0\u662f\u7531\u4e0a\u8ff0OOM\u4e8b\u4ef6\u4f34\u968f\u5bfc\u81f4\u3002 \u9700\u8981\u89e3\u51b3\u4e0a\u9762\u8fdb\u7a0bOOM\u6216\u8005\u5bb9\u5668CgroupOOM\u7684\u95ee\u9898\uff0c\u6587\u7ae0\u53c2\u8003: \u89e3\u51b3OOM \u5c0f\u6280\u5de7 \u00b6 \u5c0f\u6280\u5de7 Mac \u7ba1\u7406kubernetes\uff0c\u5408\u5e76yaml https://aisensiy.me/kubeconfig-management \u53ef\u4ee5\u901a\u8fc7journalctl -xeu kubelet \u6765\u67e5\u770b\u65e5\u5fd7","title":"\u96c6\u7fa4\u90e8\u7f72"},{"location":"basic/3-k8s-systeminit-docs/#k8s","text":"","title":"k8s\u96c6\u7fa4\u90e8\u7f72"},{"location":"basic/3-k8s-systeminit-docs/#kubeadm","text":"\u6ce8\u610f\u4ee5\u4e0b\u662fcentos\u642d\u5efa","title":"kubeadm \u521b\u5efa\u96c6\u7fa4"},{"location":"basic/3-k8s-systeminit-docs/#_1","text":"# \u5c06 SELinux \u8bbe\u7f6e\u4e3a permissive \u6a21\u5f0f\uff08\u76f8\u5f53\u4e8e\u5c06\u5176\u7981\u7528\uff09 sudo setenforce 0 sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config #\u5173\u95edswap swapoff -a sed -ri 's/.*swap.*/#&/' /etc/fstab #\u5141\u8bb8 iptables \u68c0\u67e5\u6865\u63a5\u6d41\u91cf cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sudo sysctl --system \u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes = kubernetes # \u6240\u6709\u8282\u70b9 sudo systemctl enable --now kubelet \u6e29\u99a8\u63d0\u793a kubelet \u73b0\u5728\u6bcf\u9694\u51e0\u79d2\u5c31\u4f1a\u91cd\u542f\uff0c\u56e0\u4e3a\u5b83\u9677\u5165\u4e86\u4e00\u4e2a\u7b49\u5f85 kubeadm \u6307\u4ee4\u7684\u6b7b\u5faa\u73af \u67e5\u770bkubeadm\u7684\u7248\u672c\uff0c\u5e76\u62c9\u53d6\u8be5\u7248\u672c\u7684images kubeadm config images list --kubernetes-version v1.20.9 > k8s.images root@ubuntu:/home/ubuntu/script# cat k8s.images |awk -F'/' '{print $2}' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 \u811a\u672c\u4e0b\u8f7d\u955c\u50cf root@ubuntu:/home/ubuntu/script# cat ubuntu-k8s-images.sh #!/bin/bash images=' kube-apiserver:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 kube-proxy:v1.20.9 pause:3.2 etcd:3.4.13-0 coredns:1.7.0 ' for i in $images ; do docker pull registry.aliyuncs.com/google_containers/$i done \u521d\u59cb\u5316master\u8282\u70b9\uff1a #\u6240\u6709\u673a\u5668\u6dfb\u52a0master\u57df\u540d\u6620\u5c04\uff0c\u4ee5\u4e0b\u9700\u8981\u4fee\u6539\u4e3a\u81ea\u5df1\u7684 echo \"192.168.8.70 cluster-endpoint\" >> /etc/hosts # \u4e3b\u8282\u70b9\u521d\u59cb\u5316 kubeadm init \\ --apiserver-advertise-address=192.168.159.201 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=172.16.0.0/16 #\u6240\u6709\u7f51\u7edc\u8303\u56f4\u4e0d\u91cd\u53e0 \u521d\u59cb\u5316\u53d1\u73b0\u6240\u6709\u72b6\u6001\u90fd\u662f NotReady \u5b89\u88c5\u7f51\u7edc\u7ec4\u4ef6: calico","title":"\u96c6\u7fa4\u521d\u59cb\u5316"},{"location":"basic/3-k8s-systeminit-docs/#node","text":"Warning \u65b0\u4ee4\u724c,\u9ed8\u8ba4\u7684\u4ee4\u724c24\u5c0f\u65f6\u5019\u5931\u6548 kubeadm token create --print-join-command","title":"Node\u8282\u70b9\u52a0\u5165\u96c6\u7fa4"},{"location":"basic/3-k8s-systeminit-docs/#mac","text":"Mac \u8fde\u63a5k8s\u96c6\u7fa4\u62a5\u9519 x509: certificate signed by unknown authority \u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\u6ca1\u6709\u6267\u884c\u5916\u7f51\u5730\u5740.\u5bfc\u81f4\u8bc1\u4e66\u4e0d\u80fd\u6b63\u5e38\u4f7f\u7528 \u5c06 master \u7684\u5916\u7f51\u5730\u5740\u548c\u4e3b\u673a\u540d\u89e3\u6790\u5230\u672c\u5730 hosts","title":"Mac\u8fde\u63a5\u96c6\u7fa4\u62a5\u9519"},{"location":"basic/3-k8s-systeminit-docs/#k8s_1","text":"Success [root@k8s-master ~]# kubeadm reset [root@k8s-master ~]# kubectl delete node 192.168.200.112 [root@k8s-node01 ~]# docker rm -f $(docker ps -aq) [root@k8s-node01 ~]# systemctl stop kubelet [root@k8s-node01 ~]# rm -rf /etc/kubernetes/* [root@k8s-node01 ~]# rm -rf /var/lib/kubelet/*","title":"\u6e05\u7406k8s\u96c6\u7fa4"},{"location":"basic/3-k8s-systeminit-docs/#_2","text":"\u5728\u4f7f\u7528k8s\u7684\u8fc7\u7a0b\u4e2d\uff0c\u7ecf\u5e38\u4f1a\u9047\u5230\u96c6\u7fa4\u51fa\u95ee\u9898\uff0c\u90a3\u4e48\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u67e5\u770bkubelet \u65e5\u5fd7\uff0c\u89e3\u51b3\u95ee\u9898\u3002 [ cka ] root@node1:/home/lixie# journalctl -u kubelet -f //\u7edd\u5927\u90e8\u5206\u7684\u9519\u8bef\u90fd\u9700\u8981\u770b\u65e5\u5fd7\u6765\u89e3\u51b3","title":"\u62a5\u9519\u89e3\u51b3"},{"location":"basic/3-k8s-systeminit-docs/#namespace","text":"delete namespace \u6253\u5f00\u4e00\u4e2a\u65b0\u7a97\u53e3\uff1aroot@master30:~# kubectl proxy --port=8001 \u65b9\u6cd5\u4e8c $ NAMESPACE_NAME=rook-ceph cat <<EOF | curl -X PUT \\ 127.0.0.1:8001/api/v1/namespaces/$NAMESPACE_NAME/finalize \\ -H \"Content-Type : application/json\" \\ --data-binary @- { \"kind\" : \"Namespace\" , \"apiVersion\" : \"v1\" , \"metadata\" : { \"name\" : \"$NAMESPACE_NAME\" }, \"spec\" : { \"finalizers\" : null } } EOF System OOM encountered \u4e24\u79cdOOM\uff08\u8fdb\u7a0bOOM\uff0c\u5bb9\u5668OOM\uff09\u53d1\u751f\u540e\uff0c\u90fd\u53ef\u80fd\u4f1a\u4f34\u968f\u4e00\u4e2a\u7cfb\u7edfOOM\u4e8b\u4ef6\uff0c\u8be5\u4e8b\u4ef6\u7684\u539f\u56e0\u662f\u7531\u4e0a\u8ff0OOM\u4e8b\u4ef6\u4f34\u968f\u5bfc\u81f4\u3002 \u9700\u8981\u89e3\u51b3\u4e0a\u9762\u8fdb\u7a0bOOM\u6216\u8005\u5bb9\u5668CgroupOOM\u7684\u95ee\u9898\uff0c\u6587\u7ae0\u53c2\u8003: \u89e3\u51b3OOM","title":"\u5f3a\u5236\u5220\u9664namespace"},{"location":"basic/3-k8s-systeminit-docs/#_3","text":"\u5c0f\u6280\u5de7 Mac \u7ba1\u7406kubernetes\uff0c\u5408\u5e76yaml https://aisensiy.me/kubeconfig-management \u53ef\u4ee5\u901a\u8fc7journalctl -xeu kubelet \u6765\u67e5\u770b\u65e5\u5fd7","title":"\u5c0f\u6280\u5de7"},{"location":"basic/4-ansible-install-k8s/","text":"\u9879\u76ee\u4e00: Ansible\u90e8\u7f72\u4e8c\u8fdb\u5236kubernetes\u96c6\u7fa4 \u00b6 Info \u4e8c\u8fdb\u5236\u65b9\u5f0f\u662f\u5b98\u65b9\u63a8\u8350\u7684\u90e8\u7f72\u65b9\u5f0f\u7684\u4e00\u79cd\uff0c\u4f46\u662f\u7531\u4e8e\u624b\u52a8\u90e8\u7f72\u96be\u5ea6\u8f83\u5927\uff0c\u6b64\u6b21\u91c7\u7528GitHub\u7684\u5f00\u6e90\u9879\u76eekubeasz Github \u9879\u76ee\u5730\u5740: https://github.com/easzlab/kubeasz \u73af\u5883\u914d\u7f6e \u00b6 \u89d2\u8272 IP \u63cf\u8ff0 K8s-master1 192.168.10.10 kube-scheduler\uff0ckubelet\uff0ckube-proxy\uff0cdocker\uff0ckeepalived\uff0cetcd K8s-master2 192.168.10.11 kube-scheduler\uff0ckubelet\uff0ckube-proxy\uff0cdocker\uff0ckeepalived\uff0cetcd K8s-node1 192.168.10.12 kubelet\uff0ckube-proxy\uff0cdocker \u4e00.\u914d\u7f6e\u9ad8\u53ef\u7528\u8d1f\u8f7d\u5747\u8861keepalived \u00b6 1.\u5b89\u88c5keepalived \u00b6 yum -y install ipvsadm keepalived haproxy 2.master1\u4e0a\u914d\u7f6ekeepalived \u00b6 [root@k8s-master1 ~]# cat /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface ens192 virtual_router_id 1 priority 100 advert_int 2 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.88 } } 3.master2\u4e0a\u914d\u7f6ekeepalived \u00b6 [root@k8s-master2 ~]# cat /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface ens192 virtual_router_id 1 priority 80 advert_int 2 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.88 } } 4.\u91cd\u542fkeepalived \u00b6 [root@k8s-master1 ~]# systemctl restart keepalived [root@k8s-master1 ~]# systemctl enable keepalived \u4e8c.\u914d\u7f6ehaproxy \u00b6 1.master1\u4e0a\u914d\u7f6ehaproxy \u00b6 [root@k8s-master1 ~]# vim /etc/haproxy/haproxy.cfg listen k8s_6443 mode tcp bind 192.168.10.88:6443 server 192.168.10.10 192.168.10.10:6443 check inter 5s rise 2 fall 3 server 192.168.10.11 192.168.10.11:6443 check inter 5s rise 2 fall 3 2.master2\u4e0a\u914d\u7f6ehaproxy \u00b6 [root@k8s-master1 ~]# vim /etc/haproxy/haproxy.cfg listen k8s_6443 mode tcp bind 192.168.10.88:6443 server 192.168.10.10 192.168.10.10:6443 check inter 5s rise 2 fall 3 server 192.168.10.11 192.168.10.11:6443 check inter 5s rise 2 fall 3 3.\u91cd\u542fhaproxy \u00b6 [root@k8s-master1 ~]# systemctl restart haproxy [root@k8s-master1 ~]# systemctl enable haproxy \u4e09.\u5b89\u88c5k8s(master\u4e0a\u914d\u7f6e) \u00b6 1.\u5b89\u88c5ansible \u00b6 yum -y install ansible 2.\u914d\u7f6e\u5404\u4e2a\u8282\u70b9\u514d\u5bc6\u767b\u9646 \u00b6 ssh-keygen -t rsa ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.10 ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.11 ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.12 3.yum\u5b89\u88c5docker \u00b6 3.1\u4e0b\u8f7d\u4f9d\u8d56\u8f6f\u4ef6 \u00b6 [root@jenkins ~]# yum install -y yum-utils device-mapper-persistent-data lvm2 3.2\u4e0b\u8f7ddocker\u6e90 \u00b6 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 3.3\u5b89\u88c5docker \u00b6 [root@jenkins ~]# yum -y install docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io 3.4\u4fee\u6539docker\u6839\u76ee\u5f55\u4f4d\u7f6e \u00b6 [root@jenkins ~]# mkdir -p /data/docker [root@jenkins ~]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --graph /data/docker -H fd:// [root@jenkins ~]# systemctl start docker [root@jenkins ~]# systemctl enable docker 3.5\u914d\u7f6edocker\u955c\u50cf\u4ed3\u5e93 \u00b6 [root@jenkins docker]# cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\": [\"https://7jauxlsb.mirror.aliyuncs.com\"], \"insecure-registries\":[\"192.168.10.133\"] } EOF 3.6\u91cd\u542fdocker \u00b6 [root@jenkins docker]# systemctl daemon-reload [root@jenkins docker]# systemctl restart docker 4.\u90e8\u7f72\u8282\u70b9\u7684\u5404\u9879\u76ee\u7ec4\u4ef6 \u00b6 4.1\u5b89\u88c5ezdown \u00b6 export release=3.1.1 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown 4.2\u521d\u59cb\u5316ezdown \u00b6 [root@k8s-master1 ~]# vim ezdown DOCKER_VER=19.03.15 KUBEASZ_VER=3.1.1 K8S_BIN_VER=v1.22.5 [root@k8s-master1 ~]# ./ezdown -D 4.3\u751f\u6210ansible hosts\u6587\u4ef6 \u00b6 [root@k8s-master1 ~]# cd /etc/kubeasz/ [root@k8s-master1 kubeasz]# ./ezctl new k8s-cluster1 4.4\u7f16\u8f91ansible hosts\u6587\u4ef6 \u00b6 # 'etcd' cluster should have odd member(s) (1,3,5,...) [etcd] 192.168.10.10 192.168.10.11 # master node(s) [kube_master] 192.168.10.10 192.168.10.11 # work node(s) [kube_node] 192.168.10.12 # [optional] harbor server, a private docker registry # 'NEW_INSTALL': 'true' to install a harbor server; 'false' to integrate with existed one [harbor] #192.168.1.8 NEW_INSTALL=false # [optional] loadbalance for accessing k8s from outside [ex_lb] 192.168.10.11 LB_ROLE=backup EX_APISERVER_VIP=192.168.10.88 EX_APISERVER_PORT=6443 192.168.10.10 LB_ROLE=master EX_APISERVER_VIP=192.168.10.88 EX_APISERVER_PORT=6443 # [optional] ntp server for the cluster [chrony] #192.168.1.1 [all:vars] # --------- Main Variables --------------- # Secure port for apiservers SECURE_PORT=\"6443\" # Cluster container-runtime supported: docker, containerd CONTAINER_RUNTIME=\"docker\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"172.10.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"1-65535\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"cluster.local\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/usr/bin\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/kubeasz\" # Directory for a specific cluster cluster_dir=\"{{ base_dir }}/clusters/k8s-cluster1\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\" 4.5\u7f16\u8f91config.yml\u6587\u4ef6 \u00b6 ############################ # prepare ############################ # \u53ef\u9009\u79bb\u7ebf\u5b89\u88c5\u7cfb\u7edf\u8f6f\u4ef6\u5305 (offline|online) INSTALL_SOURCE: \"online\" # \u53ef\u9009\u8fdb\u884c\u7cfb\u7edf\u5b89\u5168\u52a0\u56fa github.com/dev-sec/ansible-collection-hardening OS_HARDEN: false # \u8bbe\u7f6e\u65f6\u95f4\u6e90\u670d\u52a1\u5668\u3010\u91cd\u8981\uff1a\u96c6\u7fa4\u5185\u673a\u5668\u65f6\u95f4\u5fc5\u987b\u540c\u6b65\u3011 ntp_servers: - \"ntp1.aliyun.com\" - \"time1.cloud.tencent.com\" - \"0.cn.pool.ntp.org\" # \u8bbe\u7f6e\u5141\u8bb8\u5185\u90e8\u65f6\u95f4\u540c\u6b65\u7684\u7f51\u7edc\u6bb5\uff0c\u6bd4\u5982\"10.0.0.0/8\"\uff0c\u9ed8\u8ba4\u5168\u90e8\u5141\u8bb8 local_network: \"0.0.0.0/0\" ############################ # role:deploy ############################ # default: ca will expire in 100 years # default: certs issued by the ca will expire in 50 years CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"438000h\" # kubeconfig \u914d\u7f6e\u53c2\u6570 CLUSTER_NAME: \"cluster1\" CONTEXT_NAME: \"context-{{ CLUSTER_NAME }}\" ############################ # role:etcd ############################ # \u8bbe\u7f6e\u4e0d\u540c\u7684wal\u76ee\u5f55\uff0c\u53ef\u4ee5\u907f\u514d\u78c1\u76d8io\u7ade\u4e89\uff0c\u63d0\u9ad8\u6027\u80fd ETCD_DATA_DIR: \"/var/lib/etcd\" ETCD_WAL_DIR: \"\" ############################ # role:runtime [containerd,docker] ############################ # ------------------------------------------- containerd # [.]\u542f\u7528\u5bb9\u5668\u4ed3\u5e93\u955c\u50cf ENABLE_MIRROR_REGISTRY: true # [containerd]\u57fa\u7840\u5bb9\u5668\u955c\u50cf SANDBOX_IMAGE: \"easzlab/pause-amd64:3.5\" # [containerd]\u5bb9\u5668\u6301\u4e45\u5316\u5b58\u50a8\u76ee\u5f55 CONTAINERD_STORAGE_DIR: \"/var/lib/containerd\" # ------------------------------------------- docker # [docker]\u5bb9\u5668\u5b58\u50a8\u76ee\u5f55 DOCKER_STORAGE_DIR: \"/data/docker\" # [docker]\u5f00\u542fRestful API ENABLE_REMOTE_API: false # [docker]\u4fe1\u4efb\u7684HTTP\u4ed3\u5e93 INSECURE_REG: '[\"127.0.0.1/8\",\"192.168.10.133\"]' ############################ # role:kube-master ############################ # k8s \u96c6\u7fa4 master \u8282\u70b9\u8bc1\u4e66\u914d\u7f6e\uff0c\u53ef\u4ee5\u6dfb\u52a0\u591a\u4e2aip\u548c\u57df\u540d\uff08\u6bd4\u5982\u589e\u52a0\u516c\u7f51ip\u548c\u57df\u540d\uff09 MASTER_CERT_HOSTS: - \"10.1.1.1\" - \"k8s.test.io\" #- \"www.test.com\" # node \u8282\u70b9\u4e0a pod \u7f51\u6bb5\u63a9\u7801\u957f\u5ea6\uff08\u51b3\u5b9a\u6bcf\u4e2a\u8282\u70b9\u6700\u591a\u80fd\u5206\u914d\u7684pod ip\u5730\u5740\uff09 # \u5982\u679cflannel \u4f7f\u7528 --kube-subnet-mgr \u53c2\u6570\uff0c\u90a3\u4e48\u5b83\u5c06\u8bfb\u53d6\u8be5\u8bbe\u7f6e\u4e3a\u6bcf\u4e2a\u8282\u70b9\u5206\u914dpod\u7f51\u6bb5 # https://github.com/coreos/flannel/issues/847 NODE_CIDR_LEN: 24 ############################ # role:kube-node ############################ # Kubelet \u6839\u76ee\u5f55 KUBELET_ROOT_DIR: \"/var/lib/kubelet\" # node\u8282\u70b9\u6700\u5927pod \u6570 MAX_PODS: 300 # \u914d\u7f6e\u4e3akube\u7ec4\u4ef6\uff08kubelet,kube-proxy,dockerd\u7b49\uff09\u9884\u7559\u7684\u8d44\u6e90\u91cf # \u6570\u503c\u8bbe\u7f6e\u8be6\u89c1templates/kubelet-config.yaml.j2 KUBE_RESERVED_ENABLED: \"no\" # k8s \u5b98\u65b9\u4e0d\u5efa\u8bae\u8349\u7387\u5f00\u542f system-reserved, \u9664\u975e\u4f60\u57fa\u4e8e\u957f\u671f\u76d1\u63a7\uff0c\u4e86\u89e3\u7cfb\u7edf\u7684\u8d44\u6e90\u5360\u7528\u72b6\u51b5\uff1b # \u5e76\u4e14\u968f\u7740\u7cfb\u7edf\u8fd0\u884c\u65f6\u95f4\uff0c\u9700\u8981\u9002\u5f53\u589e\u52a0\u8d44\u6e90\u9884\u7559\uff0c\u6570\u503c\u8bbe\u7f6e\u8be6\u89c1templates/kubelet-config.yaml.j2 # \u7cfb\u7edf\u9884\u7559\u8bbe\u7f6e\u57fa\u4e8e 4c/8g \u865a\u673a\uff0c\u6700\u5c0f\u5316\u5b89\u88c5\u7cfb\u7edf\u670d\u52a1\uff0c\u5982\u679c\u4f7f\u7528\u9ad8\u6027\u80fd\u7269\u7406\u673a\u53ef\u4ee5\u9002\u5f53\u589e\u52a0\u9884\u7559 # \u53e6\u5916\uff0c\u96c6\u7fa4\u5b89\u88c5\u65f6\u5019apiserver\u7b49\u8d44\u6e90\u5360\u7528\u4f1a\u77ed\u65f6\u8f83\u5927\uff0c\u5efa\u8bae\u81f3\u5c11\u9884\u75591g\u5185\u5b58 SYS_RESERVED_ENABLED: \"no\" # haproxy balance mode BALANCE_ALG: \"roundrobin\" ############################ # role:network [flannel,calico,cilium,kube-ovn,kube-router] ############################ # ------------------------------------------- flannel # [flannel]\u8bbe\u7f6eflannel \u540e\u7aef\"host-gw\",\"vxlan\"\u7b49 FLANNEL_BACKEND: \"vxlan\" DIRECT_ROUTING: false # [flannel] flanneld_image: \"quay.io/coreos/flannel:v0.10.0-amd64\" flannelVer: \"v0.13.0-amd64\" flanneld_image: \"easzlab/flannel:{{ flannelVer }}\" # [flannel]\u79bb\u7ebf\u955c\u50cftar\u5305 flannel_offline: \"flannel_{{ flannelVer }}.tar\" # ------------------------------------------- calico # [calico]\u8bbe\u7f6e CALICO_IPV4POOL_IPIP=\u201coff\u201d,\u53ef\u4ee5\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\uff0c\u6761\u4ef6\u9650\u5236\u8be6\u89c1 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]\u8bbe\u7f6e calico-node\u4f7f\u7528\u7684host IP\uff0cbgp\u90bb\u5c45\u901a\u8fc7\u8be5\u5730\u5740\u5efa\u7acb\uff0c\u53ef\u624b\u5de5\u6307\u5b9a\u4e5f\u53ef\u4ee5\u81ea\u52a8\u53d1\u73b0 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]\u8bbe\u7f6ecalico \u7f51\u7edc backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]\u66f4\u65b0\u652f\u6301calico \u7248\u672c: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.2\" # [calico]calico \u4e3b\u7248\u672c calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\" # [calico]\u79bb\u7ebf\u955c\u50cftar\u5305 calico_offline: \"calico_{{ calico_ver }}.tar\" # ------------------------------------------- cilium # [cilium]CILIUM_ETCD_OPERATOR \u521b\u5efa\u7684 etcd \u96c6\u7fa4\u8282\u70b9\u6570 1,3,5,7... ETCD_CLUSTER_SIZE: 1 # [cilium]\u955c\u50cf\u7248\u672c cilium_ver: \"v1.4.1\" # [cilium]\u79bb\u7ebf\u955c\u50cftar\u5305 cilium_offline: \"cilium_{{ cilium_ver }}.tar\" # ------------------------------------------- kube-ovn # [kube-ovn]\u9009\u62e9 OVN DB and OVN Control Plane \u8282\u70b9\uff0c\u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u4e2amaster\u8282\u70b9 OVN_DB_NODE: \"{{ groups['kube_master'][0] }}\" # [kube-ovn]\u79bb\u7ebf\u955c\u50cftar\u5305 kube_ovn_ver: \"v1.5.3\" kube_ovn_offline: \"kube_ovn_{{ kube_ovn_ver }}.tar\" # ------------------------------------------- kube-router # [kube-router]\u516c\u6709\u4e91\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u4e00\u822c\u9700\u8981\u59cb\u7ec8\u5f00\u542f ipinip\uff1b\u81ea\u6709\u73af\u5883\u53ef\u4ee5\u8bbe\u7f6e\u4e3a \"subnet\" OVERLAY_TYPE: \"full\" # [kube-router]NetworkPolicy \u652f\u6301\u5f00\u5173 FIREWALL_ENABLE: \"true\" # [kube-router]kube-router \u955c\u50cf\u7248\u672c kube_router_ver: \"v0.3.1\" busybox_ver: \"1.28.4\" # [kube-router]kube-router \u79bb\u7ebf\u955c\u50cftar\u5305 kuberouter_offline: \"kube-router_{{ kube_router_ver }}.tar\" busybox_offline: \"busybox_{{ busybox_ver }}.tar\" ############################ # role:cluster-addon ############################ # coredns \u81ea\u52a8\u5b89\u88c5 dns_install: \"no\" corednsVer: \"1.8.4\" ENABLE_LOCAL_DNS_CACHE: true dnsNodeCacheVer: \"1.17.0\" # \u8bbe\u7f6e local dns cache \u5730\u5740 LOCAL_DNS_CACHE: \"10.10.0.2\" # metric server \u81ea\u52a8\u5b89\u88c5 metricsserver_install: \"no\" metricsVer: \"v0.5.0\" # dashboard \u81ea\u52a8\u5b89\u88c5 dashboard_install: \"no\" dashboardVer: \"v2.3.1\" dashboardMetricsScraperVer: \"v1.0.6\" # ingress \u81ea\u52a8\u5b89\u88c5 ingress_install: \"no\" ingress_backend: \"traefik\" traefik_chart_ver: \"9.12.3\" # prometheus \u81ea\u52a8\u5b89\u88c5 prom_install: \"no\" prom_namespace: \"monitor\" prom_chart_ver: \"12.10.6\" # nfs-provisioner \u81ea\u52a8\u5b89\u88c5 nfs_provisioner_install: \"no\" nfs_provisioner_namespace: \"kube-system\" nfs_provisioner_ver: \"v4.0.1\" nfs_storage_class: \"managed-nfs-storage\" nfs_server: \"192.168.1.10\" nfs_path: \"/data/nfs\" ############################ # role:harbor ############################ # harbor version\uff0c\u5b8c\u6574\u7248\u672c\u53f7 HARBOR_VER: \"v2.1.3\" HARBOR_DOMAIN: \"harbor.yourdomain.com\" HARBOR_TLS_PORT: 8443 # if set 'false', you need to put certs named harbor.pem and harbor-key.pem in directory 'down' HARBOR_SELF_SIGNED_CERT: true # install extra component HARBOR_WITH_NOTARY: false HARBOR_WITH_TRIVY: false HARBOR_WITH_CLAIR: false HARBOR_WITH_CHARTMUSEUM: true 4.6\u914d\u7f6e\u542f\u52a8\u73af\u5883 \u00b6 \u4e00\u952e\u90e8\u7f72:ezctl setup k8s-01 all [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 01 4.7\u914d\u7f6eetcd \u00b6 [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 02 //\u67e5\u770betcd\u72b6\u6001 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem --endpoints=\"https://192.168.10.10:2379,https://192.168.10.11:2379\" endpoint health --write-out=table 4.8\u5b89\u88c5docker \u00b6 [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 03 4.9\u5b89\u88c5master\u8282\u70b9 \u00b6 [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 04 4.10\u5b89\u88c5node\u8282\u70b9 \u00b6 [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 05 5.\u90e8\u7f72\u7f51\u7edc\u670d\u52a1 \u00b6 1.\u90e8\u7f72calico \u00b6 [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 06 2.\u90e8\u7f72coredns \u00b6 # __MACHINE_GENERATED_WARNING__ apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - \"\" resources: - nodes verbs: - get - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: securityContext: seccompProfile: type: RuntimeDefault priorityClassName: system-cluster-critical serviceAccountName: coredns affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: [\"kube-dns\"] topologyKey: kubernetes.io/hostname tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" nodeSelector: kubernetes.io/os: linux containers: - name: coredns image: coredns/coredns:1.8.4 imagePullPolicy: IfNotPresent resources: limits: memory: 256Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.10.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP 3.\u90e8\u7f72ingress \u00b6 apiVersion: v1 kind: Namespace metadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx automountServiceAccountToken: true --- # Source: ingress-nginx/templates/controller-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: --- # Source: ingress-nginx/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx rules: - apiGroups: - '' resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - '' resources: - nodes verbs: - get - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - '' resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch --- # Source: ingress-nginx/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - '' resources: - namespaces verbs: - get - apiGroups: - '' resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - '' resources: - configmaps resourceNames: - ingress-controller-leader verbs: - get - update - apiGroups: - '' resources: - configmaps verbs: - create - apiGroups: - '' resources: - events verbs: - create - patch --- # Source: ingress-nginx/templates/controller-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-service-webhook.yaml apiVersion: v1 kind: Service metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller-admission namespace: ingress-nginx spec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook appProtocol: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 kind: DaemonSet metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller revisionHistoryLimit: 10 minReadySeconds: 0 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: hostNetwork: true dnsPolicy: ClusterFirst containers: - name: controller image: registry.cn-beijing.aliyuncs.com/kole_chang/controller:v1.0.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE runAsUser: 101 allowPrivilegeEscalation: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP volumeMounts: - name: webhook-cert mountPath: /usr/local/certificates/ readOnly: true resources: requests: cpu: 100m memory: 90Mi nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- # Source: ingress-nginx/templates/controller-ingressclass.yaml # We don't support namespaced ingressClass yet # So a ClusterRole and a ClusterRoleBinding is required apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: nginx namespace: ingress-nginx spec: controller: k8s.io/ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml # before changing this value, check the required kubernetes version # https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook name: ingress-nginx-admission webhooks: - name: validate.nginx.ingress.kubernetes.io matchPolicy: Equivalent rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses failurePolicy: Fail sideEffects: None admissionReviewVersions: - v1 clientConfig: service: namespace: ingress-nginx name: ingress-nginx-controller-admission path: /networking/v1/ingresses --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - '' resources: - secrets verbs: - get - create --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-create namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-create labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: create image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000 --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-patch namespace: ingress-nginx annotations: helm.sh/hook: post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-patch labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: patch image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000 \u56db.k8s\u5b89\u88c5\u540e\u914d\u7f6e \u00b6 1.\u8bbe\u7f6emaster\u53ef\u4ee5\u8c03\u5ea6 \u00b6 [root@k8s-master2 ~]# kubectl get node NAME STATUS ROLES AGE VERSION 192.168.10.201 Ready,SchedulingDisabled master 21h v1.22.2 192.168.10.202 Ready,SchedulingDisabled master 21h v1.22.2 192.168.10.203 Ready node 21h v1.22.2 [root@k8s-master2 ~]# kubectl uncordon 192.168.10.10 192.168.10.11 node/192.168.10.201 uncordoned node/192.168.10.202 uncordoned [root@k8s-master2 ~]# kubectl get node NAME STATUS ROLES AGE VERSION 192.168.10.201 Ready master 23h v1.22.2 192.168.10.202 Ready master 23h v1.22.2 192.168.10.203 Ready node 23h v1.22.2 2.\u7ed9\u8282\u70b9\u6253\u6807\u7b7e \u00b6 //\u67e5\u770b\u6807\u7b7e kubectl get node --show-labels=true kubectl label nodes 192.168.10.10 label_name=kuboard kubectl label nodes 192.168.10.10 label_tools=tools-1 kubectl label nodes 192.168.10.11 label_name=view-1 kubectl label nodes 192.168.10.11 label_tools=tools-2 kubectl label nodes 192.168.10.12 label_name=server-1 kubectl label nodes 192.168.10.12 label_tools=tools-3 systemctl restart kube-apiserver systemctl restart kube-controller-manager systemctl restart kubelet systemctl restart kube-proxy systemctl restart kube-scheduler","title":"\u9879\u76ee\u4e00"},{"location":"basic/4-ansible-install-k8s/#ansiblekubernetes","text":"Info \u4e8c\u8fdb\u5236\u65b9\u5f0f\u662f\u5b98\u65b9\u63a8\u8350\u7684\u90e8\u7f72\u65b9\u5f0f\u7684\u4e00\u79cd\uff0c\u4f46\u662f\u7531\u4e8e\u624b\u52a8\u90e8\u7f72\u96be\u5ea6\u8f83\u5927\uff0c\u6b64\u6b21\u91c7\u7528GitHub\u7684\u5f00\u6e90\u9879\u76eekubeasz Github \u9879\u76ee\u5730\u5740: https://github.com/easzlab/kubeasz","title":"\u9879\u76ee\u4e00: Ansible\u90e8\u7f72\u4e8c\u8fdb\u5236kubernetes\u96c6\u7fa4"},{"location":"basic/4-ansible-install-k8s/#_1","text":"\u89d2\u8272 IP \u63cf\u8ff0 K8s-master1 192.168.10.10 kube-scheduler\uff0ckubelet\uff0ckube-proxy\uff0cdocker\uff0ckeepalived\uff0cetcd K8s-master2 192.168.10.11 kube-scheduler\uff0ckubelet\uff0ckube-proxy\uff0cdocker\uff0ckeepalived\uff0cetcd K8s-node1 192.168.10.12 kubelet\uff0ckube-proxy\uff0cdocker","title":"\u73af\u5883\u914d\u7f6e"},{"location":"basic/4-ansible-install-k8s/#keepalived","text":"","title":"\u4e00.\u914d\u7f6e\u9ad8\u53ef\u7528\u8d1f\u8f7d\u5747\u8861keepalived"},{"location":"basic/4-ansible-install-k8s/#1keepalived","text":"yum -y install ipvsadm keepalived haproxy","title":"1.\u5b89\u88c5keepalived"},{"location":"basic/4-ansible-install-k8s/#2master1keepalived","text":"[root@k8s-master1 ~]# cat /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state MASTER interface ens192 virtual_router_id 1 priority 100 advert_int 2 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.88 } }","title":"2.master1\u4e0a\u914d\u7f6ekeepalived"},{"location":"basic/4-ansible-install-k8s/#3master2keepalived","text":"[root@k8s-master2 ~]# cat /etc/keepalived/keepalived.conf vrrp_instance VI_1 { state BACKUP interface ens192 virtual_router_id 1 priority 80 advert_int 2 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.10.88 } }","title":"3.master2\u4e0a\u914d\u7f6ekeepalived"},{"location":"basic/4-ansible-install-k8s/#4keepalived","text":"[root@k8s-master1 ~]# systemctl restart keepalived [root@k8s-master1 ~]# systemctl enable keepalived","title":"4.\u91cd\u542fkeepalived"},{"location":"basic/4-ansible-install-k8s/#haproxy","text":"","title":"\u4e8c.\u914d\u7f6ehaproxy"},{"location":"basic/4-ansible-install-k8s/#1master1haproxy","text":"[root@k8s-master1 ~]# vim /etc/haproxy/haproxy.cfg listen k8s_6443 mode tcp bind 192.168.10.88:6443 server 192.168.10.10 192.168.10.10:6443 check inter 5s rise 2 fall 3 server 192.168.10.11 192.168.10.11:6443 check inter 5s rise 2 fall 3","title":"1.master1\u4e0a\u914d\u7f6ehaproxy"},{"location":"basic/4-ansible-install-k8s/#2master2haproxy","text":"[root@k8s-master1 ~]# vim /etc/haproxy/haproxy.cfg listen k8s_6443 mode tcp bind 192.168.10.88:6443 server 192.168.10.10 192.168.10.10:6443 check inter 5s rise 2 fall 3 server 192.168.10.11 192.168.10.11:6443 check inter 5s rise 2 fall 3","title":"2.master2\u4e0a\u914d\u7f6ehaproxy"},{"location":"basic/4-ansible-install-k8s/#3haproxy","text":"[root@k8s-master1 ~]# systemctl restart haproxy [root@k8s-master1 ~]# systemctl enable haproxy","title":"3.\u91cd\u542fhaproxy"},{"location":"basic/4-ansible-install-k8s/#k8smaster","text":"","title":"\u4e09.\u5b89\u88c5k8s(master\u4e0a\u914d\u7f6e)"},{"location":"basic/4-ansible-install-k8s/#1ansible","text":"yum -y install ansible","title":"1.\u5b89\u88c5ansible"},{"location":"basic/4-ansible-install-k8s/#2","text":"ssh-keygen -t rsa ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.10 ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.11 ssh-copy-id -i .ssh/id_rsa.pub root@192.168.10.12","title":"2.\u914d\u7f6e\u5404\u4e2a\u8282\u70b9\u514d\u5bc6\u767b\u9646"},{"location":"basic/4-ansible-install-k8s/#3yumdocker","text":"","title":"3.yum\u5b89\u88c5docker"},{"location":"basic/4-ansible-install-k8s/#31","text":"[root@jenkins ~]# yum install -y yum-utils device-mapper-persistent-data lvm2","title":"3.1\u4e0b\u8f7d\u4f9d\u8d56\u8f6f\u4ef6"},{"location":"basic/4-ansible-install-k8s/#32docker","text":"yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo","title":"3.2\u4e0b\u8f7ddocker\u6e90"},{"location":"basic/4-ansible-install-k8s/#33docker","text":"[root@jenkins ~]# yum -y install docker-ce-19.03.15 docker-ce-cli-19.03.15 containerd.io","title":"3.3\u5b89\u88c5docker"},{"location":"basic/4-ansible-install-k8s/#34docker","text":"[root@jenkins ~]# mkdir -p /data/docker [root@jenkins ~]# vim /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --graph /data/docker -H fd:// [root@jenkins ~]# systemctl start docker [root@jenkins ~]# systemctl enable docker","title":"3.4\u4fee\u6539docker\u6839\u76ee\u5f55\u4f4d\u7f6e"},{"location":"basic/4-ansible-install-k8s/#35docker","text":"[root@jenkins docker]# cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\": [\"https://7jauxlsb.mirror.aliyuncs.com\"], \"insecure-registries\":[\"192.168.10.133\"] } EOF","title":"3.5\u914d\u7f6edocker\u955c\u50cf\u4ed3\u5e93"},{"location":"basic/4-ansible-install-k8s/#36docker","text":"[root@jenkins docker]# systemctl daemon-reload [root@jenkins docker]# systemctl restart docker","title":"3.6\u91cd\u542fdocker"},{"location":"basic/4-ansible-install-k8s/#4","text":"","title":"4.\u90e8\u7f72\u8282\u70b9\u7684\u5404\u9879\u76ee\u7ec4\u4ef6"},{"location":"basic/4-ansible-install-k8s/#41ezdown","text":"export release=3.1.1 wget https://github.com/easzlab/kubeasz/releases/download/${release}/ezdown chmod +x ./ezdown","title":"4.1\u5b89\u88c5ezdown"},{"location":"basic/4-ansible-install-k8s/#42ezdown","text":"[root@k8s-master1 ~]# vim ezdown DOCKER_VER=19.03.15 KUBEASZ_VER=3.1.1 K8S_BIN_VER=v1.22.5 [root@k8s-master1 ~]# ./ezdown -D","title":"4.2\u521d\u59cb\u5316ezdown"},{"location":"basic/4-ansible-install-k8s/#43ansible-hosts","text":"[root@k8s-master1 ~]# cd /etc/kubeasz/ [root@k8s-master1 kubeasz]# ./ezctl new k8s-cluster1","title":"4.3\u751f\u6210ansible hosts\u6587\u4ef6"},{"location":"basic/4-ansible-install-k8s/#44ansible-hosts","text":"# 'etcd' cluster should have odd member(s) (1,3,5,...) [etcd] 192.168.10.10 192.168.10.11 # master node(s) [kube_master] 192.168.10.10 192.168.10.11 # work node(s) [kube_node] 192.168.10.12 # [optional] harbor server, a private docker registry # 'NEW_INSTALL': 'true' to install a harbor server; 'false' to integrate with existed one [harbor] #192.168.1.8 NEW_INSTALL=false # [optional] loadbalance for accessing k8s from outside [ex_lb] 192.168.10.11 LB_ROLE=backup EX_APISERVER_VIP=192.168.10.88 EX_APISERVER_PORT=6443 192.168.10.10 LB_ROLE=master EX_APISERVER_VIP=192.168.10.88 EX_APISERVER_PORT=6443 # [optional] ntp server for the cluster [chrony] #192.168.1.1 [all:vars] # --------- Main Variables --------------- # Secure port for apiservers SECURE_PORT=\"6443\" # Cluster container-runtime supported: docker, containerd CONTAINER_RUNTIME=\"docker\" # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\"calico\" # Service proxy mode of kube-proxy: 'iptables' or 'ipvs' PROXY_MODE=\"ipvs\" # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\"10.10.0.0/16\" # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\"172.10.0.0/16\" # NodePort Range NODE_PORT_RANGE=\"1-65535\" # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\"cluster.local\" # -------- Additional Variables (don't change the default value right now) --- # Binaries Directory bin_dir=\"/usr/bin\" # Deploy Directory (kubeasz workspace) base_dir=\"/etc/kubeasz\" # Directory for a specific cluster cluster_dir=\"{{ base_dir }}/clusters/k8s-cluster1\" # CA and other components cert/key Directory ca_dir=\"/etc/kubernetes/ssl\"","title":"4.4\u7f16\u8f91ansible hosts\u6587\u4ef6"},{"location":"basic/4-ansible-install-k8s/#45configyml","text":"############################ # prepare ############################ # \u53ef\u9009\u79bb\u7ebf\u5b89\u88c5\u7cfb\u7edf\u8f6f\u4ef6\u5305 (offline|online) INSTALL_SOURCE: \"online\" # \u53ef\u9009\u8fdb\u884c\u7cfb\u7edf\u5b89\u5168\u52a0\u56fa github.com/dev-sec/ansible-collection-hardening OS_HARDEN: false # \u8bbe\u7f6e\u65f6\u95f4\u6e90\u670d\u52a1\u5668\u3010\u91cd\u8981\uff1a\u96c6\u7fa4\u5185\u673a\u5668\u65f6\u95f4\u5fc5\u987b\u540c\u6b65\u3011 ntp_servers: - \"ntp1.aliyun.com\" - \"time1.cloud.tencent.com\" - \"0.cn.pool.ntp.org\" # \u8bbe\u7f6e\u5141\u8bb8\u5185\u90e8\u65f6\u95f4\u540c\u6b65\u7684\u7f51\u7edc\u6bb5\uff0c\u6bd4\u5982\"10.0.0.0/8\"\uff0c\u9ed8\u8ba4\u5168\u90e8\u5141\u8bb8 local_network: \"0.0.0.0/0\" ############################ # role:deploy ############################ # default: ca will expire in 100 years # default: certs issued by the ca will expire in 50 years CA_EXPIRY: \"876000h\" CERT_EXPIRY: \"438000h\" # kubeconfig \u914d\u7f6e\u53c2\u6570 CLUSTER_NAME: \"cluster1\" CONTEXT_NAME: \"context-{{ CLUSTER_NAME }}\" ############################ # role:etcd ############################ # \u8bbe\u7f6e\u4e0d\u540c\u7684wal\u76ee\u5f55\uff0c\u53ef\u4ee5\u907f\u514d\u78c1\u76d8io\u7ade\u4e89\uff0c\u63d0\u9ad8\u6027\u80fd ETCD_DATA_DIR: \"/var/lib/etcd\" ETCD_WAL_DIR: \"\" ############################ # role:runtime [containerd,docker] ############################ # ------------------------------------------- containerd # [.]\u542f\u7528\u5bb9\u5668\u4ed3\u5e93\u955c\u50cf ENABLE_MIRROR_REGISTRY: true # [containerd]\u57fa\u7840\u5bb9\u5668\u955c\u50cf SANDBOX_IMAGE: \"easzlab/pause-amd64:3.5\" # [containerd]\u5bb9\u5668\u6301\u4e45\u5316\u5b58\u50a8\u76ee\u5f55 CONTAINERD_STORAGE_DIR: \"/var/lib/containerd\" # ------------------------------------------- docker # [docker]\u5bb9\u5668\u5b58\u50a8\u76ee\u5f55 DOCKER_STORAGE_DIR: \"/data/docker\" # [docker]\u5f00\u542fRestful API ENABLE_REMOTE_API: false # [docker]\u4fe1\u4efb\u7684HTTP\u4ed3\u5e93 INSECURE_REG: '[\"127.0.0.1/8\",\"192.168.10.133\"]' ############################ # role:kube-master ############################ # k8s \u96c6\u7fa4 master \u8282\u70b9\u8bc1\u4e66\u914d\u7f6e\uff0c\u53ef\u4ee5\u6dfb\u52a0\u591a\u4e2aip\u548c\u57df\u540d\uff08\u6bd4\u5982\u589e\u52a0\u516c\u7f51ip\u548c\u57df\u540d\uff09 MASTER_CERT_HOSTS: - \"10.1.1.1\" - \"k8s.test.io\" #- \"www.test.com\" # node \u8282\u70b9\u4e0a pod \u7f51\u6bb5\u63a9\u7801\u957f\u5ea6\uff08\u51b3\u5b9a\u6bcf\u4e2a\u8282\u70b9\u6700\u591a\u80fd\u5206\u914d\u7684pod ip\u5730\u5740\uff09 # \u5982\u679cflannel \u4f7f\u7528 --kube-subnet-mgr \u53c2\u6570\uff0c\u90a3\u4e48\u5b83\u5c06\u8bfb\u53d6\u8be5\u8bbe\u7f6e\u4e3a\u6bcf\u4e2a\u8282\u70b9\u5206\u914dpod\u7f51\u6bb5 # https://github.com/coreos/flannel/issues/847 NODE_CIDR_LEN: 24 ############################ # role:kube-node ############################ # Kubelet \u6839\u76ee\u5f55 KUBELET_ROOT_DIR: \"/var/lib/kubelet\" # node\u8282\u70b9\u6700\u5927pod \u6570 MAX_PODS: 300 # \u914d\u7f6e\u4e3akube\u7ec4\u4ef6\uff08kubelet,kube-proxy,dockerd\u7b49\uff09\u9884\u7559\u7684\u8d44\u6e90\u91cf # \u6570\u503c\u8bbe\u7f6e\u8be6\u89c1templates/kubelet-config.yaml.j2 KUBE_RESERVED_ENABLED: \"no\" # k8s \u5b98\u65b9\u4e0d\u5efa\u8bae\u8349\u7387\u5f00\u542f system-reserved, \u9664\u975e\u4f60\u57fa\u4e8e\u957f\u671f\u76d1\u63a7\uff0c\u4e86\u89e3\u7cfb\u7edf\u7684\u8d44\u6e90\u5360\u7528\u72b6\u51b5\uff1b # \u5e76\u4e14\u968f\u7740\u7cfb\u7edf\u8fd0\u884c\u65f6\u95f4\uff0c\u9700\u8981\u9002\u5f53\u589e\u52a0\u8d44\u6e90\u9884\u7559\uff0c\u6570\u503c\u8bbe\u7f6e\u8be6\u89c1templates/kubelet-config.yaml.j2 # \u7cfb\u7edf\u9884\u7559\u8bbe\u7f6e\u57fa\u4e8e 4c/8g \u865a\u673a\uff0c\u6700\u5c0f\u5316\u5b89\u88c5\u7cfb\u7edf\u670d\u52a1\uff0c\u5982\u679c\u4f7f\u7528\u9ad8\u6027\u80fd\u7269\u7406\u673a\u53ef\u4ee5\u9002\u5f53\u589e\u52a0\u9884\u7559 # \u53e6\u5916\uff0c\u96c6\u7fa4\u5b89\u88c5\u65f6\u5019apiserver\u7b49\u8d44\u6e90\u5360\u7528\u4f1a\u77ed\u65f6\u8f83\u5927\uff0c\u5efa\u8bae\u81f3\u5c11\u9884\u75591g\u5185\u5b58 SYS_RESERVED_ENABLED: \"no\" # haproxy balance mode BALANCE_ALG: \"roundrobin\" ############################ # role:network [flannel,calico,cilium,kube-ovn,kube-router] ############################ # ------------------------------------------- flannel # [flannel]\u8bbe\u7f6eflannel \u540e\u7aef\"host-gw\",\"vxlan\"\u7b49 FLANNEL_BACKEND: \"vxlan\" DIRECT_ROUTING: false # [flannel] flanneld_image: \"quay.io/coreos/flannel:v0.10.0-amd64\" flannelVer: \"v0.13.0-amd64\" flanneld_image: \"easzlab/flannel:{{ flannelVer }}\" # [flannel]\u79bb\u7ebf\u955c\u50cftar\u5305 flannel_offline: \"flannel_{{ flannelVer }}.tar\" # ------------------------------------------- calico # [calico]\u8bbe\u7f6e CALICO_IPV4POOL_IPIP=\u201coff\u201d,\u53ef\u4ee5\u63d0\u9ad8\u7f51\u7edc\u6027\u80fd\uff0c\u6761\u4ef6\u9650\u5236\u8be6\u89c1 docs/setup/calico.md CALICO_IPV4POOL_IPIP: \"Always\" # [calico]\u8bbe\u7f6e calico-node\u4f7f\u7528\u7684host IP\uff0cbgp\u90bb\u5c45\u901a\u8fc7\u8be5\u5730\u5740\u5efa\u7acb\uff0c\u53ef\u624b\u5de5\u6307\u5b9a\u4e5f\u53ef\u4ee5\u81ea\u52a8\u53d1\u73b0 IP_AUTODETECTION_METHOD: \"can-reach={{ groups['kube_master'][0] }}\" # [calico]\u8bbe\u7f6ecalico \u7f51\u7edc backend: brid, vxlan, none CALICO_NETWORKING_BACKEND: \"brid\" # [calico]\u66f4\u65b0\u652f\u6301calico \u7248\u672c: [v3.3.x] [v3.4.x] [v3.8.x] [v3.15.x] calico_ver: \"v3.19.2\" # [calico]calico \u4e3b\u7248\u672c calico_ver_main: \"{{ calico_ver.split('.')[0] }}.{{ calico_ver.split('.')[1] }}\" # [calico]\u79bb\u7ebf\u955c\u50cftar\u5305 calico_offline: \"calico_{{ calico_ver }}.tar\" # ------------------------------------------- cilium # [cilium]CILIUM_ETCD_OPERATOR \u521b\u5efa\u7684 etcd \u96c6\u7fa4\u8282\u70b9\u6570 1,3,5,7... ETCD_CLUSTER_SIZE: 1 # [cilium]\u955c\u50cf\u7248\u672c cilium_ver: \"v1.4.1\" # [cilium]\u79bb\u7ebf\u955c\u50cftar\u5305 cilium_offline: \"cilium_{{ cilium_ver }}.tar\" # ------------------------------------------- kube-ovn # [kube-ovn]\u9009\u62e9 OVN DB and OVN Control Plane \u8282\u70b9\uff0c\u9ed8\u8ba4\u4e3a\u7b2c\u4e00\u4e2amaster\u8282\u70b9 OVN_DB_NODE: \"{{ groups['kube_master'][0] }}\" # [kube-ovn]\u79bb\u7ebf\u955c\u50cftar\u5305 kube_ovn_ver: \"v1.5.3\" kube_ovn_offline: \"kube_ovn_{{ kube_ovn_ver }}.tar\" # ------------------------------------------- kube-router # [kube-router]\u516c\u6709\u4e91\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u4e00\u822c\u9700\u8981\u59cb\u7ec8\u5f00\u542f ipinip\uff1b\u81ea\u6709\u73af\u5883\u53ef\u4ee5\u8bbe\u7f6e\u4e3a \"subnet\" OVERLAY_TYPE: \"full\" # [kube-router]NetworkPolicy \u652f\u6301\u5f00\u5173 FIREWALL_ENABLE: \"true\" # [kube-router]kube-router \u955c\u50cf\u7248\u672c kube_router_ver: \"v0.3.1\" busybox_ver: \"1.28.4\" # [kube-router]kube-router \u79bb\u7ebf\u955c\u50cftar\u5305 kuberouter_offline: \"kube-router_{{ kube_router_ver }}.tar\" busybox_offline: \"busybox_{{ busybox_ver }}.tar\" ############################ # role:cluster-addon ############################ # coredns \u81ea\u52a8\u5b89\u88c5 dns_install: \"no\" corednsVer: \"1.8.4\" ENABLE_LOCAL_DNS_CACHE: true dnsNodeCacheVer: \"1.17.0\" # \u8bbe\u7f6e local dns cache \u5730\u5740 LOCAL_DNS_CACHE: \"10.10.0.2\" # metric server \u81ea\u52a8\u5b89\u88c5 metricsserver_install: \"no\" metricsVer: \"v0.5.0\" # dashboard \u81ea\u52a8\u5b89\u88c5 dashboard_install: \"no\" dashboardVer: \"v2.3.1\" dashboardMetricsScraperVer: \"v1.0.6\" # ingress \u81ea\u52a8\u5b89\u88c5 ingress_install: \"no\" ingress_backend: \"traefik\" traefik_chart_ver: \"9.12.3\" # prometheus \u81ea\u52a8\u5b89\u88c5 prom_install: \"no\" prom_namespace: \"monitor\" prom_chart_ver: \"12.10.6\" # nfs-provisioner \u81ea\u52a8\u5b89\u88c5 nfs_provisioner_install: \"no\" nfs_provisioner_namespace: \"kube-system\" nfs_provisioner_ver: \"v4.0.1\" nfs_storage_class: \"managed-nfs-storage\" nfs_server: \"192.168.1.10\" nfs_path: \"/data/nfs\" ############################ # role:harbor ############################ # harbor version\uff0c\u5b8c\u6574\u7248\u672c\u53f7 HARBOR_VER: \"v2.1.3\" HARBOR_DOMAIN: \"harbor.yourdomain.com\" HARBOR_TLS_PORT: 8443 # if set 'false', you need to put certs named harbor.pem and harbor-key.pem in directory 'down' HARBOR_SELF_SIGNED_CERT: true # install extra component HARBOR_WITH_NOTARY: false HARBOR_WITH_TRIVY: false HARBOR_WITH_CLAIR: false HARBOR_WITH_CHARTMUSEUM: true","title":"4.5\u7f16\u8f91config.yml\u6587\u4ef6"},{"location":"basic/4-ansible-install-k8s/#46","text":"\u4e00\u952e\u90e8\u7f72:ezctl setup k8s-01 all [root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 01","title":"4.6\u914d\u7f6e\u542f\u52a8\u73af\u5883"},{"location":"basic/4-ansible-install-k8s/#47etcd","text":"[root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 02 //\u67e5\u770betcd\u72b6\u6001 etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/etcd.pem --key=/etc/kubernetes/ssl/etcd-key.pem --endpoints=\"https://192.168.10.10:2379,https://192.168.10.11:2379\" endpoint health --write-out=table","title":"4.7\u914d\u7f6eetcd"},{"location":"basic/4-ansible-install-k8s/#48docker","text":"[root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 03","title":"4.8\u5b89\u88c5docker"},{"location":"basic/4-ansible-install-k8s/#49master","text":"[root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 04","title":"4.9\u5b89\u88c5master\u8282\u70b9"},{"location":"basic/4-ansible-install-k8s/#410node","text":"[root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 05","title":"4.10\u5b89\u88c5node\u8282\u70b9"},{"location":"basic/4-ansible-install-k8s/#5","text":"","title":"5.\u90e8\u7f72\u7f51\u7edc\u670d\u52a1"},{"location":"basic/4-ansible-install-k8s/#1calico","text":"[root@k8s-master1 kubeasz]# ./ezctl setup k8s-cluster1 06","title":"1.\u90e8\u7f72calico"},{"location":"basic/4-ansible-install-k8s/#2coredns","text":"# __MACHINE_GENERATED_WARNING__ apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch - apiGroups: - \"\" resources: - nodes verbs: - get - apiGroups: - discovery.k8s.io resources: - endpointslices verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system labels: addonmanager.kubernetes.io/mode: EnsureExists data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 } prometheus :9153 forward . /etc/resolv.conf { max_concurrent 1000 } cache 30 loop reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: # replicas: not specified here: # 1. In order to make Addon Manager do not reconcile this replicas parameter. # 2. Default is 1. # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: securityContext: seccompProfile: type: RuntimeDefault priorityClassName: system-cluster-critical serviceAccountName: coredns affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: [\"kube-dns\"] topologyKey: kubernetes.io/hostname tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" nodeSelector: kubernetes.io/os: linux containers: - name: coredns image: coredns/coredns:1.8.4 imagePullPolicy: IfNotPresent resources: limits: memory: 256Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.10.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP","title":"2.\u90e8\u7f72coredns"},{"location":"basic/4-ansible-install-k8s/#3ingress","text":"apiVersion: v1 kind: Namespace metadata: name: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx --- # Source: ingress-nginx/templates/controller-serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx automountServiceAccountToken: true --- # Source: ingress-nginx/templates/controller-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx data: --- # Source: ingress-nginx/templates/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx rules: - apiGroups: - '' resources: - configmaps - endpoints - nodes - pods - secrets verbs: - list - watch - apiGroups: - '' resources: - nodes verbs: - get - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - '' resources: - events verbs: - create - patch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch --- # Source: ingress-nginx/templates/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm name: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx rules: - apiGroups: - '' resources: - namespaces verbs: - get - apiGroups: - '' resources: - configmaps - pods - secrets - endpoints verbs: - get - list - watch - apiGroups: - '' resources: - services verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses verbs: - get - list - watch - apiGroups: - networking.k8s.io resources: - ingresses/status verbs: - update - apiGroups: - networking.k8s.io resources: - ingressclasses verbs: - get - list - watch - apiGroups: - '' resources: - configmaps resourceNames: - ingress-controller-leader verbs: - get - update - apiGroups: - '' resources: - configmaps verbs: - create - apiGroups: - '' resources: - events verbs: - create - patch --- # Source: ingress-nginx/templates/controller-rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx namespace: ingress-nginx roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx subjects: - kind: ServiceAccount name: ingress-nginx namespace: ingress-nginx --- # Source: ingress-nginx/templates/controller-service-webhook.yaml apiVersion: v1 kind: Service metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller-admission namespace: ingress-nginx spec: type: ClusterIP ports: - name: https-webhook port: 443 targetPort: webhook appProtocol: https selector: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller --- # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 kind: DaemonSet metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: ingress-nginx-controller namespace: ingress-nginx spec: selector: matchLabels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller revisionHistoryLimit: 10 minReadySeconds: 0 template: metadata: labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller spec: hostNetwork: true dnsPolicy: ClusterFirst containers: - name: controller image: registry.cn-beijing.aliyuncs.com/kole_chang/controller:v1.0.0 imagePullPolicy: IfNotPresent lifecycle: preStop: exec: command: - /wait-shutdown args: - /nginx-ingress-controller - --election-id=ingress-controller-leader - --controller-class=k8s.io/ingress-nginx - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller - --validating-webhook=:8443 - --validating-webhook-certificate=/usr/local/certificates/cert - --validating-webhook-key=/usr/local/certificates/key - --watch-ingress-without-class=true securityContext: capabilities: drop: - ALL add: - NET_BIND_SERVICE runAsUser: 101 allowPrivilegeEscalation: true env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: LD_PRELOAD value: /usr/local/lib/libmimalloc.so livenessProbe: failureThreshold: 5 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 readinessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 periodSeconds: 10 successThreshold: 1 timeoutSeconds: 1 ports: - name: http containerPort: 80 protocol: TCP - name: https containerPort: 443 protocol: TCP - name: webhook containerPort: 8443 protocol: TCP volumeMounts: - name: webhook-cert mountPath: /usr/local/certificates/ readOnly: true resources: requests: cpu: 100m memory: 90Mi nodeSelector: kubernetes.io/os: linux serviceAccountName: ingress-nginx terminationGracePeriodSeconds: 300 volumes: - name: webhook-cert secret: secretName: ingress-nginx-admission --- # Source: ingress-nginx/templates/controller-ingressclass.yaml # We don't support namespaced ingressClass yet # So a ClusterRole and a ClusterRoleBinding is required apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: controller name: nginx namespace: ingress-nginx spec: controller: k8s.io/ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/validating-webhook.yaml # before changing this value, check the required kubernetes version # https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites apiVersion: admissionregistration.k8s.io/v1 kind: ValidatingWebhookConfiguration metadata: labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook name: ingress-nginx-admission webhooks: - name: validate.nginx.ingress.kubernetes.io matchPolicy: Equivalent rules: - apiGroups: - networking.k8s.io apiVersions: - v1 operations: - CREATE - UPDATE resources: - ingresses failurePolicy: Fail sideEffects: None admissionReviewVersions: - v1 clientConfig: service: namespace: ingress-nginx name: ingress-nginx-controller-admission path: /networking/v1/ingresses --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - admissionregistration.k8s.io resources: - validatingwebhookconfigurations verbs: - get - update --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/clusterrolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: ingress-nginx-admission annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook rules: - apiGroups: - '' resources: - secrets verbs: - get - create --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: ingress-nginx-admission namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade,post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: ingress-nginx-admission subjects: - kind: ServiceAccount name: ingress-nginx-admission namespace: ingress-nginx --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-createSecret.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-create namespace: ingress-nginx annotations: helm.sh/hook: pre-install,pre-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-create labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: create image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - create - --host=ingress-nginx-controller-admission,ingress-nginx-controller-admission.$(POD_NAMESPACE).svc - --namespace=$(POD_NAMESPACE) - --secret-name=ingress-nginx-admission env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000 --- # Source: ingress-nginx/templates/admission-webhooks/job-patch/job-patchWebhook.yaml apiVersion: batch/v1 kind: Job metadata: name: ingress-nginx-admission-patch namespace: ingress-nginx annotations: helm.sh/hook: post-install,post-upgrade helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: template: metadata: name: ingress-nginx-admission-patch labels: helm.sh/chart: ingress-nginx-4.0.1 app.kubernetes.io/name: ingress-nginx app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/version: 1.0.0 app.kubernetes.io/managed-by: Helm app.kubernetes.io/component: admission-webhook spec: containers: - name: patch image: registry.cn-beijing.aliyuncs.com/kole_chang/kube-webhook-certgen:v1.0 imagePullPolicy: IfNotPresent args: - patch - --webhook-name=ingress-nginx-admission - --namespace=$(POD_NAMESPACE) - --patch-mutating=false - --secret-name=ingress-nginx-admission - --patch-failure-policy=Fail env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure serviceAccountName: ingress-nginx-admission nodeSelector: kubernetes.io/os: linux securityContext: runAsNonRoot: true runAsUser: 2000","title":"3.\u90e8\u7f72ingress"},{"location":"basic/4-ansible-install-k8s/#k8s","text":"","title":"\u56db.k8s\u5b89\u88c5\u540e\u914d\u7f6e"},{"location":"basic/4-ansible-install-k8s/#1master","text":"[root@k8s-master2 ~]# kubectl get node NAME STATUS ROLES AGE VERSION 192.168.10.201 Ready,SchedulingDisabled master 21h v1.22.2 192.168.10.202 Ready,SchedulingDisabled master 21h v1.22.2 192.168.10.203 Ready node 21h v1.22.2 [root@k8s-master2 ~]# kubectl uncordon 192.168.10.10 192.168.10.11 node/192.168.10.201 uncordoned node/192.168.10.202 uncordoned [root@k8s-master2 ~]# kubectl get node NAME STATUS ROLES AGE VERSION 192.168.10.201 Ready master 23h v1.22.2 192.168.10.202 Ready master 23h v1.22.2 192.168.10.203 Ready node 23h v1.22.2","title":"1.\u8bbe\u7f6emaster\u53ef\u4ee5\u8c03\u5ea6"},{"location":"basic/4-ansible-install-k8s/#2_1","text":"//\u67e5\u770b\u6807\u7b7e kubectl get node --show-labels=true kubectl label nodes 192.168.10.10 label_name=kuboard kubectl label nodes 192.168.10.10 label_tools=tools-1 kubectl label nodes 192.168.10.11 label_name=view-1 kubectl label nodes 192.168.10.11 label_tools=tools-2 kubectl label nodes 192.168.10.12 label_name=server-1 kubectl label nodes 192.168.10.12 label_tools=tools-3 systemctl restart kube-apiserver systemctl restart kube-controller-manager systemctl restart kubelet systemctl restart kube-proxy systemctl restart kube-scheduler","title":"2.\u7ed9\u8282\u70b9\u6253\u6807\u7b7e"},{"location":"basic/5-kubernetes-UI-docs/","text":"Kuboard \u4ecb\u7ecd \u00b6 Kuboard \u662f\u4e00\u6b3e\u514d\u8d39\u7684 Kubernetes \u7ba1\u7406\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\uff0c\u7ed3\u5408\u5df2\u6709\u6216\u65b0\u5efa\u7684\u4ee3\u7801\u4ed3\u5e93\u3001\u955c\u50cf\u4ed3\u5e93\u3001CI/CD\u5de5\u5177\u7b49\uff0c\u53ef\u4ee5\u4fbf\u6377\u7684\u642d\u5efa\u4e00\u4e2a\u751f\u4ea7\u53ef\u7528\u7684 Kubernetes \u5bb9\u5668\u4e91\u5e73\u53f0\uff0c\u8f7b\u677e\u7ba1\u7406\u548c\u8fd0\u884c\u4e91\u539f\u751f\u5e94\u7528\u3002\u60a8\u4e5f\u53ef\u4ee5\u76f4\u63a5\u5c06 Kuboard \u5b89\u88c5\u5230\u73b0\u6709\u7684 Kubernetes \u96c6\u7fa4\uff0c\u901a\u8fc7 Kuboard \u63d0\u4f9b\u7684 Kubernetes RBAC \u7ba1\u7406\u754c\u9762\uff0c\u5c06 Kubernetes \u63d0\u4f9b\u7684\u80fd\u529b\u5f00\u653e\u7ed9\u60a8\u7684\u5f00\u53d1/\u6d4b\u8bd5\u56e2\u961f\u3002Kuboard \u63d0\u4f9b\u7684\u529f\u80fd\u6709\uff1a \u517c\u5bb9\u6027 \u00b6 \u5b89\u88c5 \u00b6 kubectl apply -f https://addons.kuboard.cn/kuboard/kuboard-v3.yaml \u6e29\u99a8\u63d0\u793a \u5b89\u88c5\u7684\u65f6\u5019\u6211\u9047\u5230\u670d\u52a1\u8dd1\u4e0d\u8d77\u6765\u53ef\u4ee5\u53c2\u8003\u5b98\u7f51 https://kuboard.cn/install/v3/install-in-k8s.html#%E5%AE%89%E8%A3%85-2\u6765\u8c03\u8bd5 \u51fa\u73b0\u5982\u56fe\uff0c\u4e24\u4e2a\u670d\u52a1\u90fdRunning\uff0c\u90a3\u4e48\u5c31\u90e8\u7f72\u6210\u529f\u4e86\uff01 \u8bbf\u95ee Kuboard \u00b6 \u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u94fe\u63a5 http://your-node-ip-address:30080 \u8f93\u5165\u521d\u59cb\u7528\u6237\u540d\u548c\u5bc6\u7801\uff0c\u5e76\u767b\u5f55 \u7528\u6237\u540d\uff1a admin \u5bc6\u7801\uff1a Kuboard123 \u6587\u7ae0\u53c2\u8003: https://kuboard.cn/install/v3/install-in-k8s.html#%E5%AE%89%E8%A3%85","title":"Kubernetes UI"},{"location":"basic/5-kubernetes-UI-docs/#kuboard","text":"Kuboard \u662f\u4e00\u6b3e\u514d\u8d39\u7684 Kubernetes \u7ba1\u7406\u5de5\u5177\uff0c\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u529f\u80fd\uff0c\u7ed3\u5408\u5df2\u6709\u6216\u65b0\u5efa\u7684\u4ee3\u7801\u4ed3\u5e93\u3001\u955c\u50cf\u4ed3\u5e93\u3001CI/CD\u5de5\u5177\u7b49\uff0c\u53ef\u4ee5\u4fbf\u6377\u7684\u642d\u5efa\u4e00\u4e2a\u751f\u4ea7\u53ef\u7528\u7684 Kubernetes \u5bb9\u5668\u4e91\u5e73\u53f0\uff0c\u8f7b\u677e\u7ba1\u7406\u548c\u8fd0\u884c\u4e91\u539f\u751f\u5e94\u7528\u3002\u60a8\u4e5f\u53ef\u4ee5\u76f4\u63a5\u5c06 Kuboard \u5b89\u88c5\u5230\u73b0\u6709\u7684 Kubernetes \u96c6\u7fa4\uff0c\u901a\u8fc7 Kuboard \u63d0\u4f9b\u7684 Kubernetes RBAC \u7ba1\u7406\u754c\u9762\uff0c\u5c06 Kubernetes \u63d0\u4f9b\u7684\u80fd\u529b\u5f00\u653e\u7ed9\u60a8\u7684\u5f00\u53d1/\u6d4b\u8bd5\u56e2\u961f\u3002Kuboard \u63d0\u4f9b\u7684\u529f\u80fd\u6709\uff1a","title":"Kuboard \u4ecb\u7ecd"},{"location":"basic/5-kubernetes-UI-docs/#_1","text":"","title":"\u517c\u5bb9\u6027"},{"location":"basic/5-kubernetes-UI-docs/#_2","text":"kubectl apply -f https://addons.kuboard.cn/kuboard/kuboard-v3.yaml \u6e29\u99a8\u63d0\u793a \u5b89\u88c5\u7684\u65f6\u5019\u6211\u9047\u5230\u670d\u52a1\u8dd1\u4e0d\u8d77\u6765\u53ef\u4ee5\u53c2\u8003\u5b98\u7f51 https://kuboard.cn/install/v3/install-in-k8s.html#%E5%AE%89%E8%A3%85-2\u6765\u8c03\u8bd5 \u51fa\u73b0\u5982\u56fe\uff0c\u4e24\u4e2a\u670d\u52a1\u90fdRunning\uff0c\u90a3\u4e48\u5c31\u90e8\u7f72\u6210\u529f\u4e86\uff01","title":"\u5b89\u88c5"},{"location":"basic/5-kubernetes-UI-docs/#kuboard_1","text":"\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u94fe\u63a5 http://your-node-ip-address:30080 \u8f93\u5165\u521d\u59cb\u7528\u6237\u540d\u548c\u5bc6\u7801\uff0c\u5e76\u767b\u5f55 \u7528\u6237\u540d\uff1a admin \u5bc6\u7801\uff1a Kuboard123 \u6587\u7ae0\u53c2\u8003: https://kuboard.cn/install/v3/install-in-k8s.html#%E5%AE%89%E8%A3%85","title":"\u8bbf\u95ee Kuboard"},{"location":"basic/6-kubernetes-etcd-docs/","text":"\u5b98\u7f51: https://etcd.io/ Github\u5730\u5740: https://github.com/etcd-io/etcd ETCD \u7279\u6027: \u00b6 \u5b8c\u5168\u590d\u5236: \u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ef\u4ee5\u4f7f\u7528\u5b8c\u6574\u7684\u5b58\u6863 \u9ad8\u53ef\u7528\u6027: ETCD\u53ef\u4ee5\u907f\u514d\u786c\u4ef6\u7684\u5355\u70b9\u6545\u969c\u548c\u7f51\u7edc\u95ee\u9898 \u4e00\u81f4\u6027: \u6bcf\u6b21\u5199\u5165\u90fd\u4f1a\u8fd4\u56de\u8de8\u591a\u4e3b\u673a\u7684\u6700\u65b0\u5199\u5165 \u7b80\u5355: \u5305\u62ec\u4e00\u4e2a\u5b9a\u4e49\u826f\u597d,\u9762\u5411\u7528\u6237\u7684API \u5b89\u5168: \u5b9e\u73b0\u4e86\u5e26\u6709\u53ef\u9009\u7684\u5ba2\u6237\u7aef\u8bc1\u4e66\u8eab\u4efd\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316TLS \u5feb\u901f: \u6bcf\u79d2\u4e00\u4e07\u6b21\u5199\u5165\u7684\u57fa\u51c6\u901f\u5ea6 \u53ef\u9760: \u4f7f\u7528Raft\u7b97\u6cd5\u5b9e\u73b0\u5b58\u50a8\u7684\u5408\u7406\u5206\u5e03\u5728etcd\u7684\u5de5\u4f5c\u539f\u7406 \u786c\u4ef6\u63a8\u8350 \u00b6 ETCD \u4e3b\u8981\u8003\u8651\u5185\u5b58\uff0cCPU\uff0c\u78c1\u76d8 \u5b98\u7f51\u786c\u4ef6(\u63a8\u8350): https://etcd.io/docs/v3.5/op-guide/hardware/ \u6e29\u99a8\u63d0\u793a \u5b98\u65b9\u63d0\u4f9b\u7684\u786c\u4ef6\u6709\u4e9b\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u6b63\u5f0f\u7684\u751f\u4ea7\u73af\u5883\u6700\u597d\u9ad8\u4e8e\u5b98\u65b9\u63a8\u8350\u76841.5-2\u500d\u6bd4\u8f83\u5408\u7406 ETCD \u5ba2\u6237\u7aef\u4f7f\u7528 \u00b6 root@m1-pre:~# etcdctl member list 4469cb53324fe68b, started, etcd-192.168.1.102, https://192.168.1.102:2380, https://192.168.1.102:2379, false 9f5e0acc1f346641, started, etcd-192.168.1.101, https://192.168.1.101:2380, https://192.168.1.101:2379, false e519401c4b995768, started, etcd-192.168.1.103, https://192.168.1.103:2380, https://192.168.1.103:2379, false \u9a8c\u8bc1\u5f53\u524d\u6240\u6709ETCD\u6210\u5458\u72b6\u6001 root@m1-pre:~# export NODE_IPS = \"192.168.1.101 192.168.1.102 192.168.1.103\" root@m1-pre:~# for ip in ${ NODE_IPS } ; do ETCDCTL_API = 3 /usr/bin/etcdctl --endpoints = https:// ${ ip } :2379 --cacert = /etc/kubernetes/ssl/ca.pem --cert = /etc/kubernetes/ssl/etcd.pem --key = /etc/kubernetes/ssl/etcd-key.pem endpoint health ; done https://192.168.1.101:2379 is healthy: successfully committed proposal: took = 17 .871745ms https://192.168.1.102:2379 is healthy: successfully committed proposal: took = 12 .499902ms https://192.168.1.103:2379 is healthy: successfully committed proposal: took = 10 .525834ms \u663e\u793a\u8be6\u7ec6\u4fe1\u606f root@m1-pre:~# for ip in ${ NODE_IPS } ; do ETCDCTL_API = 3 /usr/bin/etcdctl --write-out = table endpoint status --endpoints = https:// ${ ip } :2379 --cacert = /etc/kubernetes/ssl/ca.pem --cert = /etc/kubernetes/ssl/etcd.pem --key = /etc/kubernetes/ssl/etcd-key.pem ; done +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.101:2379 | 9f5e0acc1f346641 | 3 .4.13 | 2 .5 MB | true | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.102:2379 | 4469cb53324fe68b | 3 .4.13 | 2 .5 MB | false | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.103:2379 | e519401c4b995768 | 3 .4.13 | 2 .5 MB | false | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ \u67e5\u770bETCD\u6570\u636e $ etcdctl get / --prefix --keys-only $ etcdctl get /registry/pods/default/test $ etcdctl get / --prefix --keys-only | grep namespace $ etcdctl get / --prefix --keys-only | grep calico $ etcdctl get / --prefix --keys-only | grep deployment $ ETCDCTL_API = 3 etcdctl put /name \"linux60\" //\u589e $ ETCDCTL_API = 3 etcdctl get /name //\u67e5 $ ETCDCTL_API = 3 etcdctl del /name //\u5220 $ ETCDCTL_API = 3 etcdctl watch /data //\u65e0key\u4e5f\u53ef\u4ee5\u8fdb\u884cwatch watch \u5982\u540c\u6240\u793a: ETCD\u5907\u4efd\u6062\u590d \u00b6 WAL\u987e\u540d\u601d\u4e49\uff0c\u5728\u771f\u6b63\u6267\u884c\u5199\u64cd\u4f5c\u4e4b\u524d\u5148\u5199\u4e00\u4e2a\u65e5\u5fd7\uff0c\u9884\u5199\u65e5\u5fd7 WAL \u5b58\u653e\u4e86\u9884\u5199\u65e5\u5fd7\uff0c\u6700\u5927\u7684\u4f5c\u7528\u662f\u8bb0\u5f55\u4e86\u6574\u4e2a\u6570\u636e\u53d8\u5316\u7684\u7684\u5168\u90e8\u5386\u7a0b\u3002\u5728etcd\u4e2d\uff0c\u6240\u6709\u6570\u636e\u7684\u4fee\u6539\u5728\u63d0\u4ea4\u524d\u90fd\u8981\u5148\u5199\u5165WAL\u4e2d V3 \u7248\u672c\u5907\u4efd\u6570\u636e ETCDCTL_API = 3 etcdctl snapshot save <filename> \u81ea\u52a8\u5907\u4efd\u6570\u636e $ mkdir /data/etcd-backup $ cat etcd-backup.sh #!/bin/bash source /etc/profile DATE = ` date +%Y%m%d-%H%M ` ETCDCTL_API = 3 /usr/bin/etcdctl snapshot save /data/etcd-backup/etcd-snapshot- ${ DATE } .db ETCDFILE = ` find /data/etcd-backup -mtime +30 -name etcd-* | wc -l ` if [ ${ ETCDFILE } -gt 30 ] ; then find /data/etcd-backup -mtime +30 -name etcd-* -exec rm -f {} \\; fi V3 \u7248\u672c\u6062\u590d\u6570\u636e ETCDCTL_API = 3 etcdctl snapshot restore /data/etcd/etcd.db --data-dir = /opt/data // \u6062\u590d\u7684\u76ee\u5f55\u5fc5\u987b\u4e0d\u5b58\u5728\uff0c\u5426\u5219\u4f1a\u62a5\u9519","title":"ETCD \u7b80\u4ecb\u4e0e\u4f7f\u7528"},{"location":"basic/6-kubernetes-etcd-docs/#etcd","text":"\u5b8c\u5168\u590d\u5236: \u96c6\u7fa4\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u90fd\u53ef\u4ee5\u4f7f\u7528\u5b8c\u6574\u7684\u5b58\u6863 \u9ad8\u53ef\u7528\u6027: ETCD\u53ef\u4ee5\u907f\u514d\u786c\u4ef6\u7684\u5355\u70b9\u6545\u969c\u548c\u7f51\u7edc\u95ee\u9898 \u4e00\u81f4\u6027: \u6bcf\u6b21\u5199\u5165\u90fd\u4f1a\u8fd4\u56de\u8de8\u591a\u4e3b\u673a\u7684\u6700\u65b0\u5199\u5165 \u7b80\u5355: \u5305\u62ec\u4e00\u4e2a\u5b9a\u4e49\u826f\u597d,\u9762\u5411\u7528\u6237\u7684API \u5b89\u5168: \u5b9e\u73b0\u4e86\u5e26\u6709\u53ef\u9009\u7684\u5ba2\u6237\u7aef\u8bc1\u4e66\u8eab\u4efd\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316TLS \u5feb\u901f: \u6bcf\u79d2\u4e00\u4e07\u6b21\u5199\u5165\u7684\u57fa\u51c6\u901f\u5ea6 \u53ef\u9760: \u4f7f\u7528Raft\u7b97\u6cd5\u5b9e\u73b0\u5b58\u50a8\u7684\u5408\u7406\u5206\u5e03\u5728etcd\u7684\u5de5\u4f5c\u539f\u7406","title":"ETCD \u7279\u6027:"},{"location":"basic/6-kubernetes-etcd-docs/#_1","text":"ETCD \u4e3b\u8981\u8003\u8651\u5185\u5b58\uff0cCPU\uff0c\u78c1\u76d8 \u5b98\u7f51\u786c\u4ef6(\u63a8\u8350): https://etcd.io/docs/v3.5/op-guide/hardware/ \u6e29\u99a8\u63d0\u793a \u5b98\u65b9\u63d0\u4f9b\u7684\u786c\u4ef6\u6709\u4e9b\u8fc7\u4e8e\u4fdd\u5b88\uff0c\u6b63\u5f0f\u7684\u751f\u4ea7\u73af\u5883\u6700\u597d\u9ad8\u4e8e\u5b98\u65b9\u63a8\u8350\u76841.5-2\u500d\u6bd4\u8f83\u5408\u7406","title":"\u786c\u4ef6\u63a8\u8350"},{"location":"basic/6-kubernetes-etcd-docs/#etcd_1","text":"root@m1-pre:~# etcdctl member list 4469cb53324fe68b, started, etcd-192.168.1.102, https://192.168.1.102:2380, https://192.168.1.102:2379, false 9f5e0acc1f346641, started, etcd-192.168.1.101, https://192.168.1.101:2380, https://192.168.1.101:2379, false e519401c4b995768, started, etcd-192.168.1.103, https://192.168.1.103:2380, https://192.168.1.103:2379, false \u9a8c\u8bc1\u5f53\u524d\u6240\u6709ETCD\u6210\u5458\u72b6\u6001 root@m1-pre:~# export NODE_IPS = \"192.168.1.101 192.168.1.102 192.168.1.103\" root@m1-pre:~# for ip in ${ NODE_IPS } ; do ETCDCTL_API = 3 /usr/bin/etcdctl --endpoints = https:// ${ ip } :2379 --cacert = /etc/kubernetes/ssl/ca.pem --cert = /etc/kubernetes/ssl/etcd.pem --key = /etc/kubernetes/ssl/etcd-key.pem endpoint health ; done https://192.168.1.101:2379 is healthy: successfully committed proposal: took = 17 .871745ms https://192.168.1.102:2379 is healthy: successfully committed proposal: took = 12 .499902ms https://192.168.1.103:2379 is healthy: successfully committed proposal: took = 10 .525834ms \u663e\u793a\u8be6\u7ec6\u4fe1\u606f root@m1-pre:~# for ip in ${ NODE_IPS } ; do ETCDCTL_API = 3 /usr/bin/etcdctl --write-out = table endpoint status --endpoints = https:// ${ ip } :2379 --cacert = /etc/kubernetes/ssl/ca.pem --cert = /etc/kubernetes/ssl/etcd.pem --key = /etc/kubernetes/ssl/etcd-key.pem ; done +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.101:2379 | 9f5e0acc1f346641 | 3 .4.13 | 2 .5 MB | true | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.102:2379 | 4469cb53324fe68b | 3 .4.13 | 2 .5 MB | false | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://192.168.1.103:2379 | e519401c4b995768 | 3 .4.13 | 2 .5 MB | false | false | 128 | 259947 | 259947 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ \u67e5\u770bETCD\u6570\u636e $ etcdctl get / --prefix --keys-only $ etcdctl get /registry/pods/default/test $ etcdctl get / --prefix --keys-only | grep namespace $ etcdctl get / --prefix --keys-only | grep calico $ etcdctl get / --prefix --keys-only | grep deployment $ ETCDCTL_API = 3 etcdctl put /name \"linux60\" //\u589e $ ETCDCTL_API = 3 etcdctl get /name //\u67e5 $ ETCDCTL_API = 3 etcdctl del /name //\u5220 $ ETCDCTL_API = 3 etcdctl watch /data //\u65e0key\u4e5f\u53ef\u4ee5\u8fdb\u884cwatch watch \u5982\u540c\u6240\u793a:","title":"ETCD \u5ba2\u6237\u7aef\u4f7f\u7528"},{"location":"basic/6-kubernetes-etcd-docs/#etcd_2","text":"WAL\u987e\u540d\u601d\u4e49\uff0c\u5728\u771f\u6b63\u6267\u884c\u5199\u64cd\u4f5c\u4e4b\u524d\u5148\u5199\u4e00\u4e2a\u65e5\u5fd7\uff0c\u9884\u5199\u65e5\u5fd7 WAL \u5b58\u653e\u4e86\u9884\u5199\u65e5\u5fd7\uff0c\u6700\u5927\u7684\u4f5c\u7528\u662f\u8bb0\u5f55\u4e86\u6574\u4e2a\u6570\u636e\u53d8\u5316\u7684\u7684\u5168\u90e8\u5386\u7a0b\u3002\u5728etcd\u4e2d\uff0c\u6240\u6709\u6570\u636e\u7684\u4fee\u6539\u5728\u63d0\u4ea4\u524d\u90fd\u8981\u5148\u5199\u5165WAL\u4e2d V3 \u7248\u672c\u5907\u4efd\u6570\u636e ETCDCTL_API = 3 etcdctl snapshot save <filename> \u81ea\u52a8\u5907\u4efd\u6570\u636e $ mkdir /data/etcd-backup $ cat etcd-backup.sh #!/bin/bash source /etc/profile DATE = ` date +%Y%m%d-%H%M ` ETCDCTL_API = 3 /usr/bin/etcdctl snapshot save /data/etcd-backup/etcd-snapshot- ${ DATE } .db ETCDFILE = ` find /data/etcd-backup -mtime +30 -name etcd-* | wc -l ` if [ ${ ETCDFILE } -gt 30 ] ; then find /data/etcd-backup -mtime +30 -name etcd-* -exec rm -f {} \\; fi V3 \u7248\u672c\u6062\u590d\u6570\u636e ETCDCTL_API = 3 etcdctl snapshot restore /data/etcd/etcd.db --data-dir = /opt/data // \u6062\u590d\u7684\u76ee\u5f55\u5fc5\u987b\u4e0d\u5b58\u5728\uff0c\u5426\u5219\u4f1a\u62a5\u9519","title":"ETCD\u5907\u4efd\u6062\u590d"},{"location":"basic/7-kubernetes-update-docs/","text":"Master\u8282\u70b9\u7ef4\u62a4 \u00b6 \u9996\u5148\u9700\u8981\u4ece\u8d1f\u8f7d\u5747\u8861\u8e22\u51fa\u53bb \u5c06master\u8282\u70b9\u8bbe\u7f6e\u4e0d\u53ef\u8c03\u5ea6\u7684\u72b6\u6001\uff0c\u5e76\u9a71\u9010\u6240\u5728\u8282\u70b9\u7684pod\uff0c\u8fd9\u91cc\u53ef\u80fd\u4f1a\u9047\u5230\u6709\u4e9bpod\u65e0\u6cd5\u9a71\u9010\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u5f3a\u5236\u9a71\u9010 \u53bbGitHub\u4e0b\u8f7d\u8981\u5347\u7ea7\u7684\u4e8c\u8fdb\u5236\u5305\uff0c\u5e76\u8fdb\u884c\u66ff\u6362\uff08\u66ff\u6362\u524dmaster\u9700\u8981\u628a\u670d\u52a1\u90fd\u505c\u6389\uff09 \u7ec4\u4ef6\u5347\u7ea7 \u00b6 Master\u8282\u70b9\u670d\u52a1: kube-apiserver,kube-controller-manager,kubelet,kube-proxy,kube-scheduler,kubectl Node\u8282\u70b9\u670d\u52a1: kubectl,kubelet,kube-proxy kubernetes\u4e8c\u8fdb\u5236\u5305\u5982\u4f55\u83b7\u53d6\uff1f https://github.com/kubernetes/kubernetes/releases \u9009\u62e9kubernetes\u7248\u672c\uff0c\u70b9\u51fbCHANGELOG https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md \u9009\u62e9\u8981\u4e0b\u8f7d\u7684\u5305 wget https://dl.k8s.io/v1.22.17/kubernetes.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-client-linux-amd64.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-server-linux-amd64.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-node-linux-amd64.tar.gz","title":"Kubernetes \u8282\u70b9\u7ef4\u62a4"},{"location":"basic/7-kubernetes-update-docs/#master","text":"\u9996\u5148\u9700\u8981\u4ece\u8d1f\u8f7d\u5747\u8861\u8e22\u51fa\u53bb \u5c06master\u8282\u70b9\u8bbe\u7f6e\u4e0d\u53ef\u8c03\u5ea6\u7684\u72b6\u6001\uff0c\u5e76\u9a71\u9010\u6240\u5728\u8282\u70b9\u7684pod\uff0c\u8fd9\u91cc\u53ef\u80fd\u4f1a\u9047\u5230\u6709\u4e9bpod\u65e0\u6cd5\u9a71\u9010\u7684\u60c5\u51b5\uff0c\u53ef\u4ee5\u5f3a\u5236\u9a71\u9010 \u53bbGitHub\u4e0b\u8f7d\u8981\u5347\u7ea7\u7684\u4e8c\u8fdb\u5236\u5305\uff0c\u5e76\u8fdb\u884c\u66ff\u6362\uff08\u66ff\u6362\u524dmaster\u9700\u8981\u628a\u670d\u52a1\u90fd\u505c\u6389\uff09","title":"Master\u8282\u70b9\u7ef4\u62a4"},{"location":"basic/7-kubernetes-update-docs/#_1","text":"Master\u8282\u70b9\u670d\u52a1: kube-apiserver,kube-controller-manager,kubelet,kube-proxy,kube-scheduler,kubectl Node\u8282\u70b9\u670d\u52a1: kubectl,kubelet,kube-proxy kubernetes\u4e8c\u8fdb\u5236\u5305\u5982\u4f55\u83b7\u53d6\uff1f https://github.com/kubernetes/kubernetes/releases \u9009\u62e9kubernetes\u7248\u672c\uff0c\u70b9\u51fbCHANGELOG https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md \u9009\u62e9\u8981\u4e0b\u8f7d\u7684\u5305 wget https://dl.k8s.io/v1.22.17/kubernetes.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-client-linux-amd64.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-server-linux-amd64.tar.gz wget https://dl.k8s.io/v1.22.17/kubernetes-node-linux-amd64.tar.gz","title":"\u7ec4\u4ef6\u5347\u7ea7"},{"location":"basic/cncf/","text":"\u4e91\u539f\u751f\u53caCNCF \u00b6 \u4e91\u539f\u751f\u7684\u5b9a\u4e49 \u00b6 \u5b98\u65b9\u4ecb\u7ecd: \u4e91\u539f\u751f\u6280\u672f\u6709\u5229\u4e8e\u5404\u7ec4\u7ec7\u5728\u516c\u6709\u4e91\u3001\u79c1\u6709\u4e91\u548c\u6df7\u5408\u4e91\u7b49\u65b0\u578b\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u6784\u5efa\u548c\u8fd0\u884c\u53ef\u5f39\u6027\u6269\u5c55\u7684\u5e94\u7528\u3002\u4e91\u539f\u751f\u7684\u4ee3\u8868\u6280\u672f\u5305\u62ec\u5bb9\u5668\u3001\u670d\u52a1\u7f51\u683c\u3001\u5fae\u670d\u52a1\u3001\u4e0d\u53ef\u53d8\u57fa\u7840\u8bbe\u65bd\u548c\u58f0\u660e\u5f0fAPI\u3002 \u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u6784\u5efa\u5bb9\u9519\u6027\u597d\u3001\u6613\u4e8e\u7ba1\u7406\u548c\u4fbf\u4e8e\u89c2\u5bdf\u7684\u677e\u8026\u5408\u7cfb\u7edf\u3002\u7ed3\u5408\u53ef\u9760\u7684\u81ea\u52a8\u5316\u624b\u6bb5\uff0c\u4e91\u539f\u751f\u6280\u672f\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u8f7b\u677e\u5730\u5bf9\u7cfb\u7edf\u4f5c\u51fa\u9891\u7e41\u548c\u53ef\u9884\u6d4b\u7684\u91cd\u5927\u53d8\u66f4\u3002 \u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u81f4\u529b\u4e8e\u57f9\u80b2\u548c\u7ef4\u62a4\u4e00\u4e2a\u5382\u5546\u4e2d\u7acb\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u6765\u63a8\u5e7f\u4e91\u539f\u751f\u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u6700\u524d\u6cbf\u7684\u6a21\u5f0f\u6c11\u4e3b\u5316\uff0c\u8ba9\u8fd9\u4e9b\u521b\u65b0\u4e3a\u5927\u4f17\u6240\u7528\u3002 CNCF \u4e91\u539f\u751f\u5bb9\u5668\u751f\u6001\u7cfb\u7edf\u6982\u8981 \u00b6 \u5b98\u65b9\u4ecb\u7ecd\uff1a \u6587\u6863\u53c2\u8003\uff1a","title":"\u4e91\u539f\u751f\u7b80\u4ecb"},{"location":"basic/cncf/#cncf","text":"","title":"\u4e91\u539f\u751f\u53caCNCF"},{"location":"basic/cncf/#_1","text":"\u5b98\u65b9\u4ecb\u7ecd: \u4e91\u539f\u751f\u6280\u672f\u6709\u5229\u4e8e\u5404\u7ec4\u7ec7\u5728\u516c\u6709\u4e91\u3001\u79c1\u6709\u4e91\u548c\u6df7\u5408\u4e91\u7b49\u65b0\u578b\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u6784\u5efa\u548c\u8fd0\u884c\u53ef\u5f39\u6027\u6269\u5c55\u7684\u5e94\u7528\u3002\u4e91\u539f\u751f\u7684\u4ee3\u8868\u6280\u672f\u5305\u62ec\u5bb9\u5668\u3001\u670d\u52a1\u7f51\u683c\u3001\u5fae\u670d\u52a1\u3001\u4e0d\u53ef\u53d8\u57fa\u7840\u8bbe\u65bd\u548c\u58f0\u660e\u5f0fAPI\u3002 \u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u6784\u5efa\u5bb9\u9519\u6027\u597d\u3001\u6613\u4e8e\u7ba1\u7406\u548c\u4fbf\u4e8e\u89c2\u5bdf\u7684\u677e\u8026\u5408\u7cfb\u7edf\u3002\u7ed3\u5408\u53ef\u9760\u7684\u81ea\u52a8\u5316\u624b\u6bb5\uff0c\u4e91\u539f\u751f\u6280\u672f\u4f7f\u5de5\u7a0b\u5e08\u80fd\u591f\u8f7b\u677e\u5730\u5bf9\u7cfb\u7edf\u4f5c\u51fa\u9891\u7e41\u548c\u53ef\u9884\u6d4b\u7684\u91cd\u5927\u53d8\u66f4\u3002 \u4e91\u539f\u751f\u8ba1\u7b97\u57fa\u91d1\u4f1a\uff08CNCF\uff09\u81f4\u529b\u4e8e\u57f9\u80b2\u548c\u7ef4\u62a4\u4e00\u4e2a\u5382\u5546\u4e2d\u7acb\u7684\u5f00\u6e90\u751f\u6001\u7cfb\u7edf\uff0c\u6765\u63a8\u5e7f\u4e91\u539f\u751f\u6280\u672f\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u6700\u524d\u6cbf\u7684\u6a21\u5f0f\u6c11\u4e3b\u5316\uff0c\u8ba9\u8fd9\u4e9b\u521b\u65b0\u4e3a\u5927\u4f17\u6240\u7528\u3002","title":"\u4e91\u539f\u751f\u7684\u5b9a\u4e49"},{"location":"basic/cncf/#cncf_1","text":"\u5b98\u65b9\u4ecb\u7ecd\uff1a \u6587\u6863\u53c2\u8003\uff1a","title":"CNCF \u4e91\u539f\u751f\u5bb9\u5668\u751f\u6001\u7cfb\u7edf\u6982\u8981"},{"location":"basic/etcd/","text":"Etcd \u5907\u4efd \u00b6 https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ Info \u4ee5\u4e0b\u5907\u4efd\u4e3b\u8981\u662f\u4ee5kubeadm\u5b89\u88c5\u7684k8s \u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84 \u00b6 [cka] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.159.81:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.159.81:2380 - --initial-cluster=master0=https://192.168.159.81:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls=https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls=http://127.0.0.1:2381 - --listen-peer-urls=https://192.168.159.81:2380 - --name=master0 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt \u624b\u52a8\u5907\u4efd \u00b6 \u5b89\u88c5etcd\u5ba2\u6237\u7aef \u00b6 apt install etcd-client \u6307\u5b9a\u8def\u5f84\u5907\u4efd \u00b6 \u8fd9\u91cc\u5c06\u5907\u4efd\u6307\u5b9a\u5907\u4efd\u5230 /srv/data/\uff0c\u6307\u5b9a\u7684\u4e09\u4e2a\u6587\u4ef6\u5c31\u662f\u4ee5\u4e0a\u6807\u6ce8\u7684\u4e09\u4e2a\u4f4d\u7f6e export ETCDCTL_API=3 # \u58f0\u660eetcd-api mkdir /srv/data/ etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u6821\u9a8c\u7ed3\u679c\uff1a etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot status /srv/data/etcd-snapshot.db \u81ea\u52a8\u5907\u4efd \u00b6 \u8fd8\u539f\uff08kubeadm \u5b89\u88c5\u7684k8s\uff09 \u00b6 \u6a21\u62df\u6545\u969c \u00b6 \u8fd9\u91cc\u6211\u6a21\u62df\u5220\u9664\u4e86\u8fd9\u4e2a\u8d44\u6e90\uff0c\u770b\u4e00\u4f1a\u662f\u5426\u53ef\u4ee5\u8fd8\u539f [ucloud] root@master0:~# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d [ucloud] root@master0:~# k delete ds nginx-ds daemonset.apps \"nginx-ds\" deleted \u786e\u8ba4k8s\u7ec4\u4ef6\u7684\u4f4d\u7f6e \u00b6 \u67e5\u770bkubelet\u7684\u72b6\u6001 [ucloud] root@master0:~# systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d # \u67e5\u770b\u8fd9\u4e2a\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6 \u2514\u250010-kubeadm.conf Active: active (running) since Mon 2022-07-04 15:09:15 CST; 1 day 23h ago Docs: https://kubernetes.io/docs/home/ Main PID: 39279 (kubelet) Tasks: 17 (limit: 4390) Memory: 63.3M CGroup: /system.slice/kubelet.service \u67e5\u770b\u6587\u4ef6\uff1a/etc/systemd/system/kubelet.service.d/10-kubeadm.conf [ucloud] root@master0:~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # \u67e5\u770b\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS \u67e5\u770b\u6587\u4ef6\uff1a/var/lib/kubelet/config.yaml [ucloud] root@master0:~# cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: ...... staticPodPath: /etc/kubernetes/manifests # \u770b\u5230\u8fd9\u884c\uff0c\u5c31\u8bf4\u660e\u914d\u7f6e\u6587\u4ef6\u8fd9\u4e2amanifests\u4e0b streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s \u505c\u6b62api-server \u00b6 [ucloud] root@master0:~# mkdir /opt/backup/ -p [ucloud] root@master0:~# cd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# ll total 24 drwxr-xr-x 2 root root 4096 Jul 3 22:01 ./ drwxr-xr-x 5 root root 4096 Jul 3 21:15 ../ -rw------- 1 root root 2294 Jul 3 22:00 etcd.yaml -rw------- 1 root root 4036 Jul 3 22:00 kube-apiserver.yaml -rw------- 1 root root 3541 Jul 3 22:00 kube-controller-manager.yaml -rw------- 1 root root 1464 Jul 3 22:00 kube-scheduler.yaml [ucloud] root@master0:/etc/kubernetes/manifests# mv kube-* /opt/backup/ # \u79fb\u8d70\u4e4b\u540e\u6211\u4eec\u4f1a\u53d1\u73b0\u4e0d\u80fd\u4f7f\u7528kubectl\u547d\u4ee4 \u8fdb\u884c\u8fd8\u539f \u00b6 \u5982\u679c\u6062\u590d\u5931\u8d25\u9700\u8981\u6dfb\u52a0--skip-hash-check\u53c2\u6570 [ucloud] root@master0:~# etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot restore /srv/data/etcd-snapshot.db --data-dir=/var/lib/etcd-restore --skip-hash-check \u4fee\u6539etcd\u7684\u914d\u7f6e\u6587\u4ef6 \u5c06 volume \u914d\u7f6e\u7684 path: /var/lib/etcd \u6539\u6210/var/lib/etcd-restore [ucloud] root@master0:/etc/kubernetes/manifests# pwd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# vim etcd.yaml - hostPath: path: /var/lib/etcd-restore # \u5c06\u6539\u76ee\u5f55\u4fee\u6539\u4e3a\u8fd8\u539fetcd\u7684\u4f4d\u7f6e type: DirectoryOrCreate \u8fd8\u539fk8s\u7ec4\u4ef6 mv /opt/backup/* /etc/kubernetes/manifests systemctl restart kubelet \u6821\u9a8c\u7ed3\u679c\uff1a \u53d1\u73b0\u8bef\u5220\u9664\u7684pod\u8fd8\u539f\u6210\u529f [ucloud] root@master0:/etc/kubernetes/manifests# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d","title":"Etcd \u5907\u4efd"},{"location":"basic/etcd/#etcd","text":"https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ Info \u4ee5\u4e0b\u5907\u4efd\u4e3b\u8981\u662f\u4ee5kubeadm\u5b89\u88c5\u7684k8s","title":"Etcd \u5907\u4efd"},{"location":"basic/etcd/#_1","text":"[cka] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls=https://192.168.159.81:2379 - --cert-file=/etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth=true - --data-dir=/var/lib/etcd - --initial-advertise-peer-urls=https://192.168.159.81:2380 - --initial-cluster=master0=https://192.168.159.81:2380 - --key-file=/etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls=https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls=http://127.0.0.1:2381 - --listen-peer-urls=https://192.168.159.81:2380 - --name=master0 - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth=true - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count=10000 - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt","title":"\u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84"},{"location":"basic/etcd/#_2","text":"","title":"\u624b\u52a8\u5907\u4efd"},{"location":"basic/etcd/#etcd_1","text":"apt install etcd-client","title":"\u5b89\u88c5etcd\u5ba2\u6237\u7aef"},{"location":"basic/etcd/#_3","text":"\u8fd9\u91cc\u5c06\u5907\u4efd\u6307\u5b9a\u5907\u4efd\u5230 /srv/data/\uff0c\u6307\u5b9a\u7684\u4e09\u4e2a\u6587\u4ef6\u5c31\u662f\u4ee5\u4e0a\u6807\u6ce8\u7684\u4e09\u4e2a\u4f4d\u7f6e export ETCDCTL_API=3 # \u58f0\u660eetcd-api mkdir /srv/data/ etcdctl --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u6821\u9a8c\u7ed3\u679c\uff1a etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot status /srv/data/etcd-snapshot.db","title":"\u6307\u5b9a\u8def\u5f84\u5907\u4efd"},{"location":"basic/etcd/#_4","text":"","title":"\u81ea\u52a8\u5907\u4efd"},{"location":"basic/etcd/#kubeadm-k8s","text":"","title":"\u8fd8\u539f\uff08kubeadm \u5b89\u88c5\u7684k8s\uff09"},{"location":"basic/etcd/#_5","text":"\u8fd9\u91cc\u6211\u6a21\u62df\u5220\u9664\u4e86\u8fd9\u4e2a\u8d44\u6e90\uff0c\u770b\u4e00\u4f1a\u662f\u5426\u53ef\u4ee5\u8fd8\u539f [ucloud] root@master0:~# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d [ucloud] root@master0:~# k delete ds nginx-ds daemonset.apps \"nginx-ds\" deleted","title":"\u6a21\u62df\u6545\u969c"},{"location":"basic/etcd/#k8s","text":"\u67e5\u770bkubelet\u7684\u72b6\u6001 [ucloud] root@master0:~# systemctl status kubelet \u25cf kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled) Drop-In: /etc/systemd/system/kubelet.service.d # \u67e5\u770b\u8fd9\u4e2a\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6 \u2514\u250010-kubeadm.conf Active: active (running) since Mon 2022-07-04 15:09:15 CST; 1 day 23h ago Docs: https://kubernetes.io/docs/home/ Main PID: 39279 (kubelet) Tasks: 17 (limit: 4390) Memory: 63.3M CGroup: /system.slice/kubelet.service \u67e5\u770b\u6587\u4ef6\uff1a/etc/systemd/system/kubelet.service.d/10-kubeadm.conf [ucloud] root@master0:~# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\"KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\" Environment=\"KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\" # \u67e5\u770b\u8fd9\u4e2a\u914d\u7f6e\u6587\u4ef6 # This is a file that \"kubeadm init\" and \"kubeadm join\" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/default/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS \u67e5\u770b\u6587\u4ef6\uff1a/var/lib/kubelet/config.yaml [ucloud] root@master0:~# cat /var/lib/kubelet/config.yaml apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: ...... staticPodPath: /etc/kubernetes/manifests # \u770b\u5230\u8fd9\u884c\uff0c\u5c31\u8bf4\u660e\u914d\u7f6e\u6587\u4ef6\u8fd9\u4e2amanifests\u4e0b streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s","title":"\u786e\u8ba4k8s\u7ec4\u4ef6\u7684\u4f4d\u7f6e"},{"location":"basic/etcd/#api-server","text":"[ucloud] root@master0:~# mkdir /opt/backup/ -p [ucloud] root@master0:~# cd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# ll total 24 drwxr-xr-x 2 root root 4096 Jul 3 22:01 ./ drwxr-xr-x 5 root root 4096 Jul 3 21:15 ../ -rw------- 1 root root 2294 Jul 3 22:00 etcd.yaml -rw------- 1 root root 4036 Jul 3 22:00 kube-apiserver.yaml -rw------- 1 root root 3541 Jul 3 22:00 kube-controller-manager.yaml -rw------- 1 root root 1464 Jul 3 22:00 kube-scheduler.yaml [ucloud] root@master0:/etc/kubernetes/manifests# mv kube-* /opt/backup/ # \u79fb\u8d70\u4e4b\u540e\u6211\u4eec\u4f1a\u53d1\u73b0\u4e0d\u80fd\u4f7f\u7528kubectl\u547d\u4ee4","title":"\u505c\u6b62api-server"},{"location":"basic/etcd/#_6","text":"\u5982\u679c\u6062\u590d\u5931\u8d25\u9700\u8981\u6dfb\u52a0--skip-hash-check\u53c2\u6570 [ucloud] root@master0:~# etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key snapshot restore /srv/data/etcd-snapshot.db --data-dir=/var/lib/etcd-restore --skip-hash-check \u4fee\u6539etcd\u7684\u914d\u7f6e\u6587\u4ef6 \u5c06 volume \u914d\u7f6e\u7684 path: /var/lib/etcd \u6539\u6210/var/lib/etcd-restore [ucloud] root@master0:/etc/kubernetes/manifests# pwd /etc/kubernetes/manifests [ucloud] root@master0:/etc/kubernetes/manifests# vim etcd.yaml - hostPath: path: /var/lib/etcd-restore # \u5c06\u6539\u76ee\u5f55\u4fee\u6539\u4e3a\u8fd8\u539fetcd\u7684\u4f4d\u7f6e type: DirectoryOrCreate \u8fd8\u539fk8s\u7ec4\u4ef6 mv /opt/backup/* /etc/kubernetes/manifests systemctl restart kubelet \u6821\u9a8c\u7ed3\u679c\uff1a \u53d1\u73b0\u8bef\u5220\u9664\u7684pod\u8fd8\u539f\u6210\u529f [ucloud] root@master0:/etc/kubernetes/manifests# k get pod NAME READY STATUS RESTARTS AGE nginx-ds-6mnw5 1/1 Running 0 2d","title":"\u8fdb\u884c\u8fd8\u539f"},{"location":"basic/k8s-update/","text":"k8s \u7248\u672c\u5347\u7ea7 \u00b6 \u6ce8\u610f \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5347\u7ea7\u662fkubeadm\u5b89\u88c5\u7684k8s\u5662 \u7248\u672c\u9700\u6c42 \u00b6 k8s \u7684 \u7248\u672c\u4e3av1.19.16\uff0c\u5982\u679c\u8981\u5347\u7ea7\u5230v1.23.8 Warning \u6ce8\u610f\uff1a \u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c,\u56e0\u6b64\u9700\u8981\u5347\u7ea7\u52064\u6b21\u8fdb\u884c\u5347\u7ea7\uff0c\u9700\u89811.20.15--1.21.14--1.22.11--1.23.8 \uff08\u5347\u7ea7\u56db\u6b21\uff09 \u6240\u6709\u8282\u70b9\u6267\u884c\uff1aapt update \u4e3b\u8282\u70b9: \u00b6 [ ucloud ] root@master0:~# apt-cache policy kubeadm | grep 1 .20. 1 .20.15-00 500 # ************\u6240\u6709\u8282\u70b9\uff0cmaster\u548cnode\u8282\u70b9\u90fd\u9700\u8981\u5347\u7ea7********* apt-get install kubeadm = 1 .20.15-00 # \u67e5\u770bk8s\u7248\u672c\u9700\u8981\u7684\u955c\u50cf kubeadm config images list --kubernetes-version v1.20.15 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 # \u4e3b\u8282\u70b9\u5347\u7ea7\uff0c\u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u6dfb\u52a0# --etcd-upgrade=false \u53c2\u6570\u5ffd\u7565\u5373\u53ef kubeadm upgrade apply v1.20.15 \u6ce8\u610f\uff1a \u8fd9\u65f6\u6211\u4eec\u53d1\u73b0k8s\u7684\u7248\u672c\u8fd8\u662f\u6ca1\u6709\u53d8\u5316\uff0c\u66f4\u65b0kubelet\u3002 \u4e3b\u8282\u70b9\uff1a apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 Error \u8fd9\u79cd\u60c5\u51b5\u53ef\u80fd\u662f\u4e4b\u524d\u5347\u7ea7\u6ca1\u6709\u8fdb\u884cnode\u8282\u70b9\u7684kubeadm\u5347\u7ea7\u5bfc\u81f4\u7684. [sjtu] root@master:/home/lixie# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade/health] FATAL: [preflight] Some fatal errors occurred: [ERROR ControlPlaneNodesReady]: there are NotReady control-planes in the cluster: [master] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher \u89e3\u51b3\uff1a\uff08\u91cd\u65b0\u5b89\u88c5\u8be5\u7248\u672ckubelet\u548ckubectl\uff09 apt install kubelet=1.19.16-00 apt install kubectl=1.19.16-00 \u4ece\u8282\u70b9: \u00b6 node\u8282\u70b9\u5347\u7ea7kubelet\u7684\u914d\u7f6e\uff08\u6240\u6709node\u8282\u70b9\uff09 sudo kubeadm upgrade node \u5b89\u88c5kubelet\u548ckubectl apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 kubelet --version # \u67e5\u770b\u7248\u672c Kubernetes v1.23.8 \u9644\u4ef6 \u00b6 k8s\u8bc1\u4e66\u8fc7\u671f\u5904\u7406 \u00b6 \u95ee\u9898\uff1a \u5904\u7406k8s\u8bc1\u4e66\u8fc7\u671f","title":"K8S\u5347\u7ea7"},{"location":"basic/k8s-update/#k8s","text":"\u6ce8\u610f \u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5347\u7ea7\u662fkubeadm\u5b89\u88c5\u7684k8s\u5662","title":"k8s \u7248\u672c\u5347\u7ea7"},{"location":"basic/k8s-update/#_1","text":"k8s \u7684 \u7248\u672c\u4e3av1.19.16\uff0c\u5982\u679c\u8981\u5347\u7ea7\u5230v1.23.8 Warning \u6ce8\u610f\uff1a \u5347\u7ea7\u4e0d\u80fd\u6a2a\u8de8\u4e24\u4e2a\u5927\u7248\u672c,\u56e0\u6b64\u9700\u8981\u5347\u7ea7\u52064\u6b21\u8fdb\u884c\u5347\u7ea7\uff0c\u9700\u89811.20.15--1.21.14--1.22.11--1.23.8 \uff08\u5347\u7ea7\u56db\u6b21\uff09 \u6240\u6709\u8282\u70b9\u6267\u884c\uff1aapt update","title":"\u7248\u672c\u9700\u6c42"},{"location":"basic/k8s-update/#_2","text":"[ ucloud ] root@master0:~# apt-cache policy kubeadm | grep 1 .20. 1 .20.15-00 500 # ************\u6240\u6709\u8282\u70b9\uff0cmaster\u548cnode\u8282\u70b9\u90fd\u9700\u8981\u5347\u7ea7********* apt-get install kubeadm = 1 .20.15-00 # \u67e5\u770bk8s\u7248\u672c\u9700\u8981\u7684\u955c\u50cf kubeadm config images list --kubernetes-version v1.20.15 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c # \u770betcd\u7248\u672c\u662f\u5426\u53d1\u751f\u53d8\u5316 # \u4e3b\u8282\u70b9\u5347\u7ea7\uff0c\u5982\u679c\u4e0d\u9700\u8981\u5347\u7ea7etcd\uff0c\u6dfb\u52a0# --etcd-upgrade=false \u53c2\u6570\u5ffd\u7565\u5373\u53ef kubeadm upgrade apply v1.20.15 \u6ce8\u610f\uff1a \u8fd9\u65f6\u6211\u4eec\u53d1\u73b0k8s\u7684\u7248\u672c\u8fd8\u662f\u6ca1\u6709\u53d8\u5316\uff0c\u66f4\u65b0kubelet\u3002 \u4e3b\u8282\u70b9\uff1a apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 Error \u8fd9\u79cd\u60c5\u51b5\u53ef\u80fd\u662f\u4e4b\u524d\u5347\u7ea7\u6ca1\u6709\u8fdb\u884cnode\u8282\u70b9\u7684kubeadm\u5347\u7ea7\u5bfc\u81f4\u7684. [sjtu] root@master:/home/lixie# kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml' [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade/health] FATAL: [preflight] Some fatal errors occurred: [ERROR ControlPlaneNodesReady]: there are NotReady control-planes in the cluster: [master] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher \u89e3\u51b3\uff1a\uff08\u91cd\u65b0\u5b89\u88c5\u8be5\u7248\u672ckubelet\u548ckubectl\uff09 apt install kubelet=1.19.16-00 apt install kubectl=1.19.16-00","title":"\u4e3b\u8282\u70b9:"},{"location":"basic/k8s-update/#_3","text":"node\u8282\u70b9\u5347\u7ea7kubelet\u7684\u914d\u7f6e\uff08\u6240\u6709node\u8282\u70b9\uff09 sudo kubeadm upgrade node \u5b89\u88c5kubelet\u548ckubectl apt install -y kubelet = 1 .20.15-00 kubectl = 1 .20.15-00 kubelet --version # \u67e5\u770b\u7248\u672c Kubernetes v1.23.8","title":"\u4ece\u8282\u70b9:"},{"location":"basic/k8s-update/#_4","text":"","title":"\u9644\u4ef6"},{"location":"basic/k8s-update/#k8s_1","text":"\u95ee\u9898\uff1a \u5904\u7406k8s\u8bc1\u4e66\u8fc7\u671f","title":"k8s\u8bc1\u4e66\u8fc7\u671f\u5904\u7406"},{"location":"basic/overview/","text":"K8S\u7b80\u4ecb \u00b6","title":"K8S\u7b80\u4ecb"},{"location":"basic/overview/#k8s","text":"","title":"K8S\u7b80\u4ecb"},{"location":"cks/cka/","text":"cka \u8ba4\u8bc1\u8003\u8bd5 \u00b6 1. \u76d1\u63a7 pod\u65e5\u5fd7 5% \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u76d1\u63a7\u540d\u4e3a foobar \u7684 Pod \u7684\u65e5\u5fd7\uff0c\u5e76\u8fc7\u6ee4\u51fa\u5177\u6709 unable-access-website \u4fe1\u606f\u7684\u884c\uff0c\u7136\u540e\u5c06\u5199\u5165\u5230 /opt/KUTR00101/foobar \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl logs foobar | grep unable-access-website > /opt/KUTR00101/foobar 2.\u76d1\u63a7 pod \u5ea6\u91cf \u6307\u6807: 5% \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u627e\u51fa\u5177\u6709 name=cpu-user \u7684 Pod\uff0c\u5e76\u8fc7\u6ee4\u51fa\u4f7f\u7528 CPU \u6700\u9ad8\u7684 Pod\uff0c\u7136\u540e\u628a\u5b83\u7684\u540d\u5b57\u5199\u5728\u5df2\u7ecf\u5b58\u5728\u7684/opt/KUTR00401/KUTR00401.txt \u6587\u4ef6\u91cc \uff08\u6ce8\u610f\u4ed6\u6ca1\u6709\u8bf4\u6307\u5b9a namespace\u3002\u6240\u4ee5\u9700\u8981\u4f7f\u7528-A \u6307\u5b9a\u6240\u4ee5 namespace\uff09 \u6ce8\u610f\u662f \u8ffd\u52a0 \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl top pod -A -l | grep name = cpu-user # \u6ce8\u610f\u8fd9\u91cc\u7684 pod \u540d\u5b57\u4ee5\u5b9e\u9645\u540d\u5b57\u4e3a\u51c6\uff0c\u6309\u7167 CPU \u90a3\u4e00\u5217\u8fdb\u884c\u9009\u62e9\u4e00\u4e2a\u6700\u5927\u7684 Pod\uff0c\u53e6\u5916\u5982\u679c CPU \u7684\u6570\u503c\u662f 1 2 3 \u8fd9\u6837\u7684\u3002\u662f\u5927\u4e8e\u5e26 m \u8fd9\u6837\u7684\uff0c\u56e0\u4e3a 1 \u9897 CPU \u7b49\u4e8e 1000m\uff0c\u6ce8\u610f\u8981\u7528>>\u800c\u4e0d\u662f> $ echo \"coredns-54d67798b7-hl8xc\" >> /opt/KUTR00401/KUTR00401.txt 3.Deployment \u6269\u7f29\u5bb9 \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u6269\u5bb9\u540d\u5b57\u4e3a loadbalancer \u7684 deployment \u7684\u526f\u672c\u6570\u4e3a 6 \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl scale --replicas = 6 deployment loadbalancer $ kubectl edit 4. \u68c0\u67e5 Node \u8282\u70b9\u7684\u5065\u5eb7\u72b6\u6001 \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u68c0\u67e5\u96c6\u7fa4\u4e2d\u6709\u591a\u5c11\u8282\u70b9\u4e3a Ready \u72b6\u6001\uff0c\u5e76\u4e14\u53bb\u9664\u5305\u542b NoSchedule \u6c61\u70b9\u7684\u8282\u70b9\u3002\u4e4b\u540e\u5c06\u6570\u5b57\u5199\u5230/opt/KUSC00402/kusc00402.txt \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl get node | grep -i ready # \u8bb0\u5f55\u603b\u6570\u4e3a A $ kubectl describe node | grep Taint | grep NoSchedule # \u8bb0\u5f55\u603b\u6570\u4e3a B # \u5c06 A \u51cf B \u7684\u503c x \u5bfc\u5165\u5230/opt/KUSC00402/kusc00402.txt $ echo x >> /opt/KUSC00402/kusc00402.txt 5. \u8282\u70b9\u7ef4\u62a4 \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u5c06 ek8s-node-1 \u8282\u70b9\u8bbe\u7f6e\u4e3a\u4e0d\u53ef\u7528\uff0c\u7136\u540e\u91cd\u65b0\u8c03\u5ea6\u8be5\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context ek8s $ kubectl cordon ek8s-node-1 $ kubectl drain ek8s-node-1 --delete-emptydir-data --ignore-daemonsets --force $ k delete pod pod-name --grace-period = 0 --force 6. \u6307\u5b9a\u8282\u70b9\u90e8\u7f72 \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a Pod\uff0c\u540d\u5b57\u4e3a nginx-kusc00401\uff0c\u955c\u50cf\u5730\u5740\u662f nginx\uff0c\u8c03\u5ea6\u5230\u5177\u6709 disk=spinning \u6807\u7b7e\u7684\u8282\u70b9\u4e0a \uff0c \u8be5\u9898\u53ef\u4ee5\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/ \u8be5\u9898\u53ef\u4ee5\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh/docs/concepts/scheduling-eviction/assign-pod-node/ \u89e3\u9898\u6b65\u9aa4: \u9898\u76ee\u4e00\uff1a apiVersion : v1 kind : Pod metadata : name : nginx-kusc00401 labels : role : nginx-kusc00401 spec : containers : - name : nginx image : nginx nodeSelector : disk : spinning \u9898\u76ee\u4e8c\uff1a $ cat 6-2-cka.yaml apiVersion : v1 kind : Pod metadata : name : nginx-kusc00402 labels : role : nginx-kusc00402 spec : nodeName : node1 containers : - name : nginx image : nginx 7. \u4e00\u4e2a Pod \u591a\u4e2a\u5bb9\u5668 \u00b6 \u56fe\u793a\uff1a \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a Pod \uff0c \u540d\u5b57\u4e3a kucc1 \uff0c\u8fd9\u4e2a Pod \u53ef\u80fd\u5305\u542b 1-4 \u5bb9 \u5668 \uff0c \u8be5\u9898\u4e3a\u56db\u4e2a \uff1anginx+redis+memcached+consul \u89e3\u9898\u6b65\u9aa4: $ cat 7-cka.yaml apiVersion : v1 kind : Pod metadata : name : kucc1 spec : containers : - name : nginx image : nginx - name : redis image : redis - name : memcached image : memcached - name : consul image : consul 8. Service \u8003\u9898 \u00b6 \u4e2d\u6587\u89e3\u91ca: \u91cd\u65b0\u914d\u7f6e\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684 deployment front-end\uff0c\u5728\u540d\u5b57\u4e3a nginx \u7684\u5bb9\u5668\u91cc\u9762\u6dfb\u52a0\u4e00\u4e2a\u7aef\u53e3\u914d\u7f6e\uff0c\u540d\u5b57\u4e3a http\uff0c\u66b4\u9732\u7aef\u53e3\u53f7\u4e3a 80\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a service\uff0c\u540d\u5b57\u4e3a front-end-svc\uff0c\u66b4\u9732\u8be5deployment \u7684 http \u7aef\u53e3\uff0c\u5e76\u4e14 service \u7684\u7c7b\u578b\u4e3a NodePort\u3002 ports : - containerPort : 80 name : http protocol : TCP \u521b\u5efa\u4e00\u4e2asvc kubectl expose deploy front-end --name = front-end-svc --port = 80 \\ --target-port = http --type = NodePort 9. Ingress\u8003\u9898 8% \u00b6 \u53c2\u8003\u5730\u5740: https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/ \u56fe\u793a: \u73af\u5883\u51c6\u5907: [ cka ] root@master0:/home/lixie# k label node node1 ingress = true node/node1 labeled \u4e2d\u6587\u89e3\u91ca: \u5728 ing-internal \u547d\u540d\u7a7a\u95f4\u4e0b\u521b\u5efa\u4e00\u4e2a ingress\uff0c\u540d\u5b57\u4e3a pong\uff0c\u4ee3\u7406\u7684 service hi\uff0c\u7aef\u53e3\u4e3a 5678\uff0c\u914d\u7f6e\u8def\u5f84/hi\u3002 \u9a8c\u8bc1\uff1a\u8bbf\u95ee curl -kL /hi \u4f1a\u8fd4\u56de hi \u89e3\u9898\u6b65\u9aa4: $ k label node node2 ingress = true $ k apply -f ingress.yaml [ cka ] root@master0:~/cka# cat 9-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : pong # \u540d\u79f0\u9700\u8981\u8be5 namespace : ing-internal # \u9700\u8981\u8be5 spec : ingressClassName : nginx rules : - http : paths : - pathType : Prefix path : \"/hi\" # \u8bbf\u95ee\u8def\u5f84\u9700\u8981\u6539 backend : service : name : front-end-svc # \u9700\u8981\u8be5 port : number : 80 10. Sidecar 8% \u00b6 \u56fe\u793a: \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/ \u4e2d\u6587\u89e3\u91ca: \u6dfb\u52a0\u4e00\u4e2a\u540d\u4e3a busybox \u4e14\u955c\u50cf\u4e3a busybox \u7684 sidecar \u5230\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684\u540d\u4e3a legacy-app \u7684 Pod \u4e0a\uff0c\u8fd9\u4e2a sidecar \u7684\u542f\u52a8\u547d\u4ee4\u4e3a/bin/sh, -c, 'tail -n+1 -f /var/log/legacy-app.log'\u3002 \u5e76\u4e14\u8fd9\u4e2a sidecar \u548c\u539f\u6709\u7684\u955c\u50cf\u6302\u8f7d\u4e00\u4e2a\u540d\u4e3a logs \u7684 volume\uff0c\u6302\u8f7d\u7684\u76ee\u5f55\u4e3a/var/log/ \u5bfc\u51fayaml [ cka ] root@master0:~/cka# cat 10-1.yaml apiVersion : v1 kind : Pod metadata : name : legacy-app spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$(date) INFO $i\" >> /var/log/legacy-app.log; i=$((i+1)); sleep 1; done \u9996\u5148\u5c06 legacy-app \u7684 Pod \u7684 yaml \u5bfc\u51fa\uff0c\u5927\u81f4\u5982\u4e0b\uff1a kubectl get pod legacy-app -o yaml > 10-2.yaml \u8fdb\u884c\u4fee\u6539 [ cka ] root@master0:~/cka# cat 10-2.yaml apiVersion : v1 kind : Pod metadata : name : legacy-app namespace : default spec : containers : - name : busybox # \u9700\u8981\u4fee\u6539 image : busybox # \u9700\u8981\u4fee\u6539 args : [ /bin/sh , -c , 'tail -n+1 -F /var/log/legacy-app.log' ] volumeMounts : - name : logs # \u9700\u8981\u4fee\u6539 mountPath : /var/log - args : - /bin/sh - -c - | i=0; while true; do echo \"$(date) INFO $i\" >> /var/log/legacy-app.log; i=$((i+1)); sleep 1; done image : busybox imagePullPolicy : Always name : count resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : # \u9700\u8981\u6dfb\u52a0 - name : logs mountPath : /var/log volumes : - name : logs # \u9700\u8981\u4fee\u6539 emptyDir : {} 11.RBAC\u8003\u9898 \u00b6 \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a deployment-clusterrole \u7684 clusterrole\uff0c\u8be5 clusterrole \u53ea\u5141\u8bb8\u521b\u5efa Deployment\u3001 Daemonset\u3001Statefulset \u7684 create \u64cd\u4f5c\u5728\u540d\u5b57\u4e3a app-team1 \u7684 namespace \u4e0b\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a cicd-token \u7684 serviceAccount\uff0c\u5e76\u4e14\u5c06\u4e0a\u4e00\u6b65\u521b\u5efa clusterrole \u7684\u6743\u9650\u7ed1\u5b9a\u5230\u8be5 serviceAccount \u89e3\u9898\u6b65\u9aa4: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" \u88ab\u5ffd\u7565\uff0c\u56e0\u4e3a ClusterRoles \u4e0d\u53d7\u540d\u5b57\u7a7a\u95f4\u9650\u5236 name : deployment-clusterrole rules : - apiGroups : [ \"apps\" ] # \u5728 HTTP \u5c42\u9762\uff0c\u7528\u6765\u8bbf\u95ee Secret \u8d44\u6e90\u7684\u540d\u79f0\u4e3a \"secrets\" resources : [ \"deployments\" , \"daemonsets\" , \"statefulsets\" ] verbs : [ \"create\" ] \u521b\u5efaserviceAccount: $ k create ns app-team1 $ k create sa cicd-token -n app-team1 $ k create rolebinding deployment-rolebinding \\ --clusterrole = 'deployment-clusterrole' \\ --serviceaccount = app-team1:cicd-token -n app-team1 12. NetworkPolicy \u00b6 \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u5b57\u4e3a allow-port-from-namespace \u7684 NetworkPolicy\uff0c\u8fd9\u4e2a NetworkPolicy \u5141\u8bb8internal \u547d\u540d\u7a7a\u95f4\u4e0b\u7684 Pod \u8bbf\u95ee\u8be5\u547d\u540d\u7a7a\u95f4\u4e0b\u7684 9000 \u7aef\u53e3\u3002\u5e76\u4e14\u4e0d\u5141\u8bb8\u4e0d\u662f internal \u547d\u4ee4\u7a7a\u95f4\u7684\u4e0b\u7684 Pod \u8bbf\u95ee\u4e0d\u5141\u8bb8\u8bbf\u95ee\u6ca1\u6709\u76d1\u542c 9000 \u7aef\u53e3\u7684 Pod\u3002 \u89e3\u9898\u6b65\u9aa4: $ cat 12-1-cka.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-port-from-namespace namespace : internal spec : ingress : - from : - podSelector : {} ports : - port : 9000 protocol : TCP podSelector : {} policyTypes : - Ingress 13. PersistentVolume \u00b6 \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a pv\uff0c\u540d\u5b57\u4e3a app-config\uff0c\u5927\u5c0f\u4e3a 2Gi\uff0c\u8bbf\u95ee\u6743\u9650\u4e3a ReadWriteMany\u3002Volume \u7684\u7c7b\u578b\u4e3a hostPath\uff0c\u8def\u5f84\u4e3a/srv/app-config apiVersion : v1 kind : PersistentVolume metadata : name : app-config labels : type : local spec : storageClassName : manual capacity : storage : 2Gi accessModes : - ReadWriteMany hostPath : path : \"/srv/app-config\" 14. CSI & PersistentVolumeClaim \u00b6 \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u5b57\u4e3a pv-volume \u7684 pvc\uff0c\u6307\u5b9a storageClass \u4e3a csi-hostpath-sc\uff0c\u5927\u5c0f\u4e3a 10Mi \u7136\u540e\u521b\u5efa\u4e00\u4e2a Pod\uff0c\u540d\u5b57\u4e3a web-server\uff0c\u955c\u50cf\u4e3a nginx\uff0c\u5e76\u4e14\u6302\u8f7d\u8be5 PVC \u81f3/usr/share/nginx/html\uff0c \u6302\u8f7d\u7684\u6743\u9650\u4e3a ReadWriteOnce\u3002\u4e4b\u540e\u901a\u8fc7 kubectl edit \u6216\u8005 kubectl path \u5c06 pvc \u6539\u6210 70Mi\uff0c\u5e76\u4e14\u8bb0\u5f55\u4fee\u6539\u8bb0\u5f55\u3002 \u51c6\u5907\u5de5\u4f5c: $ mkdir csi-hostpath $ cd csi-hostpath $ git clone https://gitee.com/dukuan/k8s-ha-install.git $ cd k8s-ha-install/ $ git checkout manual-installation-v1.20.x-csi-hostpath $ kubectl create -f snapshotter/ $ kubectl get volumesnapshotclasses.snapshot.storage.k8s.io $ kubectl get volumesnapshots.snapshot.storage.k8s.io $ kubectl get volumesnapshotcontents.snapshot.storage.k8s.io # \u5982\u679c\u8fd4\u56de\u503c\u4e0d\u662ferror: the server doesn't have a resource type \"volumesnapshotclasses\"\u8868\u793a\u5b89\u88c5\u6210\u529f # \u6709\u8fd4\u56de\u503c\u8bf4\u660eSnapshot Controller\u5df2\u7ecf\u5b89\u88c5 $ kubectl get pods --all-namespaces -o = jsonpath = '{range .items[*]}{\"\\n\"}{range .spec.containers[*]}{.image}{\", \"}{end}{end}' | grep snapshot-controller $ \u5b89\u88c5csi-hostpath $ cd csi-hostpath/ $ kubectl apply -f . $ \u521b\u5efastorageClass $ cd examples/ $ kubectl create -f csi-storageclass.yaml \u89e3\u9898\u6b65\u9aa4: \u521b\u5efa\u4e00\u4e2apvc apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pv-volume spec : storageClassName : csi-hostpath-sc accessModes : - ReadWriteOnce resources : requests : storage : 10Mi \u521b\u5efa\u4e00\u4e2apod\u6302\u5728pvc apiVersion : v1 kind : Pod metadata : name : web-server spec : volumes : - name : pv-volume persistentVolumeClaim : claimName : pv-volume containers : - name : task-pv-container image : nginx ports : - containerPort : 80 name : \"http-server\" volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : pv-volume kubectl patch pvc pv-volume -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\": \"70Mi\"}}}}' --record 15. Etcd \u5907\u4efd\u6062\u590d \u00b6 \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ \u4e2d\u6587\u89e3\u91ca: \u9488\u5bf9 etcd \u5b9e\u4f8b https://127.0.0.1:2379 \u521b\u5efa\u4e00\u4e2a\u5feb\u7167\uff0c\u4fdd\u5b58\u5230/srv/data/etcd-snapshot.db\u3002 \u5728\u521b\u5efa\u5feb\u7167\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u5361\u4f4f\u4e86\uff0c\u5c31\u952e\u5165 ctrl+c \u7ec8\u6b62\uff0c\u7136\u540e\u91cd\u8bd5\u3002 \u7136\u540e\u6062\u590d\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684\u5feb\u7167\uff1a /var/lib/backup/etcd-snapshot-previous.db \u6267\u884c etcdctl \u547d\u4ee4\u7684\u8bc1\u4e66\u5b58\u653e\u5728\uff1a ca \u8bc1\u4e66\uff1a/opt/KUIN00601/ca.crt \u5ba2\u6237\u7aef\u8bc1\u4e66\uff1a/opt/KUIN00601/etcd-client.crt \u5ba2\u6237\u7aef\u5bc6\u94a5\uff1a/opt/KUIN00601/etcd-client.key \u89e3\u9898\u6b65\u9aa4: \u7ec3\u4e60\u65f6\u9700\u8981\u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84: [ cka ] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls = https://192.168.159.81:2379 - --cert-file = /etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth = true - --data-dir = /var/lib/etcd - --initial-advertise-peer-urls = https://192.168.159.81:2380 - --initial-cluster = master0 = https://192.168.159.81:2380 - --key-file = /etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls = https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls = http://127.0.0.1:2381 - --listen-peer-urls = https://192.168.159.81:2380 - --name = master0 - --peer-cert-file = /etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth = true - --peer-key-file = /etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count = 10000 - --trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt \u8fd9\u4e2a\u8bc1\u4e66\u7684\u8def\u5f84\u6839\u636e\u9898\u76ee\u63d0\u4f9b\u7684\u5199\u5c31\u53ef\u4ee5\u4e86 \u5907\u4efd: apt install etcd-client export ETCDCTL_API = 3 mkdir /srv/data/ etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u8fd8\u539f\u6570\u636e: \u914d\u7f6e\u6587\u4ef6\u53ef\u80fd\u4e0d\u5728\u539f\u6765\u7684\u4f4d\u7f6e,\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u7684\u65b9\u5f0f\u6765\u67e5\u627e \u6a21\u62df\u5220\u9664: (\u5220\u9664 pod \u8fd9\u6837 etcd \u7684\u6570\u636e\u5c31\u4f1a\u6ca1\u6709\u8fd9\u4e2a\u6570\u636e) \u6a21\u62df\u8fd8\u539f: mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak export ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/server.crt --key = /etc/kubernetes/pki/etcd/server.key snapshot restore /src/data/etcd-snapshot.db --data-dir = /var/lib/etcd-restore --skip-hash-check mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests systemctl restart kubelet 16. k8s \u5347\u7ea7 \u00b6 \u4e2d\u6587\u89e3\u91ca: \u4ec5\u9700\u8981\u5347\u7ea7 master \u8282\u70b9,\u548c kubelet,kubectl\u3002\u4e0d\u9700\u8981\u5347\u7ea7 etcd \u7b49\u5176\u4ed6\u7ec4\u4ef6 \u89e3\u9898\u6b65\u9aa4: $ k cordon master0 $ k drain master0 --delete-emptydir-data --ignore-daemonsets --force \u4e4b\u540e\u9700\u8981\u6309\u7167\u9898\u76ee\u63d0\u793a ssh \u5230\u4e00\u4e2a master \u8282\u70b9 $ apt update $ apt-cache policy kubeadm | grep 1 .19.0 # (\u6ce8\u610f\u7248\u672c\u7684\u5dee\u5f02\uff0c\u6709\u53ef\u80fd\u5e76\u975e 1.18.8 \u5347\u7ea7\u5230 1 .19 ) $ apt-get install kubeadm = 1 .19.0-00 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.19.0 \u5f00\u59cb\u5347\u7ea7 Master \u8282\u70b9 kubeadm upgrade apply v1.20.9 --etcd-upgrade = false # \u6ce8\u610f\u8fd9\u91cc\u4e0d\u9700\u8981\u5347\u7ea7 etcd \u5347\u7ea7kubelet\u548ckubectl apt-get install -y kubelet = 1 .19.0-00 kubectl = 1 .19.0-00 $ systemctl daemon-reload $ systemctl restart kubelet $ kubectl uncordon k8s-master 17. \u96c6\u7fa4\u6545\u969c\u6392\u67e5 \u2013 kubelet \u6545\u969c \u00b6 \u4e2d\u6587\u89e3\u91ca: \u4e00\u4e2a\u540d\u4e3a wk8s-node-0 \u7684\u8282\u70b9\u72b6\u6001\u4e3a NotReady\uff0c\u8ba9\u5176\u4ed6\u6062\u590d\u81f3\u6b63\u5e38\u72b6\u6001\uff0c\u5e76\u786e\u8ba4\u6240\u6709\u7684\u66f4\u6539\u5f00\u673a\u81ea\u52a8\u5b8c\u6210 $ ssh wk8s-node-0 $ sudo -i # systemctl status kubelet # systemctl start kubelet # systemctl enable kubelet \u53d8\u66f4\u9898: \u00b6 \u5982\u679cfubar\u6ca1\u6709\u6253\u6807\u7b7e\uff0c\u9700\u8981\u6253\u4e00\u4e2a\u6807\u7b7e $ cat network-policy.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-port-from-namespace namespace : my-app spec : egress : - to : - namespaceSelector : matchLabels : name : fubar ports : - protocol : TCP port : 53 ingress : - from : - podSelector : {} ports : - protocol : TCP port : 80 podSelector : {} policyTypes : - Ingress - Egress","title":"CKA\u8003\u9898"},{"location":"cks/cka/#cka","text":"","title":"cka \u8ba4\u8bc1\u8003\u8bd5"},{"location":"cks/cka/#1-pod-5","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u76d1\u63a7\u540d\u4e3a foobar \u7684 Pod \u7684\u65e5\u5fd7\uff0c\u5e76\u8fc7\u6ee4\u51fa\u5177\u6709 unable-access-website \u4fe1\u606f\u7684\u884c\uff0c\u7136\u540e\u5c06\u5199\u5165\u5230 /opt/KUTR00101/foobar \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl logs foobar | grep unable-access-website > /opt/KUTR00101/foobar","title":"1. \u76d1\u63a7 pod\u65e5\u5fd7 5%"},{"location":"cks/cka/#2-pod-5","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u627e\u51fa\u5177\u6709 name=cpu-user \u7684 Pod\uff0c\u5e76\u8fc7\u6ee4\u51fa\u4f7f\u7528 CPU \u6700\u9ad8\u7684 Pod\uff0c\u7136\u540e\u628a\u5b83\u7684\u540d\u5b57\u5199\u5728\u5df2\u7ecf\u5b58\u5728\u7684/opt/KUTR00401/KUTR00401.txt \u6587\u4ef6\u91cc \uff08\u6ce8\u610f\u4ed6\u6ca1\u6709\u8bf4\u6307\u5b9a namespace\u3002\u6240\u4ee5\u9700\u8981\u4f7f\u7528-A \u6307\u5b9a\u6240\u4ee5 namespace\uff09 \u6ce8\u610f\u662f \u8ffd\u52a0 \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl top pod -A -l | grep name = cpu-user # \u6ce8\u610f\u8fd9\u91cc\u7684 pod \u540d\u5b57\u4ee5\u5b9e\u9645\u540d\u5b57\u4e3a\u51c6\uff0c\u6309\u7167 CPU \u90a3\u4e00\u5217\u8fdb\u884c\u9009\u62e9\u4e00\u4e2a\u6700\u5927\u7684 Pod\uff0c\u53e6\u5916\u5982\u679c CPU \u7684\u6570\u503c\u662f 1 2 3 \u8fd9\u6837\u7684\u3002\u662f\u5927\u4e8e\u5e26 m \u8fd9\u6837\u7684\uff0c\u56e0\u4e3a 1 \u9897 CPU \u7b49\u4e8e 1000m\uff0c\u6ce8\u610f\u8981\u7528>>\u800c\u4e0d\u662f> $ echo \"coredns-54d67798b7-hl8xc\" >> /opt/KUTR00401/KUTR00401.txt","title":"2.\u76d1\u63a7 pod \u5ea6\u91cf \u6307\u6807: 5%"},{"location":"cks/cka/#3deployment","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u6269\u5bb9\u540d\u5b57\u4e3a loadbalancer \u7684 deployment \u7684\u526f\u672c\u6570\u4e3a 6 \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl scale --replicas = 6 deployment loadbalancer $ kubectl edit","title":"3.Deployment \u6269\u7f29\u5bb9"},{"location":"cks/cka/#4-node","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u68c0\u67e5\u96c6\u7fa4\u4e2d\u6709\u591a\u5c11\u8282\u70b9\u4e3a Ready \u72b6\u6001\uff0c\u5e76\u4e14\u53bb\u9664\u5305\u542b NoSchedule \u6c61\u70b9\u7684\u8282\u70b9\u3002\u4e4b\u540e\u5c06\u6570\u5b57\u5199\u5230/opt/KUSC00402/kusc00402.txt \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context k8s $ kubectl get node | grep -i ready # \u8bb0\u5f55\u603b\u6570\u4e3a A $ kubectl describe node | grep Taint | grep NoSchedule # \u8bb0\u5f55\u603b\u6570\u4e3a B # \u5c06 A \u51cf B \u7684\u503c x \u5bfc\u5165\u5230/opt/KUSC00402/kusc00402.txt $ echo x >> /opt/KUSC00402/kusc00402.txt","title":"4. \u68c0\u67e5 Node \u8282\u70b9\u7684\u5065\u5eb7\u72b6\u6001"},{"location":"cks/cka/#5","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u5c06 ek8s-node-1 \u8282\u70b9\u8bbe\u7f6e\u4e3a\u4e0d\u53ef\u7528\uff0c\u7136\u540e\u91cd\u65b0\u8c03\u5ea6\u8be5\u8282\u70b9\u4e0a\u7684\u6240\u6709 Pod \u89e3\u9898\u6b65\u9aa4: $ kubectl config use-context ek8s $ kubectl cordon ek8s-node-1 $ kubectl drain ek8s-node-1 --delete-emptydir-data --ignore-daemonsets --force $ k delete pod pod-name --grace-period = 0 --force","title":"5. \u8282\u70b9\u7ef4\u62a4"},{"location":"cks/cka/#6","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a Pod\uff0c\u540d\u5b57\u4e3a nginx-kusc00401\uff0c\u955c\u50cf\u5730\u5740\u662f nginx\uff0c\u8c03\u5ea6\u5230\u5177\u6709 disk=spinning \u6807\u7b7e\u7684\u8282\u70b9\u4e0a \uff0c \u8be5\u9898\u53ef\u4ee5\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/assign-pods-nodes/ \u8be5\u9898\u53ef\u4ee5\u53c2\u8003\u94fe\u63a5\uff1a https://kubernetes.io/zh/docs/concepts/scheduling-eviction/assign-pod-node/ \u89e3\u9898\u6b65\u9aa4: \u9898\u76ee\u4e00\uff1a apiVersion : v1 kind : Pod metadata : name : nginx-kusc00401 labels : role : nginx-kusc00401 spec : containers : - name : nginx image : nginx nodeSelector : disk : spinning \u9898\u76ee\u4e8c\uff1a $ cat 6-2-cka.yaml apiVersion : v1 kind : Pod metadata : name : nginx-kusc00402 labels : role : nginx-kusc00402 spec : nodeName : node1 containers : - name : nginx image : nginx","title":"6. \u6307\u5b9a\u8282\u70b9\u90e8\u7f72"},{"location":"cks/cka/#7-pod","text":"\u56fe\u793a\uff1a \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a Pod \uff0c \u540d\u5b57\u4e3a kucc1 \uff0c\u8fd9\u4e2a Pod \u53ef\u80fd\u5305\u542b 1-4 \u5bb9 \u5668 \uff0c \u8be5\u9898\u4e3a\u56db\u4e2a \uff1anginx+redis+memcached+consul \u89e3\u9898\u6b65\u9aa4: $ cat 7-cka.yaml apiVersion : v1 kind : Pod metadata : name : kucc1 spec : containers : - name : nginx image : nginx - name : redis image : redis - name : memcached image : memcached - name : consul image : consul","title":"7. \u4e00\u4e2a Pod \u591a\u4e2a\u5bb9\u5668"},{"location":"cks/cka/#8-service","text":"\u4e2d\u6587\u89e3\u91ca: \u91cd\u65b0\u914d\u7f6e\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684 deployment front-end\uff0c\u5728\u540d\u5b57\u4e3a nginx \u7684\u5bb9\u5668\u91cc\u9762\u6dfb\u52a0\u4e00\u4e2a\u7aef\u53e3\u914d\u7f6e\uff0c\u540d\u5b57\u4e3a http\uff0c\u66b4\u9732\u7aef\u53e3\u53f7\u4e3a 80\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a service\uff0c\u540d\u5b57\u4e3a front-end-svc\uff0c\u66b4\u9732\u8be5deployment \u7684 http \u7aef\u53e3\uff0c\u5e76\u4e14 service \u7684\u7c7b\u578b\u4e3a NodePort\u3002 ports : - containerPort : 80 name : http protocol : TCP \u521b\u5efa\u4e00\u4e2asvc kubectl expose deploy front-end --name = front-end-svc --port = 80 \\ --target-port = http --type = NodePort","title":"8. Service \u8003\u9898"},{"location":"cks/cka/#9-ingress-8","text":"\u53c2\u8003\u5730\u5740: https://kubernetes.io/zh-cn/docs/concepts/services-networking/ingress/ \u56fe\u793a: \u73af\u5883\u51c6\u5907: [ cka ] root@master0:/home/lixie# k label node node1 ingress = true node/node1 labeled \u4e2d\u6587\u89e3\u91ca: \u5728 ing-internal \u547d\u540d\u7a7a\u95f4\u4e0b\u521b\u5efa\u4e00\u4e2a ingress\uff0c\u540d\u5b57\u4e3a pong\uff0c\u4ee3\u7406\u7684 service hi\uff0c\u7aef\u53e3\u4e3a 5678\uff0c\u914d\u7f6e\u8def\u5f84/hi\u3002 \u9a8c\u8bc1\uff1a\u8bbf\u95ee curl -kL /hi \u4f1a\u8fd4\u56de hi \u89e3\u9898\u6b65\u9aa4: $ k label node node2 ingress = true $ k apply -f ingress.yaml [ cka ] root@master0:~/cka# cat 9-ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : pong # \u540d\u79f0\u9700\u8981\u8be5 namespace : ing-internal # \u9700\u8981\u8be5 spec : ingressClassName : nginx rules : - http : paths : - pathType : Prefix path : \"/hi\" # \u8bbf\u95ee\u8def\u5f84\u9700\u8981\u6539 backend : service : name : front-end-svc # \u9700\u8981\u8be5 port : number : 80","title":"9. Ingress\u8003\u9898 8%"},{"location":"cks/cka/#10-sidecar-8","text":"\u56fe\u793a: \u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/logging/ \u4e2d\u6587\u89e3\u91ca: \u6dfb\u52a0\u4e00\u4e2a\u540d\u4e3a busybox \u4e14\u955c\u50cf\u4e3a busybox \u7684 sidecar \u5230\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684\u540d\u4e3a legacy-app \u7684 Pod \u4e0a\uff0c\u8fd9\u4e2a sidecar \u7684\u542f\u52a8\u547d\u4ee4\u4e3a/bin/sh, -c, 'tail -n+1 -f /var/log/legacy-app.log'\u3002 \u5e76\u4e14\u8fd9\u4e2a sidecar \u548c\u539f\u6709\u7684\u955c\u50cf\u6302\u8f7d\u4e00\u4e2a\u540d\u4e3a logs \u7684 volume\uff0c\u6302\u8f7d\u7684\u76ee\u5f55\u4e3a/var/log/ \u5bfc\u51fayaml [ cka ] root@master0:~/cka# cat 10-1.yaml apiVersion : v1 kind : Pod metadata : name : legacy-app spec : containers : - name : count image : busybox args : - /bin/sh - -c - > i=0; while true; do echo \"$(date) INFO $i\" >> /var/log/legacy-app.log; i=$((i+1)); sleep 1; done \u9996\u5148\u5c06 legacy-app \u7684 Pod \u7684 yaml \u5bfc\u51fa\uff0c\u5927\u81f4\u5982\u4e0b\uff1a kubectl get pod legacy-app -o yaml > 10-2.yaml \u8fdb\u884c\u4fee\u6539 [ cka ] root@master0:~/cka# cat 10-2.yaml apiVersion : v1 kind : Pod metadata : name : legacy-app namespace : default spec : containers : - name : busybox # \u9700\u8981\u4fee\u6539 image : busybox # \u9700\u8981\u4fee\u6539 args : [ /bin/sh , -c , 'tail -n+1 -F /var/log/legacy-app.log' ] volumeMounts : - name : logs # \u9700\u8981\u4fee\u6539 mountPath : /var/log - args : - /bin/sh - -c - | i=0; while true; do echo \"$(date) INFO $i\" >> /var/log/legacy-app.log; i=$((i+1)); sleep 1; done image : busybox imagePullPolicy : Always name : count resources : {} terminationMessagePath : /dev/termination-log terminationMessagePolicy : File volumeMounts : # \u9700\u8981\u6dfb\u52a0 - name : logs mountPath : /var/log volumes : - name : logs # \u9700\u8981\u4fee\u6539 emptyDir : {}","title":"10. Sidecar 8%"},{"location":"cks/cka/#11rbac","text":"\u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u4e3a deployment-clusterrole \u7684 clusterrole\uff0c\u8be5 clusterrole \u53ea\u5141\u8bb8\u521b\u5efa Deployment\u3001 Daemonset\u3001Statefulset \u7684 create \u64cd\u4f5c\u5728\u540d\u5b57\u4e3a app-team1 \u7684 namespace \u4e0b\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a cicd-token \u7684 serviceAccount\uff0c\u5e76\u4e14\u5c06\u4e0a\u4e00\u6b65\u521b\u5efa clusterrole \u7684\u6743\u9650\u7ed1\u5b9a\u5230\u8be5 serviceAccount \u89e3\u9898\u6b65\u9aa4: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : # \"namespace\" \u88ab\u5ffd\u7565\uff0c\u56e0\u4e3a ClusterRoles \u4e0d\u53d7\u540d\u5b57\u7a7a\u95f4\u9650\u5236 name : deployment-clusterrole rules : - apiGroups : [ \"apps\" ] # \u5728 HTTP \u5c42\u9762\uff0c\u7528\u6765\u8bbf\u95ee Secret \u8d44\u6e90\u7684\u540d\u79f0\u4e3a \"secrets\" resources : [ \"deployments\" , \"daemonsets\" , \"statefulsets\" ] verbs : [ \"create\" ] \u521b\u5efaserviceAccount: $ k create ns app-team1 $ k create sa cicd-token -n app-team1 $ k create rolebinding deployment-rolebinding \\ --clusterrole = 'deployment-clusterrole' \\ --serviceaccount = app-team1:cicd-token -n app-team1","title":"11.RBAC\u8003\u9898"},{"location":"cks/cka/#12-networkpolicy","text":"\u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u5b57\u4e3a allow-port-from-namespace \u7684 NetworkPolicy\uff0c\u8fd9\u4e2a NetworkPolicy \u5141\u8bb8internal \u547d\u540d\u7a7a\u95f4\u4e0b\u7684 Pod \u8bbf\u95ee\u8be5\u547d\u540d\u7a7a\u95f4\u4e0b\u7684 9000 \u7aef\u53e3\u3002\u5e76\u4e14\u4e0d\u5141\u8bb8\u4e0d\u662f internal \u547d\u4ee4\u7a7a\u95f4\u7684\u4e0b\u7684 Pod \u8bbf\u95ee\u4e0d\u5141\u8bb8\u8bbf\u95ee\u6ca1\u6709\u76d1\u542c 9000 \u7aef\u53e3\u7684 Pod\u3002 \u89e3\u9898\u6b65\u9aa4: $ cat 12-1-cka.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-port-from-namespace namespace : internal spec : ingress : - from : - podSelector : {} ports : - port : 9000 protocol : TCP podSelector : {} policyTypes : - Ingress","title":"12. NetworkPolicy"},{"location":"cks/cka/#13-persistentvolume","text":"\u6587\u7ae0\u53c2\u8003: https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a pv\uff0c\u540d\u5b57\u4e3a app-config\uff0c\u5927\u5c0f\u4e3a 2Gi\uff0c\u8bbf\u95ee\u6743\u9650\u4e3a ReadWriteMany\u3002Volume \u7684\u7c7b\u578b\u4e3a hostPath\uff0c\u8def\u5f84\u4e3a/srv/app-config apiVersion : v1 kind : PersistentVolume metadata : name : app-config labels : type : local spec : storageClassName : manual capacity : storage : 2Gi accessModes : - ReadWriteMany hostPath : path : \"/srv/app-config\"","title":"13. PersistentVolume"},{"location":"cks/cka/#14-csi-persistentvolumeclaim","text":"\u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/tasks/configure-pod-container/configure-persistent-volume-storage/ \u56fe\u793a: \u4e2d\u6587\u89e3\u91ca: \u521b\u5efa\u4e00\u4e2a\u540d\u5b57\u4e3a pv-volume \u7684 pvc\uff0c\u6307\u5b9a storageClass \u4e3a csi-hostpath-sc\uff0c\u5927\u5c0f\u4e3a 10Mi \u7136\u540e\u521b\u5efa\u4e00\u4e2a Pod\uff0c\u540d\u5b57\u4e3a web-server\uff0c\u955c\u50cf\u4e3a nginx\uff0c\u5e76\u4e14\u6302\u8f7d\u8be5 PVC \u81f3/usr/share/nginx/html\uff0c \u6302\u8f7d\u7684\u6743\u9650\u4e3a ReadWriteOnce\u3002\u4e4b\u540e\u901a\u8fc7 kubectl edit \u6216\u8005 kubectl path \u5c06 pvc \u6539\u6210 70Mi\uff0c\u5e76\u4e14\u8bb0\u5f55\u4fee\u6539\u8bb0\u5f55\u3002 \u51c6\u5907\u5de5\u4f5c: $ mkdir csi-hostpath $ cd csi-hostpath $ git clone https://gitee.com/dukuan/k8s-ha-install.git $ cd k8s-ha-install/ $ git checkout manual-installation-v1.20.x-csi-hostpath $ kubectl create -f snapshotter/ $ kubectl get volumesnapshotclasses.snapshot.storage.k8s.io $ kubectl get volumesnapshots.snapshot.storage.k8s.io $ kubectl get volumesnapshotcontents.snapshot.storage.k8s.io # \u5982\u679c\u8fd4\u56de\u503c\u4e0d\u662ferror: the server doesn't have a resource type \"volumesnapshotclasses\"\u8868\u793a\u5b89\u88c5\u6210\u529f # \u6709\u8fd4\u56de\u503c\u8bf4\u660eSnapshot Controller\u5df2\u7ecf\u5b89\u88c5 $ kubectl get pods --all-namespaces -o = jsonpath = '{range .items[*]}{\"\\n\"}{range .spec.containers[*]}{.image}{\", \"}{end}{end}' | grep snapshot-controller $ \u5b89\u88c5csi-hostpath $ cd csi-hostpath/ $ kubectl apply -f . $ \u521b\u5efastorageClass $ cd examples/ $ kubectl create -f csi-storageclass.yaml \u89e3\u9898\u6b65\u9aa4: \u521b\u5efa\u4e00\u4e2apvc apiVersion : v1 kind : PersistentVolumeClaim metadata : name : pv-volume spec : storageClassName : csi-hostpath-sc accessModes : - ReadWriteOnce resources : requests : storage : 10Mi \u521b\u5efa\u4e00\u4e2apod\u6302\u5728pvc apiVersion : v1 kind : Pod metadata : name : web-server spec : volumes : - name : pv-volume persistentVolumeClaim : claimName : pv-volume containers : - name : task-pv-container image : nginx ports : - containerPort : 80 name : \"http-server\" volumeMounts : - mountPath : \"/usr/share/nginx/html\" name : pv-volume kubectl patch pvc pv-volume -p '{\"spec\":{\"resources\":{\"requests\":{\"storage\": \"70Mi\"}}}}' --record","title":"14. CSI &amp; PersistentVolumeClaim"},{"location":"cks/cka/#15-etcd","text":"\u6587\u7ae0\u53c2\u8003: https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/ \u4e2d\u6587\u89e3\u91ca: \u9488\u5bf9 etcd \u5b9e\u4f8b https://127.0.0.1:2379 \u521b\u5efa\u4e00\u4e2a\u5feb\u7167\uff0c\u4fdd\u5b58\u5230/srv/data/etcd-snapshot.db\u3002 \u5728\u521b\u5efa\u5feb\u7167\u7684\u8fc7\u7a0b\u4e2d\uff0c\u5982\u679c\u5361\u4f4f\u4e86\uff0c\u5c31\u952e\u5165 ctrl+c \u7ec8\u6b62\uff0c\u7136\u540e\u91cd\u8bd5\u3002 \u7136\u540e\u6062\u590d\u4e00\u4e2a\u5df2\u7ecf\u5b58\u5728\u7684\u5feb\u7167\uff1a /var/lib/backup/etcd-snapshot-previous.db \u6267\u884c etcdctl \u547d\u4ee4\u7684\u8bc1\u4e66\u5b58\u653e\u5728\uff1a ca \u8bc1\u4e66\uff1a/opt/KUIN00601/ca.crt \u5ba2\u6237\u7aef\u8bc1\u4e66\uff1a/opt/KUIN00601/etcd-client.crt \u5ba2\u6237\u7aef\u5bc6\u94a5\uff1a/opt/KUIN00601/etcd-client.key \u89e3\u9898\u6b65\u9aa4: \u7ec3\u4e60\u65f6\u9700\u8981\u67e5\u770b\u8bc1\u4e66\u7684\u8def\u5f84: [ cka ] root@master0:/home/lixie# cat /etc/kubernetes/manifests/etcd.yaml apiVersion: v1 kind: Pod metadata: annotations: kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.168.159.81:2379 creationTimestamp: null labels: component: etcd tier: control-plane name: etcd namespace: kube-system spec: containers: - command: - etcd - --advertise-client-urls = https://192.168.159.81:2379 - --cert-file = /etc/kubernetes/pki/etcd/server.crt # \u6307\u5b9a crt \u6587\u4ef6 - --client-cert-auth = true - --data-dir = /var/lib/etcd - --initial-advertise-peer-urls = https://192.168.159.81:2380 - --initial-cluster = master0 = https://192.168.159.81:2380 - --key-file = /etc/kubernetes/pki/etcd/server.key # \u6307\u5b9a key \u6587\u4ef6 - --listen-client-urls = https://127.0.0.1:2379,https://192.168.159.81:2379 - --listen-metrics-urls = http://127.0.0.1:2381 - --listen-peer-urls = https://192.168.159.81:2380 - --name = master0 - --peer-cert-file = /etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth = true - --peer-key-file = /etc/kubernetes/pki/etcd/peer.key - --peer-trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt # \u6307\u5b9a\u8bc1\u4e66\u6587\u4ef6 - --snapshot-count = 10000 - --trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt \u8fd9\u4e2a\u8bc1\u4e66\u7684\u8def\u5f84\u6839\u636e\u9898\u76ee\u63d0\u4f9b\u7684\u5199\u5c31\u53ef\u4ee5\u4e86 \u5907\u4efd: apt install etcd-client export ETCDCTL_API = 3 mkdir /srv/data/ etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save /srv/data/etcd-snapshot.db \u8fd8\u539f\u6570\u636e: \u914d\u7f6e\u6587\u4ef6\u53ef\u80fd\u4e0d\u5728\u539f\u6765\u7684\u4f4d\u7f6e,\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u7684\u65b9\u5f0f\u6765\u67e5\u627e \u6a21\u62df\u5220\u9664: (\u5220\u9664 pod \u8fd9\u6837 etcd \u7684\u6570\u636e\u5c31\u4f1a\u6ca1\u6709\u8fd9\u4e2a\u6570\u636e) \u6a21\u62df\u8fd8\u539f: mv /etc/kubernetes/manifests /etc/kubernetes/manifests.bak export ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 --cacert = /etc/kubernetes/pki/etcd/ca.crt --cert = /etc/kubernetes/pki/etcd/server.crt --key = /etc/kubernetes/pki/etcd/server.key snapshot restore /src/data/etcd-snapshot.db --data-dir = /var/lib/etcd-restore --skip-hash-check mv /etc/kubernetes/manifests.bak /etc/kubernetes/manifests systemctl restart kubelet","title":"15. Etcd \u5907\u4efd\u6062\u590d"},{"location":"cks/cka/#16-k8s","text":"\u4e2d\u6587\u89e3\u91ca: \u4ec5\u9700\u8981\u5347\u7ea7 master \u8282\u70b9,\u548c kubelet,kubectl\u3002\u4e0d\u9700\u8981\u5347\u7ea7 etcd \u7b49\u5176\u4ed6\u7ec4\u4ef6 \u89e3\u9898\u6b65\u9aa4: $ k cordon master0 $ k drain master0 --delete-emptydir-data --ignore-daemonsets --force \u4e4b\u540e\u9700\u8981\u6309\u7167\u9898\u76ee\u63d0\u793a ssh \u5230\u4e00\u4e2a master \u8282\u70b9 $ apt update $ apt-cache policy kubeadm | grep 1 .19.0 # (\u6ce8\u610f\u7248\u672c\u7684\u5dee\u5f02\uff0c\u6709\u53ef\u80fd\u5e76\u975e 1.18.8 \u5347\u7ea7\u5230 1 .19 ) $ apt-get install kubeadm = 1 .19.0-00 # \u9a8c\u8bc1\u5347\u7ea7\u8ba1\u5212 $ kubeadm upgrade plan # \u770b\u5230\u5982\u4e0b\u4fe1\u606f\uff0c\u53ef\u5347\u7ea7\u5230\u6307\u5b9a\u7248\u672c You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.19.0 \u5f00\u59cb\u5347\u7ea7 Master \u8282\u70b9 kubeadm upgrade apply v1.20.9 --etcd-upgrade = false # \u6ce8\u610f\u8fd9\u91cc\u4e0d\u9700\u8981\u5347\u7ea7 etcd \u5347\u7ea7kubelet\u548ckubectl apt-get install -y kubelet = 1 .19.0-00 kubectl = 1 .19.0-00 $ systemctl daemon-reload $ systemctl restart kubelet $ kubectl uncordon k8s-master","title":"16. k8s \u5347\u7ea7"},{"location":"cks/cka/#17-kubelet","text":"\u4e2d\u6587\u89e3\u91ca: \u4e00\u4e2a\u540d\u4e3a wk8s-node-0 \u7684\u8282\u70b9\u72b6\u6001\u4e3a NotReady\uff0c\u8ba9\u5176\u4ed6\u6062\u590d\u81f3\u6b63\u5e38\u72b6\u6001\uff0c\u5e76\u786e\u8ba4\u6240\u6709\u7684\u66f4\u6539\u5f00\u673a\u81ea\u52a8\u5b8c\u6210 $ ssh wk8s-node-0 $ sudo -i # systemctl status kubelet # systemctl start kubelet # systemctl enable kubelet","title":"17. \u96c6\u7fa4\u6545\u969c\u6392\u67e5 \u2013 kubelet \u6545\u969c"},{"location":"cks/cka/#_1","text":"\u5982\u679cfubar\u6ca1\u6709\u6253\u6807\u7b7e\uff0c\u9700\u8981\u6253\u4e00\u4e2a\u6807\u7b7e $ cat network-policy.yaml apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-port-from-namespace namespace : my-app spec : egress : - to : - namespaceSelector : matchLabels : name : fubar ports : - protocol : TCP port : 53 ingress : - from : - podSelector : {} ports : - protocol : TCP port : 80 podSelector : {} policyTypes : - Ingress - Egress","title":"\u53d8\u66f4\u9898:"},{"location":"docker/docker-docs/","text":"Docker \u57fa\u672c\u7ba1\u7406 \u00b6 \u968f\u7740\u8ba1\u7b97\u673a\u8fd1\u51e0\u5341\u5e74\u7684\u84ec\u52c3\u53d1\u5c55\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u4f18\u79c0\u7cfb\u7edf\u548c\u8f6f\u4ef6\u3002\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u5404 \u79cd\u8f6f\u4ef6\u5e94\u7528\u3002\u4f46\u540c\u65f6\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u9700\u8981\u7ef4\u62a4\u4e00\u4e2a\u975e\u5e38\u5e9e\u5927\u7684\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u751f\u4ea7\u73af\u5883\u3002 \u9762\u5bf9\u8fd9\u79cd \u60c5\u51b5\uff0cDocker \u5bb9\u5668\u6280\u672f\u6a2a\u7a7a\u51fa\u4e16\uff0c\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u9700\u8981\u8fc7\u591a\u5730\u6539\u53d8\u73b0\u6709 \u7684\u4f7f\u7528\u4e60\u60ef\uff0c\u5c31\u53ef\u4ee5\u548c\u5df2\u6709\u7684\u5de5\u5177\uff0c\u5982 OpenStack \u7b49\u914d\u5408\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u638c\u63e1 Docker \u76f8\u5173\u6280\u672f\u4e5f\u662f \u9014\u7ecf\u4e91\u8ba1\u7b97\u7684\u5fc5\u7ecf\u4e4b\u8def\u3002 \u672c\u7ae0\u5c06\u4f9d\u6b21\u4ecb\u7ecd Docker \u7684\u4e09\u5927\u6838\u5fc3\u6982\u5ff5\u2014\u2014\u955c\u50cf\u3001\u5bb9\u5668\u3001\u4ed3\u5e93\uff0c\u4ee5\u53ca\u5b89\u88c5 Docker \u4e0e\u4ecb\u7ecd\u56f4 \u7ed5\u955c\u50cf\u548c\u5bb9\u5668\u7684\u5177\u4f53\u64cd\u4f5c\u3002 1.1 Docker \u6982\u8ff0 \u00b6 \u56e0\u4e3a Docker \u8f7b\u4fbf\u3001\u5feb\u901f\u7684\u7279\u6027\uff0c\u53ef\u4ee5\u4f7f\u5e94\u7528\u8fbe\u5230\u5feb\u901f\u8fed\u4ee3\u7684\u76ee\u7684\u3002\u6bcf\u6b21\u5c0f\u7684\u53d8\u66f4\uff0c\u9a6c\u4e0a\u5c31\u53ef \u4ee5\u770b\u5230\u6548\u679c\uff0c\u800c\u4e0d\u7528\u5c06\u82e5\u5e72\u4e2a\u5c0f\u53d8\u66f4\u79ef\u6512\u5230\u4e00\u5b9a\u7a0b\u5ea6\u518d\u53d8\u66f4\u3002\u6bcf\u6b21\u53d8\u66f4\u4e00\u5c0f\u90e8\u5206\u5176\u5b9e\u662f\u4e00\u79cd\u975e\u5e38\u5b89 \u5168\u7684\u65b9\u5f0f\uff0c\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u80fd\u591f\u5feb\u901f\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 Docker \u5bb9\u5668\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u3001\u7cfb\u7edf\u7ba1\u7406\u5458\u3001\u8d28\u91cf\u7ba1\u7406\u548c\u7248\u672c\u63a7\u5236\u5de5\u7a0b\u5e08\u5728\u4e00\u4e2a\u751f\u4ea7\u73af\u8282\u4e2d \u4e00\u8d77\u534f\u540c\u5de5\u4f5c\u3002\u5236\u5b9a\u4e00\u5957\u5bb9\u5668\u6807\u51c6\u80fd\u591f\u4f7f\u7cfb\u7edf\u7ba1\u7406\u5458\u66f4\u6539\u5bb9\u5668\u7684\u65f6\u5019\uff0c\u7a0b\u5e8f\u5458\u4e0d\u9700\u8981\u5173\u5fc3\u5bb9\u5668\u7684\u53d8 \u5316\uff0c\u800c\u66f4\u4e13\u6ce8\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u3002\u4ece\u800c\u9694\u79bb\u5f00\u4e86\u5f00\u53d1\u548c\u7ba1\u7406\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6210\u672c\u3002 1.1.1 \u4ec0\u4e48\u662f Docker \u00b6 \u5982\u679c\u8981\u65b9\u4fbf\u7684\u521b\u5efa\u8fd0\u884c\u5728\u4e91\u5e73\u53f0\u4e0a\u7684\u5e94\u7528\uff0c\u5fc5\u987b\u8981\u8131\u79bb\u5e95\u5c42\u7684\u786c\u4ef6\uff0c\u540c\u65f6\u8fd8\u9700\u8981\u4efb\u4f55\u65f6 \u95f4\u5730 \u70b9\u53ef\u83b7\u53d6\u8fd9\u4e9b\u8d44\u6e90\uff0c\u8fd9\u6b63\u662f Docker \u6240\u80fd\u63d0\u4f9b\u7684\u3002Docker \u7684\u5bb9\u5668\u6280\u672f\u53ef\u4ee5\u5728\u4e00\u53f0\u4e3b\u673a\u4e0a\u8f7b\u677e\u4e3a\u4efb\u4f55 \u5e94\u7528\u521b\u5efa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u79fb\u690d\u7684\u3001\u81ea\u7ed9\u81ea\u8db3\u7684\u5bb9\u5668\u3002\u901a\u8fc7\u8fd9\u79cd\u5bb9\u5668\u6253\u5305\u5e94\u7528\u7a0b\u5e8f\uff0c\u610f\u5473\u7740\u7b80\u5316\u4e86 \u91cd\u65b0\u90e8\u7f72\u3001\u8c03\u8bd5\u8fd9\u4e9b\u7410\u788e\u7684\u91cd\u590d\u5de5\u4f5c\uff0c\u6781\u5927\u7684\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6548\u7387\u3002 1.1.2 Docker \u7684\u4f18\u52bf \u00b6 Docker \u5bb9\u5668\u8fd0\u884c\u901f\u5ea6\u5f88\u5feb\uff0c\u542f\u52a8\u548c\u505c\u6b62\u53ef\u4ee5\u5728\u79d2\u7ea7\u5b9e\u73b0\uff0c\u6bd4\u4f20\u7edf\u865a\u62df\u673a\u8981\u5feb\u5f88\u591a\uff1bDocker \u6838 \u5fc3\u89e3\u51b3\u7684\u95ee\u9898\u662f\u5229\u7528\u5bb9\u5668\u6765\u5b9e\u73b0\u7c7b\u4f3c\u865a\u62df\u673a\u7684\u529f\u80fd\uff0c\u4ece\u800c\u5229\u7528\u66f4\u52a0\u8282\u7701\u7684\u786c\u4ef6\u8d44\u6e90\u63d0\u4f9b\u7ed9\u7528\u6237\u66f4\u591a \u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u56e0\u6b64\uff0cDocker \u5bb9\u5668\u9664\u4e86\u8fd0\u884c\u5176\u4e2d\u7684\u5e94\u7528\u4e4b\u5916\uff0c\u57fa\u672c\u4e0d\u6d88\u8017\u989d\u5916\u7684\u7cfb\u7edf\u8d44\u6e90\uff0c\u5728\u4fdd\u8bc1 \u5e94\u7528\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53c8\u51cf\u5c0f\u4e86\u7cfb\u7edf\u5f00\u9500\uff0c\u4f7f\u5f97\u4e00\u53f0\u4e3b\u673a\u4e0a\u540c\u65f6\u8fd0\u884c\u6570\u5343\u4e2a Docker \u5bb9\u5668\u6210\u4e3a\u53ef\u80fd\u3002 Docker \u64cd\u4f5c\u65b9\u4fbf\uff0c\u53ef\u4ee5\u901a\u8fc7 Dockerfile \u914d\u7f6e\u6587\u4ef6\u652f\u6301\u7075\u6d3b\u7684\u81ea\u52a8\u5316\u521b\u5efa\u548c\u90e8\u7f72\u3002\u8868 1-1 \u5c06 Docker \u5bb9\u5668\u6280\u672f\u4e0e\u4f20\u7edf\u865a\u62df\u673a\u7684\u7279\u6027\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002 \u4e2a\u4eba\u7406\u89e3: \u865a\u62df\u673a\u548c\u5bb9\u5668\u7684\u540c\u4e2a\u65f6\u4ee3\u7684\u4e0d\u540c\u4ea7\u7269 \u88681-1 Docker \u5bb9\u5668\u4e0e\u4f20\u7edf\u865a\u62df\u673a\u7684\u533a\u522b Docker \u4e4b\u6240\u4ee5\u62e5\u6709\u4f17\u591a\u4f18\u52bf\uff0c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u865a\u62df\u5316\u81ea\u8eab\u7684\u7279\u70b9\u662f\u5206\u4e0d\u5f00\u7684\u3002\u4f20\u7edf\u865a\u62df\u673a\u9700\u8981\u6709 \u989d\u5916\u7684\u865a\u62df\u673a\u7ba1\u7406\u7a0b\u5e8f\u548c\u865a\u62df\u673a\u64cd\u4f5c\u7cfb\u7edf\u5c42\uff0c\u800c Docker \u5bb9\u5668\u5219\u662f\u76f4\u63a5\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u4e4b\u4e0a\u5b9e\u73b0\u7684 \u865a\u62df\u5316\u3002\u56fe 1.2 \u662f Docker \u4e0e\u4f20\u7edf\u865a\u62df\u673a\u67b6\u6784\u3002 1.1.3 \u955c\u50cf \u00b6 \u955c\u50cf\u3001\u5bb9\u5668\u3001\u4ed3\u5e93\u662f Docker \u7684\u4e09\u5927\u6838\u5fc3\u6982\u5ff5\u3002\u5176\u4e2d Docker \u7684\u955c\u50cf\u662f\u521b\u5efa\u5bb9\u5668\u7684\u57fa\u7840\uff0c\u7c7b\u4f3c\u865a\u62df\u673a\u7684\u5feb\u7167,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u9762\u5411 Docker \u5bb9\u5668\u5f15\u64ce\u7684\u53ea\u8bfb\u6a21\u677f\u3002\u4f8b\u5982\uff1a\u4e00\u4e2a\u955c\u50cf\u53ef\u4ee5\u662f\u4e00\u4e2a\u5b8c\u6574 \u7684 Cent OS \u64cd\u4f5c\u7cfb\u7edf\u73af\u5883\uff0c\u79f0\u4e3a\u4e00\u4e2a CentOS \u955c\u50cf\uff1b\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u5b89\u88c5\u4e86 MySQL \u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u79f0\u4e4b\u4e3a\u4e00\u4e2a MySQL \u955c\u50cf\u7b49\u7b49\u3002 \u6240\u6709\u7684\u5bb9\u5668\u90fd\u662f\u57fa\u4e8e\u955c\u50cf\u6765\u5b9e\u73b0\u7684\uff5e\uff0c\u6240\u4ee5image \u5c31\u663e\u5f97\u5f88\u91cd\u8981\uff01\uff01\uff01 1.1.4 \u5bb9\u5668 \u00b6 Docker \u7684\u5bb9\u5668\u662f\u4ece\u955c\u50cf\u521b\u5efa\u7684\u8fd0\u884c\u5b9e\u4f8b\uff0c\u5b83\u53ef\u4ee5\u88ab\u542f\u52a8\u3001\u505c\u6b62\u548c\u5220\u9664\u3002\u6240\u521b\u5efa\u7684\u6bcf\u4e00\u4e2a\u5bb9\u5668 \u90fd\u662f\u76f8\u4e92\u9694\u79bb\u3001\u4e92\u4e0d\u53ef\u89c1\uff0c\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u5e73\u53f0\u3002\u53ef\u4ee5\u5c06\u5bb9\u5668\u770b\u4f5c\u662f\u4e00\u4e2a\u7b80\u6613\u7248\u7684 Linux \u73af\u5883\uff0c Docker \u5229\u7528\u5bb9\u5668\u6765\u8fd0\u884c\u548c\u9694\u79bb\u5e94\u7528\u3002 1.1.5 \u4ed3\u5e93 \u00b6 \u6709\u4e86\u955c\u50cf\uff0c\u5f53\u7136\u9700\u8981\u6709\u5b58\u653e\u4ed6\u7684\u5730\u65b9\uff0cDocker \u4ed3\u5e93\u5c31\u662f\u7528\u6765\u96c6\u4e2d\u4fdd\u5b58\u955c\u50cf\u7684\u5730\u65b9\uff0c\u5f53\u521b\u5efa\u4e86\u81ea\u5df1\u7684\u955c\u50cf\u4e4b\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 push \u547d\u4ee4\u5c06\u5b83 \u4e0a\u4f20\u5230\u516c\u6709\u4ed3\u5e93\uff08Public\uff09\u6216\u8005\u79c1\u6709\u4ed3\u5e93\uff08Private\uff09\u3002\u5f53\u4e0b\u6b21\u8981\u5728\u53e6\u5916\u4e00\u53f0\u673a\u5668\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u955c\u50cf \u65f6\uff0c\u53ea\u9700\u4ece\u4ed3\u5e93\u83b7\u53d6\u3002 \u5e38\u89c1\u7684\u955c\u50cf\u4ed3\u5e93: \u963f\u91cc\u4e91\u955c\u50cf\u4ed3\u5e93 Ucloud \u955c\u50cf\u4ed3\u5e93 Harbor \u955c\u50cf\u4ed3\u5e93 docker hub (\u5b98\u65b9\u5e93) \u4ed3\u5e93\u6ce8\u518c\u670d\u52a1\u5668\uff08Registry\uff09\u662f\u5b58\u653e\u4ed3\u5e93\u7684\u5730\u65b9\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u591a\u4e2a\u4ed3\u5e93\u3002\u6bcf\u4e2a\u4ed3\u5e93\u96c6\u4e2d\u5b58\u653e\u67d0 \u4e00\u7c7b\u955c\u50cf\uff0c\u5e76\u4e14\u4f7f\u7528\u4e0d\u540c\u7684\u6807\u7b7e\uff08tag\uff09\u6765\u533a\u5206\u5b83\u4eec\u3002\u76ee\u524d\u6700\u5927\u7684\u516c\u5171\u4ed3\u5e93\u662f docker Hub\uff0c\u5b58\u653e\u4e86 \u6570\u91cf\u5e9e\u5927\u7684\u955c\u50cf\u4f9b\u7528\u6237\u4e0b\u8f7d\u4f7f\u7528\u3002 1.2 \u5b89\u88c5 Docker \u00b6 \u8fd9\u91cc\u4e3b\u8981\u4ecb\u7ecd\u5e38\u89c1\u7684\u51e0\u79cd\uff0cYUM\u9488\u5bf9\u4e8eCentos\u7cfb\u7edf\uff0cAPT\u9488\u5bf9\u4e8eubuntu\u7cfb\u7edf\uff0c\u811a\u672c\u5f0f\u5feb\u6377\u5b89\u88c5\u3002 \u652f\u6301\u7684\u5e73\u53f0: Docker Engine \u53ef\u901a\u8fc7 Docker Desktop \u5728\u5404\u79cdLinux \u5e73\u53f0\u3001 macOS\u548cWindows 10 \u4e0a\u4f7f\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u9759\u6001\u4e8c\u8fdb\u5236\u5b89\u88c5\u4f7f\u7528\u3002\u5728\u4e0b\u65b9\u627e\u5230\u60a8\u9996\u9009\u7684\u64cd\u4f5c\u7cfb\u7edf\u3002 \u6e29\u99a8\u63d0\u793a \u8fd9\u91cc\u9762\u4e4b\u524d\u9047\u5230\u4e00\u4e2a\u5751\uff0c\u539f\u56e0\u5c31\u662f\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u7684Mac\u529e\u516c\uff0cubuntu\u9700\u8981\u62c9\u4e0d\u6765\u955c\u50cf\uff0c\u9700\u8981Mac\u62c9\u597d\u4e0a\u4f20\u4e0a\u53bb\uff0c\u7ed3\u679cMac\u62c9\u7684\u7248\u672c\u662famd\u67b6\u6784\u7684\uff5e \u89e3\u51b3\u65b9\u6cd5: k8s.gcr.io/nfd/node-feature-discovery:v0.11.0 --platform linux/amd64 \u5b98\u7f51: https://docs.docker.com/engine/install/ 1.2.1 Apt \u65b9\u5f0f\u5b89\u88c5: \u00b6 \u79fb\u9664\u65e7docker\u5305 sudo apt-get remove docker docker-engine docker.io containerd runc \u8bbe\u7f6e\u5b58\u50a8\u5e93 \u66f4\u65b0apt\u5305\u7d22\u5f15\u5e76\u5b89\u88c5\u5305\u4ee5\u5141\u8bb8apt\u901a\u8fc7 HTTPS \u4f7f\u7528\u5b58\u50a8\u5e93\uff1a sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release \u6dfb\u52a0 Docker \u7684\u5b98\u65b9 GPG \u5bc6\u94a5\uff1a sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bbe\u7f6e\u5b58\u50a8\u5e93\uff1a echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null \u5b89\u88c5docker sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin 1.2.2 YUM \u65b9\u5f0f\u5b89\u88c5: \u00b6 \u79fb\u9664\u65e7docker\u5305 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine \u914d\u7f6eYUM\u6e90 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo \u5b89\u88c5docker-ce sudo yum install -y docker-ce docker-ce-cli containerd.io #\u4ee5\u4e0b\u662f\u5728\u5b89\u88c5k8s\u7684\u65f6\u5019\u4f7f\u7528 yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6 \u542f\u52a8 systemctl enable docker --now \u914d\u7f6e\u52a0\u901f sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json <<-'EOF' { \"registry-mirrors\": [\"https://82m9ar63.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl restart docker 1.2.3 \u811a\u672c\u5f0f\u5b89\u88c5 \u00b6 \u5b98\u65b9\u63d0\u4f9b: curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh rancher\u63d0\u4f9b: curl https://releases.rancher.com/install-docker/19.03.sh | sh \u5b89\u88c5\u597d\u7684 Docker \u7cfb\u7edf\u6709\u4e24\u4e2a\u7a0b\u5e8f\uff0cDocker \u670d\u52a1\u7aef\u548c Docker \u5ba2\u6237\u7aef\u3002\u5176\u4e2d Docker \u670d\u52a1\u7aef\u662f \u4e00\u4e2a\u670d\u52a1\u8fdb\u7a0b\uff0c\u8d1f\u8d23\u7ba1\u7406\u6240\u6709\u5bb9\u5668\u3002Docker \u5ba2\u6237\u7aef\u5219\u626e\u6f14\u7740 Docker \u670d\u52a1\u7aef\u7684\u8fdc\u7a0b\u63a7\u5236\u5668\uff0c\u53ef\u4ee5\u7528 \u6765\u63a7\u5236 Docker \u7684\u670d\u52a1\u7aef\u8fdb\u7a0b\u3002\u5927\u90e8\u5206\u60c5\u51b5\u4e0b Docker \u670d\u52a1\u7aef\u548c\u5ba2\u6237\u7aef\u8fd0\u884c\u5728\u4e00\u53f0\u673a\u5668\u4e0a\u3002 1.3 Docker \u955c\u50cf\u64cd\u4f5c \u00b6 \u8fd0\u884c Docker \u5bb9\u5668\u524d\u9700\u8981\u672c\u5730\u5b58\u5728\u5bf9\u5e94\u7684\u955c\u50cf\u3002\u5982\u679c\u4e0d\u5b58\u5728\u672c\u5730\u955c\u50cf\uff0cDocker \u5c31\u4f1a\u5c1d\u8bd5\u4ece\u9ed8\u8ba4 \u955c\u50cf\u4ed3\u5e93\u4e0b\u8f7d\u3002\u955c\u50cf\u4ed3\u5e93\u662f\u7531 Docker \u5b98\u65b9\u7ef4\u62a4\u7684\u4e00\u4e2a\u516c\u5171\u4ed3\u5e93\uff0c\u53ef\u4ee5\u6ee1\u8db3\u7528\u6237\u7684\u7edd\u5927\u90e8\u5206\u9700\u6c42\u3002 \u7528\u6237\u4e5f\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6765\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u955c\u50cf\u4ed3\u5e93\u3002 \u5e2e\u52a9\u547d\u4ee4: [ ucloud ] root@master0:~# docker --help Usage: docker [ OPTIONS ] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files ( default \"/root/.docker\" ) -c, --context string Name of the context to use to connect to the daemon ( overrides DOCKER_HOST env var and default context set with \"docker context use\" ) -D, --debug Enable debug mode -H, --host list Daemon socket ( s ) to connect to -l, --log-level string Set the logging level ( \"debug\" | \"info\" | \"warn\" | \"error\" | \"fatal\" ) ( default \"info\" ) --tls Use TLS ; implied by --tlsverify --tlscacert string Trust certs signed only by this CA ( default \"/root/.docker/ca.pem\" ) --tlscert string Path to TLS certificate file ( default \"/root/.docker/cert.pem\" ) --tlskey string Path to TLS key file ( default \"/root/.docker/key.pem\" ) --tlsverify Use TLS and verify the remote -v, --version Print version information and quit \u7ba1\u7406CLI: [ ucloud ] root@master0:~# docker search nginx NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 17448 [ OK ] \u6267\u884c docker search lamp \u547d\u4ee4\u540e\uff0c\u8fd4\u56de\u5f88\u591a\u5305\u542b lamp \u5173\u952e\u5b57\u7684\u955c\u50cf\uff0c\u5176\u4e2d\u8fd4\u56de\u4fe1\u606f\u5305\u62ec\u955c\u50cf \u540d\u79f0\uff08NAME\uff09\u3001\u63cf\u8ff0\uff08DESCRIPTION\uff09\u3001\u661f\u7ea7\uff08STARS\uff09\u3001\u662f\u5426\u5b98\u65b9\u521b\u5efa\uff08OFFICIAL\uff09\u3001\u662f \u5426\u4e3b\u52a8\u521b\u5efa\uff08AUTOMATED\uff09\u3002\u9ed8\u8ba4\u7684\u8f93\u51fa\u7ed3\u679c\u4f1a\u6309\u7167\u661f\u7ea7\u8bc4\u4ef7\u8fdb\u884c\u6392\u5e8f\uff0c\u8868\u793a\u8be5\u955c\u50cf\u53d7\u6b22\u8fce\u7a0b \u5ea6\u3002\u5728\u4e0b\u8f7d\u955c\u50cf\u65f6\uff0c\u53ef\u4ee5\u53c2\u8003\u661f\u7ea7\u3002\u5728\u641c\u7d22\u65f6\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528-s \u6216\u8005--stars=x \u663e\u793a\u6307\u5b9a\u661f\u7ea7\u4ee5\u4e0a\u7684\u955c \u50cf\uff0c\u661f\u7ea7\u8d8a\u9ad8\u8868\u793a\u8d8a\u53d7\u6b22\u8fce\uff1b\u662f\u5426\u5b98\u65b9\u521b\u5efa\u662f\u6307\u662f\u5426\u662f\u7531\u5b98\u65b9\u9879\u76ee\u7ec4\u521b\u5efa\u548c\u7ef4\u62a4\u7684\u955c\u50cf\uff0c\u4e00\u822c\u5b98\u65b9\u9879\u76ee\u7ec4\u7ef4\u62a4\u7684\u955c\u50cf\u4f7f\u7528\u5355\u4e2a\u5355\u8bcd\u4f5c\u4e3a\u955c\u50cf\u540d\u79f0\uff0c\u79f0\u4e3a\u57fa\u7840\u955c\u50cf\u6216\u8005\u6839\u955c\u50cf\u3002\u50cf reinblau/lamp \u8fd9\u79cd \u547d\u540d\u65b9\u5f0f\u7684\u955c\u50cf\uff0c\u8868\u793a\u662f\u7531 docker Hub \u7684\u7528\u6237 reinblau \u521b\u5efa\u5e76\u7ef4\u62a4\u7684\u955c\u50cf\uff0c\u5e26\u6709\u7528\u6237\u540d\u4e3a\u524d\u7f00\uff1b \u662f\u5426\u4e3b\u52a8\u521b\u5efa\u662f\u6307\u662f\u5426\u5141\u8bb8\u7528\u6237\u9a8c\u8bc1\u955c\u50cf\u7684\u6765\u6e90\u548c\u5185\u5bb9\u3002 docker pull image-name //\u83b7\u53d6\u955c\u50cf docker push image-name //\u63a8\u9001\u955c\u50cf docker images //\u67e5\u770b\u955c\u50cf docker rm -f image-name //\u5f3a\u5236\u5220\u9664\u955c\u50cf docker images -aq //\u67e5\u770b\u6240\u6709\u955c\u50cfID docker save -o \u5b58\u50a8\u6587\u4ef6\u540d \u5b58\u50a8\u7684\u955c\u50cf\u540d //\u4fdd\u5b58\u955c\u50cf docker load < \u5b58\u50a8\u6587\u4ef6\u540d //\u5bfc\u5165\u955c\u50cf \u5bf9\u4e8e Docker \u955c\u50cf\u6765\u8bf4\uff0c\u5982\u679c\u4e0b\u8f7d\u955c\u50cf\u65f6\u4e0d\u6307\u5b9a\u6807\u7b7e\uff0c\u5219\u9ed8\u8ba4\u4f1a\u4e0b\u8f7d\u4ed3\u5e93\u4e2d\u6700\u65b0\u7248\u672c\u7684\u955c \u50cf\uff0c\u5373\u9009\u62e9\u6807\u7b7e\u4e3a latest \u6807\u7b7e\uff0c\u4e5f\u53ef\u901a\u8fc7\u6307\u5b9a\u7684\u6807\u7b7e\u6765\u4e0b\u8f7d\u7279\u5b9a\u7248\u672c\u7684\u67d0\u4e00\u955c\u50cf\u3002\u8fd9\u91cc\u6807\u7b7e \uff08tag\uff09\u5c31\u662f\u7528\u6765\u533a\u5206\u955c\u50cf\u7248\u672c\u7684\u3002 \u6e29\u99a8\u63d0\u793a \u5728\u767b\u9646ucloud\u7684\u955c\u50cf\u4ed3\u5e93\uff0c\u9700\u8981\u5c06\u5bc6\u7801\u7528''\u5f15\u8d77\u6765\uff0c\u4f8b\u5982: docker login -u user-name -p 'password' uhub.service.ucloud.cn 1.4 Docker \u5bb9\u5668\u64cd\u4f5c \u00b6 \u5bb9\u5668\u662f Docker \u7684\u53e6\u4e00\u4e2a\u6838\u5fc3\u6982\u5ff5\u3002\u7b80\u5355\u8bf4\uff0c\u5bb9\u5668\u662f\u955c\u50cf\u7684\u4e00\u4e2a\u8fd0\u884c\u5b9e\u4f8b\uff0c\u662f\u72ec\u7acb\u8fd0\u884c\u7684\u4e00\u4e2a \u6216\u4e00\u7ec4\u5e94\u7528\u4ee5\u53ca\u5b83\u4eec\u6240\u5fc5\u9700\u7684\u8fd0\u884c\u73af\u5883\uff0c\u5305\u62ec\u6587\u4ef6\u7cfb\u7edf\u3001\u7cfb\u7edf\u7c7b\u5e93\u3001shell \u73af\u5883\u7b49\u3002\u955c\u50cf\u662f\u53ea\u8bfb\u6a21 \u677f\uff0c\u800c\u5bb9\u5668\u4f1a\u7ed9\u8fd9\u4e2a\u53ea\u8bfb\u6a21\u677f\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u53ef\u5199\u5c42\u3002 \u7ba1\u7406CLI: docker stop \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker start \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker restart \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker ps -aq \u67e5\u770b\u6240\u6709\u5bb9\u5668ID docker export \u5bb9\u5668ID/\u540d\u79f0 > \u6587\u4ef6\u540d //\u5bb9\u5668\u5bfc\u51fa cat \u6587\u4ef6\u540d | docker import - \u751f\u6210\u7684\u955c\u50cf\u540d\u79f0:\u6807\u7b7e //\u5bb9\u5668\u5bfc\u5165 1.5 Docker \u955c\u50cf\u7ba1\u7406 \u00b6 Docker \u955c\u50cf\u9664\u4e86\u662f Docker \u7684\u6838\u5fc3\u6280\u672f\u4e4b\u5916\uff0c\u4e5f\u662f\u5e94\u7528\u53d1\u5e03\u7684\u6807\u51c6\u683c\u5f0f\u3002\u4e00\u4e2a\u5b8c\u6574\u7684 Docker \u955c\u50cf\u53ef\u4ee5\u652f\u6491\u4e00\u4e2a Docker \u5bb9\u5668\u7684\u8fd0\u884c\uff0c\u5728 Docker \u7684\u6574\u4e2a\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8fdb\u5165\u4e00\u4e2a\u5df2\u7ecf\u5b9a\u578b\u7684\u5bb9\u5668\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5728\u5bb9\u5668\u4e2d\u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u5e38\u89c1\u7684\u64cd\u4f5c\u5c31\u662f\u5728\u5bb9\u5668\u4e2d\u5b89\u88c5\u5e94\u7528\u670d\u52a1\u3002 \u5982\u679c\u8981\u628a\u5df2\u7ecf\u5b89\u88c5\u7684\u670d\u52a1\u8fdb\u884c\u8fc1\u79fb\uff0c\u5c31\u9700\u8981\u628a\u73af\u5883\u4ee5\u53ca\u642d\u5efa\u7684\u670d\u52a1\u751f\u6210\u65b0\u7684\u955c\u50cf\u3002\u672c\u6848\u4f8b\u5c06\u4ecb\u7ecd \u5982\u4f55\u521b\u5efa Docker \u955c\u50cf\u3002 1.5.1 Docker\u955c\u50cf\u7ed3\u6784 \u00b6 \u955c\u50cf\u4e0d\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6587\u4ef6\uff0c\u800c\u662f\u6709\u591a\u5c42\u6784\u6210\u3002\u53ef\u4ee5\u901a\u8fc7 docker history \u547d\u4ee4\u67e5\u770b\u955c\u50cf\u4e2d\u5404 \u5c42\u5185\u5bb9\u53ca\u5927\u5c0f\uff0c\u6bcf\u5c42\u5bf9\u5e94\u7740 Dockerfile \u4e2d\u7684\u4e00\u6761\u6307\u4ee4\u3002Docker \u955c\u50cf\u9ed8\u8ba4\u5b58\u50a8\u5728 /var/lib/docker/ \u76ee\u5f55\u4e2d\u3002\u5bb9\u5668\u5176\u5b9e\u662f\u5728\u955c\u50cf\u7684\u6700\u4e0a\u9762\u52a0\u4e86\u4e00\u5c42\u8bfb\u5199\u5c42\uff0c \u5728\u8fd0 \u884c\u5bb9\u5668\u91cc\u505a\u7684\u4efb\u4f55\u6587\u4ef6\u6539\u52a8\uff0c\u90fd\u4f1a\u5199\u5230\u8fd9\u4e2a\u8bfb\u5199\u5c42\u3002\u5982\u679c\u5220\u9664\u4e86\u5bb9\u5668\uff0c\u4e5f\u5c31\u5220\u9664\u4e86\u5176\u6700\u4e0a\u9762\u7684 \u8bfb\u5199\u5c42\uff0c\u6587\u4ef6\u6539\u52a8\u4e5f\u5c31\u4e22\u5931\u4e86\u3002Docker \u4f7f\u7528\u5b58\u50a8\u9a71\u52a8\u7ba1\u7406\u955c\u50cf\u6bcf\u5c42\u5185\u5bb9\u53ca\u53ef\u8bfb\u5199\u5c42\u7684\u5bb9\u5668 \u5c42\u3002Docker \u955c\u50cf\u662f\u5206\u5c42\u7684\uff0c\u4e0b\u9762\u8fd9\u4e9b\u77e5\u8bc6\u70b9\u975e\u5e38\u91cd\u8981\u3002 \uff081\uff09Dockerfile \u4e2d\u7684\u6bcf\u4e2a\u6307\u4ee4\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf\u5c42\uff1b \uff082\uff09\u955c\u50cf\u5c42\u5c06\u88ab\u7f13\u5b58\u548c\u590d\u7528\uff1b \uff083\uff09 \u5f53Dockerfile \u7684\u6307\u4ee4\u4fee\u6539\u4e86\uff0c\u590d\u5236\u7684\u6587\u4ef6\u53d8\u5316\u4e86\uff0c\u6216\u8005\u6784\u5efa\u955c\u50cf\u65f6\u6307\u5b9a\u7684\u53d8\u91cf\u4e0d\u540c \u4e86\uff0c\u5bf9\u5e94\u7684\u955c\u50cf\u5c42\u7f13\u5b58\u5c31\u4f1a\u5931\u6548\uff1b \uff084\uff09\u67d0\u4e00\u5c42\u7684\u955c\u50cf\u7f13\u5b58\u5931\u6548\uff0c\u5b83\u4e4b\u540e\u7684\u955c\u50cf\u5c42\u7f13\u5b58\u90fd\u4f1a\u5931\u6548\uff1b \uff085\uff09\u955c\u50cf\u5c42\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u5982\u679c\u5728\u67d0\u4e00\u5c42\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u6587\u4ef6\uff0c\u7136\u540e\u5728\u4e0b\u4e00\u5c42\u4e2d\u5220\u9664\u5b83\uff0c\u5219\u955c \u50cf\u4e2d\u4f9d\u7136\u4f1a\u5305\u542b\u8be5\u6587\u4ef6\uff0c\u53ea\u662f\u8fd9\u4e2a\u6587\u4ef6\u5728 Docker \u5bb9\u5668\u4e2d\u4e0d\u53ef\u89c1\u4e86\u3002 1.5.2 Dockerfile\u4ecb\u7ecd \u00b6 Dockfile \u662f\u4e00\u79cd\u88ab Docker \u7a0b\u5e8f\u89e3\u91ca\u7684\u811a\u672c\uff0cDockerfile \u7531\u591a\u6761\u7684\u6307\u4ee4\u7ec4\u6210\uff0c\u6bcf\u6761\u6307\u4ee4\u5bf9 \u5e94Linux \u4e0b\u9762\u7684\u4e00\u6761\u547d\u4ee4\u3002Docker \u7a0b\u5e8f\u5c06\u8fd9\u4e9bDockerfile \u6307\u4ee4\u7ffb\u8bd1\u6210\u771f\u6b63\u7684Linux \u547d\u4ee4\u3002 Dockerfile \u6709\u81ea\u5df1\u4e66\u5199\u683c\u5f0f\u548c\u652f\u6301\u7684\u547d\u4ee4\uff0cDocker \u7a0b\u5e8f\u89e3\u51b3\u8fd9\u4e9b\u547d\u4ee4\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u7c7b\u4f3c\u4e8e Makefile\u3002Docker \u7a0b\u5e8f\u5c06\u8bfb\u53d6 Dockerfile\uff0c\u6839\u636e\u6307\u4ee4\u751f\u6210\u5b9a\u5236\u7684\u955c\u50cf\u3002\u76f8\u6bd4\u955c\u50cf\u8fd9\u79cd\u9ed1\u76d2\u5b50\uff0c Dockerfile \u8fd9\u79cd\u663e\u800c\u6613\u89c1\u7684\u811a\u672c\u66f4\u5bb9\u6613\u88ab\u4f7f\u7528\u8005\u63a5\u53d7\uff0c\u5b83\u660e\u786e\u7684\u8868\u660e\u955c\u50cf\u662f\u600e\u4e48\u4ea7\u751f\u7684\u3002\u6709\u4e86 Dockerfile\uff0c\u5f53\u6709\u5b9a\u5236\u989d\u5916\u7684\u9700\u6c42\u65f6\uff0c\u53ea\u9700\u5728 Dockerfile \u4e0a\u6dfb\u52a0\u6216\u8005\u4fee\u6539\u6307\u4ee4\uff0c \u91cd\u65b0\u751f\u6210\u955c\u50cf\u3002 \u9644\u4ef6: \u00b6","title":"Docker \u57fa\u672c\u7ba1\u7406"},{"location":"docker/docker-docs/#docker","text":"\u968f\u7740\u8ba1\u7b97\u673a\u8fd1\u51e0\u5341\u5e74\u7684\u84ec\u52c3\u53d1\u5c55\uff0c\u4ea7\u751f\u4e86\u5927\u91cf\u4f18\u79c0\u7cfb\u7edf\u548c\u8f6f\u4ef6\u3002\u8f6f\u4ef6\u5f00\u53d1\u4eba\u5458\u53ef\u4ee5\u81ea\u7531\u9009\u62e9\u5404 \u79cd\u8f6f\u4ef6\u5e94\u7528\u3002\u4f46\u540c\u65f6\u5e26\u6765\u7684\u95ee\u9898\u5c31\u662f\u9700\u8981\u7ef4\u62a4\u4e00\u4e2a\u975e\u5e38\u5e9e\u5927\u7684\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u751f\u4ea7\u73af\u5883\u3002 \u9762\u5bf9\u8fd9\u79cd \u60c5\u51b5\uff0cDocker \u5bb9\u5668\u6280\u672f\u6a2a\u7a7a\u51fa\u4e16\uff0c\u63d0\u4f9b\u4e86\u7b80\u5355\u3001\u7075\u6d3b\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e0d\u9700\u8981\u8fc7\u591a\u5730\u6539\u53d8\u73b0\u6709 \u7684\u4f7f\u7528\u4e60\u60ef\uff0c\u5c31\u53ef\u4ee5\u548c\u5df2\u6709\u7684\u5de5\u5177\uff0c\u5982 OpenStack \u7b49\u914d\u5408\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u638c\u63e1 Docker \u76f8\u5173\u6280\u672f\u4e5f\u662f \u9014\u7ecf\u4e91\u8ba1\u7b97\u7684\u5fc5\u7ecf\u4e4b\u8def\u3002 \u672c\u7ae0\u5c06\u4f9d\u6b21\u4ecb\u7ecd Docker \u7684\u4e09\u5927\u6838\u5fc3\u6982\u5ff5\u2014\u2014\u955c\u50cf\u3001\u5bb9\u5668\u3001\u4ed3\u5e93\uff0c\u4ee5\u53ca\u5b89\u88c5 Docker \u4e0e\u4ecb\u7ecd\u56f4 \u7ed5\u955c\u50cf\u548c\u5bb9\u5668\u7684\u5177\u4f53\u64cd\u4f5c\u3002","title":"Docker \u57fa\u672c\u7ba1\u7406"},{"location":"docker/docker-docs/#11-docker","text":"\u56e0\u4e3a Docker \u8f7b\u4fbf\u3001\u5feb\u901f\u7684\u7279\u6027\uff0c\u53ef\u4ee5\u4f7f\u5e94\u7528\u8fbe\u5230\u5feb\u901f\u8fed\u4ee3\u7684\u76ee\u7684\u3002\u6bcf\u6b21\u5c0f\u7684\u53d8\u66f4\uff0c\u9a6c\u4e0a\u5c31\u53ef \u4ee5\u770b\u5230\u6548\u679c\uff0c\u800c\u4e0d\u7528\u5c06\u82e5\u5e72\u4e2a\u5c0f\u53d8\u66f4\u79ef\u6512\u5230\u4e00\u5b9a\u7a0b\u5ea6\u518d\u53d8\u66f4\u3002\u6bcf\u6b21\u53d8\u66f4\u4e00\u5c0f\u90e8\u5206\u5176\u5b9e\u662f\u4e00\u79cd\u975e\u5e38\u5b89 \u5168\u7684\u65b9\u5f0f\uff0c\u5728\u5f00\u53d1\u73af\u5883\u4e2d\u80fd\u591f\u5feb\u901f\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\u3002 Docker \u5bb9\u5668\u80fd\u591f\u5e2e\u52a9\u5f00\u53d1\u4eba\u5458\u3001\u7cfb\u7edf\u7ba1\u7406\u5458\u3001\u8d28\u91cf\u7ba1\u7406\u548c\u7248\u672c\u63a7\u5236\u5de5\u7a0b\u5e08\u5728\u4e00\u4e2a\u751f\u4ea7\u73af\u8282\u4e2d \u4e00\u8d77\u534f\u540c\u5de5\u4f5c\u3002\u5236\u5b9a\u4e00\u5957\u5bb9\u5668\u6807\u51c6\u80fd\u591f\u4f7f\u7cfb\u7edf\u7ba1\u7406\u5458\u66f4\u6539\u5bb9\u5668\u7684\u65f6\u5019\uff0c\u7a0b\u5e8f\u5458\u4e0d\u9700\u8981\u5173\u5fc3\u5bb9\u5668\u7684\u53d8 \u5316\uff0c\u800c\u66f4\u4e13\u6ce8\u81ea\u5df1\u7684\u5e94\u7528\u7a0b\u5e8f\u4ee3\u7801\u3002\u4ece\u800c\u9694\u79bb\u5f00\u4e86\u5f00\u53d1\u548c\u7ba1\u7406\uff0c\u7b80\u5316\u4e86\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u6210\u672c\u3002","title":"1.1 Docker \u6982\u8ff0"},{"location":"docker/docker-docs/#111-docker","text":"\u5982\u679c\u8981\u65b9\u4fbf\u7684\u521b\u5efa\u8fd0\u884c\u5728\u4e91\u5e73\u53f0\u4e0a\u7684\u5e94\u7528\uff0c\u5fc5\u987b\u8981\u8131\u79bb\u5e95\u5c42\u7684\u786c\u4ef6\uff0c\u540c\u65f6\u8fd8\u9700\u8981\u4efb\u4f55\u65f6 \u95f4\u5730 \u70b9\u53ef\u83b7\u53d6\u8fd9\u4e9b\u8d44\u6e90\uff0c\u8fd9\u6b63\u662f Docker \u6240\u80fd\u63d0\u4f9b\u7684\u3002Docker \u7684\u5bb9\u5668\u6280\u672f\u53ef\u4ee5\u5728\u4e00\u53f0\u4e3b\u673a\u4e0a\u8f7b\u677e\u4e3a\u4efb\u4f55 \u5e94\u7528\u521b\u5efa\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u3001\u53ef\u79fb\u690d\u7684\u3001\u81ea\u7ed9\u81ea\u8db3\u7684\u5bb9\u5668\u3002\u901a\u8fc7\u8fd9\u79cd\u5bb9\u5668\u6253\u5305\u5e94\u7528\u7a0b\u5e8f\uff0c\u610f\u5473\u7740\u7b80\u5316\u4e86 \u91cd\u65b0\u90e8\u7f72\u3001\u8c03\u8bd5\u8fd9\u4e9b\u7410\u788e\u7684\u91cd\u590d\u5de5\u4f5c\uff0c\u6781\u5927\u7684\u63d0\u9ad8\u4e86\u5de5\u4f5c\u6548\u7387\u3002","title":"1.1.1 \u4ec0\u4e48\u662f Docker"},{"location":"docker/docker-docs/#112-docker","text":"Docker \u5bb9\u5668\u8fd0\u884c\u901f\u5ea6\u5f88\u5feb\uff0c\u542f\u52a8\u548c\u505c\u6b62\u53ef\u4ee5\u5728\u79d2\u7ea7\u5b9e\u73b0\uff0c\u6bd4\u4f20\u7edf\u865a\u62df\u673a\u8981\u5feb\u5f88\u591a\uff1bDocker \u6838 \u5fc3\u89e3\u51b3\u7684\u95ee\u9898\u662f\u5229\u7528\u5bb9\u5668\u6765\u5b9e\u73b0\u7c7b\u4f3c\u865a\u62df\u673a\u7684\u529f\u80fd\uff0c\u4ece\u800c\u5229\u7528\u66f4\u52a0\u8282\u7701\u7684\u786c\u4ef6\u8d44\u6e90\u63d0\u4f9b\u7ed9\u7528\u6237\u66f4\u591a \u7684\u8ba1\u7b97\u8d44\u6e90\u3002\u56e0\u6b64\uff0cDocker \u5bb9\u5668\u9664\u4e86\u8fd0\u884c\u5176\u4e2d\u7684\u5e94\u7528\u4e4b\u5916\uff0c\u57fa\u672c\u4e0d\u6d88\u8017\u989d\u5916\u7684\u7cfb\u7edf\u8d44\u6e90\uff0c\u5728\u4fdd\u8bc1 \u5e94\u7528\u6027\u80fd\u7684\u540c\u65f6\uff0c\u53c8\u51cf\u5c0f\u4e86\u7cfb\u7edf\u5f00\u9500\uff0c\u4f7f\u5f97\u4e00\u53f0\u4e3b\u673a\u4e0a\u540c\u65f6\u8fd0\u884c\u6570\u5343\u4e2a Docker \u5bb9\u5668\u6210\u4e3a\u53ef\u80fd\u3002 Docker \u64cd\u4f5c\u65b9\u4fbf\uff0c\u53ef\u4ee5\u901a\u8fc7 Dockerfile \u914d\u7f6e\u6587\u4ef6\u652f\u6301\u7075\u6d3b\u7684\u81ea\u52a8\u5316\u521b\u5efa\u548c\u90e8\u7f72\u3002\u8868 1-1 \u5c06 Docker \u5bb9\u5668\u6280\u672f\u4e0e\u4f20\u7edf\u865a\u62df\u673a\u7684\u7279\u6027\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002 \u4e2a\u4eba\u7406\u89e3: \u865a\u62df\u673a\u548c\u5bb9\u5668\u7684\u540c\u4e2a\u65f6\u4ee3\u7684\u4e0d\u540c\u4ea7\u7269 \u88681-1 Docker \u5bb9\u5668\u4e0e\u4f20\u7edf\u865a\u62df\u673a\u7684\u533a\u522b Docker \u4e4b\u6240\u4ee5\u62e5\u6709\u4f17\u591a\u4f18\u52bf\uff0c\u4e0e\u64cd\u4f5c\u7cfb\u7edf\u865a\u62df\u5316\u81ea\u8eab\u7684\u7279\u70b9\u662f\u5206\u4e0d\u5f00\u7684\u3002\u4f20\u7edf\u865a\u62df\u673a\u9700\u8981\u6709 \u989d\u5916\u7684\u865a\u62df\u673a\u7ba1\u7406\u7a0b\u5e8f\u548c\u865a\u62df\u673a\u64cd\u4f5c\u7cfb\u7edf\u5c42\uff0c\u800c Docker \u5bb9\u5668\u5219\u662f\u76f4\u63a5\u5728\u64cd\u4f5c\u7cfb\u7edf\u5c42\u9762\u4e4b\u4e0a\u5b9e\u73b0\u7684 \u865a\u62df\u5316\u3002\u56fe 1.2 \u662f Docker \u4e0e\u4f20\u7edf\u865a\u62df\u673a\u67b6\u6784\u3002","title":"1.1.2 Docker \u7684\u4f18\u52bf"},{"location":"docker/docker-docs/#113","text":"\u955c\u50cf\u3001\u5bb9\u5668\u3001\u4ed3\u5e93\u662f Docker \u7684\u4e09\u5927\u6838\u5fc3\u6982\u5ff5\u3002\u5176\u4e2d Docker \u7684\u955c\u50cf\u662f\u521b\u5efa\u5bb9\u5668\u7684\u57fa\u7840\uff0c\u7c7b\u4f3c\u865a\u62df\u673a\u7684\u5feb\u7167,\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u4e2a\u9762\u5411 Docker \u5bb9\u5668\u5f15\u64ce\u7684\u53ea\u8bfb\u6a21\u677f\u3002\u4f8b\u5982\uff1a\u4e00\u4e2a\u955c\u50cf\u53ef\u4ee5\u662f\u4e00\u4e2a\u5b8c\u6574 \u7684 Cent OS \u64cd\u4f5c\u7cfb\u7edf\u73af\u5883\uff0c\u79f0\u4e3a\u4e00\u4e2a CentOS \u955c\u50cf\uff1b\u4e5f\u53ef\u4ee5\u662f\u4e00\u4e2a\u5b89\u88c5\u4e86 MySQL \u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u79f0\u4e4b\u4e3a\u4e00\u4e2a MySQL \u955c\u50cf\u7b49\u7b49\u3002 \u6240\u6709\u7684\u5bb9\u5668\u90fd\u662f\u57fa\u4e8e\u955c\u50cf\u6765\u5b9e\u73b0\u7684\uff5e\uff0c\u6240\u4ee5image \u5c31\u663e\u5f97\u5f88\u91cd\u8981\uff01\uff01\uff01","title":"1.1.3 \u955c\u50cf"},{"location":"docker/docker-docs/#114","text":"Docker \u7684\u5bb9\u5668\u662f\u4ece\u955c\u50cf\u521b\u5efa\u7684\u8fd0\u884c\u5b9e\u4f8b\uff0c\u5b83\u53ef\u4ee5\u88ab\u542f\u52a8\u3001\u505c\u6b62\u548c\u5220\u9664\u3002\u6240\u521b\u5efa\u7684\u6bcf\u4e00\u4e2a\u5bb9\u5668 \u90fd\u662f\u76f8\u4e92\u9694\u79bb\u3001\u4e92\u4e0d\u53ef\u89c1\uff0c\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u5e73\u53f0\u3002\u53ef\u4ee5\u5c06\u5bb9\u5668\u770b\u4f5c\u662f\u4e00\u4e2a\u7b80\u6613\u7248\u7684 Linux \u73af\u5883\uff0c Docker \u5229\u7528\u5bb9\u5668\u6765\u8fd0\u884c\u548c\u9694\u79bb\u5e94\u7528\u3002","title":"1.1.4 \u5bb9\u5668"},{"location":"docker/docker-docs/#115","text":"\u6709\u4e86\u955c\u50cf\uff0c\u5f53\u7136\u9700\u8981\u6709\u5b58\u653e\u4ed6\u7684\u5730\u65b9\uff0cDocker \u4ed3\u5e93\u5c31\u662f\u7528\u6765\u96c6\u4e2d\u4fdd\u5b58\u955c\u50cf\u7684\u5730\u65b9\uff0c\u5f53\u521b\u5efa\u4e86\u81ea\u5df1\u7684\u955c\u50cf\u4e4b\u540e\uff0c\u53ef\u4ee5\u4f7f\u7528 push \u547d\u4ee4\u5c06\u5b83 \u4e0a\u4f20\u5230\u516c\u6709\u4ed3\u5e93\uff08Public\uff09\u6216\u8005\u79c1\u6709\u4ed3\u5e93\uff08Private\uff09\u3002\u5f53\u4e0b\u6b21\u8981\u5728\u53e6\u5916\u4e00\u53f0\u673a\u5668\u4e0a\u4f7f\u7528\u8fd9\u4e2a\u955c\u50cf \u65f6\uff0c\u53ea\u9700\u4ece\u4ed3\u5e93\u83b7\u53d6\u3002 \u5e38\u89c1\u7684\u955c\u50cf\u4ed3\u5e93: \u963f\u91cc\u4e91\u955c\u50cf\u4ed3\u5e93 Ucloud \u955c\u50cf\u4ed3\u5e93 Harbor \u955c\u50cf\u4ed3\u5e93 docker hub (\u5b98\u65b9\u5e93) \u4ed3\u5e93\u6ce8\u518c\u670d\u52a1\u5668\uff08Registry\uff09\u662f\u5b58\u653e\u4ed3\u5e93\u7684\u5730\u65b9\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u591a\u4e2a\u4ed3\u5e93\u3002\u6bcf\u4e2a\u4ed3\u5e93\u96c6\u4e2d\u5b58\u653e\u67d0 \u4e00\u7c7b\u955c\u50cf\uff0c\u5e76\u4e14\u4f7f\u7528\u4e0d\u540c\u7684\u6807\u7b7e\uff08tag\uff09\u6765\u533a\u5206\u5b83\u4eec\u3002\u76ee\u524d\u6700\u5927\u7684\u516c\u5171\u4ed3\u5e93\u662f docker Hub\uff0c\u5b58\u653e\u4e86 \u6570\u91cf\u5e9e\u5927\u7684\u955c\u50cf\u4f9b\u7528\u6237\u4e0b\u8f7d\u4f7f\u7528\u3002","title":"1.1.5 \u4ed3\u5e93"},{"location":"docker/docker-docs/#12-docker","text":"\u8fd9\u91cc\u4e3b\u8981\u4ecb\u7ecd\u5e38\u89c1\u7684\u51e0\u79cd\uff0cYUM\u9488\u5bf9\u4e8eCentos\u7cfb\u7edf\uff0cAPT\u9488\u5bf9\u4e8eubuntu\u7cfb\u7edf\uff0c\u811a\u672c\u5f0f\u5feb\u6377\u5b89\u88c5\u3002 \u652f\u6301\u7684\u5e73\u53f0: Docker Engine \u53ef\u901a\u8fc7 Docker Desktop \u5728\u5404\u79cdLinux \u5e73\u53f0\u3001 macOS\u548cWindows 10 \u4e0a\u4f7f\u7528\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u9759\u6001\u4e8c\u8fdb\u5236\u5b89\u88c5\u4f7f\u7528\u3002\u5728\u4e0b\u65b9\u627e\u5230\u60a8\u9996\u9009\u7684\u64cd\u4f5c\u7cfb\u7edf\u3002 \u6e29\u99a8\u63d0\u793a \u8fd9\u91cc\u9762\u4e4b\u524d\u9047\u5230\u4e00\u4e2a\u5751\uff0c\u539f\u56e0\u5c31\u662f\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u7684Mac\u529e\u516c\uff0cubuntu\u9700\u8981\u62c9\u4e0d\u6765\u955c\u50cf\uff0c\u9700\u8981Mac\u62c9\u597d\u4e0a\u4f20\u4e0a\u53bb\uff0c\u7ed3\u679cMac\u62c9\u7684\u7248\u672c\u662famd\u67b6\u6784\u7684\uff5e \u89e3\u51b3\u65b9\u6cd5: k8s.gcr.io/nfd/node-feature-discovery:v0.11.0 --platform linux/amd64 \u5b98\u7f51: https://docs.docker.com/engine/install/","title":"1.2 \u5b89\u88c5 Docker"},{"location":"docker/docker-docs/#121-apt","text":"\u79fb\u9664\u65e7docker\u5305 sudo apt-get remove docker docker-engine docker.io containerd runc \u8bbe\u7f6e\u5b58\u50a8\u5e93 \u66f4\u65b0apt\u5305\u7d22\u5f15\u5e76\u5b89\u88c5\u5305\u4ee5\u5141\u8bb8apt\u901a\u8fc7 HTTPS \u4f7f\u7528\u5b58\u50a8\u5e93\uff1a sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release \u6dfb\u52a0 Docker \u7684\u5b98\u65b9 GPG \u5bc6\u94a5\uff1a sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg \u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u8bbe\u7f6e\u5b58\u50a8\u5e93\uff1a echo \\ \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\ $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null \u5b89\u88c5docker sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-compose-plugin","title":"1.2.1 Apt \u65b9\u5f0f\u5b89\u88c5:"},{"location":"docker/docker-docs/#122-yum","text":"\u79fb\u9664\u65e7docker\u5305 sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine \u914d\u7f6eYUM\u6e90 sudo yum install -y yum-utils sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo \u5b89\u88c5docker-ce sudo yum install -y docker-ce docker-ce-cli containerd.io #\u4ee5\u4e0b\u662f\u5728\u5b89\u88c5k8s\u7684\u65f6\u5019\u4f7f\u7528 yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6 \u542f\u52a8 systemctl enable docker --now \u914d\u7f6e\u52a0\u901f sudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json <<-'EOF' { \"registry-mirrors\": [\"https://82m9ar63.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF sudo systemctl daemon-reload sudo systemctl restart docker","title":"1.2.2 YUM \u65b9\u5f0f\u5b89\u88c5:"},{"location":"docker/docker-docs/#123","text":"\u5b98\u65b9\u63d0\u4f9b: curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh rancher\u63d0\u4f9b: curl https://releases.rancher.com/install-docker/19.03.sh | sh \u5b89\u88c5\u597d\u7684 Docker \u7cfb\u7edf\u6709\u4e24\u4e2a\u7a0b\u5e8f\uff0cDocker \u670d\u52a1\u7aef\u548c Docker \u5ba2\u6237\u7aef\u3002\u5176\u4e2d Docker \u670d\u52a1\u7aef\u662f \u4e00\u4e2a\u670d\u52a1\u8fdb\u7a0b\uff0c\u8d1f\u8d23\u7ba1\u7406\u6240\u6709\u5bb9\u5668\u3002Docker \u5ba2\u6237\u7aef\u5219\u626e\u6f14\u7740 Docker \u670d\u52a1\u7aef\u7684\u8fdc\u7a0b\u63a7\u5236\u5668\uff0c\u53ef\u4ee5\u7528 \u6765\u63a7\u5236 Docker \u7684\u670d\u52a1\u7aef\u8fdb\u7a0b\u3002\u5927\u90e8\u5206\u60c5\u51b5\u4e0b Docker \u670d\u52a1\u7aef\u548c\u5ba2\u6237\u7aef\u8fd0\u884c\u5728\u4e00\u53f0\u673a\u5668\u4e0a\u3002","title":"1.2.3 \u811a\u672c\u5f0f\u5b89\u88c5"},{"location":"docker/docker-docs/#13-docker","text":"\u8fd0\u884c Docker \u5bb9\u5668\u524d\u9700\u8981\u672c\u5730\u5b58\u5728\u5bf9\u5e94\u7684\u955c\u50cf\u3002\u5982\u679c\u4e0d\u5b58\u5728\u672c\u5730\u955c\u50cf\uff0cDocker \u5c31\u4f1a\u5c1d\u8bd5\u4ece\u9ed8\u8ba4 \u955c\u50cf\u4ed3\u5e93\u4e0b\u8f7d\u3002\u955c\u50cf\u4ed3\u5e93\u662f\u7531 Docker \u5b98\u65b9\u7ef4\u62a4\u7684\u4e00\u4e2a\u516c\u5171\u4ed3\u5e93\uff0c\u53ef\u4ee5\u6ee1\u8db3\u7528\u6237\u7684\u7edd\u5927\u90e8\u5206\u9700\u6c42\u3002 \u7528\u6237\u4e5f\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6765\u4f7f\u7528\u81ea\u5b9a\u4e49\u7684\u955c\u50cf\u4ed3\u5e93\u3002 \u5e2e\u52a9\u547d\u4ee4: [ ucloud ] root@master0:~# docker --help Usage: docker [ OPTIONS ] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files ( default \"/root/.docker\" ) -c, --context string Name of the context to use to connect to the daemon ( overrides DOCKER_HOST env var and default context set with \"docker context use\" ) -D, --debug Enable debug mode -H, --host list Daemon socket ( s ) to connect to -l, --log-level string Set the logging level ( \"debug\" | \"info\" | \"warn\" | \"error\" | \"fatal\" ) ( default \"info\" ) --tls Use TLS ; implied by --tlsverify --tlscacert string Trust certs signed only by this CA ( default \"/root/.docker/ca.pem\" ) --tlscert string Path to TLS certificate file ( default \"/root/.docker/cert.pem\" ) --tlskey string Path to TLS key file ( default \"/root/.docker/key.pem\" ) --tlsverify Use TLS and verify the remote -v, --version Print version information and quit \u7ba1\u7406CLI: [ ucloud ] root@master0:~# docker search nginx NAME DESCRIPTION STARS OFFICIAL AUTOMATED nginx Official build of Nginx. 17448 [ OK ] \u6267\u884c docker search lamp \u547d\u4ee4\u540e\uff0c\u8fd4\u56de\u5f88\u591a\u5305\u542b lamp \u5173\u952e\u5b57\u7684\u955c\u50cf\uff0c\u5176\u4e2d\u8fd4\u56de\u4fe1\u606f\u5305\u62ec\u955c\u50cf \u540d\u79f0\uff08NAME\uff09\u3001\u63cf\u8ff0\uff08DESCRIPTION\uff09\u3001\u661f\u7ea7\uff08STARS\uff09\u3001\u662f\u5426\u5b98\u65b9\u521b\u5efa\uff08OFFICIAL\uff09\u3001\u662f \u5426\u4e3b\u52a8\u521b\u5efa\uff08AUTOMATED\uff09\u3002\u9ed8\u8ba4\u7684\u8f93\u51fa\u7ed3\u679c\u4f1a\u6309\u7167\u661f\u7ea7\u8bc4\u4ef7\u8fdb\u884c\u6392\u5e8f\uff0c\u8868\u793a\u8be5\u955c\u50cf\u53d7\u6b22\u8fce\u7a0b \u5ea6\u3002\u5728\u4e0b\u8f7d\u955c\u50cf\u65f6\uff0c\u53ef\u4ee5\u53c2\u8003\u661f\u7ea7\u3002\u5728\u641c\u7d22\u65f6\uff0c\u8fd8\u53ef\u4ee5\u4f7f\u7528-s \u6216\u8005--stars=x \u663e\u793a\u6307\u5b9a\u661f\u7ea7\u4ee5\u4e0a\u7684\u955c \u50cf\uff0c\u661f\u7ea7\u8d8a\u9ad8\u8868\u793a\u8d8a\u53d7\u6b22\u8fce\uff1b\u662f\u5426\u5b98\u65b9\u521b\u5efa\u662f\u6307\u662f\u5426\u662f\u7531\u5b98\u65b9\u9879\u76ee\u7ec4\u521b\u5efa\u548c\u7ef4\u62a4\u7684\u955c\u50cf\uff0c\u4e00\u822c\u5b98\u65b9\u9879\u76ee\u7ec4\u7ef4\u62a4\u7684\u955c\u50cf\u4f7f\u7528\u5355\u4e2a\u5355\u8bcd\u4f5c\u4e3a\u955c\u50cf\u540d\u79f0\uff0c\u79f0\u4e3a\u57fa\u7840\u955c\u50cf\u6216\u8005\u6839\u955c\u50cf\u3002\u50cf reinblau/lamp \u8fd9\u79cd \u547d\u540d\u65b9\u5f0f\u7684\u955c\u50cf\uff0c\u8868\u793a\u662f\u7531 docker Hub \u7684\u7528\u6237 reinblau \u521b\u5efa\u5e76\u7ef4\u62a4\u7684\u955c\u50cf\uff0c\u5e26\u6709\u7528\u6237\u540d\u4e3a\u524d\u7f00\uff1b \u662f\u5426\u4e3b\u52a8\u521b\u5efa\u662f\u6307\u662f\u5426\u5141\u8bb8\u7528\u6237\u9a8c\u8bc1\u955c\u50cf\u7684\u6765\u6e90\u548c\u5185\u5bb9\u3002 docker pull image-name //\u83b7\u53d6\u955c\u50cf docker push image-name //\u63a8\u9001\u955c\u50cf docker images //\u67e5\u770b\u955c\u50cf docker rm -f image-name //\u5f3a\u5236\u5220\u9664\u955c\u50cf docker images -aq //\u67e5\u770b\u6240\u6709\u955c\u50cfID docker save -o \u5b58\u50a8\u6587\u4ef6\u540d \u5b58\u50a8\u7684\u955c\u50cf\u540d //\u4fdd\u5b58\u955c\u50cf docker load < \u5b58\u50a8\u6587\u4ef6\u540d //\u5bfc\u5165\u955c\u50cf \u5bf9\u4e8e Docker \u955c\u50cf\u6765\u8bf4\uff0c\u5982\u679c\u4e0b\u8f7d\u955c\u50cf\u65f6\u4e0d\u6307\u5b9a\u6807\u7b7e\uff0c\u5219\u9ed8\u8ba4\u4f1a\u4e0b\u8f7d\u4ed3\u5e93\u4e2d\u6700\u65b0\u7248\u672c\u7684\u955c \u50cf\uff0c\u5373\u9009\u62e9\u6807\u7b7e\u4e3a latest \u6807\u7b7e\uff0c\u4e5f\u53ef\u901a\u8fc7\u6307\u5b9a\u7684\u6807\u7b7e\u6765\u4e0b\u8f7d\u7279\u5b9a\u7248\u672c\u7684\u67d0\u4e00\u955c\u50cf\u3002\u8fd9\u91cc\u6807\u7b7e \uff08tag\uff09\u5c31\u662f\u7528\u6765\u533a\u5206\u955c\u50cf\u7248\u672c\u7684\u3002 \u6e29\u99a8\u63d0\u793a \u5728\u767b\u9646ucloud\u7684\u955c\u50cf\u4ed3\u5e93\uff0c\u9700\u8981\u5c06\u5bc6\u7801\u7528''\u5f15\u8d77\u6765\uff0c\u4f8b\u5982: docker login -u user-name -p 'password' uhub.service.ucloud.cn","title":"1.3 Docker \u955c\u50cf\u64cd\u4f5c"},{"location":"docker/docker-docs/#14-docker","text":"\u5bb9\u5668\u662f Docker \u7684\u53e6\u4e00\u4e2a\u6838\u5fc3\u6982\u5ff5\u3002\u7b80\u5355\u8bf4\uff0c\u5bb9\u5668\u662f\u955c\u50cf\u7684\u4e00\u4e2a\u8fd0\u884c\u5b9e\u4f8b\uff0c\u662f\u72ec\u7acb\u8fd0\u884c\u7684\u4e00\u4e2a \u6216\u4e00\u7ec4\u5e94\u7528\u4ee5\u53ca\u5b83\u4eec\u6240\u5fc5\u9700\u7684\u8fd0\u884c\u73af\u5883\uff0c\u5305\u62ec\u6587\u4ef6\u7cfb\u7edf\u3001\u7cfb\u7edf\u7c7b\u5e93\u3001shell \u73af\u5883\u7b49\u3002\u955c\u50cf\u662f\u53ea\u8bfb\u6a21 \u677f\uff0c\u800c\u5bb9\u5668\u4f1a\u7ed9\u8fd9\u4e2a\u53ea\u8bfb\u6a21\u677f\u6dfb\u52a0\u4e00\u4e2a\u989d\u5916\u7684\u53ef\u5199\u5c42\u3002 \u7ba1\u7406CLI: docker stop \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker start \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker restart \u5bb9\u5668\u540d\u79f0/\u5bb9\u5668ID docker ps -aq \u67e5\u770b\u6240\u6709\u5bb9\u5668ID docker export \u5bb9\u5668ID/\u540d\u79f0 > \u6587\u4ef6\u540d //\u5bb9\u5668\u5bfc\u51fa cat \u6587\u4ef6\u540d | docker import - \u751f\u6210\u7684\u955c\u50cf\u540d\u79f0:\u6807\u7b7e //\u5bb9\u5668\u5bfc\u5165","title":"1.4 Docker \u5bb9\u5668\u64cd\u4f5c"},{"location":"docker/docker-docs/#15-docker","text":"Docker \u955c\u50cf\u9664\u4e86\u662f Docker \u7684\u6838\u5fc3\u6280\u672f\u4e4b\u5916\uff0c\u4e5f\u662f\u5e94\u7528\u53d1\u5e03\u7684\u6807\u51c6\u683c\u5f0f\u3002\u4e00\u4e2a\u5b8c\u6574\u7684 Docker \u955c\u50cf\u53ef\u4ee5\u652f\u6491\u4e00\u4e2a Docker \u5bb9\u5668\u7684\u8fd0\u884c\uff0c\u5728 Docker \u7684\u6574\u4e2a\u4f7f\u7528\u8fc7\u7a0b\u4e2d\uff0c\u8fdb\u5165\u4e00\u4e2a\u5df2\u7ecf\u5b9a\u578b\u7684\u5bb9\u5668\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u5728\u5bb9\u5668\u4e2d\u8fdb\u884c\u64cd\u4f5c\uff0c\u6700\u5e38\u89c1\u7684\u64cd\u4f5c\u5c31\u662f\u5728\u5bb9\u5668\u4e2d\u5b89\u88c5\u5e94\u7528\u670d\u52a1\u3002 \u5982\u679c\u8981\u628a\u5df2\u7ecf\u5b89\u88c5\u7684\u670d\u52a1\u8fdb\u884c\u8fc1\u79fb\uff0c\u5c31\u9700\u8981\u628a\u73af\u5883\u4ee5\u53ca\u642d\u5efa\u7684\u670d\u52a1\u751f\u6210\u65b0\u7684\u955c\u50cf\u3002\u672c\u6848\u4f8b\u5c06\u4ecb\u7ecd \u5982\u4f55\u521b\u5efa Docker \u955c\u50cf\u3002","title":"1.5 Docker \u955c\u50cf\u7ba1\u7406"},{"location":"docker/docker-docs/#151-docker","text":"\u955c\u50cf\u4e0d\u662f\u4e00\u4e2a\u5355\u4e00\u7684\u6587\u4ef6\uff0c\u800c\u662f\u6709\u591a\u5c42\u6784\u6210\u3002\u53ef\u4ee5\u901a\u8fc7 docker history \u547d\u4ee4\u67e5\u770b\u955c\u50cf\u4e2d\u5404 \u5c42\u5185\u5bb9\u53ca\u5927\u5c0f\uff0c\u6bcf\u5c42\u5bf9\u5e94\u7740 Dockerfile \u4e2d\u7684\u4e00\u6761\u6307\u4ee4\u3002Docker \u955c\u50cf\u9ed8\u8ba4\u5b58\u50a8\u5728 /var/lib/docker/ \u76ee\u5f55\u4e2d\u3002\u5bb9\u5668\u5176\u5b9e\u662f\u5728\u955c\u50cf\u7684\u6700\u4e0a\u9762\u52a0\u4e86\u4e00\u5c42\u8bfb\u5199\u5c42\uff0c \u5728\u8fd0 \u884c\u5bb9\u5668\u91cc\u505a\u7684\u4efb\u4f55\u6587\u4ef6\u6539\u52a8\uff0c\u90fd\u4f1a\u5199\u5230\u8fd9\u4e2a\u8bfb\u5199\u5c42\u3002\u5982\u679c\u5220\u9664\u4e86\u5bb9\u5668\uff0c\u4e5f\u5c31\u5220\u9664\u4e86\u5176\u6700\u4e0a\u9762\u7684 \u8bfb\u5199\u5c42\uff0c\u6587\u4ef6\u6539\u52a8\u4e5f\u5c31\u4e22\u5931\u4e86\u3002Docker \u4f7f\u7528\u5b58\u50a8\u9a71\u52a8\u7ba1\u7406\u955c\u50cf\u6bcf\u5c42\u5185\u5bb9\u53ca\u53ef\u8bfb\u5199\u5c42\u7684\u5bb9\u5668 \u5c42\u3002Docker \u955c\u50cf\u662f\u5206\u5c42\u7684\uff0c\u4e0b\u9762\u8fd9\u4e9b\u77e5\u8bc6\u70b9\u975e\u5e38\u91cd\u8981\u3002 \uff081\uff09Dockerfile \u4e2d\u7684\u6bcf\u4e2a\u6307\u4ee4\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf\u5c42\uff1b \uff082\uff09\u955c\u50cf\u5c42\u5c06\u88ab\u7f13\u5b58\u548c\u590d\u7528\uff1b \uff083\uff09 \u5f53Dockerfile \u7684\u6307\u4ee4\u4fee\u6539\u4e86\uff0c\u590d\u5236\u7684\u6587\u4ef6\u53d8\u5316\u4e86\uff0c\u6216\u8005\u6784\u5efa\u955c\u50cf\u65f6\u6307\u5b9a\u7684\u53d8\u91cf\u4e0d\u540c \u4e86\uff0c\u5bf9\u5e94\u7684\u955c\u50cf\u5c42\u7f13\u5b58\u5c31\u4f1a\u5931\u6548\uff1b \uff084\uff09\u67d0\u4e00\u5c42\u7684\u955c\u50cf\u7f13\u5b58\u5931\u6548\uff0c\u5b83\u4e4b\u540e\u7684\u955c\u50cf\u5c42\u7f13\u5b58\u90fd\u4f1a\u5931\u6548\uff1b \uff085\uff09\u955c\u50cf\u5c42\u662f\u4e0d\u53ef\u53d8\u7684\uff0c\u5982\u679c\u5728\u67d0\u4e00\u5c42\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u6587\u4ef6\uff0c\u7136\u540e\u5728\u4e0b\u4e00\u5c42\u4e2d\u5220\u9664\u5b83\uff0c\u5219\u955c \u50cf\u4e2d\u4f9d\u7136\u4f1a\u5305\u542b\u8be5\u6587\u4ef6\uff0c\u53ea\u662f\u8fd9\u4e2a\u6587\u4ef6\u5728 Docker \u5bb9\u5668\u4e2d\u4e0d\u53ef\u89c1\u4e86\u3002","title":"1.5.1 Docker\u955c\u50cf\u7ed3\u6784"},{"location":"docker/docker-docs/#152-dockerfile","text":"Dockfile \u662f\u4e00\u79cd\u88ab Docker \u7a0b\u5e8f\u89e3\u91ca\u7684\u811a\u672c\uff0cDockerfile \u7531\u591a\u6761\u7684\u6307\u4ee4\u7ec4\u6210\uff0c\u6bcf\u6761\u6307\u4ee4\u5bf9 \u5e94Linux \u4e0b\u9762\u7684\u4e00\u6761\u547d\u4ee4\u3002Docker \u7a0b\u5e8f\u5c06\u8fd9\u4e9bDockerfile \u6307\u4ee4\u7ffb\u8bd1\u6210\u771f\u6b63\u7684Linux \u547d\u4ee4\u3002 Dockerfile \u6709\u81ea\u5df1\u4e66\u5199\u683c\u5f0f\u548c\u652f\u6301\u7684\u547d\u4ee4\uff0cDocker \u7a0b\u5e8f\u89e3\u51b3\u8fd9\u4e9b\u547d\u4ee4\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u7c7b\u4f3c\u4e8e Makefile\u3002Docker \u7a0b\u5e8f\u5c06\u8bfb\u53d6 Dockerfile\uff0c\u6839\u636e\u6307\u4ee4\u751f\u6210\u5b9a\u5236\u7684\u955c\u50cf\u3002\u76f8\u6bd4\u955c\u50cf\u8fd9\u79cd\u9ed1\u76d2\u5b50\uff0c Dockerfile \u8fd9\u79cd\u663e\u800c\u6613\u89c1\u7684\u811a\u672c\u66f4\u5bb9\u6613\u88ab\u4f7f\u7528\u8005\u63a5\u53d7\uff0c\u5b83\u660e\u786e\u7684\u8868\u660e\u955c\u50cf\u662f\u600e\u4e48\u4ea7\u751f\u7684\u3002\u6709\u4e86 Dockerfile\uff0c\u5f53\u6709\u5b9a\u5236\u989d\u5916\u7684\u9700\u6c42\u65f6\uff0c\u53ea\u9700\u5728 Dockerfile \u4e0a\u6dfb\u52a0\u6216\u8005\u4fee\u6539\u6307\u4ee4\uff0c \u91cd\u65b0\u751f\u6210\u955c\u50cf\u3002","title":"1.5.2 Dockerfile\u4ecb\u7ecd"},{"location":"docker/docker-docs/#_1","text":"","title":"\u9644\u4ef6:"},{"location":"error/dev/","text":"Dev\u96c6\u7fa4\u95ee\u9898 \u00b6 rook-ceph\u96c6\u7fa4\u5d29\u6e83 \u73b0\u8c61: rook-ceph\u96c6\u7fa4\u9664\u4e86mon\uff0c\u6240\u6709pod\u7684\u72b6\u6001\u5904\u4e8eCrashLoopBackOff mgr \u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c operator\u7684log\u4e2d\u65e0\u660e\u663e\u62a5\u9519\uff0cceph-cluster-controller: failed to get ceph daemons versions, this typically happens during the first cluster initialization. failed to run 'ceph versions' mon \u88aboperator\u4e00\u5171\u8d77\u67654\u4e2adeploy \u9519\u8bef\u76f8\u4f3c\u5ea6: https://github.com/rook/rook/issues/6530 \u521d\u6b65\u6000\u7591\u662frook-ceph\u7684osd\u548c\u7cfb\u7edf\u78c1\u76d8\u4f7f\u7528\u5feb\u6ee1\u800c\u5bfc\u81f4\u7684\u96c6\u7fa4\u5d29\u6e83\uff0c\u53ea\u6709mon\u6b63\u5e38\uff0cmgr\u548cosd\u90fd\u5f02\u5e38 \u65e0\u6cd5\u6b63\u5e38running $ kubectl -n rook-ceph get pod -w ... rook-ceph-mgr-a-7fd6649d8c-jqvmv 0 /1 CrashLoopBackOff 35 20h rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:CrashLoopBackOff 1 6s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:2/4 2 18s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:Error 2 19s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:CrashLoopBackOff 2 31s \u9996\u5148\u9700\u8981\u5c06\u96c6\u7fa4\u6062\u590d\u5230\u6b63\u5e38\u7684\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u91cc\u6709\u4e2a\u95ee\u9898\u5c31\u662fmon\u4e00\u4e2a\u5904\u4e8e\u4e00\u4e2a4\u526f\u672c\u7684\u72b6\u6001,\u6709\u4e00\u4e2a\u526f\u672c\u72b6\u6001\u5f02\u5e38 \u67e5\u770b\u5f02\u5e38\u72b6\u6001\u4e0bmon\u7684configmap,\u8fd9\u9700\u8981\u5c06\u5f02\u5e38\u7684\u90a3\u4e2apod\u7684\u6240\u6709\u914d\u7f6e\u5220\u9664 $ kg configmap rook-ceph-mon-endpoints -o yaml apiVersion : v1 data : csi-cluster-config-json : '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.0.0.225:6789\",\"10.0.0.10:6789\",\"10.0.0.79:6789\",\"10.0.0.225:6789\"]}]' data : c=10.0.0.10:6789,a=10.0.0.79:6789,p=10.0.0.225:6789,q=10.0.0.225:6789 mapping : '{\"node\":{\"a\":{\"Name\":\"node1\",\"Hostname\":\"node1\",\"Address\":\"10.0.0.79\"},\"c\":{\"Name\":\"node0\",\"Hostname\":\"node0\",\"Address\":\"10.0.0.10\"},\"p\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"},\"q\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"}}}' maxMonId : \"16\" kind : ConfigMap metadata : creationTimestamp : \"2020-12-16T18:00:19Z\" name : rook-ceph-mon-endpoints namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"555335505\" uid : 1ee20194-f90e-4736-8d7f-89ac0314556c \u7559\u4e0b\u4e09\u4e2a\u6b63\u5e38\u7684mon $ k get configmap rook-ceph-mon-endpoints -o yaml apiVersion : v1 data : csi-cluster-config-json : '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.0.0.225:6789\",\"10.0.0.10:6789\",\"10.0.0.79:6789\"]}]' data : q=10.0.0.225:6789,c=10.0.0.10:6789,a=10.0.0.79:6789 mapping : '{\"node\":{\"a\":{\"Name\":\"node1\",\"Hostname\":\"node1\",\"Address\":\"10.0.0.79\"},\"c\":{\"Name\":\"node0\",\"Hostname\":\"node0\",\"Address\":\"10.0.0.10\"},\"q\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"}}}' maxMonId : \"16\" kind : ConfigMap metadata : creationTimestamp : \"2020-12-16T18:00:19Z\" name : rook-ceph-mon-endpoints namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"555840878\" uid : 1ee20194-f90e-4736-8d7f-89ac0314556c \u4fee\u6539\u5b8c\u6210\u53d1\u73b0mgr\u6b63\u5728\u6062\u590d\uff0c\u96c6\u7fa4\u7684\u72b6\u6001\u6162\u6162\u6062\u590d\u6b63\u5e38,\u4e00\u76f4\u7b49\u5230\u96c6\u7fa4\u6062\u590d\u6b63\u5e38\u3002\u8fd9\u65f6\u67e5\u770b\u96c6\u7fa4\u7684\u72b6\u6001 \u8fd8\u662f\u6709\u4e00\u4e9b\u5f02\u5e38\uff0c\u5927\u6982\u7684\u610f\u601d\u5c31\u662fmon-a\u6240\u5728\u7684\u673a\u5668\u7684\u7cfb\u7edf\u76d8\u5feb\u6ee1\u4e86 \u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65\uff0c\u6269\u5bb9\u7cfb\u7edf\u76d8\uff0c\u673a\u5668\u5728ucloud\u4e91\uff0c\u6240\u4ee5\u6211\u4eec\u5148\u5728ui\u754c\u9762\u8fdb\u884c\u6269\u5bb9\uff0c\u8fd9\u91cc\u53ef\u4ee5\u53c2\u8003 Ucloud\u6269\u5bb9 Ubuntu\uff1a sudo apt-get install cloud-initramfs-growroot LANG=en_US.UTF-8 growpart /dev/vda 1 \u8fd9\u65f6\uff0c\u6211\u4eec\u518d\u8fdb\u884c\u67e5\u770b\u6211\u4eec\u7684ceph\u96c6\u7fa4 [ root@rook-ceph-tools-84fc455b76-5dlwr / ] # ceph -s cluster: id: fc55d844-ed6b-4c0b-9f0f-a6b453ffb9b6 health: HEALTH_WARN 1 pool ( s ) do not have an application enabled 2 pool ( s ) have no replicas configured services: mon: 3 daemons, quorum a,c,q ( age 20h ) mgr: a ( active, since 18h ) mds: myfs:1 { 0 = myfs-b = up:active } 1 up:standby-replay osd: 7 osds: 7 up ( since 18h ) , 7 in ( since 18h ) task status: scrub status: mds.myfs-a: idle mds.myfs-b: idle data: pools: 6 pools, 241 pgs objects: 369 .17k objects, 92 GiB usage: 181 GiB used, 259 GiB / 440 GiB avail pgs: 241 active+clean io: client: 3 .3 KiB/s rd, 9 .3 KiB/s wr, 3 op/s rd, 1 op/s wr","title":"Dev\u73af\u5883\u96c6\u7fa4"},{"location":"error/dev/#dev","text":"rook-ceph\u96c6\u7fa4\u5d29\u6e83 \u73b0\u8c61: rook-ceph\u96c6\u7fa4\u9664\u4e86mon\uff0c\u6240\u6709pod\u7684\u72b6\u6001\u5904\u4e8eCrashLoopBackOff mgr \u65e0\u6cd5\u6b63\u5e38\u5de5\u4f5c operator\u7684log\u4e2d\u65e0\u660e\u663e\u62a5\u9519\uff0cceph-cluster-controller: failed to get ceph daemons versions, this typically happens during the first cluster initialization. failed to run 'ceph versions' mon \u88aboperator\u4e00\u5171\u8d77\u67654\u4e2adeploy \u9519\u8bef\u76f8\u4f3c\u5ea6: https://github.com/rook/rook/issues/6530 \u521d\u6b65\u6000\u7591\u662frook-ceph\u7684osd\u548c\u7cfb\u7edf\u78c1\u76d8\u4f7f\u7528\u5feb\u6ee1\u800c\u5bfc\u81f4\u7684\u96c6\u7fa4\u5d29\u6e83\uff0c\u53ea\u6709mon\u6b63\u5e38\uff0cmgr\u548cosd\u90fd\u5f02\u5e38 \u65e0\u6cd5\u6b63\u5e38running $ kubectl -n rook-ceph get pod -w ... rook-ceph-mgr-a-7fd6649d8c-jqvmv 0 /1 CrashLoopBackOff 35 20h rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:CrashLoopBackOff 1 6s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:2/4 2 18s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:Error 2 19s rook-ceph-osd-0-58c8d8ccf8-96wtd 0 /1 Init:CrashLoopBackOff 2 31s \u9996\u5148\u9700\u8981\u5c06\u96c6\u7fa4\u6062\u590d\u5230\u6b63\u5e38\u7684\u4e00\u4e2a\u72b6\u6001\uff0c\u8fd9\u91cc\u6709\u4e2a\u95ee\u9898\u5c31\u662fmon\u4e00\u4e2a\u5904\u4e8e\u4e00\u4e2a4\u526f\u672c\u7684\u72b6\u6001,\u6709\u4e00\u4e2a\u526f\u672c\u72b6\u6001\u5f02\u5e38 \u67e5\u770b\u5f02\u5e38\u72b6\u6001\u4e0bmon\u7684configmap,\u8fd9\u9700\u8981\u5c06\u5f02\u5e38\u7684\u90a3\u4e2apod\u7684\u6240\u6709\u914d\u7f6e\u5220\u9664 $ kg configmap rook-ceph-mon-endpoints -o yaml apiVersion : v1 data : csi-cluster-config-json : '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.0.0.225:6789\",\"10.0.0.10:6789\",\"10.0.0.79:6789\",\"10.0.0.225:6789\"]}]' data : c=10.0.0.10:6789,a=10.0.0.79:6789,p=10.0.0.225:6789,q=10.0.0.225:6789 mapping : '{\"node\":{\"a\":{\"Name\":\"node1\",\"Hostname\":\"node1\",\"Address\":\"10.0.0.79\"},\"c\":{\"Name\":\"node0\",\"Hostname\":\"node0\",\"Address\":\"10.0.0.10\"},\"p\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"},\"q\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"}}}' maxMonId : \"16\" kind : ConfigMap metadata : creationTimestamp : \"2020-12-16T18:00:19Z\" name : rook-ceph-mon-endpoints namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"555335505\" uid : 1ee20194-f90e-4736-8d7f-89ac0314556c \u7559\u4e0b\u4e09\u4e2a\u6b63\u5e38\u7684mon $ k get configmap rook-ceph-mon-endpoints -o yaml apiVersion : v1 data : csi-cluster-config-json : '[{\"clusterID\":\"rook-ceph\",\"monitors\":[\"10.0.0.225:6789\",\"10.0.0.10:6789\",\"10.0.0.79:6789\"]}]' data : q=10.0.0.225:6789,c=10.0.0.10:6789,a=10.0.0.79:6789 mapping : '{\"node\":{\"a\":{\"Name\":\"node1\",\"Hostname\":\"node1\",\"Address\":\"10.0.0.79\"},\"c\":{\"Name\":\"node0\",\"Hostname\":\"node0\",\"Address\":\"10.0.0.10\"},\"q\":{\"Name\":\"master0\",\"Hostname\":\"master0\",\"Address\":\"10.0.0.225\"}}}' maxMonId : \"16\" kind : ConfigMap metadata : creationTimestamp : \"2020-12-16T18:00:19Z\" name : rook-ceph-mon-endpoints namespace : rook-ceph ownerReferences : - apiVersion : ceph.rook.io/v1 blockOwnerDeletion : true controller : true kind : CephCluster name : rook-ceph uid : ee10d125-4428-4e88-983a-53190bc3411c resourceVersion : \"555840878\" uid : 1ee20194-f90e-4736-8d7f-89ac0314556c \u4fee\u6539\u5b8c\u6210\u53d1\u73b0mgr\u6b63\u5728\u6062\u590d\uff0c\u96c6\u7fa4\u7684\u72b6\u6001\u6162\u6162\u6062\u590d\u6b63\u5e38,\u4e00\u76f4\u7b49\u5230\u96c6\u7fa4\u6062\u590d\u6b63\u5e38\u3002\u8fd9\u65f6\u67e5\u770b\u96c6\u7fa4\u7684\u72b6\u6001 \u8fd8\u662f\u6709\u4e00\u4e9b\u5f02\u5e38\uff0c\u5927\u6982\u7684\u610f\u601d\u5c31\u662fmon-a\u6240\u5728\u7684\u673a\u5668\u7684\u7cfb\u7edf\u76d8\u5feb\u6ee1\u4e86 \u9700\u8981\u8fdb\u884c\u4e0b\u4e00\u6b65\uff0c\u6269\u5bb9\u7cfb\u7edf\u76d8\uff0c\u673a\u5668\u5728ucloud\u4e91\uff0c\u6240\u4ee5\u6211\u4eec\u5148\u5728ui\u754c\u9762\u8fdb\u884c\u6269\u5bb9\uff0c\u8fd9\u91cc\u53ef\u4ee5\u53c2\u8003 Ucloud\u6269\u5bb9 Ubuntu\uff1a sudo apt-get install cloud-initramfs-growroot LANG=en_US.UTF-8 growpart /dev/vda 1 \u8fd9\u65f6\uff0c\u6211\u4eec\u518d\u8fdb\u884c\u67e5\u770b\u6211\u4eec\u7684ceph\u96c6\u7fa4 [ root@rook-ceph-tools-84fc455b76-5dlwr / ] # ceph -s cluster: id: fc55d844-ed6b-4c0b-9f0f-a6b453ffb9b6 health: HEALTH_WARN 1 pool ( s ) do not have an application enabled 2 pool ( s ) have no replicas configured services: mon: 3 daemons, quorum a,c,q ( age 20h ) mgr: a ( active, since 18h ) mds: myfs:1 { 0 = myfs-b = up:active } 1 up:standby-replay osd: 7 osds: 7 up ( since 18h ) , 7 in ( since 18h ) task status: scrub status: mds.myfs-a: idle mds.myfs-b: idle data: pools: 6 pools, 241 pgs objects: 369 .17k objects, 92 GiB usage: 181 GiB used, 259 GiB / 440 GiB avail pgs: 241 active+clean io: client: 3 .3 KiB/s rd, 9 .3 KiB/s wr, 3 op/s rd, 1 op/s wr","title":"Dev\u96c6\u7fa4\u95ee\u9898"},{"location":"error/tjtu/","text":"\u5929\u6d25\u5927\u5b66\u96c6\u7fa4\u95ee\u9898 \u00b6 \u95ee\u9898\u590d\u73b0 \u4ece\u4e0b\u56fe\u53ef\u4ee5\u770b\u51fa\u6709\u5f88\u591a\u7684\u4efb\u52a1\u5904\u4e8e\u6570\u636e\u540c\u6b65\u5173\u95ed\u7684\u4e00\u4e2a\u72b6\u6001 \u95ee\u9898\u6392\u67e5 \u67e5\u770bk8s\u96c6\u7fa4\u7684\u72b6\u6001\uff0c\u53d1\u73b0\u4e00\u4e0bpod\u5904\u4e8e\u5f02\u5e38\u72b6\u6001\uff0c\u8fd9\u4e9b\u5f02\u5e38\u7684pod\u53ea\u80fd\u5168\u90e8\u5220\u9664\u91cd\u5efa \u9700\u8981\u67e5\u770b\u4e00\u4e0b\u5f02\u5e38pod\u7684\u8be6\u7ec6\u4fe1\u606f \u95ee\u9898\u51fa\u73b0 \u4ee5\u4e0a\u53ef\u4ee5\u770b\u5230pod\u5728\u521b\u5efa\u7f16\u53f7\u4e3a9\u7684gpu\u51fa\u73b0\u62a5\u9519\uff0c\u5bfc\u81f4\u8fd9\u6837\u7684\u60c5\u51b5\u53ef\u80fd\u5c31\u662fgpu\u51fa\u95ee\u9898\u4e86 \u63a5\u4e0b\u6765\u53ef\u4ee5\u68c0\u67e5\u8fd9\u4e2apod\u6240\u5728\u7684\u673a\u5668\u4e0a\u7684gpu\uff0c\u770b\u662f\u5426\u51fa\u73b0\u95ee\u9898 \u53ef\u4ee5\u901a\u8fc7 nvidia-smi -L \u547d\u4ee4\u6765\u67e5\u770b \u68c0\u67e5gpu \u51fa\u73b0\u4ee5\u4e0b\u60c5\u51b5\uff0c\u5c1d\u8bd5\u91cd\u542f\u770b\u770b\u662f\u5426\u80fd\u4fee\u590d \u5929\u6d25\u5927\u5b66\u96c6\u7fa4gpu\u786c\u4ef6\u635f\u574f GPU\u635f\u574f \u53ef\u4ee5\u901a\u8fc7nvidia-smi -L \u67e5\u770bgpu\u60c5\u51b5 nvidia-smi Unable to determine the device handle for GPU 0000 :89:00.0: Unknown Error \u611f\u89c9\u662f\u8fd9\u5757\u5361 0000:89:00.0 \u51fa\u95ee\u9898\u4e86\u3002\u7136\u540e\u53bb\u6267\u884c\u4e0b dmesg \u770b\u770b\u60c5\u51b5\uff1a $ dmesg -T [ Mon May 9 20 :37:33 2022 ] xhci_hcd 0000 :89:00.2: PCI post-resume error -19! [ Mon May 9 20 :37:33 2022 ] xhci_hcd 0000 :89:00.2: HC died ; cleaning up [ Mon May 9 20 :37:34 2022 ] nvidia-gpu 0000 :89:00.3: i2c timeout error ffffffff [ Mon May 9 20 :37:34 2022 ] ucsi_ccg 6 -0008: i2c_transfer failed -110 $ nvidia-smi drain -p 0000 :89:00.0 -m 1 Successfully set GPU 00000000 :89:00.0 drain state to: drainin \u5c4f\u853d\u5b8c\u6210\u8fd9\u53f0\u673a\u5668\uff0c\u9700\u8981\u8fdb\u884c\u91cd\u65b0\u542f\u52a8","title":"\u5929\u6d25\u96c6\u7fa4"},{"location":"error/tjtu/#_1","text":"\u95ee\u9898\u590d\u73b0 \u4ece\u4e0b\u56fe\u53ef\u4ee5\u770b\u51fa\u6709\u5f88\u591a\u7684\u4efb\u52a1\u5904\u4e8e\u6570\u636e\u540c\u6b65\u5173\u95ed\u7684\u4e00\u4e2a\u72b6\u6001 \u95ee\u9898\u6392\u67e5 \u67e5\u770bk8s\u96c6\u7fa4\u7684\u72b6\u6001\uff0c\u53d1\u73b0\u4e00\u4e0bpod\u5904\u4e8e\u5f02\u5e38\u72b6\u6001\uff0c\u8fd9\u4e9b\u5f02\u5e38\u7684pod\u53ea\u80fd\u5168\u90e8\u5220\u9664\u91cd\u5efa \u9700\u8981\u67e5\u770b\u4e00\u4e0b\u5f02\u5e38pod\u7684\u8be6\u7ec6\u4fe1\u606f \u95ee\u9898\u51fa\u73b0 \u4ee5\u4e0a\u53ef\u4ee5\u770b\u5230pod\u5728\u521b\u5efa\u7f16\u53f7\u4e3a9\u7684gpu\u51fa\u73b0\u62a5\u9519\uff0c\u5bfc\u81f4\u8fd9\u6837\u7684\u60c5\u51b5\u53ef\u80fd\u5c31\u662fgpu\u51fa\u95ee\u9898\u4e86 \u63a5\u4e0b\u6765\u53ef\u4ee5\u68c0\u67e5\u8fd9\u4e2apod\u6240\u5728\u7684\u673a\u5668\u4e0a\u7684gpu\uff0c\u770b\u662f\u5426\u51fa\u73b0\u95ee\u9898 \u53ef\u4ee5\u901a\u8fc7 nvidia-smi -L \u547d\u4ee4\u6765\u67e5\u770b \u68c0\u67e5gpu \u51fa\u73b0\u4ee5\u4e0b\u60c5\u51b5\uff0c\u5c1d\u8bd5\u91cd\u542f\u770b\u770b\u662f\u5426\u80fd\u4fee\u590d \u5929\u6d25\u5927\u5b66\u96c6\u7fa4gpu\u786c\u4ef6\u635f\u574f GPU\u635f\u574f \u53ef\u4ee5\u901a\u8fc7nvidia-smi -L \u67e5\u770bgpu\u60c5\u51b5 nvidia-smi Unable to determine the device handle for GPU 0000 :89:00.0: Unknown Error \u611f\u89c9\u662f\u8fd9\u5757\u5361 0000:89:00.0 \u51fa\u95ee\u9898\u4e86\u3002\u7136\u540e\u53bb\u6267\u884c\u4e0b dmesg \u770b\u770b\u60c5\u51b5\uff1a $ dmesg -T [ Mon May 9 20 :37:33 2022 ] xhci_hcd 0000 :89:00.2: PCI post-resume error -19! [ Mon May 9 20 :37:33 2022 ] xhci_hcd 0000 :89:00.2: HC died ; cleaning up [ Mon May 9 20 :37:34 2022 ] nvidia-gpu 0000 :89:00.3: i2c timeout error ffffffff [ Mon May 9 20 :37:34 2022 ] ucsi_ccg 6 -0008: i2c_transfer failed -110 $ nvidia-smi drain -p 0000 :89:00.0 -m 1 Successfully set GPU 00000000 :89:00.0 drain state to: drainin \u5c4f\u853d\u5b8c\u6210\u8fd9\u53f0\u673a\u5668\uff0c\u9700\u8981\u8fdb\u884c\u91cd\u65b0\u542f\u52a8","title":"\u5929\u6d25\u5927\u5b66\u96c6\u7fa4\u95ee\u9898"},{"location":"k3s/k3s-install/","text":"Docker \u8fd0\u884c\u65f6\u90e8\u7f72k3s \u00b6 \u53c2\u8003\u5730\u5740\uff1ahttps://docs.rancher.cn/docs/k3s/quick-start/_index \u57fa\u7840\u73af\u5883 \u00b6 \u73af\u5883\u8981\u6c42 Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB. \u65f6\u95f4\u540c\u6b65\uff0c\u65f6\u533a \u5173\u95ed\u9632\u706b\u5899 docker\u73af\u5883 \u5916\u7f6e\u73af\u5883\u8981\u6c42 \u9700\u8981\u5b89\u88c5docker\u73af\u5883\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u9ed8\u8ba4\u4e0d\u662fdocker. \u5b89\u88c5docker\uff1a curl https://releases.rancher.com/install-docker/19.03.sh | sh k3s\u5b89\u88c5server\u7aef \u00b6 Warning --docker \u662f\u4f7f\u7528docker\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker \u6307\u5b9a\u5b89\u88c5\uff1aINSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_VERSION=v1.22.5+k3s1 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker k3s\u90e8\u7f72agent\u7aef \u00b6 \u5148\u5728server\u7aef\u62ff\u5230token\uff1a root@ubuntu:~# cat /var/lib/rancher/k3s/server/node-token K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR = cn K3S_URL = https://192.168.0.202:6443 K3S_TOKEN = K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 INSTALL_K3S_EXEC = \"--docker\" sh - \u6307\u5b9a\u5b89\u88c5\uff1a curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10e3cfbb8fc176b66cb7957997cd7d01958fbbdd507d3c49b75ff819cd0a93905b::server:cba4412e36cc16e5a92137bf987d575d INSTALL_K3S_EXEC=\"--docker\" sh - \u66f4\u65b0\u7248\uff1a \u00b6 k3s\u5b89\u88c5server\u7aef \u00b6 curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker k3s \u90e8\u7f72agent\u7aef \u00b6 curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10ab8ec9db721e53cf65e395d44b2b59a48ee1744dcf7c691733f907af47c88630::server:806d6ca499e7ae52c75c89a7f47961b4 sh -s - --docker \u5378\u8f7dk3s \u00b6 \u8fd9\u91cc\u53ef\u4ee5\u901a\u8fc7 k3s \u5b98\u65b9\u65b9\u5f0f\u5220\u9664 install k3s curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.23.10+k3s1 sh -s - --docker","title":"K3S\u5b89\u88c5"},{"location":"k3s/k3s-install/#docker-k3s","text":"\u53c2\u8003\u5730\u5740\uff1ahttps://docs.rancher.cn/docs/k3s/quick-start/_index","title":"Docker \u8fd0\u884c\u65f6\u90e8\u7f72k3s"},{"location":"k3s/k3s-install/#_1","text":"\u73af\u5883\u8981\u6c42 Lightweight Kubernetes. Easy to install, half the memory, all in a binary of less than 100 MB. \u65f6\u95f4\u540c\u6b65\uff0c\u65f6\u533a \u5173\u95ed\u9632\u706b\u5899 docker\u73af\u5883 \u5916\u7f6e\u73af\u5883\u8981\u6c42 \u9700\u8981\u5b89\u88c5docker\u73af\u5883\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6\uff0c\u9ed8\u8ba4\u4e0d\u662fdocker. \u5b89\u88c5docker\uff1a curl https://releases.rancher.com/install-docker/19.03.sh | sh","title":"\u57fa\u7840\u73af\u5883"},{"location":"k3s/k3s-install/#k3sserver","text":"Warning --docker \u662f\u4f7f\u7528docker\u4f5c\u4e3a\u5bb9\u5668\u8fd0\u884c\u65f6 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker \u6307\u5b9a\u5b89\u88c5\uff1aINSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_VERSION=v1.22.5+k3s1 curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker","title":"k3s\u5b89\u88c5server\u7aef"},{"location":"k3s/k3s-install/#k3sagent","text":"\u5148\u5728server\u7aef\u62ff\u5230token\uff1a root@ubuntu:~# cat /var/lib/rancher/k3s/server/node-token K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 \u9ed8\u8ba4\u5b89\u88c5\uff1a\uff08\u9ed8\u8ba4\u662f\u53ea\u5b89\u88c5\u6700\u65b0\u7248\uff09 curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR = cn K3S_URL = https://192.168.0.202:6443 K3S_TOKEN = K10c549fbf4c0197251998eff2e9f451222f297839d4c9150543e6b1b7935a46936::server:588e0646787eb8610efd7b9c1e9fcef0 INSTALL_K3S_EXEC = \"--docker\" sh - \u6307\u5b9a\u5b89\u88c5\uff1a curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | INSTALL_K3S_VERSION=v1.22.5+k3s1 INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10e3cfbb8fc176b66cb7957997cd7d01958fbbdd507d3c49b75ff819cd0a93905b::server:cba4412e36cc16e5a92137bf987d575d INSTALL_K3S_EXEC=\"--docker\" sh -","title":"k3s\u90e8\u7f72agent\u7aef"},{"location":"k3s/k3s-install/#_2","text":"","title":"\u66f4\u65b0\u7248\uff1a"},{"location":"k3s/k3s-install/#k3sserver_1","text":"curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 sh -s - --docker","title":"k3s\u5b89\u88c5server\u7aef"},{"location":"k3s/k3s-install/#k3s-agent","text":"curl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.22.5+k3s1 K3S_URL=https://192.168.0.202:6443 K3S_TOKEN=K10ab8ec9db721e53cf65e395d44b2b59a48ee1744dcf7c691733f907af47c88630::server:806d6ca499e7ae52c75c89a7f47961b4 sh -s - --docker","title":"k3s \u90e8\u7f72agent\u7aef"},{"location":"k3s/k3s-install/#k3s","text":"\u8fd9\u91cc\u53ef\u4ee5\u901a\u8fc7 k3s \u5b98\u65b9\u65b9\u5f0f\u5220\u9664 install k3s curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn INSTALL_K3S_VERSION=v1.23.10+k3s1 sh -s - --docker","title":"\u5378\u8f7dk3s"},{"location":"k8s/base/k8s-overview/","text":"k8s \u7b80\u4ecb \u00b6 \u5f00\u6e90\u5bb9\u5668\u5316\u7f16\u6392\u5f15\u64ce\uff0c\u4e13\u7528\u4e8e\u7ba1\u7406\u5bb9\u5668\u5316\u5e94\u7528\u548c\u670d\u52a1\u96c6\u7fa4\u3002 \u6838\u5fc3\u7ec4\u4ef6\u4ecb\u7ecd \u00b6 \u63a7\u5236\u5e73\u9762\u7ec4\u4ef6 \u00b6 kube-apiserver: \u5b98\u65b9\u89e3\u91ca: API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u7ec4\u4ef6\uff0c \u8be5\u7ec4\u4ef6\u8d1f\u8d23\u516c\u5f00\u4e86 Kubernetes API\uff0c\u8d1f\u8d23\u5904\u7406\u63a5\u53d7\u8bf7\u6c42\u7684\u5de5\u4f5c\u3002 API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u524d\u7aef\u3002 Kubernetes API \u670d\u52a1\u5668\u7684\u4e3b\u8981\u5b9e\u73b0\u662f kube-apiserver\u3002 kube-apiserver \u8bbe\u8ba1\u4e0a\u8003\u8651\u4e86\u6c34\u5e73\u6269\u7f29\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u53ef\u901a\u8fc7\u90e8\u7f72\u591a\u4e2a\u5b9e\u4f8b\u6765\u8fdb\u884c\u6269\u7f29\u3002 \u4f60\u53ef\u4ee5\u8fd0\u884c kube-apiserver \u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u5728\u8fd9\u4e9b\u5b9e\u4f8b\u4e4b\u95f4\u5e73\u8861\u6d41\u91cf\u3002 \u4e2a\u4eba\u7406\u89e3: k8s\u7684\u8bbf\u95ee\u5165\u53e3\uff0c\u5982\u679c\u60f3\u64cd\u4f5c\u5bb9\u5668\uff0c\u5fc5\u987b\u8981\u901a\u8fc7api-server\u624d\u53ef\u4ee5\u3002k8s\u96c6\u7fa4\u6240\u6709\u7684node\u90fd\u8981\u4e0eapi-server\u8fdb\u884c\u901a\u4fe1\uff0c\u6240\u4ee5node\u7684\u6570\u91cf\u8d8a\u591a\u7ed9api-server\u7684\u538b\u529b\u5c31\u4f1a\u8d8a\u5927\u3002\u6240\u4ee5\u4e00\u822ck8s-master\u90fd\u662f\u96c6\u7fa4\u6765\u9ad8\u53ef\u7528\u7684\uff0c\u901a\u5e38\u6765\u8bf4\u90fd\u662f\u4e09\u4e2a\u8282\u70b9\u3002 kube-scheduler: \u5b98\u65b9\u89e3\u91ca: \u662f\u8d1f\u8d23\u8d44\u6e90\u8c03\u5ea6\u7684\u8fdb\u7a0b\uff0c\u76d1\u89c6\u65b0\u521b\u5efa\u4e14\u6ca1\u6709\u5206\u914d\u5230Node\u7684Pod\uff0c\u4e3aPod \u9009\u62e9\u4e00\u4e2aNode\uff1b \u8c03\u5ea6\u51b3\u7b56\u8003\u8651\u7684\u56e0\u7d20\u5305\u62ec\u5355\u4e2a Pod \u53ca Pods \u96c6\u5408\u7684\u8d44\u6e90\u9700\u6c42\u3001 \u8f6f\u786c\u4ef6\u53ca\u7b56\u7565\u7ea6\u675f\u3001 \u4eb2\u548c\u6027\u53ca\u53cd\u4eb2\u548c\u6027\u89c4\u8303\u3001\u6570\u636e\u4f4d\u7f6e\u3001\u5de5\u4f5c\u8d1f\u8f7d\u95f4\u7684\u5e72\u6270\u53ca\u6700\u540e\u65f6\u9650 . \u4e2a\u4eba\u7406\u89e3: \u7ba1\u7406\u5458\u53d1\u9001\u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\u7684\u6307\u4ee4\u5230api-server\uff0capi-server\u6536\u5230\u8fd9\u4e2a\u6307\u4ee4\u5c31\u5b58\u50a8\u5728etcd\u4e2d\uff0ckube-scheduler\u4f1a\u76d1\u542capi-server\u770b\u770b\u6709\u6ca1\u6709pod\u7684\u64cd\u4f5c\u4e8b\u4ef6\uff0c\u5982\u679c\u6ca1\u6709\u4e0b\u6b21\u63a5\u7740\u67e5\u8be2\uff0c\u5982\u679c\u6709\u7684\u8bdd\uff0ckube-scheduler\u4f1a\u901a\u8fc7api-server\u62ff\u5230etcd\u8fd9\u4e2a\u4e8b\u4ef6\u3002\u7136\u540ekube-scheduler\u4f1a\u6839\u636enode\u7684\u8d44\u6e90\u5229\u7528\u7387\u6765\u8fdb\u884c\u8c03\u5ea6\u3002\u8c03\u5ea6\u4e4b\u540ekube-scheduler\u4f1a\u628a\u7ed3\u679c\u8fd4\u56de\u7ed9api-server\uff0capi-server\u518d\u628a\u7ed3\u679c\u5199\u5230etcd\u3002 kube-controller-manager: \u5b98\u65b9\u89e3\u91ca: \u8fd0\u884c\u7ba1\u7406\u63a7\u5236\u5668\uff0c\u662f\u96c6\u7fa4\u4e2d\u5904\u7406\u5e38\u89c4\u4efb\u52a1\u7684\u540e\u53f0\u8fdb\u7a0b\uff0c\u662fKubernetes \u91cc\u6240\u6709\u8d44\u6e90\u5bf9\u8c61\u7684\u81ea\u52a8\u5316\u63a7\u5236\u4e2d\u5fc3\u3002\u903b\u8f91\u4e0a\uff0c\u6bcf\u4e2a\u63a7\u5236\u5668\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u8fdb\u7a0b\uff0c\u4f46\u4e3a\u4e86\u964d\u4f4e\u590d\u6742\u6027\uff0c\u5b83\u4eec\u90fd\u88ab\u7f16\u8bd1\u6210\u5355\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5e76\u5728\u5355\u4e2a\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u3002\u8fd9\u4e9b\u63a7\u5236\u5668\u4e3b\u8981\u5305\u62ec\u3002 \u8282\u70b9\u63a7\u5236\u5668\uff08Node Controller\uff09\uff1a\u8d1f\u8d23\u5728Node\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\u53ca\u65f6\u53d1\u73b0\u548c\u54cd\u5e94\uff1b \u590d\u5236\u63a7\u5236\u5668\uff08Replication Controller\uff09\uff1a\u8d1f\u8d23\u7ef4\u62a4\u6b63\u786e\u6570\u91cf\u7684Pod\uff1b \u7aef\u70b9\u63a7\u5236\u5668\uff08Endpoints Controller\uff09\uff1a\u586b\u5145\u7aef\u70b9\u5bf9\u8c61\uff08\u5373\u8fde\u63a5Services\u548cPods\uff09\uff1b \u670d\u52a1\u5e10\u6237\u548c\u4ee4\u724c\u63a7\u5236\u5668\uff08Service Account & Token Controllers\uff09\uff1a\u4e3a\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u9ed8\u8ba4\u5e10\u6237\u548cAPI\u8bbf\u95ee\u4ee4\u724c\u3002 \u4e2a\u4eba\u7406\u89e3: etcd: \u5b98\u65b9\u89e3\u91ca: \u662fKubernetes \u63d0\u4f9b\u7684\u9ed8\u8ba4\u5b58\u50a8\uff0c\u6240\u6709\u96c6\u7fa4\u6570\u636e\u90fd\u4fdd\u5b58\u5728Etcd\u4e2d\uff0c\u4f7f\u7528\u65f6\u5efa\u8bae\u4e3aEtcd \u6570\u636e\u63d0\u4f9b\u5907\u4efd\u8ba1\u5212\uff1b \u4e2a\u4eba\u7406\u89e3: \u4e00\u5806\u63a7\u5236\u5668\uff08\u526f\u672c\u63a7\u5236\u5668\uff0c\u8282\u70b9\u63a7\u5236\u5668\uff0c\u547d\u4ee4\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u670d\u52a1\u5668\u8d26\u53f7\u63a7\u5236\u5668\u7b49\uff09\uff0c\u63a7\u5236\u5668\u505a\u4e3a\u96c6\u7fa4\u5185\u90e8\u7ba1\u7406\u63a7\u5236\u4e2d\u5fc3\uff0c\u8d1f\u8d23\u96c6\u7fa4\u5185\u7684node\u3001pod\u3001\u670d\u52a1\u7aef\u70b9\u3001\u547d\u540d\u7a7a\u95f4\u3001\u670d\u52a1\u5668\u8d26\u53f7\uff0c\u8d44\u6e90\u5b9a\u989d\u7684\u7ba1\u7406\uff0c\u5f53\u67d0\u4e2anode\u610f\u5916\u5b95\u673a\uff0cController-manager\u4f1a\u81ea\u52a8\u53d1\u73b0\u5e76\u6267\u884c\u81ea\u52a8\u6062\u590d\u6d41\u7a0b\u3002\u786e\u4fdd\u96c6\u7fa4\u4e2dpod\u7684\u526f\u672c\u59cb\u7ec8\u4fdd\u6301\u9884\u671f\u5de5\u4f5c\u7684\u72b6\u6001\u3002 Node \u7ec4\u4ef6 \u00b6 kubelet\uff1a \u5b98\u65b9\u89e3\u91ca: \u8d1f\u8d23Pod\u5bf9\u5e94\u5bb9\u5668\u7684\u521b\u5efa\u3001\u8d77\u505c\u7b49\u4efb\u52a1\uff0c\u540c\u65f6\u4e0eMaster \u8282\u70b9\u5bc6\u5207\u534f\u4f5c\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7ba1\u7406\u7684\u57fa\u672c\u529f\u80fd\u3002 \u4e2a\u4eba\u7406\u89e3: kubelet\u662f\u7ef4\u62a4node\u4e0apod\u7684\u72b6\u6001\uff0c\u5982\u679cpod\u6302\u4e86\uff0c\u4ed6\u4f1a\u5c06\u4e8b\u4ef6\u53d1\u9001\u7ed9api-server\uff0capi-server\u5c06\u4e8b\u4ef6\u5199\u5165etcd\uff0ckube-scheduler\u6216\u8005Controller-manager\u4f1a\u62ff\u5230api-server\u4e0a\u7684\u4e8b\u4ef6\uff0c\u5bf9pod\u8fdb\u884c\u91cd\u5efa\u7b49\u64cd\u4f5c\u3002 kube-proxy\uff1a \u5b98\u65b9\u89e3\u91ca: \u5b98\u7f51\u89e3\u91ca\uff1a\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u7f51\u7edc\u4ee3\u7406\uff0c \u5b9e\u73b0 Kubernetes \u670d\u52a1\uff08Service\uff09 \u6982\u5ff5\u7684\u4e00\u90e8\u5206 \u4e2a\u4eba\u7406\u89e3: \u4e2a\u4eba\u7406\u89e3\uff1a\u7ef4\u62a4\u5f53\u524d\u4e3b\u673a\u7684\u7f51\u7edc\u89c4\u5219 \u5176\u4ed6\u7ec4\u4ef6 \u00b6 helm install mysql presslabs/mysql-operator \\ -f 1-config.yaml \\ -n infra helm install mysql-operator bitpoke/mysql-operator \\ -f 1-config.yaml \\ -n infra","title":"K8S\u7b80\u4ecb"},{"location":"k8s/base/k8s-overview/#k8s","text":"\u5f00\u6e90\u5bb9\u5668\u5316\u7f16\u6392\u5f15\u64ce\uff0c\u4e13\u7528\u4e8e\u7ba1\u7406\u5bb9\u5668\u5316\u5e94\u7528\u548c\u670d\u52a1\u96c6\u7fa4\u3002","title":"k8s \u7b80\u4ecb"},{"location":"k8s/base/k8s-overview/#_1","text":"","title":"\u6838\u5fc3\u7ec4\u4ef6\u4ecb\u7ecd"},{"location":"k8s/base/k8s-overview/#_2","text":"kube-apiserver: \u5b98\u65b9\u89e3\u91ca: API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u7ec4\u4ef6\uff0c \u8be5\u7ec4\u4ef6\u8d1f\u8d23\u516c\u5f00\u4e86 Kubernetes API\uff0c\u8d1f\u8d23\u5904\u7406\u63a5\u53d7\u8bf7\u6c42\u7684\u5de5\u4f5c\u3002 API \u670d\u52a1\u5668\u662f Kubernetes \u63a7\u5236\u5e73\u9762\u7684\u524d\u7aef\u3002 Kubernetes API \u670d\u52a1\u5668\u7684\u4e3b\u8981\u5b9e\u73b0\u662f kube-apiserver\u3002 kube-apiserver \u8bbe\u8ba1\u4e0a\u8003\u8651\u4e86\u6c34\u5e73\u6269\u7f29\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u53ef\u901a\u8fc7\u90e8\u7f72\u591a\u4e2a\u5b9e\u4f8b\u6765\u8fdb\u884c\u6269\u7f29\u3002 \u4f60\u53ef\u4ee5\u8fd0\u884c kube-apiserver \u7684\u591a\u4e2a\u5b9e\u4f8b\uff0c\u5e76\u5728\u8fd9\u4e9b\u5b9e\u4f8b\u4e4b\u95f4\u5e73\u8861\u6d41\u91cf\u3002 \u4e2a\u4eba\u7406\u89e3: k8s\u7684\u8bbf\u95ee\u5165\u53e3\uff0c\u5982\u679c\u60f3\u64cd\u4f5c\u5bb9\u5668\uff0c\u5fc5\u987b\u8981\u901a\u8fc7api-server\u624d\u53ef\u4ee5\u3002k8s\u96c6\u7fa4\u6240\u6709\u7684node\u90fd\u8981\u4e0eapi-server\u8fdb\u884c\u901a\u4fe1\uff0c\u6240\u4ee5node\u7684\u6570\u91cf\u8d8a\u591a\u7ed9api-server\u7684\u538b\u529b\u5c31\u4f1a\u8d8a\u5927\u3002\u6240\u4ee5\u4e00\u822ck8s-master\u90fd\u662f\u96c6\u7fa4\u6765\u9ad8\u53ef\u7528\u7684\uff0c\u901a\u5e38\u6765\u8bf4\u90fd\u662f\u4e09\u4e2a\u8282\u70b9\u3002 kube-scheduler: \u5b98\u65b9\u89e3\u91ca: \u662f\u8d1f\u8d23\u8d44\u6e90\u8c03\u5ea6\u7684\u8fdb\u7a0b\uff0c\u76d1\u89c6\u65b0\u521b\u5efa\u4e14\u6ca1\u6709\u5206\u914d\u5230Node\u7684Pod\uff0c\u4e3aPod \u9009\u62e9\u4e00\u4e2aNode\uff1b \u8c03\u5ea6\u51b3\u7b56\u8003\u8651\u7684\u56e0\u7d20\u5305\u62ec\u5355\u4e2a Pod \u53ca Pods \u96c6\u5408\u7684\u8d44\u6e90\u9700\u6c42\u3001 \u8f6f\u786c\u4ef6\u53ca\u7b56\u7565\u7ea6\u675f\u3001 \u4eb2\u548c\u6027\u53ca\u53cd\u4eb2\u548c\u6027\u89c4\u8303\u3001\u6570\u636e\u4f4d\u7f6e\u3001\u5de5\u4f5c\u8d1f\u8f7d\u95f4\u7684\u5e72\u6270\u53ca\u6700\u540e\u65f6\u9650 . \u4e2a\u4eba\u7406\u89e3: \u7ba1\u7406\u5458\u53d1\u9001\u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\u7684\u6307\u4ee4\u5230api-server\uff0capi-server\u6536\u5230\u8fd9\u4e2a\u6307\u4ee4\u5c31\u5b58\u50a8\u5728etcd\u4e2d\uff0ckube-scheduler\u4f1a\u76d1\u542capi-server\u770b\u770b\u6709\u6ca1\u6709pod\u7684\u64cd\u4f5c\u4e8b\u4ef6\uff0c\u5982\u679c\u6ca1\u6709\u4e0b\u6b21\u63a5\u7740\u67e5\u8be2\uff0c\u5982\u679c\u6709\u7684\u8bdd\uff0ckube-scheduler\u4f1a\u901a\u8fc7api-server\u62ff\u5230etcd\u8fd9\u4e2a\u4e8b\u4ef6\u3002\u7136\u540ekube-scheduler\u4f1a\u6839\u636enode\u7684\u8d44\u6e90\u5229\u7528\u7387\u6765\u8fdb\u884c\u8c03\u5ea6\u3002\u8c03\u5ea6\u4e4b\u540ekube-scheduler\u4f1a\u628a\u7ed3\u679c\u8fd4\u56de\u7ed9api-server\uff0capi-server\u518d\u628a\u7ed3\u679c\u5199\u5230etcd\u3002 kube-controller-manager: \u5b98\u65b9\u89e3\u91ca: \u8fd0\u884c\u7ba1\u7406\u63a7\u5236\u5668\uff0c\u662f\u96c6\u7fa4\u4e2d\u5904\u7406\u5e38\u89c4\u4efb\u52a1\u7684\u540e\u53f0\u8fdb\u7a0b\uff0c\u662fKubernetes \u91cc\u6240\u6709\u8d44\u6e90\u5bf9\u8c61\u7684\u81ea\u52a8\u5316\u63a7\u5236\u4e2d\u5fc3\u3002\u903b\u8f91\u4e0a\uff0c\u6bcf\u4e2a\u63a7\u5236\u5668\u662f\u4e00\u4e2a\u5355\u72ec\u7684\u8fdb\u7a0b\uff0c\u4f46\u4e3a\u4e86\u964d\u4f4e\u590d\u6742\u6027\uff0c\u5b83\u4eec\u90fd\u88ab\u7f16\u8bd1\u6210\u5355\u4e2a\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u5e76\u5728\u5355\u4e2a\u8fdb\u7a0b\u4e2d\u8fd0\u884c\u3002\u8fd9\u4e9b\u63a7\u5236\u5668\u4e3b\u8981\u5305\u62ec\u3002 \u8282\u70b9\u63a7\u5236\u5668\uff08Node Controller\uff09\uff1a\u8d1f\u8d23\u5728Node\u8282\u70b9\u51fa\u73b0\u6545\u969c\u65f6\u53ca\u65f6\u53d1\u73b0\u548c\u54cd\u5e94\uff1b \u590d\u5236\u63a7\u5236\u5668\uff08Replication Controller\uff09\uff1a\u8d1f\u8d23\u7ef4\u62a4\u6b63\u786e\u6570\u91cf\u7684Pod\uff1b \u7aef\u70b9\u63a7\u5236\u5668\uff08Endpoints Controller\uff09\uff1a\u586b\u5145\u7aef\u70b9\u5bf9\u8c61\uff08\u5373\u8fde\u63a5Services\u548cPods\uff09\uff1b \u670d\u52a1\u5e10\u6237\u548c\u4ee4\u724c\u63a7\u5236\u5668\uff08Service Account & Token Controllers\uff09\uff1a\u4e3a\u65b0\u7684\u547d\u540d\u7a7a\u95f4\u521b\u5efa\u9ed8\u8ba4\u5e10\u6237\u548cAPI\u8bbf\u95ee\u4ee4\u724c\u3002 \u4e2a\u4eba\u7406\u89e3: etcd: \u5b98\u65b9\u89e3\u91ca: \u662fKubernetes \u63d0\u4f9b\u7684\u9ed8\u8ba4\u5b58\u50a8\uff0c\u6240\u6709\u96c6\u7fa4\u6570\u636e\u90fd\u4fdd\u5b58\u5728Etcd\u4e2d\uff0c\u4f7f\u7528\u65f6\u5efa\u8bae\u4e3aEtcd \u6570\u636e\u63d0\u4f9b\u5907\u4efd\u8ba1\u5212\uff1b \u4e2a\u4eba\u7406\u89e3: \u4e00\u5806\u63a7\u5236\u5668\uff08\u526f\u672c\u63a7\u5236\u5668\uff0c\u8282\u70b9\u63a7\u5236\u5668\uff0c\u547d\u4ee4\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u670d\u52a1\u5668\u8d26\u53f7\u63a7\u5236\u5668\u7b49\uff09\uff0c\u63a7\u5236\u5668\u505a\u4e3a\u96c6\u7fa4\u5185\u90e8\u7ba1\u7406\u63a7\u5236\u4e2d\u5fc3\uff0c\u8d1f\u8d23\u96c6\u7fa4\u5185\u7684node\u3001pod\u3001\u670d\u52a1\u7aef\u70b9\u3001\u547d\u540d\u7a7a\u95f4\u3001\u670d\u52a1\u5668\u8d26\u53f7\uff0c\u8d44\u6e90\u5b9a\u989d\u7684\u7ba1\u7406\uff0c\u5f53\u67d0\u4e2anode\u610f\u5916\u5b95\u673a\uff0cController-manager\u4f1a\u81ea\u52a8\u53d1\u73b0\u5e76\u6267\u884c\u81ea\u52a8\u6062\u590d\u6d41\u7a0b\u3002\u786e\u4fdd\u96c6\u7fa4\u4e2dpod\u7684\u526f\u672c\u59cb\u7ec8\u4fdd\u6301\u9884\u671f\u5de5\u4f5c\u7684\u72b6\u6001\u3002","title":"\u63a7\u5236\u5e73\u9762\u7ec4\u4ef6"},{"location":"k8s/base/k8s-overview/#node","text":"kubelet\uff1a \u5b98\u65b9\u89e3\u91ca: \u8d1f\u8d23Pod\u5bf9\u5e94\u5bb9\u5668\u7684\u521b\u5efa\u3001\u8d77\u505c\u7b49\u4efb\u52a1\uff0c\u540c\u65f6\u4e0eMaster \u8282\u70b9\u5bc6\u5207\u534f\u4f5c\uff0c\u5b9e\u73b0\u96c6\u7fa4\u7ba1\u7406\u7684\u57fa\u672c\u529f\u80fd\u3002 \u4e2a\u4eba\u7406\u89e3: kubelet\u662f\u7ef4\u62a4node\u4e0apod\u7684\u72b6\u6001\uff0c\u5982\u679cpod\u6302\u4e86\uff0c\u4ed6\u4f1a\u5c06\u4e8b\u4ef6\u53d1\u9001\u7ed9api-server\uff0capi-server\u5c06\u4e8b\u4ef6\u5199\u5165etcd\uff0ckube-scheduler\u6216\u8005Controller-manager\u4f1a\u62ff\u5230api-server\u4e0a\u7684\u4e8b\u4ef6\uff0c\u5bf9pod\u8fdb\u884c\u91cd\u5efa\u7b49\u64cd\u4f5c\u3002 kube-proxy\uff1a \u5b98\u65b9\u89e3\u91ca: \u5b98\u7f51\u89e3\u91ca\uff1a\u96c6\u7fa4\u4e2d\u6bcf\u4e2a\u8282\u70b9\u4e0a\u8fd0\u884c\u7684\u7f51\u7edc\u4ee3\u7406\uff0c \u5b9e\u73b0 Kubernetes \u670d\u52a1\uff08Service\uff09 \u6982\u5ff5\u7684\u4e00\u90e8\u5206 \u4e2a\u4eba\u7406\u89e3: \u4e2a\u4eba\u7406\u89e3\uff1a\u7ef4\u62a4\u5f53\u524d\u4e3b\u673a\u7684\u7f51\u7edc\u89c4\u5219","title":"Node \u7ec4\u4ef6"},{"location":"k8s/base/k8s-overview/#_3","text":"helm install mysql presslabs/mysql-operator \\ -f 1-config.yaml \\ -n infra helm install mysql-operator bitpoke/mysql-operator \\ -f 1-config.yaml \\ -n infra","title":"\u5176\u4ed6\u7ec4\u4ef6"},{"location":"k8s/controller/StatefulSet/","text":"","title":"StatefulSet"},{"location":"k8s/controller/deployment/","text":"","title":"Deployment"},{"location":"k8s/controller/rs/","text":"","title":"Rs"},{"location":"k8s/mysql-operator/1-install-mysql-operator/","text":"Mysql-operator \u5b89\u88c5\u90e8\u7f72 \u00b6 \u73af\u5883\u8981\u6c42 \u5b89\u88c5Helm\u5305\u7ba1\u7406\u5de5\u5177 \u5b89\u88c5rook-ceph\u4f5c\u4e3a\u540e\u7aef\u5b58\u50a8 \u5b89\u88c5operator \u5b89\u88c5mysql\u670d\u52a1 \u5b89\u88c5 \u00b6 Helm\u548crook-ceph\u8fd9\u91cc\u5c31\u4e0d\u8fdb\u884c\u6f14\u793a\u5b89\u88c5\u4e86\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e4b\u524d\u7684\u6587\u7ae0\u6765\u5b89\u88c5 \u5b89\u88c5operator \u00b6 github\u4e0a\u6709\u4e00\u4e2a\u9879\u76ee\u53ef\u4ee5\u5e2e\u6211\u4eec\u5feb\u901f\u7684\u6765\u5b89\u88c5 mysql \u4f18\u52bf: \u5feb\u901f\u90e8\u7f72Mysql\u670d\u52a1 \u89e3\u51b3\u4e86\u76d1\u63a7\u3001\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5907\u4efd\u95ee\u9898 \u901a\u8fc7storageClass\u6765\u89e3\u51b3\u5b58\u50a8\u95ee\u9898 \u5f00\u7bb1\u5373\u7528\u7684\u5907\u4efd\uff08\u8ba1\u5212\u548c\u6309\u9700\uff09\u548c\u65f6\u95f4\u70b9\u6062\u590d \u6dfb\u52a0chart\u5730\u5740 \u00b6 helm repo add bitpoke https://helm-charts.bitpoke.io helm repo update \u5b89\u88c5mysql-operator \u00b6 $ helm install mysql-operator bitpoke/mysql-operator \\ -f 1 -config.yaml \\ -n infra WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config NAME: mysql-operator LAST DEPLOYED: Thu Aug 25 10 :24:53 2022 NAMESPACE: infra STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat <<EOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF \u8fd9\u4e2a1-config.yaml\u6587\u4ef6\u662f\u7ed9operator\u6765\u4f7f\u7528\u7684\uff0c\u8fd9\u91cc\u5b9a\u4e49\u4e86storageClass\u548c\u5bb9\u91cf $ cat 1 -config.yaml orchestrator: persistence: enabled: true storageClass: \"rook-ceph-block\" accessMode: \"ReadWriteOnce\" size: 10Gi \u9a8c\u8bc1\uff1a $ k get pod NAME READY STATUS RESTARTS AGE mysql-operator-0 2 /2 Running 2 ( 6m46s ago ) 6m47s \u8fd9\u65f6\uff0cmysql-operator\u5c31\u5b89\u88c5\u597d\u4e86\uff0c\u7b2c\u4e8c\u6b65\u5c31\u5b89\u88c5Mysql \u5b89\u88c5mysql \u00b6 $ k apply -f 2 -openbayes-mysql.yaml secret/openbayes-db-secret created mysqlcluster.mysql.presslabs.org/openbayes created","title":"Mysql-operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql-operator","text":"\u73af\u5883\u8981\u6c42 \u5b89\u88c5Helm\u5305\u7ba1\u7406\u5de5\u5177 \u5b89\u88c5rook-ceph\u4f5c\u4e3a\u540e\u7aef\u5b58\u50a8 \u5b89\u88c5operator \u5b89\u88c5mysql\u670d\u52a1","title":"Mysql-operator \u5b89\u88c5\u90e8\u7f72"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#_1","text":"Helm\u548crook-ceph\u8fd9\u91cc\u5c31\u4e0d\u8fdb\u884c\u6f14\u793a\u5b89\u88c5\u4e86\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e4b\u524d\u7684\u6587\u7ae0\u6765\u5b89\u88c5","title":"\u5b89\u88c5"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#operator","text":"github\u4e0a\u6709\u4e00\u4e2a\u9879\u76ee\u53ef\u4ee5\u5e2e\u6211\u4eec\u5feb\u901f\u7684\u6765\u5b89\u88c5 mysql \u4f18\u52bf: \u5feb\u901f\u90e8\u7f72Mysql\u670d\u52a1 \u89e3\u51b3\u4e86\u76d1\u63a7\u3001\u53ef\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5907\u4efd\u95ee\u9898 \u901a\u8fc7storageClass\u6765\u89e3\u51b3\u5b58\u50a8\u95ee\u9898 \u5f00\u7bb1\u5373\u7528\u7684\u5907\u4efd\uff08\u8ba1\u5212\u548c\u6309\u9700\uff09\u548c\u65f6\u95f4\u70b9\u6062\u590d","title":"\u5b89\u88c5operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#chart","text":"helm repo add bitpoke https://helm-charts.bitpoke.io helm repo update","title":"\u6dfb\u52a0chart\u5730\u5740"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql-operator_1","text":"$ helm install mysql-operator bitpoke/mysql-operator \\ -f 1 -config.yaml \\ -n infra WARNING: Kubernetes configuration file is group-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config WARNING: Kubernetes configuration file is world-readable. This is insecure. Location: /Users/beiyiwangdejiyi/.kube/config NAME: mysql-operator LAST DEPLOYED: Thu Aug 25 10 :24:53 2022 NAMESPACE: infra STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: You can create a new cluster by issuing: cat <<EOF | kubectl apply -f- apiVersion: mysql.presslabs.org/v1alpha1 kind: MysqlCluster metadata: name: my-cluster spec: replicas: 1 secretName: my-cluster-secret --- apiVersion: v1 kind: Secret metadata: name: my-cluster-secret type: Opaque data: ROOT_PASSWORD: $(echo -n \"not-so-secure\" | base64) EOF \u8fd9\u4e2a1-config.yaml\u6587\u4ef6\u662f\u7ed9operator\u6765\u4f7f\u7528\u7684\uff0c\u8fd9\u91cc\u5b9a\u4e49\u4e86storageClass\u548c\u5bb9\u91cf $ cat 1 -config.yaml orchestrator: persistence: enabled: true storageClass: \"rook-ceph-block\" accessMode: \"ReadWriteOnce\" size: 10Gi \u9a8c\u8bc1\uff1a $ k get pod NAME READY STATUS RESTARTS AGE mysql-operator-0 2 /2 Running 2 ( 6m46s ago ) 6m47s \u8fd9\u65f6\uff0cmysql-operator\u5c31\u5b89\u88c5\u597d\u4e86\uff0c\u7b2c\u4e8c\u6b65\u5c31\u5b89\u88c5Mysql","title":"\u5b89\u88c5mysql-operator"},{"location":"k8s/mysql-operator/1-install-mysql-operator/#mysql","text":"$ k apply -f 2 -openbayes-mysql.yaml secret/openbayes-db-secret created mysqlcluster.mysql.presslabs.org/openbayes created","title":"\u5b89\u88c5mysql"},{"location":"k8s/yaml/yaml/","text":"\u8d44\u6e90\u6e05\u5355 \u00b6 YAML \u662f \"YAML Ain't a Markup Language\"\uff08YAML \u4e0d\u662f\u4e00\u79cd\u6807\u8bb0\u8bed\u8a00\uff09\u7684\u9012\u5f52\u7f29\u5199\u3002AML \u7684\u8bed\u6cd5\u548c\u5176\u4ed6\u9ad8\u7ea7\u8bed\u8a00\u7c7b\u4f3c\uff0c\u5e76\u4e14\u53ef\u4ee5\u7b80\u5355\u8868\u8fbe\u6e05\u5355\u3001\u6563\u5217\u8868\uff0c\u6807\u91cf\u7b49\u6570\u636e\u5f62\u6001\u3002\u5b83\u4f7f\u7528\u7a7a\u767d\u7b26\u53f7\u7f29\u8fdb\u548c\u5927\u91cf\u4f9d\u8d56\u5916\u89c2\u7684\u7279\u8272\uff0c\u7279\u522b\u9002\u5408\u7528\u6765\u8868\u8fbe\u6216\u7f16\u8f91\u6570\u636e\u7ed3\u6784\u3001\u5404\u79cd\u914d\u7f6e\u6587\u4ef6\u3001\u503e\u5370\u8c03\u8bd5\u5185\u5bb9\u3001\u6587\u4ef6\u5927\u7eb2\uff08\u4f8b\u5982\uff1a\u8bb8\u591a\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u683c\u5f0f\u548cYAML\u975e\u5e38\u63a5\u8fd1\uff09\u3002 YAML \u7684\u914d\u7f6e\u6587\u4ef6\u540e\u7f00\u4e3a .yml\uff0c\u5982\uff1arunoob.yml \u3002 \u57fa\u672c\u8bed\u6cd5\uff1a \u5927\u5c0f\u5199\u654f\u611f \u4f7f\u7528\u7f29\u8fdb\u8868\u793a\u5c42\u7ea7\u5173\u7cfb \u7f29\u8fdb\u4e0d\u5141\u8bb8\u4f7f\u7528tab\uff0c\u53ea\u5141\u8bb8\u7a7a\u683c \u7f29\u8fdb\u7684\u7a7a\u683c\u6570\u4e0d\u91cd\u8981\uff0c\u53ea\u8981\u76f8\u540c\u5c42\u7ea7\u7684\u5143\u7d20\u5de6\u5bf9\u9f50\u5373\u53ef '#'\u8868\u793a\u6ce8\u91ca \u8d44\u6e90\u6e05\u5355\u89e3\u8bfb \u00b6 apiVersion : v1 #\u7248\u672c\u53f7\uff0c\u4f8b\u5982v1 kind : Pod #\u8d44\u6e90\u7c7b\u578b\uff0c\u5982Pod metadata : #\u5143\u6570\u636e name : string # Pod\u540d\u5b57 namespace : string # Pod\u6240\u5c5e\u7684\u547d\u540d\u7a7a\u95f4 labels : #\u81ea\u5b9a\u4e49\u6807\u7b7e - name : string #\u81ea\u5b9a\u4e49\u6807\u7b7e\u540d\u5b57 annotations : #\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5217\u8868 - name : string spec : # Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 containers : # Pod\u4e2d\u5bb9\u5668\u5217\u8868 - name : string #\u5bb9\u5668\u540d\u79f0 image : string #\u5bb9\u5668\u7684\u955c\u50cf\u540d\u79f0 imagePullPolicy : [ Always | Never | IfNotPresent ] #\u83b7\u53d6\u955c\u50cf\u7684\u7b56\u7565 Alawys\u8868\u793a\u4e0b\u8f7d\u955c\u50cf IfnotPresent\u8868\u793a\u4f18\u5148\u4f7f\u7528\u672c\u5730\u955c\u50cf\uff0c\u5426\u5219\u4e0b\u8f7d\u955c\u50cf\uff0cNerver\u8868\u793a\u4ec5\u4f7f\u7528\u672c\u5730\u955c\u50cf command : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u5217\u8868\uff0c\u5982\u4e0d\u6307\u5b9a\uff0c\u4f7f\u7528\u6253\u5305\u65f6\u4f7f\u7528\u7684\u542f\u52a8\u547d\u4ee4 args : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u53c2\u6570\u5217\u8868 workingDir : string #\u5bb9\u5668\u7684\u5de5\u4f5c\u76ee\u5f55 volumeMounts : #\u6302\u8f7d\u5230\u5bb9\u5668\u5185\u90e8\u7684\u5b58\u50a8\u5377\u914d\u7f6e - name : string #\u5f15\u7528pod\u5b9a\u4e49\u7684\u5171\u4eab\u5b58\u50a8\u5377\u7684\u540d\u79f0\uff0c\u9700\u7528volumes[]\u90e8\u5206\u5b9a\u4e49\u7684\u7684\u5377\u540d mountPath : string #\u5b58\u50a8\u5377\u5728\u5bb9\u5668\u5185mount\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u5e94\u5c11\u4e8e512\u5b57\u7b26 readOnly : boolean #\u662f\u5426\u4e3a\u53ea\u8bfb\u6a21\u5f0f ports : # \u9700\u8981\u66b4\u9732\u7684\u7aef\u53e3\u5e93\u53f7 - name : string # \u7aef\u53e3\u53f7\u540d\u79f0 containerPort : int #\u5bb9\u5668\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7 hostPort : int #\u5bb9\u5668\u6240\u5728\u4e3b\u673a\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7\uff0c\u9ed8\u8ba4\u4e0eContainer\u76f8\u540c protocol : string #\u7aef\u53e3\u534f\u8bae\uff0c\u652f\u6301TCP\u548cUDP\uff0c\u9ed8\u8ba4TCP env : #\u5bb9\u5668\u8fd0\u884c\u524d\u9700\u8bbe\u7f6e\u7684\u73af\u5883\u53d8\u91cf\u5217\u8868 - name : string #\u73af\u5883\u53d8\u91cf\u540d\u79f0 value : string #\u73af\u5883\u53d8\u91cf\u7684\u503c resources : #\u8d44\u6e90\u9650\u5236\u548c\u8bf7\u6c42\u7684\u8bbe\u7f6e limits : #\u8d44\u6e90\u9650\u5236\u7684\u8bbe\u7f6e cpu : string #cpu\u7684\u9650\u5236\uff0c\u5355\u4f4d\u4e3acore\u6570 memory : string #\u5185\u5b58\u9650\u5236\uff0c\u5355\u4f4d\u53ef\u4ee5\u4e3aMib/Gib requests : #\u8d44\u6e90\u8bf7\u6c42\u7684\u8bbe\u7f6e cpu : string #cpu\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u6570\u91cf memory : string #\u5185\u5b58\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u5185\u5b58 livenessProbe : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u7684\u8bbe\u7f6e\uff0c\u5f53\u63a2\u6d4b\u65e0\u54cd\u5e94\u51e0\u6b21\u540e\u5c06\u81ea\u52a8\u91cd\u542f\u8be5\u5bb9\u5668\uff0c\u68c0\u67e5\u65b9\u6cd5\u6709exec\u3001httpGet\u548ctcpSocket\uff0c\u5bf9\u4e00\u4e2a\u5bb9\u5668\u53ea\u9700\u8bbe\u7f6e\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5373\u53ef exec : #\u5bf9Pod\u5bb9\u5668\u5185\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3aexec\u65b9\u5f0f command : [ string ] #exec\u65b9\u5f0f\u9700\u8981\u5236\u5b9a\u7684\u547d\u4ee4\u6216\u811a\u672c httpGet : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u6cd5\u8bbe\u7f6e\u4e3aHttpGet\uff0c\u9700\u8981\u5236\u5b9aPath\u3001port path : string port : number host : string scheme : string HttpHeaders : - name : string value : string tcpSocket : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3atcpSocket\u65b9\u5f0f port : number initialDelaySeconds : 0 #\u5bb9\u5668\u542f\u52a8\u5b8c\u6210\u540e\u9996\u6b21\u63a2\u6d4b\u7684\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2 timeoutSeconds : 0 #\u5bf9\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u63a2\u6d4b\u7b49\u5f85\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba41\u79d2 periodSeconds : 0 #\u5bf9\u5bb9\u5668\u76d1\u63a7\u68c0\u67e5\u7684\u5b9a\u671f\u63a2\u6d4b\u65f6\u95f4\u8bbe\u7f6e\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba410\u79d2\u4e00\u6b21 successThreshold : 0 failureThreshold : 0 securityContext : privileged:false restartPolicy : [ Always | Never | OnFailure ] #Pod\u7684\u91cd\u542f\u7b56\u7565\uff0cAlways\u8868\u793a\u4e00\u65e6\u4e0d\u7ba1\u4ee5\u4f55\u79cd\u65b9\u5f0f\u7ec8\u6b62\u8fd0\u884c\uff0ckubelet\u90fd\u5c06\u91cd\u542f\uff0cOnFailure\u8868\u793a\u53ea\u6709Pod\u4ee5\u975e0\u9000\u51fa\u7801\u9000\u51fa\u624d\u91cd\u542f\uff0cNerver\u8868\u793a\u4e0d\u518d\u91cd\u542f\u8be5Pod nodeSelector : obeject #\u8bbe\u7f6eNodeSelector\u8868\u793a\u5c06\u8be5Pod\u8c03\u5ea6\u5230\u5305\u542b\u8fd9\u4e2alabel\u7684node\u4e0a\uff0c\u4ee5key\uff1avalue\u7684\u683c\u5f0f\u6307\u5b9a imagePullSecrets : #Pull\u955c\u50cf\u65f6\u4f7f\u7528\u7684secret\u540d\u79f0\uff0c\u4ee5key\uff1asecretkey\u683c\u5f0f\u6307\u5b9a - name : string hostNetwork:false #\u662f\u5426\u4f7f\u7528\u4e3b\u673a\u7f51\u7edc\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3afalse\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3atrue\uff0c\u8868\u793a\u4f7f\u7528\u5bbf\u4e3b\u673a\u7f51\u7edc volumes : #\u5728\u8be5pod\u4e0a\u5b9a\u4e49\u5171\u4eab\u5b58\u50a8\u5377\u5217\u8868 - name : string #\u5171\u4eab\u5b58\u50a8\u5377\u540d\u79f0 \uff08volumes\u7c7b\u578b\u6709\u5f88\u591a\u79cd\uff09 emptyDir : {} #\u7c7b\u578b\u4e3aemtyDir\u7684\u5b58\u50a8\u5377\uff0c\u4e0ePod\u540c\u751f\u547d\u5468\u671f\u7684\u4e00\u4e2a\u4e34\u65f6\u76ee\u5f55\u3002\u4e3a\u7a7a\u503c hostPath : string #\u7c7b\u578b\u4e3ahostPath\u7684\u5b58\u50a8\u5377\uff0c\u8868\u793a\u6302\u8f7dPod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55 path : string #Pod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55\uff0c\u5c06\u88ab\u7528\u4e8e\u540c\u671f\u4e2dmount\u7684\u76ee\u5f55 secret : #\u7c7b\u578b\u4e3asecret\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u96c6\u7fa4\u4e0e\u5b9a\u4e49\u7684secre\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 scretname : string items : - key : string path : string configMap : #\u7c7b\u578b\u4e3aconfigMap\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u9884\u5b9a\u4e49\u7684configMap\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 name : string items : - key : string path : string API\u548ckubernetes\u7684\u5bf9\u5e94\u5173\u7cfb rancher\u751f\u4ea7\u6848\u4f8bYaml \u00b6 ####\u542f\u52a8docker#### eval PROJECT_ID='$'$k8s_group if [ $ms_group == 'center' ] then replicas=1 else replicas=1 fi /opt/rancher/rancher context switch $PROJECT_ID cat <<EOF | /opt/rancher/rancher kubectl apply -f - apiVersion : apps/v1 # \u8868\u793aapi\u7248\u672c\uff0cv1 kind : Deployment # kind\u8868\u793a\u8d44\u6e90\u7c7b\u578b\uff0c\u8fd9\u91cc\u662fDeployment metadata : # \u5143\u6570\u636e labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name name : $server_name # \u670d\u52a1\u540d namespace : $name_space # \u547d\u540d\u7a7a\u95f4 spec : replicas : $replicas Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 selector : matchLabels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name template : metadata : labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name spec : imagePullSecrets : - name : old-harbor # \u955c\u50cf \u4ed3\u5e93\u540d restartPolicy : Always containers : - image : $harbor_addr/$name_space/$image_name:$image_tag # \u5bb9\u5668\u7684\u955c\u50cf\u540d imagePullPolicy : Always readinessProbe : failureThreshold : 60 initialDelaySeconds : 5 periodSeconds : 5 successThreshold : 1 tcpSocket : port : $NodePort # \u7aef\u53e3\u66b4\u9732\u65b9\u5f0f timeoutSeconds : 1 livenessProbe : failureThreshold : 3 initialDelaySeconds : 300 periodSeconds : 2 successThreshold : 1 tcpSocket : port : $NodePort timeoutSeconds : 1 env : - name : CSProjFile value : $csproj_file - name : FOR_GODS_SAKE_PLEASE_REDEPLOY value : \"`date +%s`\" name : $server_name ports : - containerPort : $NodePort resources : requests : memory : $memory args : [ \"bash\" , \"-c\" , \"dotnet /opt/$csproj_file/$csproj_file.dll --serviceName $server_name \\ --webApiServiceAddress http://0.0.0.0:$NodePort --zkConfigServer $zk_configserver \\ --zkAppRole $zk_approle --runScope $run_scope --msGroup $ms_group \\ --KPversion 2 --psapp v2 --ser protobuf \\ --zkTimeOut 15000 --mcTimeOut 30000 --Cors *.xxxxx.net \\ --trace kafka --webApiHelp off $other_parameters\" ] # localtime volumeMounts : - mountPath : /etc/localtime name : localtime readOnly : true volumes : - hostPath : path : /etc/localtime type : \"\" name : localtime --- apiVersion : v1 kind : Service metadata : name : $server_name namespace : $name_space spec : type : NodePort ports : - name : default nodePort : $NodePort port : $NodePort protocol : TCP targetPort : $NodePort selector : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name EOF jenkins\u4f20\u6570 \u00b6 \u8fd9\u91ccJenkins\u4f5c\u4e3a\u4e0a\u7ea7\u9879\u76ee\uff0c\u5b9a\u4e49\u53d8\u91cf\u4e3a\u4e0b\u7ea7\u9879\u76ee\u4f20\u9012\u53c2\u6570 image_name = accountwebapiserver name_space = webapi server_name = account csproj_file = AccountWebAPIServer zk_approle = Common-AccountWebApi Controller = account run_scope = Core991 ms_group = $ms_group zk_configserver = w1.confandsa.zk.group.hex.com:2181,w2.confandsa.zk.group.hex.com:2181,w3.confandsa.zk.group.hex.com:2181 image_tag = $image_tag memory = 1Gi maxmemory = 2 .2Gi NodePort = 32037 k8s_group = $k8s_group other_parameters = --UrlPrefix tms-zw4 \u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u5bb9\u5668\u5316\u793a\u4f8b\uff1a \u00b6 $ cat nginx-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-yyds namespace : web spec : selector : matchLabels : app : nginx replicas : 2 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 \u8fd0\u884cYaml\u6587\u4ef6 k apply -f nginx-deploy.yaml \u67e5\u770bpod\u7684\u72b6\u6001 $ k get pod NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 47m nginx-yyds-585449566-vzwkw 1 /1 Running 0 47m \u5e38\u7528\u7684\u7ba1\u7406\u547d\u4ee4\uff1a \u00b6 \u67e5\u770b\u63a7\u5236\u5668 $ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-yyds 2 /2 2 2 51m \u67e5\u770bpod\u7684\u8be6\u7ec6\u4fe1\u606f $ k get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-yyds-585449566-76hgr 1 /1 Running 0 53m 10 .42.2.130 node1 <none> <none> nginx-yyds-585449566-vzwkw 1 /1 Running 0 53m 10 .42.1.127 node0 <none> <none \u901a\u8fc7\u6807\u7b7e\u6765\u67e5\u627epod $ k get pod -l app = nginx NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 56m nginx-yyds-585449566-vzwkw 1 /1 Running 0 56m $ k describe pod nginx-yyds-585449566-76hgr Name: nginx-yyds-585449566-76hgr Namespace: web Priority: 0 Node: node1/192.168.0.151 Start Time: Tue, 23 Aug 2022 15 :46:16 +0800 Labels: app = nginx pod-template-hash = 585449566 Annotations: <none> Status: Running IP: 10 .42.2.130 IPs: IP: 10 .42.2.130 Controlled By: ReplicaSet/nginx-yyds-585449566 Containers: nginx: Container ID: docker://7994248f600aa93444272eff8092938c866a5648db1126545d28635d41251b51 Image: nginx:latest Image ID: docker-pullable://nginx@sha256:dc29f133a33a1d6311807f3b88134000ce67318a40517b1060b929b84b0bbea0 Port: 80 /TCP Host Port: 0 /TCP State: Running Started: Tue, 23 Aug 2022 15 :48:42 +0800 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rf7j8 ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-rf7j8: Type: Projected ( a volume that contains injected data from multiple sources ) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op = Exists for 300s node.kubernetes.io/unreachable:NoExecute op = Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 58m default-scheduler Successfully assigned web/nginx-yyds-585449566-76hgr to node1 Normal Pulling 58m kubelet Pulling image \"nginx:latest\" Normal Pulled 55m kubelet Successfully pulled image \"nginx:latest\" in 2m24.557367751s Normal Created 55m kubelet Created container nginx Normal Started 55m kubelet Started container nginx \u901a\u8fc7explain\u6765\u67e5\u770bYaml\u6587\u4ef6\u5199\u6cd5 $ k explain","title":"\u8d44\u6e90\u6e05\u5355"},{"location":"k8s/yaml/yaml/#_1","text":"YAML \u662f \"YAML Ain't a Markup Language\"\uff08YAML \u4e0d\u662f\u4e00\u79cd\u6807\u8bb0\u8bed\u8a00\uff09\u7684\u9012\u5f52\u7f29\u5199\u3002AML \u7684\u8bed\u6cd5\u548c\u5176\u4ed6\u9ad8\u7ea7\u8bed\u8a00\u7c7b\u4f3c\uff0c\u5e76\u4e14\u53ef\u4ee5\u7b80\u5355\u8868\u8fbe\u6e05\u5355\u3001\u6563\u5217\u8868\uff0c\u6807\u91cf\u7b49\u6570\u636e\u5f62\u6001\u3002\u5b83\u4f7f\u7528\u7a7a\u767d\u7b26\u53f7\u7f29\u8fdb\u548c\u5927\u91cf\u4f9d\u8d56\u5916\u89c2\u7684\u7279\u8272\uff0c\u7279\u522b\u9002\u5408\u7528\u6765\u8868\u8fbe\u6216\u7f16\u8f91\u6570\u636e\u7ed3\u6784\u3001\u5404\u79cd\u914d\u7f6e\u6587\u4ef6\u3001\u503e\u5370\u8c03\u8bd5\u5185\u5bb9\u3001\u6587\u4ef6\u5927\u7eb2\uff08\u4f8b\u5982\uff1a\u8bb8\u591a\u7535\u5b50\u90ae\u4ef6\u6807\u9898\u683c\u5f0f\u548cYAML\u975e\u5e38\u63a5\u8fd1\uff09\u3002 YAML \u7684\u914d\u7f6e\u6587\u4ef6\u540e\u7f00\u4e3a .yml\uff0c\u5982\uff1arunoob.yml \u3002 \u57fa\u672c\u8bed\u6cd5\uff1a \u5927\u5c0f\u5199\u654f\u611f \u4f7f\u7528\u7f29\u8fdb\u8868\u793a\u5c42\u7ea7\u5173\u7cfb \u7f29\u8fdb\u4e0d\u5141\u8bb8\u4f7f\u7528tab\uff0c\u53ea\u5141\u8bb8\u7a7a\u683c \u7f29\u8fdb\u7684\u7a7a\u683c\u6570\u4e0d\u91cd\u8981\uff0c\u53ea\u8981\u76f8\u540c\u5c42\u7ea7\u7684\u5143\u7d20\u5de6\u5bf9\u9f50\u5373\u53ef '#'\u8868\u793a\u6ce8\u91ca","title":"\u8d44\u6e90\u6e05\u5355"},{"location":"k8s/yaml/yaml/#_2","text":"apiVersion : v1 #\u7248\u672c\u53f7\uff0c\u4f8b\u5982v1 kind : Pod #\u8d44\u6e90\u7c7b\u578b\uff0c\u5982Pod metadata : #\u5143\u6570\u636e name : string # Pod\u540d\u5b57 namespace : string # Pod\u6240\u5c5e\u7684\u547d\u540d\u7a7a\u95f4 labels : #\u81ea\u5b9a\u4e49\u6807\u7b7e - name : string #\u81ea\u5b9a\u4e49\u6807\u7b7e\u540d\u5b57 annotations : #\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5217\u8868 - name : string spec : # Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 containers : # Pod\u4e2d\u5bb9\u5668\u5217\u8868 - name : string #\u5bb9\u5668\u540d\u79f0 image : string #\u5bb9\u5668\u7684\u955c\u50cf\u540d\u79f0 imagePullPolicy : [ Always | Never | IfNotPresent ] #\u83b7\u53d6\u955c\u50cf\u7684\u7b56\u7565 Alawys\u8868\u793a\u4e0b\u8f7d\u955c\u50cf IfnotPresent\u8868\u793a\u4f18\u5148\u4f7f\u7528\u672c\u5730\u955c\u50cf\uff0c\u5426\u5219\u4e0b\u8f7d\u955c\u50cf\uff0cNerver\u8868\u793a\u4ec5\u4f7f\u7528\u672c\u5730\u955c\u50cf command : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u5217\u8868\uff0c\u5982\u4e0d\u6307\u5b9a\uff0c\u4f7f\u7528\u6253\u5305\u65f6\u4f7f\u7528\u7684\u542f\u52a8\u547d\u4ee4 args : [ string ] #\u5bb9\u5668\u7684\u542f\u52a8\u547d\u4ee4\u53c2\u6570\u5217\u8868 workingDir : string #\u5bb9\u5668\u7684\u5de5\u4f5c\u76ee\u5f55 volumeMounts : #\u6302\u8f7d\u5230\u5bb9\u5668\u5185\u90e8\u7684\u5b58\u50a8\u5377\u914d\u7f6e - name : string #\u5f15\u7528pod\u5b9a\u4e49\u7684\u5171\u4eab\u5b58\u50a8\u5377\u7684\u540d\u79f0\uff0c\u9700\u7528volumes[]\u90e8\u5206\u5b9a\u4e49\u7684\u7684\u5377\u540d mountPath : string #\u5b58\u50a8\u5377\u5728\u5bb9\u5668\u5185mount\u7684\u7edd\u5bf9\u8def\u5f84\uff0c\u5e94\u5c11\u4e8e512\u5b57\u7b26 readOnly : boolean #\u662f\u5426\u4e3a\u53ea\u8bfb\u6a21\u5f0f ports : # \u9700\u8981\u66b4\u9732\u7684\u7aef\u53e3\u5e93\u53f7 - name : string # \u7aef\u53e3\u53f7\u540d\u79f0 containerPort : int #\u5bb9\u5668\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7 hostPort : int #\u5bb9\u5668\u6240\u5728\u4e3b\u673a\u9700\u8981\u76d1\u542c\u7684\u7aef\u53e3\u53f7\uff0c\u9ed8\u8ba4\u4e0eContainer\u76f8\u540c protocol : string #\u7aef\u53e3\u534f\u8bae\uff0c\u652f\u6301TCP\u548cUDP\uff0c\u9ed8\u8ba4TCP env : #\u5bb9\u5668\u8fd0\u884c\u524d\u9700\u8bbe\u7f6e\u7684\u73af\u5883\u53d8\u91cf\u5217\u8868 - name : string #\u73af\u5883\u53d8\u91cf\u540d\u79f0 value : string #\u73af\u5883\u53d8\u91cf\u7684\u503c resources : #\u8d44\u6e90\u9650\u5236\u548c\u8bf7\u6c42\u7684\u8bbe\u7f6e limits : #\u8d44\u6e90\u9650\u5236\u7684\u8bbe\u7f6e cpu : string #cpu\u7684\u9650\u5236\uff0c\u5355\u4f4d\u4e3acore\u6570 memory : string #\u5185\u5b58\u9650\u5236\uff0c\u5355\u4f4d\u53ef\u4ee5\u4e3aMib/Gib requests : #\u8d44\u6e90\u8bf7\u6c42\u7684\u8bbe\u7f6e cpu : string #cpu\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u6570\u91cf memory : string #\u5185\u5b58\u8bf7\u6c42\uff0c\u5bb9\u5668\u542f\u52a8\u7684\u521d\u59cb\u53ef\u7528\u5185\u5b58 livenessProbe : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u7684\u8bbe\u7f6e\uff0c\u5f53\u63a2\u6d4b\u65e0\u54cd\u5e94\u51e0\u6b21\u540e\u5c06\u81ea\u52a8\u91cd\u542f\u8be5\u5bb9\u5668\uff0c\u68c0\u67e5\u65b9\u6cd5\u6709exec\u3001httpGet\u548ctcpSocket\uff0c\u5bf9\u4e00\u4e2a\u5bb9\u5668\u53ea\u9700\u8bbe\u7f6e\u5176\u4e2d\u4e00\u79cd\u65b9\u6cd5\u5373\u53ef exec : #\u5bf9Pod\u5bb9\u5668\u5185\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3aexec\u65b9\u5f0f command : [ string ] #exec\u65b9\u5f0f\u9700\u8981\u5236\u5b9a\u7684\u547d\u4ee4\u6216\u811a\u672c httpGet : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u6cd5\u8bbe\u7f6e\u4e3aHttpGet\uff0c\u9700\u8981\u5236\u5b9aPath\u3001port path : string port : number host : string scheme : string HttpHeaders : - name : string value : string tcpSocket : #\u5bf9Pod\u5185\u4e2a\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u65b9\u5f0f\u8bbe\u7f6e\u4e3atcpSocket\u65b9\u5f0f port : number initialDelaySeconds : 0 #\u5bb9\u5668\u542f\u52a8\u5b8c\u6210\u540e\u9996\u6b21\u63a2\u6d4b\u7684\u65f6\u95f4\uff0c\u5355\u4f4d\u4e3a\u79d2 timeoutSeconds : 0 #\u5bf9\u5bb9\u5668\u5065\u5eb7\u68c0\u67e5\u63a2\u6d4b\u7b49\u5f85\u54cd\u5e94\u7684\u8d85\u65f6\u65f6\u95f4\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba41\u79d2 periodSeconds : 0 #\u5bf9\u5bb9\u5668\u76d1\u63a7\u68c0\u67e5\u7684\u5b9a\u671f\u63a2\u6d4b\u65f6\u95f4\u8bbe\u7f6e\uff0c\u5355\u4f4d\u79d2\uff0c\u9ed8\u8ba410\u79d2\u4e00\u6b21 successThreshold : 0 failureThreshold : 0 securityContext : privileged:false restartPolicy : [ Always | Never | OnFailure ] #Pod\u7684\u91cd\u542f\u7b56\u7565\uff0cAlways\u8868\u793a\u4e00\u65e6\u4e0d\u7ba1\u4ee5\u4f55\u79cd\u65b9\u5f0f\u7ec8\u6b62\u8fd0\u884c\uff0ckubelet\u90fd\u5c06\u91cd\u542f\uff0cOnFailure\u8868\u793a\u53ea\u6709Pod\u4ee5\u975e0\u9000\u51fa\u7801\u9000\u51fa\u624d\u91cd\u542f\uff0cNerver\u8868\u793a\u4e0d\u518d\u91cd\u542f\u8be5Pod nodeSelector : obeject #\u8bbe\u7f6eNodeSelector\u8868\u793a\u5c06\u8be5Pod\u8c03\u5ea6\u5230\u5305\u542b\u8fd9\u4e2alabel\u7684node\u4e0a\uff0c\u4ee5key\uff1avalue\u7684\u683c\u5f0f\u6307\u5b9a imagePullSecrets : #Pull\u955c\u50cf\u65f6\u4f7f\u7528\u7684secret\u540d\u79f0\uff0c\u4ee5key\uff1asecretkey\u683c\u5f0f\u6307\u5b9a - name : string hostNetwork:false #\u662f\u5426\u4f7f\u7528\u4e3b\u673a\u7f51\u7edc\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3afalse\uff0c\u5982\u679c\u8bbe\u7f6e\u4e3atrue\uff0c\u8868\u793a\u4f7f\u7528\u5bbf\u4e3b\u673a\u7f51\u7edc volumes : #\u5728\u8be5pod\u4e0a\u5b9a\u4e49\u5171\u4eab\u5b58\u50a8\u5377\u5217\u8868 - name : string #\u5171\u4eab\u5b58\u50a8\u5377\u540d\u79f0 \uff08volumes\u7c7b\u578b\u6709\u5f88\u591a\u79cd\uff09 emptyDir : {} #\u7c7b\u578b\u4e3aemtyDir\u7684\u5b58\u50a8\u5377\uff0c\u4e0ePod\u540c\u751f\u547d\u5468\u671f\u7684\u4e00\u4e2a\u4e34\u65f6\u76ee\u5f55\u3002\u4e3a\u7a7a\u503c hostPath : string #\u7c7b\u578b\u4e3ahostPath\u7684\u5b58\u50a8\u5377\uff0c\u8868\u793a\u6302\u8f7dPod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55 path : string #Pod\u6240\u5728\u5bbf\u4e3b\u673a\u7684\u76ee\u5f55\uff0c\u5c06\u88ab\u7528\u4e8e\u540c\u671f\u4e2dmount\u7684\u76ee\u5f55 secret : #\u7c7b\u578b\u4e3asecret\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u96c6\u7fa4\u4e0e\u5b9a\u4e49\u7684secre\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 scretname : string items : - key : string path : string configMap : #\u7c7b\u578b\u4e3aconfigMap\u7684\u5b58\u50a8\u5377\uff0c\u6302\u8f7d\u9884\u5b9a\u4e49\u7684configMap\u5bf9\u8c61\u5230\u5bb9\u5668\u5185\u90e8 name : string items : - key : string path : string API\u548ckubernetes\u7684\u5bf9\u5e94\u5173\u7cfb","title":"\u8d44\u6e90\u6e05\u5355\u89e3\u8bfb"},{"location":"k8s/yaml/yaml/#rancheryaml","text":"####\u542f\u52a8docker#### eval PROJECT_ID='$'$k8s_group if [ $ms_group == 'center' ] then replicas=1 else replicas=1 fi /opt/rancher/rancher context switch $PROJECT_ID cat <<EOF | /opt/rancher/rancher kubectl apply -f - apiVersion : apps/v1 # \u8868\u793aapi\u7248\u672c\uff0cv1 kind : Deployment # kind\u8868\u793a\u8d44\u6e90\u7c7b\u578b\uff0c\u8fd9\u91cc\u662fDeployment metadata : # \u5143\u6570\u636e labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name name : $server_name # \u670d\u52a1\u540d namespace : $name_space # \u547d\u540d\u7a7a\u95f4 spec : replicas : $replicas Pod\u4e2d\u5bb9\u5668\u7684\u8be6\u7ec6\u5b9a\u4e49 selector : matchLabels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name template : metadata : labels : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name spec : imagePullSecrets : - name : old-harbor # \u955c\u50cf \u4ed3\u5e93\u540d restartPolicy : Always containers : - image : $harbor_addr/$name_space/$image_name:$image_tag # \u5bb9\u5668\u7684\u955c\u50cf\u540d imagePullPolicy : Always readinessProbe : failureThreshold : 60 initialDelaySeconds : 5 periodSeconds : 5 successThreshold : 1 tcpSocket : port : $NodePort # \u7aef\u53e3\u66b4\u9732\u65b9\u5f0f timeoutSeconds : 1 livenessProbe : failureThreshold : 3 initialDelaySeconds : 300 periodSeconds : 2 successThreshold : 1 tcpSocket : port : $NodePort timeoutSeconds : 1 env : - name : CSProjFile value : $csproj_file - name : FOR_GODS_SAKE_PLEASE_REDEPLOY value : \"`date +%s`\" name : $server_name ports : - containerPort : $NodePort resources : requests : memory : $memory args : [ \"bash\" , \"-c\" , \"dotnet /opt/$csproj_file/$csproj_file.dll --serviceName $server_name \\ --webApiServiceAddress http://0.0.0.0:$NodePort --zkConfigServer $zk_configserver \\ --zkAppRole $zk_approle --runScope $run_scope --msGroup $ms_group \\ --KPversion 2 --psapp v2 --ser protobuf \\ --zkTimeOut 15000 --mcTimeOut 30000 --Cors *.xxxxx.net \\ --trace kafka --webApiHelp off $other_parameters\" ] # localtime volumeMounts : - mountPath : /etc/localtime name : localtime readOnly : true volumes : - hostPath : path : /etc/localtime type : \"\" name : localtime --- apiVersion : v1 kind : Service metadata : name : $server_name namespace : $name_space spec : type : NodePort ports : - name : default nodePort : $NodePort port : $NodePort protocol : TCP targetPort : $NodePort selector : workload.user.cattle.io/workloadselector : deployment-$name_space-$server_name EOF","title":"rancher\u751f\u4ea7\u6848\u4f8bYaml"},{"location":"k8s/yaml/yaml/#jenkins","text":"\u8fd9\u91ccJenkins\u4f5c\u4e3a\u4e0a\u7ea7\u9879\u76ee\uff0c\u5b9a\u4e49\u53d8\u91cf\u4e3a\u4e0b\u7ea7\u9879\u76ee\u4f20\u9012\u53c2\u6570 image_name = accountwebapiserver name_space = webapi server_name = account csproj_file = AccountWebAPIServer zk_approle = Common-AccountWebApi Controller = account run_scope = Core991 ms_group = $ms_group zk_configserver = w1.confandsa.zk.group.hex.com:2181,w2.confandsa.zk.group.hex.com:2181,w3.confandsa.zk.group.hex.com:2181 image_tag = $image_tag memory = 1Gi maxmemory = 2 .2Gi NodePort = 32037 k8s_group = $k8s_group other_parameters = --UrlPrefix tms-zw4","title":"jenkins\u4f20\u6570"},{"location":"k8s/yaml/yaml/#_3","text":"$ cat nginx-deploy.yaml apiVersion : apps/v1 kind : Deployment metadata : name : nginx-yyds namespace : web spec : selector : matchLabels : app : nginx replicas : 2 template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:latest ports : - containerPort : 80 \u8fd0\u884cYaml\u6587\u4ef6 k apply -f nginx-deploy.yaml \u67e5\u770bpod\u7684\u72b6\u6001 $ k get pod NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 47m nginx-yyds-585449566-vzwkw 1 /1 Running 0 47m","title":"\u7b2c\u4e00\u4e2a\u7b80\u5355\u7684\u5bb9\u5668\u5316\u793a\u4f8b\uff1a"},{"location":"k8s/yaml/yaml/#_4","text":"\u67e5\u770b\u63a7\u5236\u5668 $ k get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-yyds 2 /2 2 2 51m \u67e5\u770bpod\u7684\u8be6\u7ec6\u4fe1\u606f $ k get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-yyds-585449566-76hgr 1 /1 Running 0 53m 10 .42.2.130 node1 <none> <none> nginx-yyds-585449566-vzwkw 1 /1 Running 0 53m 10 .42.1.127 node0 <none> <none \u901a\u8fc7\u6807\u7b7e\u6765\u67e5\u627epod $ k get pod -l app = nginx NAME READY STATUS RESTARTS AGE nginx-yyds-585449566-76hgr 1 /1 Running 0 56m nginx-yyds-585449566-vzwkw 1 /1 Running 0 56m $ k describe pod nginx-yyds-585449566-76hgr Name: nginx-yyds-585449566-76hgr Namespace: web Priority: 0 Node: node1/192.168.0.151 Start Time: Tue, 23 Aug 2022 15 :46:16 +0800 Labels: app = nginx pod-template-hash = 585449566 Annotations: <none> Status: Running IP: 10 .42.2.130 IPs: IP: 10 .42.2.130 Controlled By: ReplicaSet/nginx-yyds-585449566 Containers: nginx: Container ID: docker://7994248f600aa93444272eff8092938c866a5648db1126545d28635d41251b51 Image: nginx:latest Image ID: docker-pullable://nginx@sha256:dc29f133a33a1d6311807f3b88134000ce67318a40517b1060b929b84b0bbea0 Port: 80 /TCP Host Port: 0 /TCP State: Running Started: Tue, 23 Aug 2022 15 :48:42 +0800 Ready: True Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rf7j8 ( ro ) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-rf7j8: Type: Projected ( a volume that contains injected data from multiple sources ) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: <nil> DownwardAPI: true QoS Class: BestEffort Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute op = Exists for 300s node.kubernetes.io/unreachable:NoExecute op = Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 58m default-scheduler Successfully assigned web/nginx-yyds-585449566-76hgr to node1 Normal Pulling 58m kubelet Pulling image \"nginx:latest\" Normal Pulled 55m kubelet Successfully pulled image \"nginx:latest\" in 2m24.557367751s Normal Created 55m kubelet Created container nginx Normal Started 55m kubelet Started container nginx \u901a\u8fc7explain\u6765\u67e5\u770bYaml\u6587\u4ef6\u5199\u6cd5 $ k explain","title":"\u5e38\u7528\u7684\u7ba1\u7406\u547d\u4ee4\uff1a"},{"location":"mylife/life-docs1/","text":"\u6211\u76842021 \u00b6 Hello! \u5927\u5bb6\u597d\uff0c\u6211\u662f\u6765\u81ea\u5c71\u897f\u7684\u5317\u6f02\u4e00\u65cf\uff0c\u8bb0\u5f55\u6587\u7ae0\u4e3b\u8981\u662f\u5b66\u4e60\u548c\u4e2a\u4eba\u7684\u4e00\u4e9b\u7231\u597d\uff01 \u8fd9\u91cc\u4e3b\u8981\u629b\u5f00\u5de5\u4f5c\uff0c\u6765\u804a\u804a\u81ea\u5df1\u7684\u4e2a\u4eba\u751f\u6d3b\u548c\u672a\u6765\u89c4\u5212\u3002 \u90a32021\u5462\uff0c\u662f\u6211\u6765\u5317\u4eac\u5de5\u4f5c\u7684\u7b2c\u4e00\u5e74\uff0c\u4e5f\u662f\u548c\u5927\u591a\u6570\u7684\u4e0a\u73ed\u65cf\u4e00\u6837\uff0c\u62b1\u7740\u8ff7\u832b\u7684\u773c\u795e\u6765\u5230\u8fd9\u4e2a\u5927\u7684\u57ce\u5e02\uff0c\u4e5f\u4e0d\u77e5\u5230\u672a\u6765\u4f1a\u548b\u6837\uff0c\u4f46\u662f\u8fd8\u662f\u60f3\u7740\u8bf4\u5e74\u8f7b\u5e94\u8be5\u591a\u95ef\u8361\u4e00\u4e0b\uff0c\u6bd5\u7adf\u5e74\u8f7b\u6709\u8bd5\u9519\u7684\u673a\u4f1a\uff5e \uff0c\u4e07\u4e00\u5bf9\u5427\uff0c\u6df7\u7684\u4e0d\u9519\uff0c\u4e5f\u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u9009\u62e9\uff0c\u6b63\u5982\u738b\u8001\u677f\u8bf4\u7684\u6e05\u534e\u5317\u5927\u4e0d\u5982\u80c6\u5b50\u5927\uff0c\u54c8\u54c8\u54c8\uff0c\u5f53\u7136\u662f\u51e1\u5c14\u8d5b\u3002\u8c01\u8fd8\u6ca1\u6709\u4e00\u4ebf\u7684\u5c0f\u76ee\u6807\u5462\uff01 \u5148\u4ecb\u7ecd\u4e00\u4e0b\u6765\u5317\u4eac\u7b2c\u4e00\u5bb6\u516c\u53f8\uff0c\u662f\u505a\u4e92\u8054\u7f51\u6559\u80b2\u7684\uff0c\u5b98\u65b9\u4e00\u70b9\u7684\u8bdd\u53eb\u6559\u5b66\u4e91\u670d\u52a1\u5e73\u53f0\uff0c\u8bf4\u767d\u4e86\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5e2e\u52a9\u8001\u5e08\u6559\u5b66\u7684\u4e00\u6b3e\u5de5\u5177\uff0c\u5982\u679c\u5b9e\u5728\u7406\u89e3\u4e0d\u4e86\uff0c\u5c31\u6bd4\u5982\u6709\u4e9b\u4eba\u79bb\u5f00github\uff0c\u8fd9\u4e2a\u4ee3\u7801\u6211\u5c31\u4e0d\u4f1a\u5199\u4e0d\u4e86\uff5e \u4e0d\u8fc7\u603b\u7684\u6765\u8bf4\u6211\u4e5f\u7ecf\u5386\u7684\u6765\u4e00\u6b21\u516c\u53f8\u7684\u642c\u5bb6\uff0c\u4ece\u524d\u516b\u5bb6\u642c\u5230\u6c47\u667a\u5927\u53a6\uff0c\u4e0b\u9762\u6765\u6211\u62cd\u4e86\u4e00\u5f20\u56fe\u53ef\u4ee5\u770b\u4e00\u4e0b \u53ef\u4ee5\u5148\u770b\u4e00\u4e0b\u88c5\u4fee\u524d\u7684\u6837\u5b50 \u597d\u5bb6\u4f19\uff0c\u4e0d\u77e5\u9053\u7684\u8fd8\u4ee5\u4e3a\u662f\u5e9f\u589f\u5462 \u88c5\u4fee\u597d\u4e4b\u540e\u8fd8\u662f\u5f88\u597d\u7684 \u5176\u5b9e\u8fd8\u662f\u76f8\u5f53\u5927\u7684\uff0c\u6bd4\u8d77\u4e4b\u524d\u7684\u90a3\u4e2a\u73af\u5883\u8981\u597d\u4e0d\u5c11\u3002\u524d\u9762\u90a3\u4f4d\u662f\u6211\u7684leader\uff01 \u4e0d\u8fc7\u5728\u8fd9\u91cc\u5de5\u4f5c\u6211\u8ba4\u4e3a\u6700\u4e0d\u8212\u670d\u7684\u5c31\u662f\u6709\u70b9\u592a\u8d76\u4e86\uff0c\u5c31\u50cf\u662f\u960e\u738b\u7237\u50ac\u547d\u4f3c\u7684\uff0c\u800c\u4e14\u5929\u5929\u5f00\u4f1a\u8fd9\u4e2a\u771f\u7684\u8ba9\u4eba\u65e0\u6cd5\u7406\u89e3\uff0c\u5f53\u7136\u5927\u90e8\u5206\u516c\u53f8\u90fd\u662f\u8fd9\u6837\uff0c\u4e5f\u662f\u6ca1\u5f97\u529e\u6cd5\uff0c\u6253\u5de5\u4eba\u4e0d\u5c31\u662f\u4eba\u5bb6\u8ba9\u5e72\u5565\u548b\u5c31\u5e72\u5565\uff5e \u4e5f\u662f\u5927\u90e8\u5206\u4e2d\u56fd\u516c\u53f8\u7684\u901a\u75c5\uff0c\u4e0d\u8fc7\u4e2a\u4eba\u633a\u4e0d\u559c\u6b22\u8fd9\u6837\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u6709\u6bb5\u65f6\u95f4\u7b80\u76f4\u6709\u70b9\u5c0f\u5d29\u6e83\uff0c\u540e\u6765\u60f3\u60f3\u65e2\u7136\u89e3\u51b3\u4e0d\u4e86\u95ee\u9898\uff0c\u5c31\u89e3\u51b3\u6211\uff0c\u54c8\u54c8\u54c8\uff5e \u603b\u7684\u6765\u8bf4: \u4e2a\u4eba\u8bc4\u4ef7\u4e0d\u4ee3\u8868\u6240\u6709\u4eba\u7684\u770b\u6cd5\uff0c\u516c\u53f8\u7684\u5927\u90e8\u5206\u540c\u4e8b\u90fd\u662f\u7279\u522b\u597d\u7684\uff0c\u9886\u5bfc\u4e5f\u6bd4\u8f83\u7684\u968f\u548c\uff0c\u6ca1\u6709\u4ec0\u4e48\u5b98\u67b6\u5b50\uff0c\u9664\u4e86\u516c\u53f8\u5236\u5ea6\u4e2a\u4eba\u6709\u70b9\u63a5\u53d7\u4e0d\u4e86\u4ee5\u5916\uff0c\u8fd8\u662f\u633a\u4e0d\u9519\u7684\u3002 \u8981\u8bf4\u5728\u5317\u4eac\u9664\u4e86\u5de5\u4f5c\uff0c\u4ec0\u4e48\u6700\u7d2f\uff1f \u4e0d\u7528\u8bf4\uff0c\u642c\u5bb6\uff01 \u90a3\u4e48\u6211\u662f\u4ece\u8042\u5404\u5e84\u642c\u5230\u5929\u901a\u82d1\uff08\u4e9a\u6d32\u6700\u5927\u793e\u533a\uff09 \u8fd8\u771f\u7684\u662f\uff0c\u4e0d\u6765\u4e0d\u77e5\u9053\uff0c\u6765\u4e86\u5c31\u518d\u4e5f\u4e0d\u60f3\u6765\u4e86\uff01 \u90a3\u5c31\u6709\u5f88\u591a\u4e0d\u7406\u89e3\u7684\u5c0f\u660e\u8bf4\u4e3a\u5565\u4e0d\u60f3\u6765\u4e86\uff0c\u53ef\u4ee5\u770b\u770b\u4e0b\u9762\u8fd9\u5f20\u56fe \u770b\u628a\u8fd9\u5927\u59b9\u5b50\u6324\u7684\uff0c\u8981\u8bf4\u600e\u4e48\u624d\u80fd\u6324\u4e0a\u5730\u94c1\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e0b\u4e24\u7c73\u51b2\u523a\u4e4b\u7c7b\u7684\u524d\u51b2\u52a8\u4f5c\uff0c\u5728\u5f00\u95e8\u7684\u4e00\u77ac\u95f4\u8d77\u8df3\u3002 \u5982\u679c\u4f60\u540e\u9762\u6709\u4e94\u7c73\u9ad8\u7684\u58ee\u6c49\uff0c\u90a3\u606d\u559c\u4f60\u51b2\u523a\u5c31\u4e0d\u9700\u8981\u4e86\uff0c\u8fd9\u4e2a\u808c\u8089\u731b\u7537\u53ef\u4ee5\u76f4\u63a5\u5c06\u4f60\u9001\u4f60\u5230\u8f66\u53a2\u7684\u6b63\u4e2d\u95f4\u3002\u800c\u4e14\u5929\u901a\u82d1\u79df\u623f\u5b50\u73af\u5883\u5f88\u4e0d\u53cb\u597d\uff0c\u4e00\u822c\u90fd\u662f \u7fa4\u5c45 \uff0c\u5efa\u8bae\u6709\u5173\u90e8\u95e8\u53ef\u4ee5\u597d\u597d\u67e5\u67e5\uff5e \u901a\u8fc7\u8fd9\u4e24\u70b9\u5c31\u8db3\u4ee5\u88ab\u8bc4\u4e0a\u6700\u5dee\u5e78\u798f\u6307\u6570\u7b2c\u4e00\u540d\u3002 \u4e0d\u8fc7\u503c\u5f97\u9ad8\u5174\u7684\u662f\uff0c\u4e0a\u73ed\u7684\u65f6\u95f4\u4ece\u957f\u8fbe1\u4e2a\u5c0f\u65f640\u5206\u949f\u51cf\u5c11\u52301\u4e2a\u5c0f\u65f6\u3002\uff08\u53ef\u4ee5\u591a\u7761\u534a\u5c0f\u65f6\uff09 \u5f53\u7136\u79df\u623f\u5343\u4e07\u4e0d\u8981\u5927\u610f\uff0c\u4e0d\u8981\u4fe1\u4efb\u4f55\u4e2d\u4ecb\u6ca1\u6709\u4e66\u9762\u5408\u540c\u7684\u9b3c\u8bdd\uff0c\u4eba\u751f\u7ecf\u9a8c\u554a\uff01\u4e0d\u7136\u7684\u8bdd\uff0c\u4e2d\u4ecb\u4f1a\u548c\u4f60\u7b7e\u7f72\u4e00\u7cfb\u5217\u7684\u4e0d\u5e73\u7b49\u6761\u7ea6\uff0c\u6700\u540e\u8fd8\u8981\u60f3\u5c3d\u529e\u6cd5\u6263\u4f60\u7684\u62bc\u91d1\uff0c\u4e0d\u8981\u95ee\u6211\u600e\u4e48\u77e5\u9053\u7684\uff5e \u6240\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u9075\u7eaa\u5b88\u6cd5\u7684\u597d\u516c\u6c11\uff0c\u6211\u80af\u5b9a\u4f1a\u5148\u95ee\u5019\u7684\u4ed6\u4eec\u7684\u7956\u5b97\u5341\u516b\u4ee3\u4e4b\u540e\uff0c\u518d\u6253\u4e2a\u6295\u8bc9\u7535\u8bdd\uff0c\u8ba9\u76f8\u5173\u90e8\u95e8\u5904\u7406\uff0c\u4e00\u4e2a\u4e0d\u5c0f\u5fc3\u5c31\u5168\u62c6\u4e86\u3002\u7531\u6b64\u53ef\u4ee5\u5f97\u51fa\u4e00\u4e2a\u5e7f\u544a\u8bed: \u5927\u5bb6\u597d\uff0c\u624d\u662f\u771f\u7684\u597d\u3002","title":"\u6211\u76842021"},{"location":"mylife/life-docs1/#2021","text":"Hello! \u5927\u5bb6\u597d\uff0c\u6211\u662f\u6765\u81ea\u5c71\u897f\u7684\u5317\u6f02\u4e00\u65cf\uff0c\u8bb0\u5f55\u6587\u7ae0\u4e3b\u8981\u662f\u5b66\u4e60\u548c\u4e2a\u4eba\u7684\u4e00\u4e9b\u7231\u597d\uff01 \u8fd9\u91cc\u4e3b\u8981\u629b\u5f00\u5de5\u4f5c\uff0c\u6765\u804a\u804a\u81ea\u5df1\u7684\u4e2a\u4eba\u751f\u6d3b\u548c\u672a\u6765\u89c4\u5212\u3002 \u90a32021\u5462\uff0c\u662f\u6211\u6765\u5317\u4eac\u5de5\u4f5c\u7684\u7b2c\u4e00\u5e74\uff0c\u4e5f\u662f\u548c\u5927\u591a\u6570\u7684\u4e0a\u73ed\u65cf\u4e00\u6837\uff0c\u62b1\u7740\u8ff7\u832b\u7684\u773c\u795e\u6765\u5230\u8fd9\u4e2a\u5927\u7684\u57ce\u5e02\uff0c\u4e5f\u4e0d\u77e5\u5230\u672a\u6765\u4f1a\u548b\u6837\uff0c\u4f46\u662f\u8fd8\u662f\u60f3\u7740\u8bf4\u5e74\u8f7b\u5e94\u8be5\u591a\u95ef\u8361\u4e00\u4e0b\uff0c\u6bd5\u7adf\u5e74\u8f7b\u6709\u8bd5\u9519\u7684\u673a\u4f1a\uff5e \uff0c\u4e07\u4e00\u5bf9\u5427\uff0c\u6df7\u7684\u4e0d\u9519\uff0c\u4e5f\u662f\u4e00\u4e2a\u4e0d\u9519\u7684\u9009\u62e9\uff0c\u6b63\u5982\u738b\u8001\u677f\u8bf4\u7684\u6e05\u534e\u5317\u5927\u4e0d\u5982\u80c6\u5b50\u5927\uff0c\u54c8\u54c8\u54c8\uff0c\u5f53\u7136\u662f\u51e1\u5c14\u8d5b\u3002\u8c01\u8fd8\u6ca1\u6709\u4e00\u4ebf\u7684\u5c0f\u76ee\u6807\u5462\uff01 \u5148\u4ecb\u7ecd\u4e00\u4e0b\u6765\u5317\u4eac\u7b2c\u4e00\u5bb6\u516c\u53f8\uff0c\u662f\u505a\u4e92\u8054\u7f51\u6559\u80b2\u7684\uff0c\u5b98\u65b9\u4e00\u70b9\u7684\u8bdd\u53eb\u6559\u5b66\u4e91\u670d\u52a1\u5e73\u53f0\uff0c\u8bf4\u767d\u4e86\u5176\u5b9e\u5c31\u662f\u4e00\u4e2a\u5e2e\u52a9\u8001\u5e08\u6559\u5b66\u7684\u4e00\u6b3e\u5de5\u5177\uff0c\u5982\u679c\u5b9e\u5728\u7406\u89e3\u4e0d\u4e86\uff0c\u5c31\u6bd4\u5982\u6709\u4e9b\u4eba\u79bb\u5f00github\uff0c\u8fd9\u4e2a\u4ee3\u7801\u6211\u5c31\u4e0d\u4f1a\u5199\u4e0d\u4e86\uff5e \u4e0d\u8fc7\u603b\u7684\u6765\u8bf4\u6211\u4e5f\u7ecf\u5386\u7684\u6765\u4e00\u6b21\u516c\u53f8\u7684\u642c\u5bb6\uff0c\u4ece\u524d\u516b\u5bb6\u642c\u5230\u6c47\u667a\u5927\u53a6\uff0c\u4e0b\u9762\u6765\u6211\u62cd\u4e86\u4e00\u5f20\u56fe\u53ef\u4ee5\u770b\u4e00\u4e0b \u53ef\u4ee5\u5148\u770b\u4e00\u4e0b\u88c5\u4fee\u524d\u7684\u6837\u5b50 \u597d\u5bb6\u4f19\uff0c\u4e0d\u77e5\u9053\u7684\u8fd8\u4ee5\u4e3a\u662f\u5e9f\u589f\u5462 \u88c5\u4fee\u597d\u4e4b\u540e\u8fd8\u662f\u5f88\u597d\u7684 \u5176\u5b9e\u8fd8\u662f\u76f8\u5f53\u5927\u7684\uff0c\u6bd4\u8d77\u4e4b\u524d\u7684\u90a3\u4e2a\u73af\u5883\u8981\u597d\u4e0d\u5c11\u3002\u524d\u9762\u90a3\u4f4d\u662f\u6211\u7684leader\uff01 \u4e0d\u8fc7\u5728\u8fd9\u91cc\u5de5\u4f5c\u6211\u8ba4\u4e3a\u6700\u4e0d\u8212\u670d\u7684\u5c31\u662f\u6709\u70b9\u592a\u8d76\u4e86\uff0c\u5c31\u50cf\u662f\u960e\u738b\u7237\u50ac\u547d\u4f3c\u7684\uff0c\u800c\u4e14\u5929\u5929\u5f00\u4f1a\u8fd9\u4e2a\u771f\u7684\u8ba9\u4eba\u65e0\u6cd5\u7406\u89e3\uff0c\u5f53\u7136\u5927\u90e8\u5206\u516c\u53f8\u90fd\u662f\u8fd9\u6837\uff0c\u4e5f\u662f\u6ca1\u5f97\u529e\u6cd5\uff0c\u6253\u5de5\u4eba\u4e0d\u5c31\u662f\u4eba\u5bb6\u8ba9\u5e72\u5565\u548b\u5c31\u5e72\u5565\uff5e \u4e5f\u662f\u5927\u90e8\u5206\u4e2d\u56fd\u516c\u53f8\u7684\u901a\u75c5\uff0c\u4e0d\u8fc7\u4e2a\u4eba\u633a\u4e0d\u559c\u6b22\u8fd9\u6837\u7684\u5de5\u4f5c\u65b9\u5f0f\uff0c\u6709\u6bb5\u65f6\u95f4\u7b80\u76f4\u6709\u70b9\u5c0f\u5d29\u6e83\uff0c\u540e\u6765\u60f3\u60f3\u65e2\u7136\u89e3\u51b3\u4e0d\u4e86\u95ee\u9898\uff0c\u5c31\u89e3\u51b3\u6211\uff0c\u54c8\u54c8\u54c8\uff5e \u603b\u7684\u6765\u8bf4: \u4e2a\u4eba\u8bc4\u4ef7\u4e0d\u4ee3\u8868\u6240\u6709\u4eba\u7684\u770b\u6cd5\uff0c\u516c\u53f8\u7684\u5927\u90e8\u5206\u540c\u4e8b\u90fd\u662f\u7279\u522b\u597d\u7684\uff0c\u9886\u5bfc\u4e5f\u6bd4\u8f83\u7684\u968f\u548c\uff0c\u6ca1\u6709\u4ec0\u4e48\u5b98\u67b6\u5b50\uff0c\u9664\u4e86\u516c\u53f8\u5236\u5ea6\u4e2a\u4eba\u6709\u70b9\u63a5\u53d7\u4e0d\u4e86\u4ee5\u5916\uff0c\u8fd8\u662f\u633a\u4e0d\u9519\u7684\u3002 \u8981\u8bf4\u5728\u5317\u4eac\u9664\u4e86\u5de5\u4f5c\uff0c\u4ec0\u4e48\u6700\u7d2f\uff1f \u4e0d\u7528\u8bf4\uff0c\u642c\u5bb6\uff01 \u90a3\u4e48\u6211\u662f\u4ece\u8042\u5404\u5e84\u642c\u5230\u5929\u901a\u82d1\uff08\u4e9a\u6d32\u6700\u5927\u793e\u533a\uff09 \u8fd8\u771f\u7684\u662f\uff0c\u4e0d\u6765\u4e0d\u77e5\u9053\uff0c\u6765\u4e86\u5c31\u518d\u4e5f\u4e0d\u60f3\u6765\u4e86\uff01 \u90a3\u5c31\u6709\u5f88\u591a\u4e0d\u7406\u89e3\u7684\u5c0f\u660e\u8bf4\u4e3a\u5565\u4e0d\u60f3\u6765\u4e86\uff0c\u53ef\u4ee5\u770b\u770b\u4e0b\u9762\u8fd9\u5f20\u56fe \u770b\u628a\u8fd9\u5927\u59b9\u5b50\u6324\u7684\uff0c\u8981\u8bf4\u600e\u4e48\u624d\u80fd\u6324\u4e0a\u5730\u94c1\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e0b\u4e24\u7c73\u51b2\u523a\u4e4b\u7c7b\u7684\u524d\u51b2\u52a8\u4f5c\uff0c\u5728\u5f00\u95e8\u7684\u4e00\u77ac\u95f4\u8d77\u8df3\u3002 \u5982\u679c\u4f60\u540e\u9762\u6709\u4e94\u7c73\u9ad8\u7684\u58ee\u6c49\uff0c\u90a3\u606d\u559c\u4f60\u51b2\u523a\u5c31\u4e0d\u9700\u8981\u4e86\uff0c\u8fd9\u4e2a\u808c\u8089\u731b\u7537\u53ef\u4ee5\u76f4\u63a5\u5c06\u4f60\u9001\u4f60\u5230\u8f66\u53a2\u7684\u6b63\u4e2d\u95f4\u3002\u800c\u4e14\u5929\u901a\u82d1\u79df\u623f\u5b50\u73af\u5883\u5f88\u4e0d\u53cb\u597d\uff0c\u4e00\u822c\u90fd\u662f \u7fa4\u5c45 \uff0c\u5efa\u8bae\u6709\u5173\u90e8\u95e8\u53ef\u4ee5\u597d\u597d\u67e5\u67e5\uff5e \u901a\u8fc7\u8fd9\u4e24\u70b9\u5c31\u8db3\u4ee5\u88ab\u8bc4\u4e0a\u6700\u5dee\u5e78\u798f\u6307\u6570\u7b2c\u4e00\u540d\u3002 \u4e0d\u8fc7\u503c\u5f97\u9ad8\u5174\u7684\u662f\uff0c\u4e0a\u73ed\u7684\u65f6\u95f4\u4ece\u957f\u8fbe1\u4e2a\u5c0f\u65f640\u5206\u949f\u51cf\u5c11\u52301\u4e2a\u5c0f\u65f6\u3002\uff08\u53ef\u4ee5\u591a\u7761\u534a\u5c0f\u65f6\uff09 \u5f53\u7136\u79df\u623f\u5343\u4e07\u4e0d\u8981\u5927\u610f\uff0c\u4e0d\u8981\u4fe1\u4efb\u4f55\u4e2d\u4ecb\u6ca1\u6709\u4e66\u9762\u5408\u540c\u7684\u9b3c\u8bdd\uff0c\u4eba\u751f\u7ecf\u9a8c\u554a\uff01\u4e0d\u7136\u7684\u8bdd\uff0c\u4e2d\u4ecb\u4f1a\u548c\u4f60\u7b7e\u7f72\u4e00\u7cfb\u5217\u7684\u4e0d\u5e73\u7b49\u6761\u7ea6\uff0c\u6700\u540e\u8fd8\u8981\u60f3\u5c3d\u529e\u6cd5\u6263\u4f60\u7684\u62bc\u91d1\uff0c\u4e0d\u8981\u95ee\u6211\u600e\u4e48\u77e5\u9053\u7684\uff5e \u6240\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u9075\u7eaa\u5b88\u6cd5\u7684\u597d\u516c\u6c11\uff0c\u6211\u80af\u5b9a\u4f1a\u5148\u95ee\u5019\u7684\u4ed6\u4eec\u7684\u7956\u5b97\u5341\u516b\u4ee3\u4e4b\u540e\uff0c\u518d\u6253\u4e2a\u6295\u8bc9\u7535\u8bdd\uff0c\u8ba9\u76f8\u5173\u90e8\u95e8\u5904\u7406\uff0c\u4e00\u4e2a\u4e0d\u5c0f\u5fc3\u5c31\u5168\u62c6\u4e86\u3002\u7531\u6b64\u53ef\u4ee5\u5f97\u51fa\u4e00\u4e2a\u5e7f\u544a\u8bed: \u5927\u5bb6\u597d\uff0c\u624d\u662f\u771f\u7684\u597d\u3002","title":"\u6211\u76842021"},{"location":"network/3-network-docs/","text":"\u7f51\u7edc\u901a\u4fe1\u57fa\u7840\uff1a CNI \u901a\u5e38\u6709\u4e09\u79cd\u5b9e\u73b0\u6a21\u5f0f \u5b98\u7f51: https://kubernetes.io/zh-cn/docs/concepts/cluster-administration/networking/ flannel\u4ecb\u7ecd: \u00b6 \u7f51\u7edc\u6a21\u5f0f \u00b6 github \u5730\u5740: https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md flannel \u7684\u7f51\u7edc\u6a21\u578b\uff08\u540e\u7aef\uff09\uff1a\u76ee\u524d\u6709\u4e09\u79cd\u65b9\u5f0f\u5b9e\u73b0UDP/VXLAN/host-gw UDP : \u5728\u6700\u65e9\u671fflannel\u4f7f\u7528UDP\u5c01\u88c5\u6765\u5b8c\u6210\u62a5\u6587\u7684\u8de8\u4e3b\u673a\u8f6c\u53d1,\u5b89\u5168\u6027\u548c\u6027\u80fd\u7565\u6709\u4e0d\u8db3(\u5df2\u5e9f\u5f03) VXLAN: Linux\u5185\u6838\u5728\u57282012\u5e74\u5e95\u7684v3.7.0\u4e4b\u540e\u52a0\u5165\u4e86VXLAN\u534f\u8bae\u652f\u6301\uff0c\u56e0\u6b64\u65b0\u7248\u672c\u7684Flanne1\u4e5f\u6709UDP\u8f6c\u6362\u4e3aVXLAN, VXLAN\u672c\u8d28\u4e0a\u662f\u4e00\u79cd tunnel (\u96a7\u9053)\u534f\u8bae\uff0c\u7528\u6765\u57fa\u4e8e3\u5c42\u7f51\u7edc\u5b9e\u73b0\u865a\u62df\u76842\u5c42\u7f51\u7edc\uff0c\u76ee\u524dflannel\u7684\u7f51\u7edc\u6a21\u578b\u5df2\u7ecf\u662f\u57fa\u4e8eVXLAN\u7684\u53e0\u52a0(\u8986\u76d6)\u7f51\u7edc\uff0c\u76ee\u524d\u63a8\u8350\u4f7f\u7528 vxlan\u4f5c\u4e3a\u5176\u7f51\u7edc\u6a21\u578b\u3002(\u4f7f\u7528\u6700\u591a) Host-gw:\u4e5f\u5c31\u662fHost GateWay, \u901a\u8fc7\u5728node\u8282\u70b9\u4e0a\u521b\u5efa\u5230\u8fbe\u5404\u76ee\u6807\u5bb9\u5668\u5730\u5740\u7684\u8def\u7531\u8868\u800c\u5b8c\u6210\u62a5\u6587\u7684\u8f6c\u53d1\uff0c\u56e0\u6b64\u8fd9\u79cd\u65b9\u5f0f\u8981\u6c42\u5404node\u8282\u70b9\u672c\u8eab \u5fc5\u987b\u5904\u4e8e\u540c\u4e00\u4e2a\u5c40\u57df\u7f51(\u4e8c\u5c42\u7f51\u7edc)\u4e2d\uff0c\u56e0\u6b64\u4e0d\u9002\u7528\u4e8e\u7f51\u7edc\u53d8\u52a8\u9891\u7e41\u6216\u6bd4\u8f83\u5927\u578b\u7684\u7f51\u7edc\u73af\u5883\uff0c\u4f46\u662f\u5176\u6027\u80fd\u8f83\u597d\u3002 Flannel\u7ec4\u4ef6\u7684\u8bf4\u660e: \u00b6 Cni0:\u7f51\u6865\u8bbe\u5907\uff0c\u6bcf\u521b\u5efa\u4e00\u4e2apod\u90fd\u4f1a\u521b\u5efa\u5bf9veth pair, \u5176\u4e2d\u4e00\u7aef\u662fpod\u4e2d\u7684eth0,\u53e6\u4e00\u7aef\u662fCni0\u7f51\u6865\u4e2d\u7684\u7aef\u53e3(\u7f51\u5361) \uff0cPod\u4e2d\u4ece\u7f51\u5361 eth0\u53d1\u51fa\u7684\u6d41\u91cf\u90fd\u4f1a\u53d1\u9001\u5230Cni0\u7f51\u6865\u8bbe\u5907\u7684\u7aef\u53e3(\u7f51\u5361)\u4e0a\uff0cCni0\u8bbe\u5907\u83b7\u5f97\u7684ip\u5730\u5740\u662f\u8be5\u8282\u70b9\u5206\u914d\u5230\u7684\u7f51\u6bb5\u7684\u7b2c\u4e00\u4e2a\u5730\u5740\u3002 \u539f\u751f\u6a21\u5f0f: Flannel.1: overlay\u7f51\u7edc\u7684\u8bbe\u5907\uff0c\u7528\u6765\u8fdb\u884cvxlan\u62a5\u6587\u7684\u5904\u7406(\u5c01\u5305\u548c\u89e3\u5305)\uff0c \u4e0d\u540cnode\u4e4b\u95f4\u7684pod\u6570\u636e\u6d41\u91cf\u90fd\u4eceoverlay\u8bbe\u5907\u4ee5\u96a7\u9053\u7684\u5f62\u5f0f \u53d1\u9001\u5230\u5bf9\u7aef\u3002 flannel \u7684\u7cfb\u7edf\u6587\u4ef6\u53ca\u76ee\u5f55: \u00b6 root@node2:~# find / -name flannel VXLAN \u539f\u751f\u6a21\u5f0f: \u00b6 k3s \u9ed8\u8ba4\u7684 flannel \u4e5f\u9ed8\u8ba4vxlan,\u4f46\u662f\u4ecev1.26 \u5f00\u59cb\u79fb\u9664\u4e86 wireguard \u540e\u7aef\uff0c\u4ece\u800c\u652f\u6301 Flannel \u539f\u751f\u7684 wireguard-native \u540e\u7aef https://docs.k3s.io/installation/network-options sshd\u4e3b\u673a\u4e0a\u7684\u5bb9\u5668\u8bbf\u95ee\u522b\u7684\u5bb9\u5668,\u9996\u5148\u901a\u8fc7\u7f51\u6865\u5224\u65ad\u76ee\u7684\u5730\u5740\u662f\u8bbf\u95ee\u5f53\u524d\u7684\u5730\u5740\u8fd8\u662f\u5176\u4ed6\u4e3b\u673a,\u5982\u679c\u5f53\u524d\u4e3b\u673a\u7684\u5bb9\u5668\u4e0d\u8d70flannel.1\u5c31\u76f4\u63a5\u8fd4\u56de\u4e86,\u5982\u679c\u662f\u8bbf\u95ee\u5176\u4ed6 \u6709\u4e24\u79cd\u573a\u666f; \u573a\u666f\u4e00: \u8bbf\u95ee\u522b\u7684 node \u4e0a\u7684\u5bb9\u5668,\u8fd9\u65f6\u4ed6\u7684\u4e3b\u673a\u4e0a\u5c31\u4f1a\u6709\u5bf9\u5e94\u8def\u7531\u8868,\u4f8b\u5982\u4e0a\u56fe\u4e2d,\u53ef\u80fd\u4ed6\u8981\u8bbf\u95ee\u7684\u5413\u4e00\u8df3\u5c31\u662f172.16.78.0/21,\u90a3\u4e48\u5c31\u4f1a\u8d70flannel.1\u6765\u505a\u62a5\u6587\u7684\u5c01\u88c5;\u8fd9\u65f6\u5c31\u8d70\u96a7\u9053\u4e86,\u4e0d\u4f1a\u5728\u7269\u7406\u7f51\u5361\u6765\u505a\u62a5\u6587\u7684\u5c01\u88c5. \u573a\u666f\u4e8c: \u5982\u679c\u76ee\u7684\u5730\u5740\u4e0d\u662f k8s \u7684\u73af\u5883,\u800c\u662f\u5916\u7f51,\u90a3\u4e48\u5c31\u4f1a\u5339\u914d\u8def\u7531\u7684\u65f6\u5019\u5c31\u4e0d\u4f1a\u8d70\u96a7\u9053\u8bbe\u5907,\u5728\u5185\u6838\u505a\u7269\u7406\u7f51\u5361\u7684\u5c01\u88c5,\u901a\u8fc7\u5bbf\u4e3b\u673a\u51fa\u53bb. \u4e0d\u540c\u7684\u4e3b\u673a\u7684\u5bb9\u5668\u901a\u4fe1\u9700\u8981\u901a\u8fc7flannel.1 \u8fdb\u884c\u5c01\u88c5\u548c\u89e3\u5c01 \u597d\u5904: \u53ef\u4ee5\u8de8\u5b50\u7f51\u901a\u4fe1 \u7f3a\u70b9: \u6027\u80fd\u5dee\u70b9 DirectRouting\u6a21\u5f0f: \u00b6 VXLAN DirectRoutinghost-gw\uff08\u5e03\u5c14\u503c\uff09\uff1a\u5f53\u4e3b\u673a\u4f4d\u4e8e\u540c\u4e00\u5b50\u7f51\u4e0a\u65f6\u542f\u7528\u76f4\u63a5\u8def\u7531\uff08\u5982\uff09\u3002VXLAN \u5c06\u4ec5\u7528\u4e8e\u5c01\u88c5\u6570\u636e\u5305\u5230\u4e0d\u540c\u5b50\u7f51\u4e0a\u7684\u4e3b\u673a\u3002\u9ed8\u8ba4\u4e3afalse. Windows \u4e0d\u652f\u6301 DirectRouting \u5f00\u542f\u4e86\u8fd9\u4e2a\u6a21\u5f0f,\u90a3\u7f51\u7edc\u5c31\u4f1a\u63d0\u5347,\u539f\u56e0\u5c31\u662f\u5728\u540c\u7f51\u6bb5\u4e0d\u8fdb\u884c\u5c01\u88c5,\u53ea\u6709\u8de8\u5b50\u7f51\u624d\u8fdb\u884c\u5c01\u88c5. \u76d1\u542c\u7aef\u53e3: [ openbayes-enflame ] root@node1:~# ss -auntp | grep 8472 udp UNCONN 0 0 0 .0.0.0:8472 0 .0.0.0:* host-gw \u6a21\u5f0f\uff1a \u00b6 \u548cDirectRouting\u7c7b\u4f3c,\u4f46\u662f\u4e0d\u80fd\u8de8\u5b50\u7f51 Use host-gw to create IP routes to subnets via remote machine IPs. Requires direct layer2 connectivity between hosts running flannel. host-gw provides good performance, with few dependencies, and easy set up. flannel \u603b\u7ed3: \u63a8\u8350\u4f7f\u7528VXLAN DirectRouting \u76f8\u540c\u7684 node \u5b50\u7f51\u901a\u4fe1,\u4e0d\u9700\u8981overlay\u5c01\u88c5\u548c\u89e3\u5c01\u88c5,\u800c\u4e14\u8fd8\u53ef\u4ee5\u652f\u6301 node \u8de8\u5b50\u7f51 \u7f51\u7edc\u7ec4\u4ef6caclio \u00b6 \u5b98\u7f51\u5730\u5740: https://projectcalico.docs.tigera.io GitHub \u5730\u5740: https://github.com/projectcalico/calico Calico\u662f\u4e00\u4e2a\u7eaf\u4e09\u5c42\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5bb9\u5668\u63d0\u4f9b\u591anode\u95f4\u7684\u8bbf\u95ee\u901a\u4fe1\uff0ccalico\u5c06\u6bcf\u4e00\u4e2anode\u8282\u70b9\u90fd\u5f53\u505a\u4e3a\u4e00\u4e2a\u8def\u7531\u5668(router), \u5404\u8282\u70b9\u901a\u8fc7BGP(Border Gateway Protocol)\u8fb9\u754c\u7f51\u5173\u534f\u8bae\u5b66\u4e60\u5e76\u5728node\u8282\u70b9\u751f\u6210\u8def\u7531\u89c4\u5219\uff0c\u4ece\u800c\u5c06\u4e0d\u540cnode\u8282\u70b9\u4e0a\u7684pod\u8fde\u63a5\u8d77\u6765\u8fdb\u884c\u901a\u4fe1\u3002 Calico \u4ecb\u7ecd: \u7f51\u7edc\u901a\u8fc7\u7b2c3\u5c42\u8def\u7531\u6280\u672f(\u5982\u9759\u6001\u8def\u7531\u6216BGP\u8def\u7531\u5206\u914d)\u6216\u7b2c2\u5c42\u5730\u5740\u5b66\u4e60\u6765\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u5c06\u672a\u5c01\u88c5\u7684\u6d41\u91cf\u8def\u7531\u5230\u4f5c\u4e3a\u6700\u7ec8\u76ee\u7684\u5730\u7684\u7aef\u70b9\u7684\u6b63\u786e\u4e3b\u673a\u3002\u4f46\u662f\u5e76\u975e\u6240\u6709\u7f51\u7edc\u90fd\u80fd\u591f\u8def\u7531\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002\u4f8b\u5982\u516c\u5171\u4e91\u73af\u5883\u3001\u8de8VPC\u5b50\u7f51\u8fb9\u754c\u7684AWS,\u4ee5\u53ca\u65e0\u6cd5\u901a\u8fc7BGP \u3001Calico\u5bf9\u5e94\u5230under lay\u7f51\u7edc\u6216\u65e0\u6cd5\u8f7b\u677e\u914d\u7f6e\u9759\u6001\u8def\u7531\u7684\u5176\u4ed6\u573a\u666f\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48Calico\u652f\u6301\u5c01\u88c5\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u5728\u5de5\u4f5c\u8d1f\u8f7d\u4e4b\u95f4\u53d1\u9001\u6d41\u91cf\uff0c\u800c\u65e0\u9700\u5e95\u5c42\u7f51\u7edc\u77e5\u9053\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002 calico\u5c01\u88c5\u7c7b\u578b: Calico\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u5c01\u88c5: VXLAN\u548cIP-in-IP, VXLAN\u5728IP\u4e2d\u6ca1\u6709 IP\u7684\u67d0\u4e9b\u73af\u5883\u4e2d\u53d7\u652f\u6301(\u4f8b\u5982Azure)\uff0cVXLAN\u7684\u6bcf \u6570\u636e\u5305\u5f00\u9500\u7a0d\u9ad8\uff0c\u56e0\u4e3a\u62a5\u5934\u8f83\u5927\uff0c\u4f46\u9664\u975e\u60a8\u8fd0\u884c\u7684\u662f\u7f51\u7edc\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5426\u5219\u60a8\u901a\u5e38\u4e0d\u4f1a\u6ce8\u610f\u5230\u8fd9\u79cd\u5dee\u5f02\u3002\u8fd9\u4e24\u79cd\u5c01\u88c5\u4e4b\u95f4\u7684\u53e6\u4e00\u4e2a\u5c0f\u5dee\u5f02\u662fCal ico\u7684VXLAN\u5b9e\u73b0\u4e0d\u4f7f\u7528BGP, Calico\u7684IP-in-IP\u662f\u5728Calico\u8282\u70b9\u4e4b\u95f4\u4f7f\u7528BGP\u534f\u8bae\u5b9e\u73b0\u8de8\u5b50\u7f51\u3002 Calico\u4e24\u79cd\u7f51\u7edc\u6a21\u5f0f Calico\u672c\u8eab\u652f\u6301\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0c\u4eceoverlay\u548cunderlay\u4e0a\u533a\u5206\u3002Calico overlay \u6a21\u5f0f\uff0c\u4e00\u822c\u4e5f\u79f0Calico IPIP\u6216VXLAN\u6a21\u5f0f\uff0c\u4e0d\u540cNode\u95f4Pod\u4f7f\u7528IPIP\u6216VXLAN\u96a7\u9053\u8fdb\u884c\u901a\u4fe1\u3002Calico underlay \u6a21\u5f0f\uff0c\u4e00\u822c\u4e5f\u79f0calico BGP\u6a21\u5f0f\uff0c\u4e0d\u540cNode Pod\u4f7f\u7528\u76f4\u63a5\u8def\u7531\u8fdb\u884c\u901a\u4fe1\u3002\u5728overlay\u548cunderlay\u90fd\u6709nodetonode mesh(\u5168\u7f51\u4e92\u8054)\u548cRoute Reflector(\u8def\u7531\u53cd\u5c04\u5668)\u3002\u5982\u679c\u6709\u5b89\u5168\u7ec4\u7b56\u7565\u9700\u8981\u5f00\u653eIPIP\u534f\u8bae\uff1b\u8981\u6c42Node\u5141\u8bb8BGP\u534f\u8bae\uff0c\u5982\u679c\u6709\u5b89\u5168\u7ec4\u7b56\u7565\u9700\u8981\u5f00\u653eTCP 179\u7aef\u53e3\uff1b\u5b98\u65b9\u63a8\u8350\u4f7f\u7528\u5728Node\u5c0f\u4e8e100\u7684\u96c6\u7fa4\uff0c\u6211\u4eec\u5728\u4f7f\u7528\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u901a\u8fc7IPIP\u6a21\u5f0f\u652f\u6491\u4e86100-200\u89c4\u6a21\u7684\u96c6\u7fa4\u7a33\u5b9a\u8fd0\u884c\u3002 BGP\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u8bae\uff0c\u5b83\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u548c\u7ef4\u62a4\u8def\u7531\u8868\u5b9e\u73b0\u7f51\u7edc\u7684\u53ef\u7528\u6027\uff0c\u4f46\u662f\u5e76\u4e0d\u662f\u6240\u6709\u7684\u7f51\u7edc\u90fd\u652f\u6301BGP,\u53e6\u5916\u4e3a\u4e86\u8de8\u7f51\u7edc \u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u7f51\u7edc\u7ba1\u7406\uff0ccalico \u8fd8\u652f\u6301IP-in-IP\u7684\u53e0\u52a0\u6a21\u578b\uff0c\u7b80\u79f0IPIP, IPIP\u53ef\u4ee5\u5b9e\u73b0\u8de8\u4e0d\u540c\u7f51\u6bb5\u5efa\u7acb \u8def\u7531\u901a\u4fe1\uff0c\u4f46\u662f\u4f1a\u5b58\u5728\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5176\u5728\u5185\u6838\u5185\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7Calico\u7684\u914d\u7f6e\u6587\u4ef6\u8bbe\u7f6e\u662f\u5426\u542f\u7528IPIP\uff0c \u5728\u516c\u53f8\u5185\u90e8\u5982\u679ck8s\u7684node\u8282\u70b9\u6ca1\u6709\u8de8\u8d8a\u7f51\u6bb5\u5efa\u8bae\u5173\u95edIPIP\u3002 IPIP\u662f\u4e00-\u79cd\u5c06\u5404Node\u7684\u8def\u7531\u4e4b\u95f4\u505a\u4e00\u4e2atunnel, \u518d\u628a\u4e24\u4e2a\u7f51\u7edc\u8fde\u63a5\u8d77\u6765\u7684\u6a21\u5f0f\u3002\u542f\u7528IPIP\u6a21\u5f0f\u65f6\uff0cCalico\u5c06\u5728\u5404Node\u4e0a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\" tun10\"\u7684\u865a\u62df\u7f51\u7edc\u63a5\u53e3\u3002 BGP\u6a21\u5f0f\u5219\u76f4\u63a5\u4f7f\u7528\u7269\u7406\u673a\u4f5c\u4e3a\u865a\u62df\u8def\u7531\u8def(vRouter) \uff0c\u4e0d\u518d\u521b\u5efa\u989d\u5916\u7684tunnel. Calio \u67b6\u6784: Calio \u7ec4\u4ef6: Felix: calico\u7684agent, \u8fd0\u884c\u5728\u6bcf\u4e00\u53f0node\u8282\u70b9\u4e0a\uff0c \u5176\u4e3b\u8981\u662f\u7ef4\u62a4\u8def\u7531\u89c4\u5219\u3001\u6c47\u62a5\u5f53\u524d\u8282\u70b9\u72b6\u6001\u4ee5\u786e\u4fddpod\u7684\u5938\u4e3b\u673a\u901a\u4fe1\u3002 BGP Client: \u6bcf\u53f0node\u90fd\u8fd0\u884c\uff0c\u5176\u4e3b\u8981\u8d1f\u8d23\u76d1\u542cnode\u8282\u70b9\u4e0a\u7531felix\u751f\u6210\u7684\u8def\u7531\u4fe1\u606f\uff0c\u7136\u540e\u901a\u8fc7BGP\u534f\u8bae\u5e7f\u64ad\u81f3\u5176\u4ed6\u5269\u4f59\u7684node\u8282\u70b9\uff0c\u4ece\u800c\u76f8\u4e92\u5b66\u4e60\u8def\u7531\u5b9e\u73b0pod\u901a\u4fe1\u3002 Route Reflector:\u96c6\u4e2d\u5f0f\u7684\u8def\u7531\u53cd\u5c04\u5668\uff0ccalico v3.3\u5f00\u59cb\u652f\u6301\uff0c\u5f53Calico BGP\u5ba2\u6237\u7aef\u5c06\u8def\u7531\u4ece\u5176FIB(Forward InformationdataBase,\u8f6c\u53d1\u4fe1\u606f\u5e93)\u901a\u544a\u5230Route Reflector\u65f6\uff0cRoute Reflector \u4f1a\u5c06\u8fd9\u4e9b\u8def\u7531\u901a\u544a\u7ed9\u90e8\u7f72\u96c6\u7fa4\u4e2d\u7684\u5176\u4ed6\u8282\u70b9\uff0cRoute Reflector\u4e13\u95e8\u7528\u4e8e\u7ba1\u7406BGP\u7f51\u7edc\u8def\u7531\u89c4\u5219\uff0c\u4e0d\u4f1a\u4ea7\u751fpod\u6570\u636e\u901a\u4fe1\u3002 \u6ce8: calico\u9ed8\u8ba4\u5de5\u4f5c\u6a21\u5f0f\u662fBGP\u7684node- to-node mesh,\u5982\u679c\u8981\u4f7f\u7528Route Reflector \u9700\u8981\u8fdb\u884c\u76f8\u5173\u914d\u7f6e\u3002 https://docs.projectcalico.org/v3.4/usage/routereflector https://docs.projectcalico.org/v3.2/usage/routereflector/calico-routereflector https://blog.kelu.org/category/tech.html https://blog.kelu.org/tech/2020/01/11/calico-series-3-calico-components-and-arch.html \u5f00\u542f ip-ip \u6a21\u5f0f,\u53ef\u4ee5\u770b\u5230\u540e\u9762\u6709\u4e2a\u96a7\u9053tunl0 (\u517c\u5bb9\u6027\u597d,\u53ef\u4ee5\u8de8\u5b50\u7f51) \u603b\u7ed3: \u539f\u5219\u4f7f\u7528: \u81ea\u5efa\u673a\u623f\u4f18\u5148\u4f7f\u7528 caclio caclio \u652f\u6301\u7f51\u7edc\u7b56\u7565 \u6027\u80fd\u9ad8\u4e8e flannel \u4f7f\u7528\u542f\u7528overlay \u8003\u8651\u540e\u671f\u7684\u7f51\u7edc\u6269\u5c55,node \u8282\u70b9\u662f\u5426\u5b58\u5728\u8de8\u5b50\u7f51\u7684\u95ee\u9898 \u4e0d\u542f\u7528 overlay flannel host-gw caclio bgp (\u76f4\u63a5\u8def\u7531) \u542f\u7528 overlay caclio IPIP flannel vxlan CrossSubnet: \u8868\u793a\uff08ipip-bgp\u6df7\u5408\u6a21\u5f0f\uff09\uff0c\u6307\u201c\u540c\u5b50\u7f51\u5185\u8def\u7531\u91c7\u7528bgp\uff0c\u8de8\u5b50\u7f51\u8def\u7531\u91c7\u7528ipip \u5728\u4f7f\u7528\u573a\u666f\u4e0a\u5f53\u4e3b\u673a\u8282\u70b9\u5904\u4e8e\u4e0d\u540c\u7f51\u7edc\u5206\u6bb5\uff0c\u9700\u8981\u8de8\u7f51\u6bb5\u901a\u4fe1\u65f6\uff0cBGP\u6a21\u5f0f\u5c06\u4f1a\u5931\u6548\uff0c\u6b64\u65f6\u8981\u4f7f\u7528IPIP\u6a21\u5f0f\u3002 Calico\u7684BGP Mesh\u6a21\u5f0f\u9002\u5408\u5728\u5c0f\u89c4\u6a21\u96c6\u7fa4(\u8282\u70b9\u6570\u91cf\u5c0f\u4e8e100\u4e2a)\u4e2d\u76f4\u63a5\u4e92\u8054\uff0c\u7531\u4e8e\u968f\u7740\u96c6\u7fa4\u8282\u70b9\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8def\u7531\u89c4\u5219\u5c06\u6210\u6307\u6570\u7ea7\u589e\u957f\u4f1a\u7ed9\u96c6\u7fa4\u7f51\u7edc\u5e26\u6765\u5f88\u5927\u538b\u529b\u3002\u5927\u89c4\u6a21\u96c6\u7fa4\u9700\u8981\u4f7f\u7528BGP Route Reflector GitHub \u5730\u5740: https://github.com/projectcalico/calico/blob/79b442a53adb7d7f1fd62927d9322daf87dce9de/calico/reference/public-cloud/aws.md \u4e0d\u540c\u7ec4\u4ef6\u538b\u6d4b\u8c03\u7814 \u00b6 \u8c03\u7814\u4e0d\u540c\u7f51\u7edc\u7ec4\u4ef6\u5bf9\u7f51\u7edc\u7684\u5f71\u54cd:(\u540c\u7b49\u914d\u7f6e\u4e0b) docker-compose \u5c31\u4e0d\u9700\u8981\u505a overlay \u7684\u5c01\u88c5\u548c\u89e3\u5c01\u88c5,\u6027\u80fd\u662f\u6700\u597d\u7684. \u538b\u6d4b\u8c03\u7814: flannel \u7f51\u7edc\u7ec4\u4ef6 \u538b\u6d4b\u8c03\u7814: DirectRouting\u6a21\u5f0f: \u5347\u7ea7\u7cfb\u7edf calico \u7ec4\u4ef6: \u5173\u95ed IPIP \u4f7f\u7528 BGP \u4f46\u662f\u8fd9\u4e2a\u7ed3\u679c\u8fd8\u662f\u8fbe\u4e0d\u5230\u9884\u671f\u7684\u6548\u679c,\u6700\u540e\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5355\u673a\u591a\u526f\u672c \u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5bf9\u6bd4 \u00b6 \u81ea\u5df1\u538b\u6d4b: caclio \u7684IPIP\u6a21\u5f0f: \u6587\u7ae0\u53c2\u8003: https://system51.github.io/2020/05/27/using-calico/ https://projectcalico.docs.tigera.io/ https://blog.frognew.com/2021/07/relearning-container-22.html#%E5%8F%82%E8%80%83 https://kiddie92.github.io/2019/01/23/kubernetes-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B/ https://gist.github.com/baymaxium/7797f226fe03d38461f33fdd02145b11","title":"kubernetes \u7f51\u7edc\u7ec4\u4ef6"},{"location":"network/3-network-docs/#flannel","text":"","title":"flannel\u4ecb\u7ecd:"},{"location":"network/3-network-docs/#_1","text":"github \u5730\u5740: https://github.com/flannel-io/flannel/blob/master/Documentation/backends.md flannel \u7684\u7f51\u7edc\u6a21\u578b\uff08\u540e\u7aef\uff09\uff1a\u76ee\u524d\u6709\u4e09\u79cd\u65b9\u5f0f\u5b9e\u73b0UDP/VXLAN/host-gw UDP : \u5728\u6700\u65e9\u671fflannel\u4f7f\u7528UDP\u5c01\u88c5\u6765\u5b8c\u6210\u62a5\u6587\u7684\u8de8\u4e3b\u673a\u8f6c\u53d1,\u5b89\u5168\u6027\u548c\u6027\u80fd\u7565\u6709\u4e0d\u8db3(\u5df2\u5e9f\u5f03) VXLAN: Linux\u5185\u6838\u5728\u57282012\u5e74\u5e95\u7684v3.7.0\u4e4b\u540e\u52a0\u5165\u4e86VXLAN\u534f\u8bae\u652f\u6301\uff0c\u56e0\u6b64\u65b0\u7248\u672c\u7684Flanne1\u4e5f\u6709UDP\u8f6c\u6362\u4e3aVXLAN, VXLAN\u672c\u8d28\u4e0a\u662f\u4e00\u79cd tunnel (\u96a7\u9053)\u534f\u8bae\uff0c\u7528\u6765\u57fa\u4e8e3\u5c42\u7f51\u7edc\u5b9e\u73b0\u865a\u62df\u76842\u5c42\u7f51\u7edc\uff0c\u76ee\u524dflannel\u7684\u7f51\u7edc\u6a21\u578b\u5df2\u7ecf\u662f\u57fa\u4e8eVXLAN\u7684\u53e0\u52a0(\u8986\u76d6)\u7f51\u7edc\uff0c\u76ee\u524d\u63a8\u8350\u4f7f\u7528 vxlan\u4f5c\u4e3a\u5176\u7f51\u7edc\u6a21\u578b\u3002(\u4f7f\u7528\u6700\u591a) Host-gw:\u4e5f\u5c31\u662fHost GateWay, \u901a\u8fc7\u5728node\u8282\u70b9\u4e0a\u521b\u5efa\u5230\u8fbe\u5404\u76ee\u6807\u5bb9\u5668\u5730\u5740\u7684\u8def\u7531\u8868\u800c\u5b8c\u6210\u62a5\u6587\u7684\u8f6c\u53d1\uff0c\u56e0\u6b64\u8fd9\u79cd\u65b9\u5f0f\u8981\u6c42\u5404node\u8282\u70b9\u672c\u8eab \u5fc5\u987b\u5904\u4e8e\u540c\u4e00\u4e2a\u5c40\u57df\u7f51(\u4e8c\u5c42\u7f51\u7edc)\u4e2d\uff0c\u56e0\u6b64\u4e0d\u9002\u7528\u4e8e\u7f51\u7edc\u53d8\u52a8\u9891\u7e41\u6216\u6bd4\u8f83\u5927\u578b\u7684\u7f51\u7edc\u73af\u5883\uff0c\u4f46\u662f\u5176\u6027\u80fd\u8f83\u597d\u3002","title":"\u7f51\u7edc\u6a21\u5f0f"},{"location":"network/3-network-docs/#flannel_1","text":"Cni0:\u7f51\u6865\u8bbe\u5907\uff0c\u6bcf\u521b\u5efa\u4e00\u4e2apod\u90fd\u4f1a\u521b\u5efa\u5bf9veth pair, \u5176\u4e2d\u4e00\u7aef\u662fpod\u4e2d\u7684eth0,\u53e6\u4e00\u7aef\u662fCni0\u7f51\u6865\u4e2d\u7684\u7aef\u53e3(\u7f51\u5361) \uff0cPod\u4e2d\u4ece\u7f51\u5361 eth0\u53d1\u51fa\u7684\u6d41\u91cf\u90fd\u4f1a\u53d1\u9001\u5230Cni0\u7f51\u6865\u8bbe\u5907\u7684\u7aef\u53e3(\u7f51\u5361)\u4e0a\uff0cCni0\u8bbe\u5907\u83b7\u5f97\u7684ip\u5730\u5740\u662f\u8be5\u8282\u70b9\u5206\u914d\u5230\u7684\u7f51\u6bb5\u7684\u7b2c\u4e00\u4e2a\u5730\u5740\u3002 \u539f\u751f\u6a21\u5f0f: Flannel.1: overlay\u7f51\u7edc\u7684\u8bbe\u5907\uff0c\u7528\u6765\u8fdb\u884cvxlan\u62a5\u6587\u7684\u5904\u7406(\u5c01\u5305\u548c\u89e3\u5305)\uff0c \u4e0d\u540cnode\u4e4b\u95f4\u7684pod\u6570\u636e\u6d41\u91cf\u90fd\u4eceoverlay\u8bbe\u5907\u4ee5\u96a7\u9053\u7684\u5f62\u5f0f \u53d1\u9001\u5230\u5bf9\u7aef\u3002","title":"Flannel\u7ec4\u4ef6\u7684\u8bf4\u660e:"},{"location":"network/3-network-docs/#flannel_2","text":"root@node2:~# find / -name flannel","title":"flannel \u7684\u7cfb\u7edf\u6587\u4ef6\u53ca\u76ee\u5f55:"},{"location":"network/3-network-docs/#vxlan","text":"k3s \u9ed8\u8ba4\u7684 flannel \u4e5f\u9ed8\u8ba4vxlan,\u4f46\u662f\u4ecev1.26 \u5f00\u59cb\u79fb\u9664\u4e86 wireguard \u540e\u7aef\uff0c\u4ece\u800c\u652f\u6301 Flannel \u539f\u751f\u7684 wireguard-native \u540e\u7aef https://docs.k3s.io/installation/network-options sshd\u4e3b\u673a\u4e0a\u7684\u5bb9\u5668\u8bbf\u95ee\u522b\u7684\u5bb9\u5668,\u9996\u5148\u901a\u8fc7\u7f51\u6865\u5224\u65ad\u76ee\u7684\u5730\u5740\u662f\u8bbf\u95ee\u5f53\u524d\u7684\u5730\u5740\u8fd8\u662f\u5176\u4ed6\u4e3b\u673a,\u5982\u679c\u5f53\u524d\u4e3b\u673a\u7684\u5bb9\u5668\u4e0d\u8d70flannel.1\u5c31\u76f4\u63a5\u8fd4\u56de\u4e86,\u5982\u679c\u662f\u8bbf\u95ee\u5176\u4ed6 \u6709\u4e24\u79cd\u573a\u666f; \u573a\u666f\u4e00: \u8bbf\u95ee\u522b\u7684 node \u4e0a\u7684\u5bb9\u5668,\u8fd9\u65f6\u4ed6\u7684\u4e3b\u673a\u4e0a\u5c31\u4f1a\u6709\u5bf9\u5e94\u8def\u7531\u8868,\u4f8b\u5982\u4e0a\u56fe\u4e2d,\u53ef\u80fd\u4ed6\u8981\u8bbf\u95ee\u7684\u5413\u4e00\u8df3\u5c31\u662f172.16.78.0/21,\u90a3\u4e48\u5c31\u4f1a\u8d70flannel.1\u6765\u505a\u62a5\u6587\u7684\u5c01\u88c5;\u8fd9\u65f6\u5c31\u8d70\u96a7\u9053\u4e86,\u4e0d\u4f1a\u5728\u7269\u7406\u7f51\u5361\u6765\u505a\u62a5\u6587\u7684\u5c01\u88c5. \u573a\u666f\u4e8c: \u5982\u679c\u76ee\u7684\u5730\u5740\u4e0d\u662f k8s \u7684\u73af\u5883,\u800c\u662f\u5916\u7f51,\u90a3\u4e48\u5c31\u4f1a\u5339\u914d\u8def\u7531\u7684\u65f6\u5019\u5c31\u4e0d\u4f1a\u8d70\u96a7\u9053\u8bbe\u5907,\u5728\u5185\u6838\u505a\u7269\u7406\u7f51\u5361\u7684\u5c01\u88c5,\u901a\u8fc7\u5bbf\u4e3b\u673a\u51fa\u53bb. \u4e0d\u540c\u7684\u4e3b\u673a\u7684\u5bb9\u5668\u901a\u4fe1\u9700\u8981\u901a\u8fc7flannel.1 \u8fdb\u884c\u5c01\u88c5\u548c\u89e3\u5c01 \u597d\u5904: \u53ef\u4ee5\u8de8\u5b50\u7f51\u901a\u4fe1 \u7f3a\u70b9: \u6027\u80fd\u5dee\u70b9","title":"VXLAN \u539f\u751f\u6a21\u5f0f:"},{"location":"network/3-network-docs/#directrouting","text":"VXLAN DirectRoutinghost-gw\uff08\u5e03\u5c14\u503c\uff09\uff1a\u5f53\u4e3b\u673a\u4f4d\u4e8e\u540c\u4e00\u5b50\u7f51\u4e0a\u65f6\u542f\u7528\u76f4\u63a5\u8def\u7531\uff08\u5982\uff09\u3002VXLAN \u5c06\u4ec5\u7528\u4e8e\u5c01\u88c5\u6570\u636e\u5305\u5230\u4e0d\u540c\u5b50\u7f51\u4e0a\u7684\u4e3b\u673a\u3002\u9ed8\u8ba4\u4e3afalse. Windows \u4e0d\u652f\u6301 DirectRouting \u5f00\u542f\u4e86\u8fd9\u4e2a\u6a21\u5f0f,\u90a3\u7f51\u7edc\u5c31\u4f1a\u63d0\u5347,\u539f\u56e0\u5c31\u662f\u5728\u540c\u7f51\u6bb5\u4e0d\u8fdb\u884c\u5c01\u88c5,\u53ea\u6709\u8de8\u5b50\u7f51\u624d\u8fdb\u884c\u5c01\u88c5. \u76d1\u542c\u7aef\u53e3: [ openbayes-enflame ] root@node1:~# ss -auntp | grep 8472 udp UNCONN 0 0 0 .0.0.0:8472 0 .0.0.0:*","title":"DirectRouting\u6a21\u5f0f:"},{"location":"network/3-network-docs/#host-gw","text":"\u548cDirectRouting\u7c7b\u4f3c,\u4f46\u662f\u4e0d\u80fd\u8de8\u5b50\u7f51 Use host-gw to create IP routes to subnets via remote machine IPs. Requires direct layer2 connectivity between hosts running flannel. host-gw provides good performance, with few dependencies, and easy set up. flannel \u603b\u7ed3: \u63a8\u8350\u4f7f\u7528VXLAN DirectRouting \u76f8\u540c\u7684 node \u5b50\u7f51\u901a\u4fe1,\u4e0d\u9700\u8981overlay\u5c01\u88c5\u548c\u89e3\u5c01\u88c5,\u800c\u4e14\u8fd8\u53ef\u4ee5\u652f\u6301 node \u8de8\u5b50\u7f51","title":"host-gw \u6a21\u5f0f\uff1a"},{"location":"network/3-network-docs/#caclio","text":"\u5b98\u7f51\u5730\u5740: https://projectcalico.docs.tigera.io GitHub \u5730\u5740: https://github.com/projectcalico/calico Calico\u662f\u4e00\u4e2a\u7eaf\u4e09\u5c42\u7684\u7f51\u7edc\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5bb9\u5668\u63d0\u4f9b\u591anode\u95f4\u7684\u8bbf\u95ee\u901a\u4fe1\uff0ccalico\u5c06\u6bcf\u4e00\u4e2anode\u8282\u70b9\u90fd\u5f53\u505a\u4e3a\u4e00\u4e2a\u8def\u7531\u5668(router), \u5404\u8282\u70b9\u901a\u8fc7BGP(Border Gateway Protocol)\u8fb9\u754c\u7f51\u5173\u534f\u8bae\u5b66\u4e60\u5e76\u5728node\u8282\u70b9\u751f\u6210\u8def\u7531\u89c4\u5219\uff0c\u4ece\u800c\u5c06\u4e0d\u540cnode\u8282\u70b9\u4e0a\u7684pod\u8fde\u63a5\u8d77\u6765\u8fdb\u884c\u901a\u4fe1\u3002 Calico \u4ecb\u7ecd: \u7f51\u7edc\u901a\u8fc7\u7b2c3\u5c42\u8def\u7531\u6280\u672f(\u5982\u9759\u6001\u8def\u7531\u6216BGP\u8def\u7531\u5206\u914d)\u6216\u7b2c2\u5c42\u5730\u5740\u5b66\u4e60\u6765\u611f\u77e5\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002\u56e0\u6b64\u5b83\u4eec\u53ef\u4ee5\u5c06\u672a\u5c01\u88c5\u7684\u6d41\u91cf\u8def\u7531\u5230\u4f5c\u4e3a\u6700\u7ec8\u76ee\u7684\u5730\u7684\u7aef\u70b9\u7684\u6b63\u786e\u4e3b\u673a\u3002\u4f46\u662f\u5e76\u975e\u6240\u6709\u7f51\u7edc\u90fd\u80fd\u591f\u8def\u7531\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002\u4f8b\u5982\u516c\u5171\u4e91\u73af\u5883\u3001\u8de8VPC\u5b50\u7f51\u8fb9\u754c\u7684AWS,\u4ee5\u53ca\u65e0\u6cd5\u901a\u8fc7BGP \u3001Calico\u5bf9\u5e94\u5230under lay\u7f51\u7edc\u6216\u65e0\u6cd5\u8f7b\u677e\u914d\u7f6e\u9759\u6001\u8def\u7531\u7684\u5176\u4ed6\u573a\u666f\uff0c\u8fd9\u5c31\u662f\u4e3a\u4ec0\u4e48Calico\u652f\u6301\u5c01\u88c5\uff0c\u56e0\u6b64\u60a8\u53ef\u4ee5\u5728\u5de5\u4f5c\u8d1f\u8f7d\u4e4b\u95f4\u53d1\u9001\u6d41\u91cf\uff0c\u800c\u65e0\u9700\u5e95\u5c42\u7f51\u7edc\u77e5\u9053\u5de5\u4f5c\u8d1f\u8f7dIP\u5730\u5740\u3002 calico\u5c01\u88c5\u7c7b\u578b: Calico\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u5c01\u88c5: VXLAN\u548cIP-in-IP, VXLAN\u5728IP\u4e2d\u6ca1\u6709 IP\u7684\u67d0\u4e9b\u73af\u5883\u4e2d\u53d7\u652f\u6301(\u4f8b\u5982Azure)\uff0cVXLAN\u7684\u6bcf \u6570\u636e\u5305\u5f00\u9500\u7a0d\u9ad8\uff0c\u56e0\u4e3a\u62a5\u5934\u8f83\u5927\uff0c\u4f46\u9664\u975e\u60a8\u8fd0\u884c\u7684\u662f\u7f51\u7edc\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u5426\u5219\u60a8\u901a\u5e38\u4e0d\u4f1a\u6ce8\u610f\u5230\u8fd9\u79cd\u5dee\u5f02\u3002\u8fd9\u4e24\u79cd\u5c01\u88c5\u4e4b\u95f4\u7684\u53e6\u4e00\u4e2a\u5c0f\u5dee\u5f02\u662fCal ico\u7684VXLAN\u5b9e\u73b0\u4e0d\u4f7f\u7528BGP, Calico\u7684IP-in-IP\u662f\u5728Calico\u8282\u70b9\u4e4b\u95f4\u4f7f\u7528BGP\u534f\u8bae\u5b9e\u73b0\u8de8\u5b50\u7f51\u3002 Calico\u4e24\u79cd\u7f51\u7edc\u6a21\u5f0f Calico\u672c\u8eab\u652f\u6301\u591a\u79cd\u7f51\u7edc\u6a21\u5f0f\uff0c\u4eceoverlay\u548cunderlay\u4e0a\u533a\u5206\u3002Calico overlay \u6a21\u5f0f\uff0c\u4e00\u822c\u4e5f\u79f0Calico IPIP\u6216VXLAN\u6a21\u5f0f\uff0c\u4e0d\u540cNode\u95f4Pod\u4f7f\u7528IPIP\u6216VXLAN\u96a7\u9053\u8fdb\u884c\u901a\u4fe1\u3002Calico underlay \u6a21\u5f0f\uff0c\u4e00\u822c\u4e5f\u79f0calico BGP\u6a21\u5f0f\uff0c\u4e0d\u540cNode Pod\u4f7f\u7528\u76f4\u63a5\u8def\u7531\u8fdb\u884c\u901a\u4fe1\u3002\u5728overlay\u548cunderlay\u90fd\u6709nodetonode mesh(\u5168\u7f51\u4e92\u8054)\u548cRoute Reflector(\u8def\u7531\u53cd\u5c04\u5668)\u3002\u5982\u679c\u6709\u5b89\u5168\u7ec4\u7b56\u7565\u9700\u8981\u5f00\u653eIPIP\u534f\u8bae\uff1b\u8981\u6c42Node\u5141\u8bb8BGP\u534f\u8bae\uff0c\u5982\u679c\u6709\u5b89\u5168\u7ec4\u7b56\u7565\u9700\u8981\u5f00\u653eTCP 179\u7aef\u53e3\uff1b\u5b98\u65b9\u63a8\u8350\u4f7f\u7528\u5728Node\u5c0f\u4e8e100\u7684\u96c6\u7fa4\uff0c\u6211\u4eec\u5728\u4f7f\u7528\u7684\u8fc7\u7a0b\u4e2d\u5df2\u7ecf\u901a\u8fc7IPIP\u6a21\u5f0f\u652f\u6491\u4e86100-200\u89c4\u6a21\u7684\u96c6\u7fa4\u7a33\u5b9a\u8fd0\u884c\u3002 BGP\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u534f\u8bae\uff0c\u5b83\u901a\u8fc7\u81ea\u52a8\u5b66\u4e60\u548c\u7ef4\u62a4\u8def\u7531\u8868\u5b9e\u73b0\u7f51\u7edc\u7684\u53ef\u7528\u6027\uff0c\u4f46\u662f\u5e76\u4e0d\u662f\u6240\u6709\u7684\u7f51\u7edc\u90fd\u652f\u6301BGP,\u53e6\u5916\u4e3a\u4e86\u8de8\u7f51\u7edc \u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u7f51\u7edc\u7ba1\u7406\uff0ccalico \u8fd8\u652f\u6301IP-in-IP\u7684\u53e0\u52a0\u6a21\u578b\uff0c\u7b80\u79f0IPIP, IPIP\u53ef\u4ee5\u5b9e\u73b0\u8de8\u4e0d\u540c\u7f51\u6bb5\u5efa\u7acb \u8def\u7531\u901a\u4fe1\uff0c\u4f46\u662f\u4f1a\u5b58\u5728\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5176\u5728\u5185\u6838\u5185\u7f6e\uff0c\u53ef\u4ee5\u901a\u8fc7Calico\u7684\u914d\u7f6e\u6587\u4ef6\u8bbe\u7f6e\u662f\u5426\u542f\u7528IPIP\uff0c \u5728\u516c\u53f8\u5185\u90e8\u5982\u679ck8s\u7684node\u8282\u70b9\u6ca1\u6709\u8de8\u8d8a\u7f51\u6bb5\u5efa\u8bae\u5173\u95edIPIP\u3002 IPIP\u662f\u4e00-\u79cd\u5c06\u5404Node\u7684\u8def\u7531\u4e4b\u95f4\u505a\u4e00\u4e2atunnel, \u518d\u628a\u4e24\u4e2a\u7f51\u7edc\u8fde\u63a5\u8d77\u6765\u7684\u6a21\u5f0f\u3002\u542f\u7528IPIP\u6a21\u5f0f\u65f6\uff0cCalico\u5c06\u5728\u5404Node\u4e0a\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a\" tun10\"\u7684\u865a\u62df\u7f51\u7edc\u63a5\u53e3\u3002 BGP\u6a21\u5f0f\u5219\u76f4\u63a5\u4f7f\u7528\u7269\u7406\u673a\u4f5c\u4e3a\u865a\u62df\u8def\u7531\u8def(vRouter) \uff0c\u4e0d\u518d\u521b\u5efa\u989d\u5916\u7684tunnel. Calio \u67b6\u6784: Calio \u7ec4\u4ef6: Felix: calico\u7684agent, \u8fd0\u884c\u5728\u6bcf\u4e00\u53f0node\u8282\u70b9\u4e0a\uff0c \u5176\u4e3b\u8981\u662f\u7ef4\u62a4\u8def\u7531\u89c4\u5219\u3001\u6c47\u62a5\u5f53\u524d\u8282\u70b9\u72b6\u6001\u4ee5\u786e\u4fddpod\u7684\u5938\u4e3b\u673a\u901a\u4fe1\u3002 BGP Client: \u6bcf\u53f0node\u90fd\u8fd0\u884c\uff0c\u5176\u4e3b\u8981\u8d1f\u8d23\u76d1\u542cnode\u8282\u70b9\u4e0a\u7531felix\u751f\u6210\u7684\u8def\u7531\u4fe1\u606f\uff0c\u7136\u540e\u901a\u8fc7BGP\u534f\u8bae\u5e7f\u64ad\u81f3\u5176\u4ed6\u5269\u4f59\u7684node\u8282\u70b9\uff0c\u4ece\u800c\u76f8\u4e92\u5b66\u4e60\u8def\u7531\u5b9e\u73b0pod\u901a\u4fe1\u3002 Route Reflector:\u96c6\u4e2d\u5f0f\u7684\u8def\u7531\u53cd\u5c04\u5668\uff0ccalico v3.3\u5f00\u59cb\u652f\u6301\uff0c\u5f53Calico BGP\u5ba2\u6237\u7aef\u5c06\u8def\u7531\u4ece\u5176FIB(Forward InformationdataBase,\u8f6c\u53d1\u4fe1\u606f\u5e93)\u901a\u544a\u5230Route Reflector\u65f6\uff0cRoute Reflector \u4f1a\u5c06\u8fd9\u4e9b\u8def\u7531\u901a\u544a\u7ed9\u90e8\u7f72\u96c6\u7fa4\u4e2d\u7684\u5176\u4ed6\u8282\u70b9\uff0cRoute Reflector\u4e13\u95e8\u7528\u4e8e\u7ba1\u7406BGP\u7f51\u7edc\u8def\u7531\u89c4\u5219\uff0c\u4e0d\u4f1a\u4ea7\u751fpod\u6570\u636e\u901a\u4fe1\u3002 \u6ce8: calico\u9ed8\u8ba4\u5de5\u4f5c\u6a21\u5f0f\u662fBGP\u7684node- to-node mesh,\u5982\u679c\u8981\u4f7f\u7528Route Reflector \u9700\u8981\u8fdb\u884c\u76f8\u5173\u914d\u7f6e\u3002 https://docs.projectcalico.org/v3.4/usage/routereflector https://docs.projectcalico.org/v3.2/usage/routereflector/calico-routereflector https://blog.kelu.org/category/tech.html https://blog.kelu.org/tech/2020/01/11/calico-series-3-calico-components-and-arch.html \u5f00\u542f ip-ip \u6a21\u5f0f,\u53ef\u4ee5\u770b\u5230\u540e\u9762\u6709\u4e2a\u96a7\u9053tunl0 (\u517c\u5bb9\u6027\u597d,\u53ef\u4ee5\u8de8\u5b50\u7f51) \u603b\u7ed3: \u539f\u5219\u4f7f\u7528: \u81ea\u5efa\u673a\u623f\u4f18\u5148\u4f7f\u7528 caclio caclio \u652f\u6301\u7f51\u7edc\u7b56\u7565 \u6027\u80fd\u9ad8\u4e8e flannel \u4f7f\u7528\u542f\u7528overlay \u8003\u8651\u540e\u671f\u7684\u7f51\u7edc\u6269\u5c55,node \u8282\u70b9\u662f\u5426\u5b58\u5728\u8de8\u5b50\u7f51\u7684\u95ee\u9898 \u4e0d\u542f\u7528 overlay flannel host-gw caclio bgp (\u76f4\u63a5\u8def\u7531) \u542f\u7528 overlay caclio IPIP flannel vxlan CrossSubnet: \u8868\u793a\uff08ipip-bgp\u6df7\u5408\u6a21\u5f0f\uff09\uff0c\u6307\u201c\u540c\u5b50\u7f51\u5185\u8def\u7531\u91c7\u7528bgp\uff0c\u8de8\u5b50\u7f51\u8def\u7531\u91c7\u7528ipip \u5728\u4f7f\u7528\u573a\u666f\u4e0a\u5f53\u4e3b\u673a\u8282\u70b9\u5904\u4e8e\u4e0d\u540c\u7f51\u7edc\u5206\u6bb5\uff0c\u9700\u8981\u8de8\u7f51\u6bb5\u901a\u4fe1\u65f6\uff0cBGP\u6a21\u5f0f\u5c06\u4f1a\u5931\u6548\uff0c\u6b64\u65f6\u8981\u4f7f\u7528IPIP\u6a21\u5f0f\u3002 Calico\u7684BGP Mesh\u6a21\u5f0f\u9002\u5408\u5728\u5c0f\u89c4\u6a21\u96c6\u7fa4(\u8282\u70b9\u6570\u91cf\u5c0f\u4e8e100\u4e2a)\u4e2d\u76f4\u63a5\u4e92\u8054\uff0c\u7531\u4e8e\u968f\u7740\u96c6\u7fa4\u8282\u70b9\u6570\u91cf\u7684\u589e\u52a0\uff0c\u8def\u7531\u89c4\u5219\u5c06\u6210\u6307\u6570\u7ea7\u589e\u957f\u4f1a\u7ed9\u96c6\u7fa4\u7f51\u7edc\u5e26\u6765\u5f88\u5927\u538b\u529b\u3002\u5927\u89c4\u6a21\u96c6\u7fa4\u9700\u8981\u4f7f\u7528BGP Route Reflector GitHub \u5730\u5740: https://github.com/projectcalico/calico/blob/79b442a53adb7d7f1fd62927d9322daf87dce9de/calico/reference/public-cloud/aws.md","title":"\u7f51\u7edc\u7ec4\u4ef6caclio"},{"location":"network/3-network-docs/#_2","text":"\u8c03\u7814\u4e0d\u540c\u7f51\u7edc\u7ec4\u4ef6\u5bf9\u7f51\u7edc\u7684\u5f71\u54cd:(\u540c\u7b49\u914d\u7f6e\u4e0b) docker-compose \u5c31\u4e0d\u9700\u8981\u505a overlay \u7684\u5c01\u88c5\u548c\u89e3\u5c01\u88c5,\u6027\u80fd\u662f\u6700\u597d\u7684. \u538b\u6d4b\u8c03\u7814: flannel \u7f51\u7edc\u7ec4\u4ef6 \u538b\u6d4b\u8c03\u7814: DirectRouting\u6a21\u5f0f: \u5347\u7ea7\u7cfb\u7edf calico \u7ec4\u4ef6: \u5173\u95ed IPIP \u4f7f\u7528 BGP \u4f46\u662f\u8fd9\u4e2a\u7ed3\u679c\u8fd8\u662f\u8fbe\u4e0d\u5230\u9884\u671f\u7684\u6548\u679c,\u6700\u540e\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5355\u673a\u591a\u526f\u672c","title":"\u4e0d\u540c\u7ec4\u4ef6\u538b\u6d4b\u8c03\u7814"},{"location":"network/3-network-docs/#_3","text":"\u81ea\u5df1\u538b\u6d4b: caclio \u7684IPIP\u6a21\u5f0f: \u6587\u7ae0\u53c2\u8003: https://system51.github.io/2020/05/27/using-calico/ https://projectcalico.docs.tigera.io/ https://blog.frognew.com/2021/07/relearning-container-22.html#%E5%8F%82%E8%80%83 https://kiddie92.github.io/2019/01/23/kubernetes-%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E6%96%B9%E6%B3%95%E7%AE%80%E4%BB%8B/ https://gist.github.com/baymaxium/7797f226fe03d38461f33fdd02145b11","title":"\u7f51\u7edc\u6027\u80fd\u6d4b\u8bd5\u5bf9\u6bd4"},{"location":"network/4-network-docs/","text":"\u79d1\u5b66\u4e0a\u7f51 \u6211\u662f\u4e00\u540d\u6280\u672f\u4eba\u5458\uff0c\u5e73\u65f6\u6bcf\u5929\u9700\u8981\u5927\u91cf\u7684Google\u53bb\u770b\u4e00\u4e9b\u6587\u7ae0\uff0c\u5f53\u7136\u4e5f\u56de\u53bb\u6cb9\u7ba1\u901b\u901b\uff0c\u90a3\u4e48\u56fd\u5185\u4e0a\u7f51\u7684\u73af\u5883\u5e76\u4e0d\u662f\u5f88\u7406\u60f3\uff0c\u800c\u4e14\u8d44\u6e90\u4e5f\u6709\u9650\uff0c \u6240\u4ee5\u9700\u8981\u79d1\u5b66\u4e0a\u7f51\uff0c\u90a3\u4e48\u6211\u63a8\u8350\u4e00\u4e2a\u6bd4\u8f83\u597d\u7528\u7684\u673a\u573a\u3002 WgetCloud \uff08\u539fGaCloud \uff09\u662f\u4e00\u5bb6\u7cbe\u54c1\u673a\u573a\uff0c\u7ebf\u8def\u6570\u91cf\u4e0d\u7b97\u591a\uff0c\u4f46\u90fd\u662f\u5341\u5206\u4f18\u8d28\u7684\u4f4e\u5ef6\u8fdfBGP\u4e2d\u8f6c\u7ebf\u8def\u3002 WgetCloud \u540c\u65f6\u6709\u9999\u6e2f\u81ea\u5efa\u673a\u623f\uff0c\u7ebf\u8def\u72ec\u4eab\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u9ad8\u5cf0\u671f\u7a33\u5b9a\u7ffb\u5899\uff0c\u6240\u6709\u4ee3\u7406\u670d\u52a1\u5668\u90fd\u652f\u6301\u6d41\u7545\u89e3\u9501\u5f53\u5730\u6d41\u5a92\u4f53\u670d\u52a1\u3002 \u5982\u4f55\u5728iPhone\u3001\u5b89\u5353\u624b\u673a\u548c\u7535\u8111\u4e0a\u4f7f\u7528 WgetCloud \u5168\u7403\u52a0\u901f\uff1f WgetCloud \u4e5f\u662f\u76ee\u524d\u4e2a\u4eba\u4f7f\u7528\u540e\u89c9\u5f97\u6700\u7a33\u5b9a\u7684\u7ffb\u5899\u670d\u52a1\uff0c\u4f46\u5957\u9910\u6d41\u91cf\u4e0d\u591a\uff0c\u4ef7\u683c\u6700\u4f4e\uffe5468\u4e00\u5e74\uff0c\u4eab\u53d7\u6d41\u91cf\u7ffb\u500d200G\u6bcf\u6708\uff0c\u7531\u4e8e\u7a33\u5b9a\u6027\u51fa\u8272\uff0c\u975e\u5e38\u9002\u5408\u5546\u52a1\u529e\u516c\u5916\u8d38\u7fa4\u4f53\u3002 \u6211\u4e2a\u4eba\u4f7f\u7528\u6d4b\u8bd5\u8fc7\uff0c\u8fd8\u662f\u5f88\u7ed9\u529b,\u7ed9\u5de5\u4f5c\u548c\u751f\u6d3b\u5e26\u6765\u4e86\u5f88\u5927\u4fbf\u5229 WgetCloud\u63a8\u8350: https://invite.wgetcloud.ltd/auth/register?code=Apl0","title":"\u79d1\u5b66\u4e0a\u7f51"},{"location":"network/ZeroTier/","text":"ZeroTier \u5185\u7f51 \u00b6 \u80cc\u666f \u00b6 \u968f\u7740\u4e92\u8054\u7f51\u7684\u666e\u53ca\uff0c\u53ef\u7528\u7684\u516c\u7f51 IPv4 \u5730\u5740\u8d8a\u6765\u8d8a\u5c11\uff0c\u73b0\u5728\u7684\u8fd0\u8425\u5546\u57fa\u672c\u4e0d\u7ed9\u5bb6\u7528\u5bbd\u5e26\u5206\u914d\u516c\u7f51 IP \u4e86\u3002\u5982\u679c\u4f60\u60f3\u901a\u8fc7\u5916\u7f51\u8bbf\u95ee\u5230\u5185\u7f51\u7684\u8d44\u6e90\uff0c\u76ee\u524d\u53ea\u80fd\u91c7\u7528\u5185\u7f51\u7a7f\u900f\u7684\u8f6f\u4ef6\u6765\u5b9e\u73b0\u3002\u5e38\u89c1\u7684\u5185\u7f51\u7a7f\u900f\u5982: frp \u8fd9\u6837\u7684\u5de5\u5177\uff0c\u4f46\u662f\u5982\u679c\u8981\u4f7f\u7528\u8fd9\u6837\u7684\u5de5\u5177\u9700\u8981\u6709\u4e00\u53f0\u5177\u5907\u516c\u7f51\u5730\u5740\u7684\u670d\u52a1\u5668\u624d\u53ef\u4ee5\u3002 \u505a\u4e3a\u4e00\u540d\u4f18\u79c0\u7684\u6280\u672f\u4eba\u5458\uff0c\u5f53\u7136\u662f\u80fd\u767d\u5ad6\u5c31\u767d\u5ad6\uff0c\u4e0b\u9762\u4e3b\u8981\u6765\u4ecb\u7ecd\u4e00\u6b3e\u4e0d\u9700\u8981\u516c\u7f51\u7684P2P\u5de5\u5177ZeroTier \u539f\u7406 \u00b6 ZeroTier \u8fd9\u4e00\u7c7b P2P VPN \u662f\u5728\u4e92\u8054\u7f51\u7684\u57fa\u7840\u4e0a\u5c06\u81ea\u5df1\u7684\u6240\u6709\u8bbe\u5907\u7ec4\u6210\u4e00\u4e2a\u79c1\u6709\u7684\u7f51\u7edc\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e92\u8054\u7f51\u8fde\u63a5\u7684\u5c40\u57df\u7f51\u3002 \u5927\u767d\u8bdd\u5c31\u662f\u4e00\u5806\u670d\u52a1\u5668\u5b89\u88c5\u4e0a\u8fd9\u4e2a\u8f6f\u4ef6\u5c31\u7ec4\u5efa\u4e86\u4e00\u4e2a\u5185\u7f51\uff0c\u5927\u5bb6\u901a\u8fc7\u8fd9\u4e2a\u7279\u5b9a\u7f51\u6bb5\u5185\u7f51\u53ef\u4ee5\u4e92\u76f8\u8bbf\u95ee\u3002 \u4f18\u70b9 \u5185\u7f51\u7a7f\u900f\u5de5\u5177\uff08\u514d\u8d39\uff09 ZeroTier \u652f\u6301 Windows\u3001macOS\u3001Linux \u4e09\u5927\u4e3b\u6d41\u5e73\u53f0\uff0ciOS\u3001Android \u4e24\u5927\u79fb\u52a8\u5e73\u53f0 \u6211\u4eec\u6b63\u5728\u9010\u6b65\u7528 Tailscale \u66ff\u6362\u6389 ZeroTier \u7f51\u7edc\uff0cZeroTier \u5c06\u4f5c\u4e3a Tailscale \u4e0d\u53ef\u7528\u65f6\u7684\u5907\u9009\u65b9\u6848 \u5b98\u7f51\u5730\u5740: https://www.zerotier.com/ \u4e0b\u8f7d\u5ba2\u6237\u7aef: https://www.zerotier.com/download/ Linux (DEB/RPM) \u00b6 curl -s https://install.zerotier.com | sudo bash macOS \u00b6 \u53ef\u9009\u62e9\u5b98\u65b9\u7684 DMG \u5b89\u88c5\uff0c\u6216\u4ece Homebrew \u8fdb\u884c\u5b89\u88c5\uff1a brew cask install zerotier-one \u6e29\u99a8\u63d0\u793a \u6211\u8fd9\u8fb9\u4e3b\u8981\u662f\u4ee5\u4f7f\u7528linux\u548cmac\u4e3a\u4e3b\uff0c\u5176\u4ed6\u5e73\u53f0\u8bf7\u76f4\u63a5\u8bbf\u95ee\u4e0a\u65b9\u7684 Zerotier \u5b98\u7f51\u8fdb\u884c\u4e0b\u8f7d \u5b89\u88c5 \u00b6 \u64cd\u4f5c\u6b65\u9aa4 Linux\u6216\u8005Mac\u5b89\u88c5zerotier \u52a0\u5165\u5230zerotier\u7f51\u7edc zerotier\u7684UI\u754c\u9762\u52fe\u9009\u521a\u521a\u6dfb\u52a0\u7684\u8282\u70b9 \u5b89\u88c5: \u53c2\u8003\u5b98\u7f51 curl -s https://install.zerotier.com | sudo bash \u8282\u70b9\u52a0\u5165zerotier\u7f51\u7edc\uff0c\u52a0\u5165\u7f51\u7edc\u4e4b\u524d\u9700\u8981\u63d0\u524d\u521b\u5efa\u8fd9\u4e2a\u7f51\u7edc root@user:/home/user# zerotier-cli join b15644912ebd050d 200 join OK \u767b\u9646\u81ea\u5df1\u7684zerotier\u5e10\u53f7\uff0c\u52fe\u9009\u5141\u8bb8\u5bf9\u5e94\u673a\u5668\u5c31\u53ef\u4ee5\u4e86\uff0c\u8fd9\u91cc\u63a8\u8350\u4f7f\u7528github\u5e10\u53f7\u3002","title":"ZeroTier\u7ec4\u7f51"},{"location":"network/ZeroTier/#zerotier","text":"","title":"ZeroTier \u5185\u7f51"},{"location":"network/ZeroTier/#_1","text":"\u968f\u7740\u4e92\u8054\u7f51\u7684\u666e\u53ca\uff0c\u53ef\u7528\u7684\u516c\u7f51 IPv4 \u5730\u5740\u8d8a\u6765\u8d8a\u5c11\uff0c\u73b0\u5728\u7684\u8fd0\u8425\u5546\u57fa\u672c\u4e0d\u7ed9\u5bb6\u7528\u5bbd\u5e26\u5206\u914d\u516c\u7f51 IP \u4e86\u3002\u5982\u679c\u4f60\u60f3\u901a\u8fc7\u5916\u7f51\u8bbf\u95ee\u5230\u5185\u7f51\u7684\u8d44\u6e90\uff0c\u76ee\u524d\u53ea\u80fd\u91c7\u7528\u5185\u7f51\u7a7f\u900f\u7684\u8f6f\u4ef6\u6765\u5b9e\u73b0\u3002\u5e38\u89c1\u7684\u5185\u7f51\u7a7f\u900f\u5982: frp \u8fd9\u6837\u7684\u5de5\u5177\uff0c\u4f46\u662f\u5982\u679c\u8981\u4f7f\u7528\u8fd9\u6837\u7684\u5de5\u5177\u9700\u8981\u6709\u4e00\u53f0\u5177\u5907\u516c\u7f51\u5730\u5740\u7684\u670d\u52a1\u5668\u624d\u53ef\u4ee5\u3002 \u505a\u4e3a\u4e00\u540d\u4f18\u79c0\u7684\u6280\u672f\u4eba\u5458\uff0c\u5f53\u7136\u662f\u80fd\u767d\u5ad6\u5c31\u767d\u5ad6\uff0c\u4e0b\u9762\u4e3b\u8981\u6765\u4ecb\u7ecd\u4e00\u6b3e\u4e0d\u9700\u8981\u516c\u7f51\u7684P2P\u5de5\u5177ZeroTier","title":"\u80cc\u666f"},{"location":"network/ZeroTier/#_2","text":"ZeroTier \u8fd9\u4e00\u7c7b P2P VPN \u662f\u5728\u4e92\u8054\u7f51\u7684\u57fa\u7840\u4e0a\u5c06\u81ea\u5df1\u7684\u6240\u6709\u8bbe\u5907\u7ec4\u6210\u4e00\u4e2a\u79c1\u6709\u7684\u7f51\u7edc\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e92\u8054\u7f51\u8fde\u63a5\u7684\u5c40\u57df\u7f51\u3002 \u5927\u767d\u8bdd\u5c31\u662f\u4e00\u5806\u670d\u52a1\u5668\u5b89\u88c5\u4e0a\u8fd9\u4e2a\u8f6f\u4ef6\u5c31\u7ec4\u5efa\u4e86\u4e00\u4e2a\u5185\u7f51\uff0c\u5927\u5bb6\u901a\u8fc7\u8fd9\u4e2a\u7279\u5b9a\u7f51\u6bb5\u5185\u7f51\u53ef\u4ee5\u4e92\u76f8\u8bbf\u95ee\u3002 \u4f18\u70b9 \u5185\u7f51\u7a7f\u900f\u5de5\u5177\uff08\u514d\u8d39\uff09 ZeroTier \u652f\u6301 Windows\u3001macOS\u3001Linux \u4e09\u5927\u4e3b\u6d41\u5e73\u53f0\uff0ciOS\u3001Android \u4e24\u5927\u79fb\u52a8\u5e73\u53f0 \u6211\u4eec\u6b63\u5728\u9010\u6b65\u7528 Tailscale \u66ff\u6362\u6389 ZeroTier \u7f51\u7edc\uff0cZeroTier \u5c06\u4f5c\u4e3a Tailscale \u4e0d\u53ef\u7528\u65f6\u7684\u5907\u9009\u65b9\u6848 \u5b98\u7f51\u5730\u5740: https://www.zerotier.com/ \u4e0b\u8f7d\u5ba2\u6237\u7aef: https://www.zerotier.com/download/","title":"\u539f\u7406"},{"location":"network/ZeroTier/#linux-debrpm","text":"curl -s https://install.zerotier.com | sudo bash","title":"Linux (DEB/RPM)"},{"location":"network/ZeroTier/#macos","text":"\u53ef\u9009\u62e9\u5b98\u65b9\u7684 DMG \u5b89\u88c5\uff0c\u6216\u4ece Homebrew \u8fdb\u884c\u5b89\u88c5\uff1a brew cask install zerotier-one \u6e29\u99a8\u63d0\u793a \u6211\u8fd9\u8fb9\u4e3b\u8981\u662f\u4ee5\u4f7f\u7528linux\u548cmac\u4e3a\u4e3b\uff0c\u5176\u4ed6\u5e73\u53f0\u8bf7\u76f4\u63a5\u8bbf\u95ee\u4e0a\u65b9\u7684 Zerotier \u5b98\u7f51\u8fdb\u884c\u4e0b\u8f7d","title":"macOS"},{"location":"network/ZeroTier/#_3","text":"\u64cd\u4f5c\u6b65\u9aa4 Linux\u6216\u8005Mac\u5b89\u88c5zerotier \u52a0\u5165\u5230zerotier\u7f51\u7edc zerotier\u7684UI\u754c\u9762\u52fe\u9009\u521a\u521a\u6dfb\u52a0\u7684\u8282\u70b9 \u5b89\u88c5: \u53c2\u8003\u5b98\u7f51 curl -s https://install.zerotier.com | sudo bash \u8282\u70b9\u52a0\u5165zerotier\u7f51\u7edc\uff0c\u52a0\u5165\u7f51\u7edc\u4e4b\u524d\u9700\u8981\u63d0\u524d\u521b\u5efa\u8fd9\u4e2a\u7f51\u7edc root@user:/home/user# zerotier-cli join b15644912ebd050d 200 join OK \u767b\u9646\u81ea\u5df1\u7684zerotier\u5e10\u53f7\uff0c\u52fe\u9009\u5141\u8bb8\u5bf9\u5e94\u673a\u5668\u5c31\u53ef\u4ee5\u4e86\uff0c\u8fd9\u91cc\u63a8\u8350\u4f7f\u7528github\u5e10\u53f7\u3002","title":"\u5b89\u88c5"},{"location":"network/frp/","text":"\u4e00\u3001\u5185\u7f51\u7a7f\u900fFrp\uff1a \u00b6 \u4e0b\u8f7d\u5730\u5740\uff1a https://github.com/fatedier/frp/releases 1.1\u3001\u6982\u8ff0\uff1a \u00b6 1.1.1\uff1afrp\u5185\u7f51\u7a7f\u900f\u5de5\u5177\uff1a \u00b6 \u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u53cd\u5411\u4ee3\u7406\u5e94\u7528\uff0c\u53ef\u4ee5\u5e2e\u52a9\u60a8\u8f7b\u677e\u5730\u8fdb\u884c\u5185\u7f51\u7a7f\u900f\uff0c\u5bf9\u5916\u7f51\u63d0\u4f9b\u670d\u52a1\uff0c\u652f\u6301 tcp, http, https \u7b49\u534f\u8bae\u7c7b\u578b\uff0c\u5e76\u4e14 web \u670d\u52a1\u652f\u6301\u6839\u636e\u57df\u540d\u8fdb\u884c\u8def\u7531\u8f6c\u53d1\u3002 1.1.2\uff1afrp\u7684\u4f5c\u7528\uff1a \u00b6 \u5229\u7528\u5904\u4e8e\u5185\u7f51\u6216\u9632\u706b\u5899\u540e\u7684\u673a\u5668\uff0c\u5bf9\u5916\u7f51\u73af\u5883\u63d0\u4f9b http \u6216 https \u670d\u52a1\u3002 \u5bf9\u4e8e http \u670d\u52a1\u652f\u6301\u57fa\u4e8e\u57df\u540d\u7684\u865a\u62df\u4e3b\u673a\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u57df\u540d\u7ed1\u5b9a\uff0c\u4f7f\u591a\u4e2a\u57df\u540d\u53ef\u4ee5\u5171\u7528\u4e00\u4e2a80\u7aef\u53e3\u3002 \u5229\u7528\u5904\u4e8e\u5185\u7f51\u6216\u9632\u706b\u5899\u540e\u7684\u673a\u5668\uff0c\u5bf9\u5916\u7f51\u73af\u5883\u63d0\u4f9b tcp \u670d\u52a1\uff0c\u4f8b\u5982\u5728\u5bb6\u91cc\u901a\u8fc7 ssh \u8bbf\u95ee\u5904\u4e8e\u516c\u53f8\u5185\u7f51\u73af\u5883\u5185\u7684\u4e3b\u673a\u3002 \u53ef\u67e5\u770b\u901a\u8fc7\u4ee3\u7406\u7684\u6240\u6709 http \u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\uff08\u5f85\u5f00\u53d1\uff09 2.1\u3001\u9879\u76ee\u601d\u8def\u53ca\u91cd\u96be\u70b9\u5185\u5bb9 \u00b6 \u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u6b65\u9aa4\u53ea\u9488\u5bf9\u9488\u5bf9\u4e8ecentos \u64cd\u4f5c\u7cfb\u7edf 2.1.1\uff1a\u9879\u76ee\u62d3\u6251\u56fe\uff1a \u00b6 2.1.2\uff1a\u5b9e\u9a8c\u601d\u8def\uff1a \u00b6 \u7b2c\u4e00\u6b65 \u8d2d\u4e70\u4e00\u53f0\u4e91\u670d\u52a1\u5668\u4f5c\u4e3afrp-server\u7aef\uff0c\u4e3b\u8981\u662f\u9700\u8981\u4e00\u4e2a\u56fa\u5b9a\u516c\u7f51\u5730\u5740\uff0c\u5982\u679c\u516c\u53f8\u6709\u56fa\u5b9a\u516c\u7f51\u5730\u5740\u6700\u597d \u7b2c\u4e8c\u6b65 \u5728\u516c\u53f8\u6216\u8005\u5185\u7f51\u51c6\u5907\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3afrp-client\u7aef \u7b2c\u4e09\u6b65 \u5728\u516c\u53f8\u6216\u8005\u5185\u7f51\u51c6\u5907\u4e00\u53f0\u4e3b\u673a\u6765\u90e8\u7f72vpn \u7b2c\u56db\u6b65 \u5229\u7528\u7a7f\u900f\u6765\u5b9e\u73b0\u8fde\u63a5VPN 2.1.3\uff1afrp\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e\uff1a \u00b6 [ root@localhost frpc ] # ll \u603b\u7528\u91cf 21136 -rwxr-xr-x 1 root root 10466752 8\u6708 18 2021 frpc -rw-r--r-- 1 root root 6818 8\u6708 18 2021 frpc_full.ini -rw-r--r-- 1 root root 283 5\u6708 2 23 :35 frpc.ini -rwxr-xr-x 1 root root 11133376 8\u6708 18 2021 frps -rw-r--r-- 1 root root 2199 8\u6708 18 2021 frps_full.ini -rw-r--r-- 1 root root 26 8\u6708 18 2021 frps.ini -rw-r--r-- 1 root root 11358 8\u6708 18 2021 LICENSE -rw-r--r-- 1 root root 21 8\u6708 18 2021 run.sh drwxrwxr-x 2 root root 88 8\u6708 18 2021 systemd \u6587\u4ef6\u8bf4\u660e\uff1a frpc # \u5ba2\u6237\u7aef\u4e8c\u8fdb\u5236\u6587\u4ef6 frpc_full.ini # \u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u5b8c\u6574\u793a\u4f8b frpc.ini # \u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 frps # \u670d\u52a1\u7aef\u4e8c\u8fdb\u5236\u6587\u4ef6 frps_full.ini # \u670d\u52a1\u7aef\u914d\u7f6e\u6587\u4ef6\u5b8c\u6574\u793a\u4f8b frps.in1 # \u670d\u52a1\u7aef\u914d\u7f6e\u6587\u4ef6 3.1\u3001Frp\u90e8\u7f72\uff1a \u00b6 3.1.1\uff1a\u90e8\u7f72frps\uff1a \u00b6 \u6ce8\u91ca\uff1a\u4e00\u822cfrps \u662f\u516c\u7f51\u4e0a\u7684\u4e00\u53f0\u4e91\u670d\u52a1\u5668\uff0c\u8fd9\u91cc\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u4e91\u5e73\u53f0\uff0c\u6ce8\u610f\u4e00\u5b9a\u8bb0\u5f97\u5f00\u5b89\u5168\u7ec4\uff01\uff01\uff01 [ root@VM-16-9-centos frps ] # cat frps.ini [ common ] bind_port = 50070 vhost_http_port = 50080 vhost_https_port = 50443 token = $pF @7zz^LDuh7^xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u8fd9\u4e2atoken\u662f\u7528\u4e8e\u670d\u52a1\u7aef\u548c\u5ba2\u6237\u7aef\u8fde\u63a5 # \u542f\u52a8 ./frps -c ./frps.ini 3.1.2\uff1a\u90e8\u7f72frpc\uff1a \u00b6 [ root@moban frp_0.38.0_linux_amd64 ] # cat frpc.ini [ common ] server_addr = 115 .29.189.57 # server_addr\u8868\u793a\u4e0efrps\u8fde\u63a5\u7684\u5730\u5740 server_port = 50081 # \u4e0efrps\u8fde\u63a5\u7684\u7aef\u53e3 token = pass@word1 # \u914d\u7f6etoken\u7528\u6765\u9a8c\u8bc1 [ openvpn ] type = udp local_ip = 192 .168.8.82 local_port = 51194 remote_port = 51194 # remote_port\u8868\u793a\u7528\u6237\u8bbf\u95ee\u516c\u7f51\u5730\u5740+\u7aef\u53e3,\u5c31\u53cd\u5411\u4ee3\u7406\u5230local_ip+\u7aef\u53e3 # \u542f\u52a8 ./frps -c ./frps.ini 3.1.3\uff1anj\u5730\u533a\uff1a192.165.0.20 \u00b6 [ root@moban frp_0.26.0_linux_amd64 ] # cat frpc.ini [ common ] tls_enable = true server_addr = 39 .104.160.84 server_port = 50070 token = $pF @xxxxxx [ openvpn-nj ] type = udp local_ip = 192 .165.0.21 local_port = 31194 remote_port = 31194 [ root@moban frp_0.26.0_linux_amd64 ] # cat frp.sh ./frpc -c ./frpc.ini 3.1.4\uff1abj\u5730\u533a\uff1a \u00b6 [ root@frp-40 frpc ] # cat frpc.ini [ common ] tls_enable = true server_addr = 39 .104.160.84 server_port = 50070 token = $pF @7zz^LDuh7^X0Uxxxxx 4.1\u3001vpn\u90e8\u7f72\uff1a \u00b6 4.4.1\uff1avpn\u90e8\u7f72\u811a\u672c\uff1a \u00b6 #!/bin/bash # \u642d\u5efavpenvpn\u811a\u672c # \u4f5c\u8005:lixie # \u65e5\u671f:2021-10-19 # openvpn\u662f\u670d\u52a1\u7aef,easy-rsa\u662f\u8bc1\u4e66\u7ba1\u7406\u5de5\u5177 yum -y install wget mkdir -p /etc/yum.repos.d/backup mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum -y install openvpn easy-rsa & > /dev/null # \u83b7\u53d6openvpn\u548ceasy-rsa\u7684\u7248\u672c OPENVER = $( rpm -qi openvpn | awk -F \": \" 'NR==2{print $2}' ) EASYVER = $( rpm -qi easy-rsa | awk -F \": \" 'NR==2{print $2}' ) #echo \"openvpn-$OPENVER\" #echo \"easy-rsa-$EASYVER\" #\u751f\u6210\u670d\u52a1\u5668\u914d\u7f6e\u6587\u4ef6 #cp /usr/share/doc/openvpn-$OPENVER/sample/sample-config-files/server.conf /etc/openvpn/ cat << EOF >> /etc/openvpn/server/server.conf local 0.0.0.0 port 41194 proto udp dev tun ca /etc/openvpn/certs/ca.crt cert /etc/openvpn/certs/server.crt dh /etc/openvpn/certs/dh.pem push \"route 192.168.0.0 255.255.0.0\" server 10.10.0.0 255.255.255.0 ifconfig-pool-persist /var/log/openvpn/ipp.txt client-to-client keepalive 10 120 comp-lzo max-clients 100 persist-key persist-tun status /var/log/openvpn/openvpn-status.log log-append /var/log/openvpn/openvpn.log verb 3 key /etc/openvpn/certs/server.key cipher AES-256-CBC client-config-dir /etc/openvpn/ccd auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env username-as-common-name script-security 3 verify-client-cert none EOF # \u6ce8\u610f\uff01\uff01\uff01 \u8fd9\u91cc\u521b\u5efa\u5b8c\u914d\u7f6e\u6587\u4ef6\u540e\uff0c\u9700\u8981\u505a\u4e2a\u914d\u7f6e\u6587\u4ef6\u7684\u8f6f\u8fde\u63a5\uff0c\u56e0\u4e3a\u5f53\u524d\u7248\u672c\u7684 openvpn systemd \u542f\u52a8\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684\u662f.service.conf\u914d\u7f6e cd /etc/openvpn/server/ && ln -sf server.conf .service.conf #\u51c6\u5907\u8bc1\u4e66\u7b7e\u53d1\u76f8\u5173\u6587\u4ef6 cp -r /usr/share/easy-rsa/ /etc/openvpn/easy-rsa-server #\u51c6\u5907\u7b7e\u53d1\u8bc1\u4e66\u76f8\u5173\u53d8\u91cf\u7684\u914d\u7f6e\u6587\u4ef6 cp /usr/share/doc/easy-rsa- $EASYVER /vars.example /etc/openvpn/easy-rsa-server/3/vars #CA\u7684\u8bc1\u4e66\u6709\u6548\u671f\u9ed8\u4e3a\u4e3a10\u5e74,\u53ef\u4ee5\u9002\u5f53\u5ef6\u957f,\u6bd4\u5982:36500\u5929 sed -i '/3650/ s/3650/36500/g' /etc/openvpn/easy-rsa-server/3/vars #\u670d\u52a1\u5668\u8bc1\u4e66\u9ed8\u4e3a\u4e3a825\u5929 sed -i '/825/ s/825/82500/g' /etc/openvpn/easy-rsa-server/3/vars \u521d\u59cb\u5316\u6570\u636e,\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u751f\u6210pki\u76ee\u5f55\u53ca\u76f8\u5173\u6587\u4ef6 cd /etc/openvpn/easy-rsa-server/3/ pwd ./easyrsa init-pki \u521b\u5efaCA\u673a\u6784 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa build-ca nopass <<EOF EOF # \u521b\u5efa\u670d\u52a1\u7aef\u8bc1\u4e66\u7533\u8bf7 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa gen-req server nopass <<EOF EOF ## \u9881\u53d1\u670d\u52a1\u7aef\u8bc1\u4e66 # cd /etc/openvpn/easy-rsa-server/3 ./easyrsa sign server server <<EOF yes EOF # \u521b\u5efa Diffie-Hellman \u5bc6\u94a5,\u8fd9\u4e2a\u6b65\u9aa4\u5b8c\u4e86\u670d\u52a1\u5668\u90e8\u5206\u5c31\u5b8c\u6210\u4e86 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa gen-dh #\u4e3a\u5ba2\u6237\u7aef\u51c6\u5907\u8bc1\u4e66\u73af\u5883 cp -r /usr/share/easy-rsa/ /etc/openvpn/easy-rsa-client cp /usr/share/doc/easy-rsa- $EASYVER /vars.example /etc/openvpn/easy-rsa-client/3/vars cd /etc/openvpn/easy-rsa-client/3/ ./easyrsa init-pki # \u5c06CA\u548c\u670d\u52a1\u5668\u8bc1\u4e66\u76f8\u5173\u6587\u4ef6\u590d\u5236\u5230\u670d\u52a1\u5668\u76f8\u5e94\u7684\u76ee\u5f55 mkdir /etc/openvpn/certs cp /etc/openvpn/easy-rsa-server/3/pki/ca.crt /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/issued/server.crt /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/private/server.key /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/dh.pem /etc/openvpn/certs/ # \u4fee\u6539\u670d\u52a1\u5668\u7aef\u914d\u7f6e\u6587\u4ef6 # \u521b\u5efa\u65e5\u5fd7\u76ee\u5f55 mkdir /etc/openvpn/ccd mkdir /var/log/openvpn chown openvpn.openvpn /var/log/openvpn cat <<EOF>> /etc/openvpn/psw-file LIXIE ABC123, EOF chmod 600 /etc/openvpn/psw-file chown openvpn:openvpn /etc/openvpn/psw-file chmod +x /etc/openvpn/checkpsw.sh chown -R openvpn:openvpn /etc/openvpn/ #\u5728\u670d\u52a1\u5668\u5f00\u542fip_forward\u8f6c\u53d1\u529f\u80fd echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf && sysctl -p & > /dev/null echo 'iptables -t nat -A POSTROUTING -s 10.10.0.0/24 -j MASQUERADE' >> /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local /etc/rc.d/rc.local systemctl daemon-reload systemctl start openvpn-server@.service.service systemctl enable openvpn-server@.service.service 4.4.2\uff1a\u65b0\u589e\u7528\u6237\u811a\u672c\uff1a \u00b6 cat <<EOF>> /etc/openvpn/checkpsw.sh #!/bin/sh ########################################################### # checkpsw.sh (C) 2004 Mathias Sundman <mathias@openvpn.se> # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password. PASSFILE=\"/etc/openvpn/psw-file\" LOG_FILE=\"/var/log/openvpn/password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` ########################################################### if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password= \\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password= \\\"${password}\\\".\" >> ${LOG_FILE} exit 1 EOF \u6ce8\u610f\uff1a\u9700\u8981\u7ed9\u8fd9\u4e2a\u811a\u672c\u4e00\u4e2a\u6267\u884c\u6743\u9650 4.4.3\uff1a\u90e8\u7f72iptables\uff1a \u00b6 \u9632\u706b\u5899\u914d\u7f6e\uff08firewalld\u914d\u7f6e\u6216\u8005iptables\u914d\u7f6e\u9009\u4e00\u4e2a\uff09 # firewalld\u914d\u7f6e firewall-cmd --permanent --add-masquerade firewall-cmd --permanent --add-service = openvpn # \u6216\u8005\u6dfb\u52a0\u81ea\u5b9a\u4e49\u7aef\u53e3 # firewall-cmd --permanent --add-port=1194/tcp firewall-cmd --permanent --direct --passthrough ipv4 -t nat -A POSTROUTING -s 10 .8.0.0/24 -o eth0 -j MASQUERADE firewall-cmd --reload # iptables \uff08\u548c\u4e0a\u9762\u7684\u7f51\u6bb5\u914d\u7f6e\u4e0d\u540c\uff0c\u4fee\u6539\u4e0b\u9762\u7684\u7f51\u6bb5\u5373\u53ef\uff09 yum install iptables-services systemctl enable iptables iptables -X iptables -F iptables -Z iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT service iptables save iptables -t nat -A POSTROUTING -s 10 .10.0.0/255.255.255.0 -o eth0 -j MASQUERADE iptables -A FORWARD -s 10 .10.0.0/24 -d 192 .168.0.0/24 -i tun0 -j ACCEPT iptables -A FORWARD -s 192 .168.0.0/24 -d 10 .10.0.0/24 -i eth0 -j ACCEPT service iptables save 4.4.4\uff1a\u51c6\u5907\u5ba2\u6237\u7aef\u8bc1\u4e66\uff1a \u00b6 \u5907\u6ce8\uff1a\u5b89\u88c5openvpn\u5ba2\u6237\u7aef\u8f6f\u4ef6\uff1aconfig\u914d\u7f6e\u6587\u4ef6\u4e2d\u6dfb\u52a0ca\u8bc1\u4e66\u548c.opvn client dev tun proto udp # \u516c\u7f51ip\u9700\u8981\u4fee\u6539 remote \u516c\u7f51\u5730\u5740 41194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt comp-lzo verb 3 cipher AES-256-CBC auth-user-pass 5.1\uff1a\u5176\u4ed6\u6848\u4f8b\u914d\u7f6e\uff1a \u00b6 \u9644\u4ef6\uff1a \u00b6 \u5b66\u4e60\u535a\u5ba2\uff1a https://www.jianshu.com/p/09603d9e0b6c","title":"frp \u5185\u7f51\u7a7f\u900f"},{"location":"network/frp/#frp","text":"\u4e0b\u8f7d\u5730\u5740\uff1a https://github.com/fatedier/frp/releases","title":"\u4e00\u3001\u5185\u7f51\u7a7f\u900fFrp\uff1a"},{"location":"network/frp/#11","text":"","title":"1.1\u3001\u6982\u8ff0\uff1a"},{"location":"network/frp/#111frp","text":"\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u7684\u53cd\u5411\u4ee3\u7406\u5e94\u7528\uff0c\u53ef\u4ee5\u5e2e\u52a9\u60a8\u8f7b\u677e\u5730\u8fdb\u884c\u5185\u7f51\u7a7f\u900f\uff0c\u5bf9\u5916\u7f51\u63d0\u4f9b\u670d\u52a1\uff0c\u652f\u6301 tcp, http, https \u7b49\u534f\u8bae\u7c7b\u578b\uff0c\u5e76\u4e14 web \u670d\u52a1\u652f\u6301\u6839\u636e\u57df\u540d\u8fdb\u884c\u8def\u7531\u8f6c\u53d1\u3002","title":"1.1.1\uff1afrp\u5185\u7f51\u7a7f\u900f\u5de5\u5177\uff1a"},{"location":"network/frp/#112frp","text":"\u5229\u7528\u5904\u4e8e\u5185\u7f51\u6216\u9632\u706b\u5899\u540e\u7684\u673a\u5668\uff0c\u5bf9\u5916\u7f51\u73af\u5883\u63d0\u4f9b http \u6216 https \u670d\u52a1\u3002 \u5bf9\u4e8e http \u670d\u52a1\u652f\u6301\u57fa\u4e8e\u57df\u540d\u7684\u865a\u62df\u4e3b\u673a\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u57df\u540d\u7ed1\u5b9a\uff0c\u4f7f\u591a\u4e2a\u57df\u540d\u53ef\u4ee5\u5171\u7528\u4e00\u4e2a80\u7aef\u53e3\u3002 \u5229\u7528\u5904\u4e8e\u5185\u7f51\u6216\u9632\u706b\u5899\u540e\u7684\u673a\u5668\uff0c\u5bf9\u5916\u7f51\u73af\u5883\u63d0\u4f9b tcp \u670d\u52a1\uff0c\u4f8b\u5982\u5728\u5bb6\u91cc\u901a\u8fc7 ssh \u8bbf\u95ee\u5904\u4e8e\u516c\u53f8\u5185\u7f51\u73af\u5883\u5185\u7684\u4e3b\u673a\u3002 \u53ef\u67e5\u770b\u901a\u8fc7\u4ee3\u7406\u7684\u6240\u6709 http \u8bf7\u6c42\u548c\u54cd\u5e94\u7684\u8be6\u7ec6\u4fe1\u606f\u3002\uff08\u5f85\u5f00\u53d1\uff09","title":"1.1.2\uff1afrp\u7684\u4f5c\u7528\uff1a"},{"location":"network/frp/#21","text":"\u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u6b65\u9aa4\u53ea\u9488\u5bf9\u9488\u5bf9\u4e8ecentos \u64cd\u4f5c\u7cfb\u7edf","title":"2.1\u3001\u9879\u76ee\u601d\u8def\u53ca\u91cd\u96be\u70b9\u5185\u5bb9"},{"location":"network/frp/#211","text":"","title":"2.1.1\uff1a\u9879\u76ee\u62d3\u6251\u56fe\uff1a"},{"location":"network/frp/#212","text":"\u7b2c\u4e00\u6b65 \u8d2d\u4e70\u4e00\u53f0\u4e91\u670d\u52a1\u5668\u4f5c\u4e3afrp-server\u7aef\uff0c\u4e3b\u8981\u662f\u9700\u8981\u4e00\u4e2a\u56fa\u5b9a\u516c\u7f51\u5730\u5740\uff0c\u5982\u679c\u516c\u53f8\u6709\u56fa\u5b9a\u516c\u7f51\u5730\u5740\u6700\u597d \u7b2c\u4e8c\u6b65 \u5728\u516c\u53f8\u6216\u8005\u5185\u7f51\u51c6\u5907\u4e00\u53f0\u4e3b\u673a\u4f5c\u4e3afrp-client\u7aef \u7b2c\u4e09\u6b65 \u5728\u516c\u53f8\u6216\u8005\u5185\u7f51\u51c6\u5907\u4e00\u53f0\u4e3b\u673a\u6765\u90e8\u7f72vpn \u7b2c\u56db\u6b65 \u5229\u7528\u7a7f\u900f\u6765\u5b9e\u73b0\u8fde\u63a5VPN","title":"2.1.2\uff1a\u5b9e\u9a8c\u601d\u8def\uff1a"},{"location":"network/frp/#213frp","text":"[ root@localhost frpc ] # ll \u603b\u7528\u91cf 21136 -rwxr-xr-x 1 root root 10466752 8\u6708 18 2021 frpc -rw-r--r-- 1 root root 6818 8\u6708 18 2021 frpc_full.ini -rw-r--r-- 1 root root 283 5\u6708 2 23 :35 frpc.ini -rwxr-xr-x 1 root root 11133376 8\u6708 18 2021 frps -rw-r--r-- 1 root root 2199 8\u6708 18 2021 frps_full.ini -rw-r--r-- 1 root root 26 8\u6708 18 2021 frps.ini -rw-r--r-- 1 root root 11358 8\u6708 18 2021 LICENSE -rw-r--r-- 1 root root 21 8\u6708 18 2021 run.sh drwxrwxr-x 2 root root 88 8\u6708 18 2021 systemd \u6587\u4ef6\u8bf4\u660e\uff1a frpc # \u5ba2\u6237\u7aef\u4e8c\u8fdb\u5236\u6587\u4ef6 frpc_full.ini # \u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6\u5b8c\u6574\u793a\u4f8b frpc.ini # \u5ba2\u6237\u7aef\u914d\u7f6e\u6587\u4ef6 frps # \u670d\u52a1\u7aef\u4e8c\u8fdb\u5236\u6587\u4ef6 frps_full.ini # \u670d\u52a1\u7aef\u914d\u7f6e\u6587\u4ef6\u5b8c\u6574\u793a\u4f8b frps.in1 # \u670d\u52a1\u7aef\u914d\u7f6e\u6587\u4ef6","title":"2.1.3\uff1afrp\u914d\u7f6e\u6587\u4ef6\u8bf4\u660e\uff1a"},{"location":"network/frp/#31frp","text":"","title":"3.1\u3001Frp\u90e8\u7f72\uff1a"},{"location":"network/frp/#311frps","text":"\u6ce8\u91ca\uff1a\u4e00\u822cfrps \u662f\u516c\u7f51\u4e0a\u7684\u4e00\u53f0\u4e91\u670d\u52a1\u5668\uff0c\u8fd9\u91cc\u53ef\u4ee5\u662f\u4efb\u610f\u7684\u4e91\u5e73\u53f0\uff0c\u6ce8\u610f\u4e00\u5b9a\u8bb0\u5f97\u5f00\u5b89\u5168\u7ec4\uff01\uff01\uff01 [ root@VM-16-9-centos frps ] # cat frps.ini [ common ] bind_port = 50070 vhost_http_port = 50080 vhost_https_port = 50443 token = $pF @7zz^LDuh7^xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \u8fd9\u4e2atoken\u662f\u7528\u4e8e\u670d\u52a1\u7aef\u548c\u5ba2\u6237\u7aef\u8fde\u63a5 # \u542f\u52a8 ./frps -c ./frps.ini","title":"3.1.1\uff1a\u90e8\u7f72frps\uff1a"},{"location":"network/frp/#312frpc","text":"[ root@moban frp_0.38.0_linux_amd64 ] # cat frpc.ini [ common ] server_addr = 115 .29.189.57 # server_addr\u8868\u793a\u4e0efrps\u8fde\u63a5\u7684\u5730\u5740 server_port = 50081 # \u4e0efrps\u8fde\u63a5\u7684\u7aef\u53e3 token = pass@word1 # \u914d\u7f6etoken\u7528\u6765\u9a8c\u8bc1 [ openvpn ] type = udp local_ip = 192 .168.8.82 local_port = 51194 remote_port = 51194 # remote_port\u8868\u793a\u7528\u6237\u8bbf\u95ee\u516c\u7f51\u5730\u5740+\u7aef\u53e3,\u5c31\u53cd\u5411\u4ee3\u7406\u5230local_ip+\u7aef\u53e3 # \u542f\u52a8 ./frps -c ./frps.ini","title":"3.1.2\uff1a\u90e8\u7f72frpc\uff1a"},{"location":"network/frp/#313nj192165020","text":"[ root@moban frp_0.26.0_linux_amd64 ] # cat frpc.ini [ common ] tls_enable = true server_addr = 39 .104.160.84 server_port = 50070 token = $pF @xxxxxx [ openvpn-nj ] type = udp local_ip = 192 .165.0.21 local_port = 31194 remote_port = 31194 [ root@moban frp_0.26.0_linux_amd64 ] # cat frp.sh ./frpc -c ./frpc.ini","title":"3.1.3\uff1anj\u5730\u533a\uff1a192.165.0.20"},{"location":"network/frp/#314bj","text":"[ root@frp-40 frpc ] # cat frpc.ini [ common ] tls_enable = true server_addr = 39 .104.160.84 server_port = 50070 token = $pF @7zz^LDuh7^X0Uxxxxx","title":"3.1.4\uff1abj\u5730\u533a\uff1a"},{"location":"network/frp/#41vpn","text":"","title":"4.1\u3001vpn\u90e8\u7f72\uff1a"},{"location":"network/frp/#441vpn","text":"#!/bin/bash # \u642d\u5efavpenvpn\u811a\u672c # \u4f5c\u8005:lixie # \u65e5\u671f:2021-10-19 # openvpn\u662f\u670d\u52a1\u7aef,easy-rsa\u662f\u8bc1\u4e66\u7ba1\u7406\u5de5\u5177 yum -y install wget mkdir -p /etc/yum.repos.d/backup mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/ wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum -y install openvpn easy-rsa & > /dev/null # \u83b7\u53d6openvpn\u548ceasy-rsa\u7684\u7248\u672c OPENVER = $( rpm -qi openvpn | awk -F \": \" 'NR==2{print $2}' ) EASYVER = $( rpm -qi easy-rsa | awk -F \": \" 'NR==2{print $2}' ) #echo \"openvpn-$OPENVER\" #echo \"easy-rsa-$EASYVER\" #\u751f\u6210\u670d\u52a1\u5668\u914d\u7f6e\u6587\u4ef6 #cp /usr/share/doc/openvpn-$OPENVER/sample/sample-config-files/server.conf /etc/openvpn/ cat << EOF >> /etc/openvpn/server/server.conf local 0.0.0.0 port 41194 proto udp dev tun ca /etc/openvpn/certs/ca.crt cert /etc/openvpn/certs/server.crt dh /etc/openvpn/certs/dh.pem push \"route 192.168.0.0 255.255.0.0\" server 10.10.0.0 255.255.255.0 ifconfig-pool-persist /var/log/openvpn/ipp.txt client-to-client keepalive 10 120 comp-lzo max-clients 100 persist-key persist-tun status /var/log/openvpn/openvpn-status.log log-append /var/log/openvpn/openvpn.log verb 3 key /etc/openvpn/certs/server.key cipher AES-256-CBC client-config-dir /etc/openvpn/ccd auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env username-as-common-name script-security 3 verify-client-cert none EOF # \u6ce8\u610f\uff01\uff01\uff01 \u8fd9\u91cc\u521b\u5efa\u5b8c\u914d\u7f6e\u6587\u4ef6\u540e\uff0c\u9700\u8981\u505a\u4e2a\u914d\u7f6e\u6587\u4ef6\u7684\u8f6f\u8fde\u63a5\uff0c\u56e0\u4e3a\u5f53\u524d\u7248\u672c\u7684 openvpn systemd \u542f\u52a8\u6587\u4ef6\u4e2d\u8bfb\u53d6\u7684\u662f.service.conf\u914d\u7f6e cd /etc/openvpn/server/ && ln -sf server.conf .service.conf #\u51c6\u5907\u8bc1\u4e66\u7b7e\u53d1\u76f8\u5173\u6587\u4ef6 cp -r /usr/share/easy-rsa/ /etc/openvpn/easy-rsa-server #\u51c6\u5907\u7b7e\u53d1\u8bc1\u4e66\u76f8\u5173\u53d8\u91cf\u7684\u914d\u7f6e\u6587\u4ef6 cp /usr/share/doc/easy-rsa- $EASYVER /vars.example /etc/openvpn/easy-rsa-server/3/vars #CA\u7684\u8bc1\u4e66\u6709\u6548\u671f\u9ed8\u4e3a\u4e3a10\u5e74,\u53ef\u4ee5\u9002\u5f53\u5ef6\u957f,\u6bd4\u5982:36500\u5929 sed -i '/3650/ s/3650/36500/g' /etc/openvpn/easy-rsa-server/3/vars #\u670d\u52a1\u5668\u8bc1\u4e66\u9ed8\u4e3a\u4e3a825\u5929 sed -i '/825/ s/825/82500/g' /etc/openvpn/easy-rsa-server/3/vars \u521d\u59cb\u5316\u6570\u636e,\u5728\u5f53\u524d\u76ee\u5f55\u4e0b\u751f\u6210pki\u76ee\u5f55\u53ca\u76f8\u5173\u6587\u4ef6 cd /etc/openvpn/easy-rsa-server/3/ pwd ./easyrsa init-pki \u521b\u5efaCA\u673a\u6784 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa build-ca nopass <<EOF EOF # \u521b\u5efa\u670d\u52a1\u7aef\u8bc1\u4e66\u7533\u8bf7 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa gen-req server nopass <<EOF EOF ## \u9881\u53d1\u670d\u52a1\u7aef\u8bc1\u4e66 # cd /etc/openvpn/easy-rsa-server/3 ./easyrsa sign server server <<EOF yes EOF # \u521b\u5efa Diffie-Hellman \u5bc6\u94a5,\u8fd9\u4e2a\u6b65\u9aa4\u5b8c\u4e86\u670d\u52a1\u5668\u90e8\u5206\u5c31\u5b8c\u6210\u4e86 cd /etc/openvpn/easy-rsa-server/3 ./easyrsa gen-dh #\u4e3a\u5ba2\u6237\u7aef\u51c6\u5907\u8bc1\u4e66\u73af\u5883 cp -r /usr/share/easy-rsa/ /etc/openvpn/easy-rsa-client cp /usr/share/doc/easy-rsa- $EASYVER /vars.example /etc/openvpn/easy-rsa-client/3/vars cd /etc/openvpn/easy-rsa-client/3/ ./easyrsa init-pki # \u5c06CA\u548c\u670d\u52a1\u5668\u8bc1\u4e66\u76f8\u5173\u6587\u4ef6\u590d\u5236\u5230\u670d\u52a1\u5668\u76f8\u5e94\u7684\u76ee\u5f55 mkdir /etc/openvpn/certs cp /etc/openvpn/easy-rsa-server/3/pki/ca.crt /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/issued/server.crt /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/private/server.key /etc/openvpn/certs/ cp /etc/openvpn/easy-rsa-server/3/pki/dh.pem /etc/openvpn/certs/ # \u4fee\u6539\u670d\u52a1\u5668\u7aef\u914d\u7f6e\u6587\u4ef6 # \u521b\u5efa\u65e5\u5fd7\u76ee\u5f55 mkdir /etc/openvpn/ccd mkdir /var/log/openvpn chown openvpn.openvpn /var/log/openvpn cat <<EOF>> /etc/openvpn/psw-file LIXIE ABC123, EOF chmod 600 /etc/openvpn/psw-file chown openvpn:openvpn /etc/openvpn/psw-file chmod +x /etc/openvpn/checkpsw.sh chown -R openvpn:openvpn /etc/openvpn/ #\u5728\u670d\u52a1\u5668\u5f00\u542fip_forward\u8f6c\u53d1\u529f\u80fd echo net.ipv4.ip_forward = 1 >> /etc/sysctl.conf && sysctl -p & > /dev/null echo 'iptables -t nat -A POSTROUTING -s 10.10.0.0/24 -j MASQUERADE' >> /etc/rc.d/rc.local chmod +x /etc/rc.d/rc.local /etc/rc.d/rc.local systemctl daemon-reload systemctl start openvpn-server@.service.service systemctl enable openvpn-server@.service.service","title":"4.4.1\uff1avpn\u90e8\u7f72\u811a\u672c\uff1a"},{"location":"network/frp/#442","text":"cat <<EOF>> /etc/openvpn/checkpsw.sh #!/bin/sh ########################################################### # checkpsw.sh (C) 2004 Mathias Sundman <mathias@openvpn.se> # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password. PASSFILE=\"/etc/openvpn/psw-file\" LOG_FILE=\"/var/log/openvpn/password.log\" TIME_STAMP=`date \"+%Y-%m-%d %T\"` ########################################################### if [ ! -r \"${PASSFILE}\" ]; then echo \"${TIME_STAMP}: Could not open password file \\\"${PASSFILE}\\\" for reading.\" >> ${LOG_FILE} exit 1 fi CORRECT_PASSWORD=`awk '!/^;/&&!/^#/&&$1==\"'${username}'\"{print $2;exit}' ${PASSFILE}` if [ \"${CORRECT_PASSWORD}\" = \"\" ]; then echo \"${TIME_STAMP}: User does not exist: username=\\\"${username}\\\", password= \\\"${password}\\\".\" >> ${LOG_FILE} exit 1 fi if [ \"${password}\" = \"${CORRECT_PASSWORD}\" ]; then echo \"${TIME_STAMP}: Successful authentication: username=\\\"${username}\\\".\" >> ${LOG_FILE} exit 0 fi echo \"${TIME_STAMP}: Incorrect password: username=\\\"${username}\\\", password= \\\"${password}\\\".\" >> ${LOG_FILE} exit 1 EOF \u6ce8\u610f\uff1a\u9700\u8981\u7ed9\u8fd9\u4e2a\u811a\u672c\u4e00\u4e2a\u6267\u884c\u6743\u9650","title":"4.4.2\uff1a\u65b0\u589e\u7528\u6237\u811a\u672c\uff1a"},{"location":"network/frp/#443iptables","text":"\u9632\u706b\u5899\u914d\u7f6e\uff08firewalld\u914d\u7f6e\u6216\u8005iptables\u914d\u7f6e\u9009\u4e00\u4e2a\uff09 # firewalld\u914d\u7f6e firewall-cmd --permanent --add-masquerade firewall-cmd --permanent --add-service = openvpn # \u6216\u8005\u6dfb\u52a0\u81ea\u5b9a\u4e49\u7aef\u53e3 # firewall-cmd --permanent --add-port=1194/tcp firewall-cmd --permanent --direct --passthrough ipv4 -t nat -A POSTROUTING -s 10 .8.0.0/24 -o eth0 -j MASQUERADE firewall-cmd --reload # iptables \uff08\u548c\u4e0a\u9762\u7684\u7f51\u6bb5\u914d\u7f6e\u4e0d\u540c\uff0c\u4fee\u6539\u4e0b\u9762\u7684\u7f51\u6bb5\u5373\u53ef\uff09 yum install iptables-services systemctl enable iptables iptables -X iptables -F iptables -Z iptables -P INPUT ACCEPT iptables -P OUTPUT ACCEPT iptables -P FORWARD ACCEPT service iptables save iptables -t nat -A POSTROUTING -s 10 .10.0.0/255.255.255.0 -o eth0 -j MASQUERADE iptables -A FORWARD -s 10 .10.0.0/24 -d 192 .168.0.0/24 -i tun0 -j ACCEPT iptables -A FORWARD -s 192 .168.0.0/24 -d 10 .10.0.0/24 -i eth0 -j ACCEPT service iptables save","title":"4.4.3\uff1a\u90e8\u7f72iptables\uff1a"},{"location":"network/frp/#444","text":"\u5907\u6ce8\uff1a\u5b89\u88c5openvpn\u5ba2\u6237\u7aef\u8f6f\u4ef6\uff1aconfig\u914d\u7f6e\u6587\u4ef6\u4e2d\u6dfb\u52a0ca\u8bc1\u4e66\u548c.opvn client dev tun proto udp # \u516c\u7f51ip\u9700\u8981\u4fee\u6539 remote \u516c\u7f51\u5730\u5740 41194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt comp-lzo verb 3 cipher AES-256-CBC auth-user-pass","title":"4.4.4\uff1a\u51c6\u5907\u5ba2\u6237\u7aef\u8bc1\u4e66\uff1a"},{"location":"network/frp/#51","text":"","title":"5.1\uff1a\u5176\u4ed6\u6848\u4f8b\u914d\u7f6e\uff1a"},{"location":"network/frp/#_1","text":"\u5b66\u4e60\u535a\u5ba2\uff1a https://www.jianshu.com/p/09603d9e0b6c","title":"\u9644\u4ef6\uff1a"},{"location":"prometheus/1.%20prometheus/","text":"curl -X POST -H \"Content-Type: application/json\" -d '{\"name\":\"apikeycurl\", \"role\": \"Admin\"}' http://admin:strongpassword@localhost:3000/api/auth/keys {\"id\":1,\"name\":\"apikeycurl\",\"key\":\"eyJrIjoiVHV2czQxNTdiQnFEWDJ6VjRXMjJpUTc1bGtkR2NmQUoiLCJuIjoiYXBpa2V5Y3VybCIsImlkIjoxfQ==\"}%","title":"1. prometheus"},{"location":"ubuntu/1-system-docs/","text":"\u7cfb\u7edf\u5b89\u88c5 \u00b6 \u6b63\u6240\u8c13\u4e0d\u4f1a\u88c5\u7cfb\u7edf\u7684\u8fd0\u7ef4\u5c31\u4e0d\u662f\u597d\u8fd0\u7ef4\u7684\u7406\u5ff5\uff0c\u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0bubuntu\u7cfb\u7edf\u5b89\u88c5 \u51c6\u5907\u5de5\u4f5c \u6b65\u9aa4\u4e00: \u4e0b\u8f7diso\u955c\u50cf \u4e0b\u8f7d\u5730\u5740: https://mirrors.aliyun.com/ubuntu-releases/ \u6b65\u9aa4\u4e8c: \u5236\u4f5c\u7cfb\u7edf\u76d8 \u53ef\u4ee5\u53c2\u8003\u4f7f\u7528\u6280\u5de7\u4e2d\u7684Mac\u5236\u4f5c\u7cfb\u7edf\u76d8\u8fd9\u7bc7\u6587\u7ae0 \u6b65\u9aa4\u4e09: \u88c5\u5c31\u5b8c\u4e8b\u4e86 1.1. \u9009\u62e9\u8bed\u8a00 1.2. \u9009\u62e9\u952e\u76d8\uff08\u672c\u6b65\u9aa4\u76f4\u63a5\u9ed8\u8ba4\u6309\u56de\u8f66\u5373\u53ef\u3002\uff09 1.3. \u914d\u7f6e\u7f51\u7edc\uff08\u4e00\u822c\u60c5\u51b5\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e00\u6b65\uff09 1.4. \u9009\u62e9\u4ee3\u7406\uff08\u9ed8\u8ba4\u56de\u8f66\u8df3\u8fc7\uff09 1.5. \u914d\u7f6e\u955c\u50cf\u6e90\uff08\u8df3\u8fc7\uff09 1.6. \u9009\u62e9\u78c1\u76d8\uff08\u8fd9\u4e2a\u6b65\u9aa4\u6bd4\u8f83\u5173\u952e\uff09 \u9009\u62e9\u78c1\u76d8\u8fd9\u4e00\u6b65\u9700\u8981\u6ce8\u610f\uff0c\u9700\u8981\u6240\u6709\u78c1\u76d8\u7a7a\u95f4\u5206\u7ed9\u6839\u5206\u533a 1.7. \u7528\u6237\u4fe1\u606f 1.8. openssh server \u5207\u8bb0\u8981\u9009\u62e9\u4e0a ( \u5207\u8bb0 ) \u7cfb\u7edf\u521d\u59cb\u5316 \u00b6 \u521d\u59cb\u5316\u6b65\u9aa4 \u6dfb\u52a0hosts\u4fe1\u606f \u4fee\u6539\u56fd\u5185apt\u6e90 \u6e05\u534e\u6e90 \u963f\u91cc\u6e90 \u6dfb\u52a0\u7ba1\u7406\u5458\u7528\u6237 \u901a\u5e38\u51e0\u4e2a\u7ba1\u7406\u4eba\u5458\u51e0\u4e2a\u7ba1\u7406\u7528\u6237 \u4fee\u6539\u5185\u6838\u53c2\u6570 \u5b89\u88c5\u57fa\u7840\u8f6f\u4ef6\uff0cgpu\u9a71\u52a8 \u5b89\u88c5docker \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6 \u5b89\u88c5kubernetes \u5b89\u88c5\u6307\u5b9akubeadm\u7248\u672c \u4ee5\u4e0a\u521d\u59cb\u5316\u53ef\u4ee5\u901a\u8fc7\u8dd1ansible\u6765\u5b9e\u73b0\uff0c\u4e0b\u9762\u5177\u4f53\u62c6\u5206\u6765\u914d\u7f6e\u4e00\u4e0b \u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u662f\u4ee5\u6700\u65b0\u7684ubuntu22.04 \u4e3a\u4f8b\u5b50\u6765\u6f14\u793a \u66f4\u6362\u56fd\u5185\u6e90 \u00b6 ubuntu22.04 \u6e05\u534e\u6e90 \u9700\u8981\u6ce8\u610f\u4e00\u4e0b\uff0c\u5982\u679capt update \u62a5\u9519\uff0c\u5c31\u5c06https\u6539\u6210http \u5982\u679c\u9700\u8981\u6dfb\u52a0\u5176\u4ed6\u7684\u7248\u672c\u7684\u6e90\u53ef\u4ee5\u8bbf\u95ee: https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse \u5b89\u5168 \u00b6 \u4e94: \u7cfb\u7edf\u5b89\u5168 \u00b6 \u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002 5.1: \u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09 \u00b6 \u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e 5.2: \u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668 \u00b6 \u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 ) \u5f53\u7136\u4e86\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e0b\u8fd9\u6837\u6dfb\u52a0\u81ea\u5df1\u7684\u516c\u94a5 curl https://openbayes.com/api/users/lixie/keys.txt >> authorized_keys \u7981\u6b62\u7528\u6237\u5bc6\u7801\u767b\u9646 \u4e3a\u4e86\u5b89\u5168\u7684\u8003\u8651\uff0c\u6211\u4eec\u9700\u8981\u5173\u95ed\u7528\u6237\u5bc6\u7801\u767b\u9646\u7684\u8fd9\u79cd\u65b9\u5f0f PubkeyAuthentication yes # \u542f\u7528\u516c\u544a\u5bc6\u94a5\u914d\u5bf9\u8ba4\u8bc1\u65b9\u5f0f RSAAuthentication yes # \u5141\u8bb8RSA\u5bc6\u94a5 PasswordAuthentication no # \u7981\u6b62\u5bc6\u7801\u9a8c\u8bc1\u767b\u5f55,\u5982\u679c\u542f\u7528\u7684\u8bdd,RSA\u8ba4\u8bc1\u767b\u5f55\u5c31\u6ca1\u6709\u610f\u4e49\u4e86 PermitRootLogin no # \u7981\u7528root\u8d26\u6237\u767b\u5f55\uff0c\u975e\u5fc5\u8981\uff0c\u4f46\u4e3a\u4e86\u5b89\u5168\u6027\uff0c\u8bf7\u914d\u7f6e \u8fd9\u6837\u7ed3\u5408\u4e0a\u4e00\u6b65\u9aa4\uff0c\u5173\u95ed\u7528\u6237\u8d26\u53f7\u5bc6\u7801\u9a8c\u8bc1\u65b9\u5f0f\uff0c\u53ea\u91c7\u7528\u5bc6\u94a5\u5bf9\u4f1a\u5b89\u5168\u5f88\u591a\u3002 SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/ Ubuntu \u7cfb\u7edf\u7ba1\u7406 && \u5b89\u88c5\u53ca\u7ba1\u7406\u7a0b\u5e8f: \u00b6 dpkg \u5305\u5b89\u88c5 \u00b6 \uff081\uff09\u683c\u5f0f \u00b6 dpkg [\u9009\u9879] \u5305\u6587\u4ef6 \uff082\uff09\u7528\u6cd5 \u00b6 \u53c2\u6570 Description - i \u5b89\u88c5 deb \u8f6f\u4ef6\u5305 - r \u5220\u9664 deb \u8f6f\u4ef6\u5305 -r --purge \u8fde\u540c\u914d\u7f6e\u6587\u4ef6\u4e00\u8d77\u5220\u9664 -l \u67e5\u770b\u7cfb\u7edf\u4e2d\u5df2\u5b89\u88c5\u8f6f\u4ef6\u5305\u4fe1\u606f -p \u5378\u8f7d\u8f6f\u4ef6\u5305\u53ca\u5176\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u4f9d\u8d56\u5173\u7cfb \uff083\uff09\u8f85\u52a9\u9009\u9879 \u00b6 --force-all \u5f3a\u5236\u5b89\u88c5\u4e00\u4e2a\u5305(\u5ffd\u7565\u4f9d\u8d56\u53ca\u5176\u5b83\u95ee\u9898) --no-install-recommends \u53c2\u6570\u6765\u907f\u514d\u5b89\u88c5\u975e\u5fc5\u987b\u7684\u6587\u4ef6\uff0c\u4ece\u800c\u51cf\u5c0f\u955c\u50cf\u7684\u4f53\u79ef apt \u5305\u5b89\u88c5 \u5378\u8f7d \u00b6 \uff081\uff09\u683c\u5f0f \u00b6 apt [options] [command] [package ...] \uff082\uff09\u7528\u6cd5 \u00b6 apt install -y package_name //\u5b89\u88c5 apt remove package_name //\u5378\u8f7d apt update //\u5217\u51fa\u6240\u6709\u53ef\u66f4\u65b0\u7684\u8f6f\u4ef6\u6e05\u5355\u547d\u4ee4 apt update \u66f4\u65b0\u62a5\u9519 root@node2:/etc/apt# apt update Reading package lists... Done E: Could not get lock /var/lib/apt/lists/lock. It is held by process 27056 ( apt-get ) N: Be aware that removing the lock file is not a solution and may break your system. E: Unable to lock directory /var/lib/apt/lists/ \u8fd9\u4e2a\u4e3b\u8981\u7684\u539f\u56e0\u662f\u6709\u522b\u7684\u8fdb\u7a0b\u5360\u7528apt\u8fd9\u4e2a\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e0b\u65b9\u6cd5\u8fdb\u884c\u6392\u67e5 ps aux | grep -i apt //\u8fc7\u6ee4\u51fa\u6765apt\u8fdb\u7a0b\uff0c\u5982\u679c\u6ca1\u6709\u7528\u53ef\u4ee5kill\u6389\uff0c\u6216\u8005\u7b49\u5f85\u8fdb\u7a0b\u7ed3\u675f \uff083\uff09\u6848\u4f8b \u00b6 // \u8fc7\u6ee4\u51fa\u6765\u4ee5rc\u5f00\u5934\u548cnvidia\u7684\u5305\u5e76\u5378\u8f7d dpkg -l | grep nvidia | grep \"^rc\" | awk '{print $2}' | grep -E 'nvidia' | xargs dpkg --purge dpkg -l | grep nvidia | grep \"^ii\" | awk '{print $2}' | grep -E '^nvidia' | xargs dpkg --force-all -r dpkg -l | grep nvidia | awk '{print $2}' | xargs apt remove -y dpkg -l | grep nvidia | awk '{print $2}' | xargs apt purge -y ubuntu \u5173\u673a \u00b6 echo b > /proc/sysrq-trigger \u5b98\u7f51: \u53c2\u8003\u5730\u5740\u94fe\u63a5 Linux\u7cfb\u7edf\u7ba1\u740607-\u6587\u4ef6\u7cfb\u7edf\u4e0eLVM \u00b6 inode \u77e5\u8bc6\u70b9\u8865\u5145 \u00b6 \u5f53\u6211\u4eec\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u5076\u7136\u4f1a\u9047\u5230\u4e00\u4e9b\u7279\u6b8a\u683c\u5f0f\u7684\u6587\u4ef6\u6216\u8005\u76ee\u5f55\uff0c\u901a\u8fc7\u4f7f\u7528rm \u662f\u65e0\u6cd5\u76f4\u63a5\u5220\u9664\u7684\uff0c\u8fd9\u65f6\u53ef\u4ee5\u5229\u7528inode\u53f7\u5220\u9664\u6587\u4ef6\u6216\u8005\u76ee\u5f55\u3002 [ cka ] root@node1:/# mkdir /share [ cka ] root@node1:/# ll -i //\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6587\u4ef6\u7684inode\u4e3a805024 total 2097232 805024 drwxr-xr-x 2 root root 4096 Nov 11 17 :22 share/ 655362 drwxr-xr-x 6 root root 4096 Feb 23 2022 snap/ find . -inum inode\u53f7 -delete // \u6839\u636einode\u6765\u5220\u9664\u8be5\u76ee\u5f55 \u9644\u4ef6: \u00b6 \u6587\u7ae0\u5730\u5740: ubuntu\u5b89\u88c5\u53c2\u8003:","title":"Linux \u7cfb\u7edf\u7ba1\u7406"},{"location":"ubuntu/1-system-docs/#_1","text":"\u6b63\u6240\u8c13\u4e0d\u4f1a\u88c5\u7cfb\u7edf\u7684\u8fd0\u7ef4\u5c31\u4e0d\u662f\u597d\u8fd0\u7ef4\u7684\u7406\u5ff5\uff0c\u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0bubuntu\u7cfb\u7edf\u5b89\u88c5 \u51c6\u5907\u5de5\u4f5c \u6b65\u9aa4\u4e00: \u4e0b\u8f7diso\u955c\u50cf \u4e0b\u8f7d\u5730\u5740: https://mirrors.aliyun.com/ubuntu-releases/ \u6b65\u9aa4\u4e8c: \u5236\u4f5c\u7cfb\u7edf\u76d8 \u53ef\u4ee5\u53c2\u8003\u4f7f\u7528\u6280\u5de7\u4e2d\u7684Mac\u5236\u4f5c\u7cfb\u7edf\u76d8\u8fd9\u7bc7\u6587\u7ae0 \u6b65\u9aa4\u4e09: \u88c5\u5c31\u5b8c\u4e8b\u4e86 1.1. \u9009\u62e9\u8bed\u8a00 1.2. \u9009\u62e9\u952e\u76d8\uff08\u672c\u6b65\u9aa4\u76f4\u63a5\u9ed8\u8ba4\u6309\u56de\u8f66\u5373\u53ef\u3002\uff09 1.3. \u914d\u7f6e\u7f51\u7edc\uff08\u4e00\u822c\u60c5\u51b5\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e00\u6b65\uff09 1.4. \u9009\u62e9\u4ee3\u7406\uff08\u9ed8\u8ba4\u56de\u8f66\u8df3\u8fc7\uff09 1.5. \u914d\u7f6e\u955c\u50cf\u6e90\uff08\u8df3\u8fc7\uff09 1.6. \u9009\u62e9\u78c1\u76d8\uff08\u8fd9\u4e2a\u6b65\u9aa4\u6bd4\u8f83\u5173\u952e\uff09 \u9009\u62e9\u78c1\u76d8\u8fd9\u4e00\u6b65\u9700\u8981\u6ce8\u610f\uff0c\u9700\u8981\u6240\u6709\u78c1\u76d8\u7a7a\u95f4\u5206\u7ed9\u6839\u5206\u533a 1.7. \u7528\u6237\u4fe1\u606f 1.8. openssh server \u5207\u8bb0\u8981\u9009\u62e9\u4e0a ( \u5207\u8bb0 )","title":"\u7cfb\u7edf\u5b89\u88c5"},{"location":"ubuntu/1-system-docs/#_2","text":"\u521d\u59cb\u5316\u6b65\u9aa4 \u6dfb\u52a0hosts\u4fe1\u606f \u4fee\u6539\u56fd\u5185apt\u6e90 \u6e05\u534e\u6e90 \u963f\u91cc\u6e90 \u6dfb\u52a0\u7ba1\u7406\u5458\u7528\u6237 \u901a\u5e38\u51e0\u4e2a\u7ba1\u7406\u4eba\u5458\u51e0\u4e2a\u7ba1\u7406\u7528\u6237 \u4fee\u6539\u5185\u6838\u53c2\u6570 \u5b89\u88c5\u57fa\u7840\u8f6f\u4ef6\uff0cgpu\u9a71\u52a8 \u5b89\u88c5docker \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6 \u5b89\u88c5kubernetes \u5b89\u88c5\u6307\u5b9akubeadm\u7248\u672c \u4ee5\u4e0a\u521d\u59cb\u5316\u53ef\u4ee5\u901a\u8fc7\u8dd1ansible\u6765\u5b9e\u73b0\uff0c\u4e0b\u9762\u5177\u4f53\u62c6\u5206\u6765\u914d\u7f6e\u4e00\u4e0b \u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u662f\u4ee5\u6700\u65b0\u7684ubuntu22.04 \u4e3a\u4f8b\u5b50\u6765\u6f14\u793a","title":"\u7cfb\u7edf\u521d\u59cb\u5316"},{"location":"ubuntu/1-system-docs/#_3","text":"ubuntu22.04 \u6e05\u534e\u6e90 \u9700\u8981\u6ce8\u610f\u4e00\u4e0b\uff0c\u5982\u679capt update \u62a5\u9519\uff0c\u5c31\u5c06https\u6539\u6210http \u5982\u679c\u9700\u8981\u6dfb\u52a0\u5176\u4ed6\u7684\u7248\u672c\u7684\u6e90\u53ef\u4ee5\u8bbf\u95ee: https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse","title":"\u66f4\u6362\u56fd\u5185\u6e90"},{"location":"ubuntu/1-system-docs/#_4","text":"","title":"\u5b89\u5168"},{"location":"ubuntu/1-system-docs/#_5","text":"\u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002","title":"\u4e94: \u7cfb\u7edf\u5b89\u5168"},{"location":"ubuntu/1-system-docs/#51-root-centosubuntu","text":"\u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e","title":"5.1: \u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09"},{"location":"ubuntu/1-system-docs/#52","text":"\u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 ) \u5f53\u7136\u4e86\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e0b\u8fd9\u6837\u6dfb\u52a0\u81ea\u5df1\u7684\u516c\u94a5 curl https://openbayes.com/api/users/lixie/keys.txt >> authorized_keys \u7981\u6b62\u7528\u6237\u5bc6\u7801\u767b\u9646 \u4e3a\u4e86\u5b89\u5168\u7684\u8003\u8651\uff0c\u6211\u4eec\u9700\u8981\u5173\u95ed\u7528\u6237\u5bc6\u7801\u767b\u9646\u7684\u8fd9\u79cd\u65b9\u5f0f PubkeyAuthentication yes # \u542f\u7528\u516c\u544a\u5bc6\u94a5\u914d\u5bf9\u8ba4\u8bc1\u65b9\u5f0f RSAAuthentication yes # \u5141\u8bb8RSA\u5bc6\u94a5 PasswordAuthentication no # \u7981\u6b62\u5bc6\u7801\u9a8c\u8bc1\u767b\u5f55,\u5982\u679c\u542f\u7528\u7684\u8bdd,RSA\u8ba4\u8bc1\u767b\u5f55\u5c31\u6ca1\u6709\u610f\u4e49\u4e86 PermitRootLogin no # \u7981\u7528root\u8d26\u6237\u767b\u5f55\uff0c\u975e\u5fc5\u8981\uff0c\u4f46\u4e3a\u4e86\u5b89\u5168\u6027\uff0c\u8bf7\u914d\u7f6e \u8fd9\u6837\u7ed3\u5408\u4e0a\u4e00\u6b65\u9aa4\uff0c\u5173\u95ed\u7528\u6237\u8d26\u53f7\u5bc6\u7801\u9a8c\u8bc1\u65b9\u5f0f\uff0c\u53ea\u91c7\u7528\u5bc6\u94a5\u5bf9\u4f1a\u5b89\u5168\u5f88\u591a\u3002 SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/","title":"5.2: \u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668"},{"location":"ubuntu/1-system-docs/#ubuntu","text":"","title":"Ubuntu \u7cfb\u7edf\u7ba1\u7406 &amp;&amp; \u5b89\u88c5\u53ca\u7ba1\u7406\u7a0b\u5e8f:"},{"location":"ubuntu/1-system-docs/#dpkg","text":"","title":"dpkg \u5305\u5b89\u88c5"},{"location":"ubuntu/1-system-docs/#1","text":"dpkg [\u9009\u9879] \u5305\u6587\u4ef6","title":"\uff081\uff09\u683c\u5f0f"},{"location":"ubuntu/1-system-docs/#2","text":"\u53c2\u6570 Description - i \u5b89\u88c5 deb \u8f6f\u4ef6\u5305 - r \u5220\u9664 deb \u8f6f\u4ef6\u5305 -r --purge \u8fde\u540c\u914d\u7f6e\u6587\u4ef6\u4e00\u8d77\u5220\u9664 -l \u67e5\u770b\u7cfb\u7edf\u4e2d\u5df2\u5b89\u88c5\u8f6f\u4ef6\u5305\u4fe1\u606f -p \u5378\u8f7d\u8f6f\u4ef6\u5305\u53ca\u5176\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u4f9d\u8d56\u5173\u7cfb","title":"\uff082\uff09\u7528\u6cd5"},{"location":"ubuntu/1-system-docs/#3","text":"--force-all \u5f3a\u5236\u5b89\u88c5\u4e00\u4e2a\u5305(\u5ffd\u7565\u4f9d\u8d56\u53ca\u5176\u5b83\u95ee\u9898) --no-install-recommends \u53c2\u6570\u6765\u907f\u514d\u5b89\u88c5\u975e\u5fc5\u987b\u7684\u6587\u4ef6\uff0c\u4ece\u800c\u51cf\u5c0f\u955c\u50cf\u7684\u4f53\u79ef","title":"\uff083\uff09\u8f85\u52a9\u9009\u9879"},{"location":"ubuntu/1-system-docs/#apt","text":"","title":"apt \u5305\u5b89\u88c5 \u5378\u8f7d"},{"location":"ubuntu/1-system-docs/#1_1","text":"apt [options] [command] [package ...]","title":"\uff081\uff09\u683c\u5f0f"},{"location":"ubuntu/1-system-docs/#2_1","text":"apt install -y package_name //\u5b89\u88c5 apt remove package_name //\u5378\u8f7d apt update //\u5217\u51fa\u6240\u6709\u53ef\u66f4\u65b0\u7684\u8f6f\u4ef6\u6e05\u5355\u547d\u4ee4 apt update \u66f4\u65b0\u62a5\u9519 root@node2:/etc/apt# apt update Reading package lists... Done E: Could not get lock /var/lib/apt/lists/lock. It is held by process 27056 ( apt-get ) N: Be aware that removing the lock file is not a solution and may break your system. E: Unable to lock directory /var/lib/apt/lists/ \u8fd9\u4e2a\u4e3b\u8981\u7684\u539f\u56e0\u662f\u6709\u522b\u7684\u8fdb\u7a0b\u5360\u7528apt\u8fd9\u4e2a\u8fdb\u7a0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e0b\u65b9\u6cd5\u8fdb\u884c\u6392\u67e5 ps aux | grep -i apt //\u8fc7\u6ee4\u51fa\u6765apt\u8fdb\u7a0b\uff0c\u5982\u679c\u6ca1\u6709\u7528\u53ef\u4ee5kill\u6389\uff0c\u6216\u8005\u7b49\u5f85\u8fdb\u7a0b\u7ed3\u675f","title":"\uff082\uff09\u7528\u6cd5"},{"location":"ubuntu/1-system-docs/#3_1","text":"// \u8fc7\u6ee4\u51fa\u6765\u4ee5rc\u5f00\u5934\u548cnvidia\u7684\u5305\u5e76\u5378\u8f7d dpkg -l | grep nvidia | grep \"^rc\" | awk '{print $2}' | grep -E 'nvidia' | xargs dpkg --purge dpkg -l | grep nvidia | grep \"^ii\" | awk '{print $2}' | grep -E '^nvidia' | xargs dpkg --force-all -r dpkg -l | grep nvidia | awk '{print $2}' | xargs apt remove -y dpkg -l | grep nvidia | awk '{print $2}' | xargs apt purge -y","title":"\uff083\uff09\u6848\u4f8b"},{"location":"ubuntu/1-system-docs/#ubuntu_1","text":"echo b > /proc/sysrq-trigger \u5b98\u7f51: \u53c2\u8003\u5730\u5740\u94fe\u63a5","title":"ubuntu \u5173\u673a"},{"location":"ubuntu/1-system-docs/#linux07-lvm","text":"","title":"Linux\u7cfb\u7edf\u7ba1\u740607-\u6587\u4ef6\u7cfb\u7edf\u4e0eLVM"},{"location":"ubuntu/1-system-docs/#inode","text":"\u5f53\u6211\u4eec\u5728Linux\u7cfb\u7edf\u4e2d\uff0c\u5076\u7136\u4f1a\u9047\u5230\u4e00\u4e9b\u7279\u6b8a\u683c\u5f0f\u7684\u6587\u4ef6\u6216\u8005\u76ee\u5f55\uff0c\u901a\u8fc7\u4f7f\u7528rm \u662f\u65e0\u6cd5\u76f4\u63a5\u5220\u9664\u7684\uff0c\u8fd9\u65f6\u53ef\u4ee5\u5229\u7528inode\u53f7\u5220\u9664\u6587\u4ef6\u6216\u8005\u76ee\u5f55\u3002 [ cka ] root@node1:/# mkdir /share [ cka ] root@node1:/# ll -i //\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6587\u4ef6\u7684inode\u4e3a805024 total 2097232 805024 drwxr-xr-x 2 root root 4096 Nov 11 17 :22 share/ 655362 drwxr-xr-x 6 root root 4096 Feb 23 2022 snap/ find . -inum inode\u53f7 -delete // \u6839\u636einode\u6765\u5220\u9664\u8be5\u76ee\u5f55","title":"inode \u77e5\u8bc6\u70b9\u8865\u5145"},{"location":"ubuntu/1-system-docs/#_6","text":"\u6587\u7ae0\u5730\u5740: ubuntu\u5b89\u88c5\u53c2\u8003:","title":"\u9644\u4ef6:"},{"location":"ubuntu/ubuntu-bmc/","text":"\u670d\u52a1\u5668BMC\uff08\u5e26\u5916\uff09 \u00b6 \u7b80\u4ecb \u00b6 \u670d\u52a1\u5668\u9664\u4e86\u88c5linux\uff0cwindows\u7cfb\u7edf\u5916\uff0c\u76f8\u5e94\u8fd8\u6709\u4e00\u4e2a\u53ef\u901a\u8fc7\u7f51\u7ebf\uff08\u670d\u52a1\u5668\u9ed8\u8ba4\u5e26\u5916\u5730\u5740--\u53ef\u6539\uff09\u8fde\u63a5\u5177\u4f53\u5382\u5546\u670d\u52a1\u5668\u7684BMC\uff08Baseboard Management Controller\uff0c\u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668\uff09 \u667a\u80fd\u5e73\u53f0\u7ba1\u7406\u63a5\u53e3 (IPMI) \u662f\u4e00\u79cd\u5f00\u653e\u6807\u51c6\u7684\u786c\u4ef6\u7ba1\u7406\u63a5\u53e3\u89c4\u683c\uff0c\u5b9a\u4e49\u4e86\u5d4c\u5165\u5f0f\u7ba1\u7406\u5b50\u7cfb\u7edf\u8fdb\u884c\u901a\u4fe1\u7684\u7279\u5b9a\u65b9\u6cd5\u3002IPMI \u4fe1\u606f\u901a\u8fc7\u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668 (BMC)\uff08\u4f4d\u4e8e IPMI \u89c4\u683c\u7684\u786c\u4ef6\u7ec4\u4ef6\u4e0a\uff09\u8fdb\u884c\u4ea4\u6d41\u3002\u4f7f\u7528\u4f4e\u7ea7\u786c\u4ef6\u667a\u80fd\u7ba1\u7406\u800c\u4e0d\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u884c\u7ba1\u7406\uff0c\u5177\u6709\u4e24\u4e2a\u4e3b\u8981\u4f18\u70b9\uff1a \u9996\u5148\uff0c\u6b64\u914d\u7f6e\u5141\u8bb8\u8fdb\u884c\u5e26\u5916\u670d\u52a1\u5668\u7ba1\u7406\uff1b\u5176\u6b21\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u5fc5\u8d1f\u62c5\u4f20\u8f93\u7cfb\u7edf\u72b6\u6001\u6570\u636e\u7684\u4efb\u52a1\u3002\u4e00\u822c\u7edf\u79f0Mgmt\u7ba1\u7406\u7f51\u53e3\uff0c\u534e\u4e3a\u7684\u767d\u76ae\u4e66\u53ebiBMC\uff0c\u6234\u5c14\u53ebidrac\uff0c\u5176\u5b9e\u90fd\u662f\u517c\u5bb9ipmi\u534f\u8bae\u7684\u7f51\u53e3\u800c\u5df2~ BMC\u7cfb\u7edf\u72ec\u7acb\uff0c\u7ba1\u7406\u786c\u4ef6\uff08cpu\uff0c\u98ce\u6247\u7b49\u4fe1\u606f\uff09\uff0c\u6253\u5f00\u63a7\u5236\u53f0,\u6765\u8fdc\u7a0b\u7ba1\u7406\u6211\u4eec\u7684\u670d\u52a1\u5668\uff0c\u8ba9\u8fd0\u7ef4\u7684\u540c\u5b66\u5c11\u8dd1\u4e00\u4e07\u6b21\u7684\u673a\u623f\uff5e \u5b9e\u8df5 \u00b6 \u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0b\u6211\u8fd9\u8fb9\u66d9\u5149GPU\u670d\u52a1\u5668\u7684\u5e26\u5916\u7ba1\u7406\u3002 $ ssh -L 3443 :192.168.2.x:443 routerx.c1 \u8fd9\u4e2a\u6211\u4eec\u662f\u7528\u53e6\u4e00\u53f0\u673a\u5668\u505a\u8df3\u677f\u624d\u80fd\u767b\u9646\uff0c\u6240\u4ee5\u9700\u8981\u6267\u884c\u4ee5\u4e0a\u547d\u4ee4\uff5e \u81ea\u5e26\u6d4f\u89c8\u8bbf\u95ee: https://127.0.0.1:3443 \u9700\u8981\u8f93\u5165\u5e10\u53f7\u5bc6\u7801 \u901a\u8fc7\u4ee5\u4e0a\u7684\u65b9\u5f0f,\u5c31\u53ef\u4ee5\u5bf9\u670d\u52a1\u5668\u8fdb\u884c\u91cd\u542f,\u5173\u673a,\u91cd\u88c5\u7cfb\u7edf\u7b49\u64cd\u4f5c~ \u9644\u4ef6 \u00b6 Linux\u914d\u7f6e apt-get install ipmitool ipmitool lan print # \u67e5\u770bBMC\u7684\u5730\u5740 ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr 192 .168.2.21 ipmitool lan set 1 netmask 255 .255.255.0 ipmitool lan set 1 defgw ipaddr 192 .168.2.1","title":"Linux \u5e26\u5916\u7ba1\u7406"},{"location":"ubuntu/ubuntu-bmc/#bmc","text":"","title":"\u670d\u52a1\u5668BMC\uff08\u5e26\u5916\uff09"},{"location":"ubuntu/ubuntu-bmc/#_1","text":"\u670d\u52a1\u5668\u9664\u4e86\u88c5linux\uff0cwindows\u7cfb\u7edf\u5916\uff0c\u76f8\u5e94\u8fd8\u6709\u4e00\u4e2a\u53ef\u901a\u8fc7\u7f51\u7ebf\uff08\u670d\u52a1\u5668\u9ed8\u8ba4\u5e26\u5916\u5730\u5740--\u53ef\u6539\uff09\u8fde\u63a5\u5177\u4f53\u5382\u5546\u670d\u52a1\u5668\u7684BMC\uff08Baseboard Management Controller\uff0c\u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668\uff09 \u667a\u80fd\u5e73\u53f0\u7ba1\u7406\u63a5\u53e3 (IPMI) \u662f\u4e00\u79cd\u5f00\u653e\u6807\u51c6\u7684\u786c\u4ef6\u7ba1\u7406\u63a5\u53e3\u89c4\u683c\uff0c\u5b9a\u4e49\u4e86\u5d4c\u5165\u5f0f\u7ba1\u7406\u5b50\u7cfb\u7edf\u8fdb\u884c\u901a\u4fe1\u7684\u7279\u5b9a\u65b9\u6cd5\u3002IPMI \u4fe1\u606f\u901a\u8fc7\u57fa\u677f\u7ba1\u7406\u63a7\u5236\u5668 (BMC)\uff08\u4f4d\u4e8e IPMI \u89c4\u683c\u7684\u786c\u4ef6\u7ec4\u4ef6\u4e0a\uff09\u8fdb\u884c\u4ea4\u6d41\u3002\u4f7f\u7528\u4f4e\u7ea7\u786c\u4ef6\u667a\u80fd\u7ba1\u7406\u800c\u4e0d\u4f7f\u7528\u64cd\u4f5c\u7cfb\u7edf\u8fdb\u884c\u7ba1\u7406\uff0c\u5177\u6709\u4e24\u4e2a\u4e3b\u8981\u4f18\u70b9\uff1a \u9996\u5148\uff0c\u6b64\u914d\u7f6e\u5141\u8bb8\u8fdb\u884c\u5e26\u5916\u670d\u52a1\u5668\u7ba1\u7406\uff1b\u5176\u6b21\uff0c\u64cd\u4f5c\u7cfb\u7edf\u4e0d\u5fc5\u8d1f\u62c5\u4f20\u8f93\u7cfb\u7edf\u72b6\u6001\u6570\u636e\u7684\u4efb\u52a1\u3002\u4e00\u822c\u7edf\u79f0Mgmt\u7ba1\u7406\u7f51\u53e3\uff0c\u534e\u4e3a\u7684\u767d\u76ae\u4e66\u53ebiBMC\uff0c\u6234\u5c14\u53ebidrac\uff0c\u5176\u5b9e\u90fd\u662f\u517c\u5bb9ipmi\u534f\u8bae\u7684\u7f51\u53e3\u800c\u5df2~ BMC\u7cfb\u7edf\u72ec\u7acb\uff0c\u7ba1\u7406\u786c\u4ef6\uff08cpu\uff0c\u98ce\u6247\u7b49\u4fe1\u606f\uff09\uff0c\u6253\u5f00\u63a7\u5236\u53f0,\u6765\u8fdc\u7a0b\u7ba1\u7406\u6211\u4eec\u7684\u670d\u52a1\u5668\uff0c\u8ba9\u8fd0\u7ef4\u7684\u540c\u5b66\u5c11\u8dd1\u4e00\u4e07\u6b21\u7684\u673a\u623f\uff5e","title":"\u7b80\u4ecb"},{"location":"ubuntu/ubuntu-bmc/#_2","text":"\u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0b\u6211\u8fd9\u8fb9\u66d9\u5149GPU\u670d\u52a1\u5668\u7684\u5e26\u5916\u7ba1\u7406\u3002 $ ssh -L 3443 :192.168.2.x:443 routerx.c1 \u8fd9\u4e2a\u6211\u4eec\u662f\u7528\u53e6\u4e00\u53f0\u673a\u5668\u505a\u8df3\u677f\u624d\u80fd\u767b\u9646\uff0c\u6240\u4ee5\u9700\u8981\u6267\u884c\u4ee5\u4e0a\u547d\u4ee4\uff5e \u81ea\u5e26\u6d4f\u89c8\u8bbf\u95ee: https://127.0.0.1:3443 \u9700\u8981\u8f93\u5165\u5e10\u53f7\u5bc6\u7801 \u901a\u8fc7\u4ee5\u4e0a\u7684\u65b9\u5f0f,\u5c31\u53ef\u4ee5\u5bf9\u670d\u52a1\u5668\u8fdb\u884c\u91cd\u542f,\u5173\u673a,\u91cd\u88c5\u7cfb\u7edf\u7b49\u64cd\u4f5c~","title":"\u5b9e\u8df5"},{"location":"ubuntu/ubuntu-bmc/#_3","text":"Linux\u914d\u7f6e apt-get install ipmitool ipmitool lan print # \u67e5\u770bBMC\u7684\u5730\u5740 ipmitool lan set 1 ipsrc static ipmitool lan set 1 ipaddr 192 .168.2.21 ipmitool lan set 1 netmask 255 .255.255.0 ipmitool lan set 1 defgw ipaddr 192 .168.2.1","title":"\u9644\u4ef6"},{"location":"ubuntu/ubuntu-system/","text":"\u7cfb\u7edf\u5b89\u88c5 \u00b6 \u6b63\u6240\u8c13\u4e0d\u4f1a\u88c5\u7cfb\u7edf\u7684\u8fd0\u7ef4\u5c31\u4e0d\u662f\u597d\u8fd0\u7ef4\u7684\u7406\u5ff5\uff0c\u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0bubuntu\u7cfb\u7edf\u5b89\u88c5 \u51c6\u5907\u5de5\u4f5c \u6b65\u9aa4\u4e00: \u4e0b\u8f7diso\u955c\u50cf \u4e0b\u8f7d\u5730\u5740: https://mirrors.aliyun.com/ubuntu-releases/ \u6b65\u9aa4\u4e8c: \u5236\u4f5c\u7cfb\u7edf\u76d8 \u53ef\u4ee5\u53c2\u8003\u4f7f\u7528\u6280\u5de7\u4e2d\u7684Mac\u5236\u4f5c\u7cfb\u7edf\u76d8\u8fd9\u7bc7\u6587\u7ae0 \u6b65\u9aa4\u4e09: \u88c5\u5c31\u5b8c\u4e8b\u4e86 1.1. \u9009\u62e9\u8bed\u8a00 1.2. \u9009\u62e9\u952e\u76d8\uff08\u672c\u6b65\u9aa4\u76f4\u63a5\u9ed8\u8ba4\u6309\u56de\u8f66\u5373\u53ef\u3002\uff09 1.3. \u914d\u7f6e\u7f51\u7edc\uff08\u4e00\u822c\u60c5\u51b5\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e00\u6b65\uff09 1.4. \u9009\u62e9\u4ee3\u7406\uff08\u9ed8\u8ba4\u56de\u8f66\u8df3\u8fc7\uff09 1.5. \u914d\u7f6e\u955c\u50cf\u6e90\uff08\u8df3\u8fc7\uff09 1.6. \u9009\u62e9\u78c1\u76d8\uff08\u8fd9\u4e2a\u6b65\u9aa4\u6bd4\u8f83\u5173\u952e\uff09 \u9009\u62e9\u78c1\u76d8\u8fd9\u4e00\u6b65\u9700\u8981\u6ce8\u610f\uff0c\u9700\u8981\u6240\u6709\u78c1\u76d8\u7a7a\u95f4\u5206\u7ed9\u6839\u5206\u533a 1.7. \u7528\u6237\u4fe1\u606f 1.8. openssh server \u5207\u8bb0\u8981\u9009\u62e9\u4e0a ( \u5207\u8bb0 ) \u7cfb\u7edf\u521d\u59cb\u5316 \u00b6 \u521d\u59cb\u5316\u6b65\u9aa4 \u6dfb\u52a0hosts\u4fe1\u606f \u4fee\u6539\u56fd\u5185apt\u6e90 \u6e05\u534e\u6e90 \u963f\u91cc\u6e90 \u6dfb\u52a0\u7ba1\u7406\u5458\u7528\u6237 \u901a\u5e38\u51e0\u4e2a\u7ba1\u7406\u4eba\u5458\u51e0\u4e2a\u7ba1\u7406\u7528\u6237 \u4fee\u6539\u5185\u6838\u53c2\u6570 \u5b89\u88c5\u57fa\u7840\u8f6f\u4ef6\uff0cgpu\u9a71\u52a8 \u5b89\u88c5docker \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6 \u5b89\u88c5kubernetes \u5b89\u88c5\u6307\u5b9akubeadm\u7248\u672c \u4ee5\u4e0a\u521d\u59cb\u5316\u53ef\u4ee5\u901a\u8fc7\u8dd1ansible\u6765\u5b9e\u73b0\uff0c\u4e0b\u9762\u5177\u4f53\u62c6\u5206\u6765\u914d\u7f6e\u4e00\u4e0b \u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u662f\u4ee5\u6700\u65b0\u7684ubuntu22.04 \u4e3a\u4f8b\u5b50\u6765\u6f14\u793a \u66f4\u6362\u56fd\u5185\u6e90 \u00b6 ubuntu22.04 \u6e05\u534e\u6e90 \u9700\u8981\u6ce8\u610f\u4e00\u4e0b\uff0c\u5982\u679capt update \u62a5\u9519\uff0c\u5c31\u5c06https\u6539\u6210http \u5982\u679c\u9700\u8981\u6dfb\u52a0\u5176\u4ed6\u7684\u7248\u672c\u7684\u6e90\u53ef\u4ee5\u8bbf\u95ee: https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse \u5b89\u5168 \u00b6 \u4e94: \u7cfb\u7edf\u5b89\u5168 \u00b6 \u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002 5.1: \u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09 \u00b6 \u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e 5.2: \u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668 \u00b6 \u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 ) \u7981\u6b62\u7528\u6237\u5bc6\u7801\u767b\u9646 \u4e3a\u4e86\u5b89\u5168\u7684\u8003\u8651\uff0c\u6211\u4eec\u9700\u8981\u5173\u95ed\u7528\u6237\u5bc6\u7801\u767b\u9646\u7684\u8fd9\u79cd\u65b9\u5f0f PubkeyAuthentication yes # \u542f\u7528\u516c\u544a\u5bc6\u94a5\u914d\u5bf9\u8ba4\u8bc1\u65b9\u5f0f RSAAuthentication yes # \u5141\u8bb8RSA\u5bc6\u94a5 PasswordAuthentication no # \u7981\u6b62\u5bc6\u7801\u9a8c\u8bc1\u767b\u5f55,\u5982\u679c\u542f\u7528\u7684\u8bdd,RSA\u8ba4\u8bc1\u767b\u5f55\u5c31\u6ca1\u6709\u610f\u4e49\u4e86 PermitRootLogin no # \u7981\u7528root\u8d26\u6237\u767b\u5f55\uff0c\u975e\u5fc5\u8981\uff0c\u4f46\u4e3a\u4e86\u5b89\u5168\u6027\uff0c\u8bf7\u914d\u7f6e \u8fd9\u6837\u7ed3\u5408\u4e0a\u4e00\u6b65\u9aa4\uff0c\u5173\u95ed\u7528\u6237\u8d26\u53f7\u5bc6\u7801\u9a8c\u8bc1\u65b9\u5f0f\uff0c\u53ea\u91c7\u7528\u5bc6\u94a5\u5bf9\u4f1a\u5b89\u5168\u5f88\u591a\u3002 SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/ Ubuntu \u7cfb\u7edf\u7ba1\u7406 && \u5b89\u88c5\u53ca\u7ba1\u7406\u7a0b\u5e8f: \u00b6 dpkg \u5305\u5b89\u88c5 \u00b6 \uff081\uff09\u683c\u5f0f \u00b6 dpkg [\u9009\u9879] \u5305\u6587\u4ef6 \uff082\uff09\u7528\u6cd5 \u00b6 \u53c2\u6570 Description - i \u5b89\u88c5 deb \u8f6f\u4ef6\u5305 - r \u5220\u9664 deb \u8f6f\u4ef6\u5305 -r --purge \u8fde\u540c\u914d\u7f6e\u6587\u4ef6\u4e00\u8d77\u5220\u9664 -l \u67e5\u770b\u7cfb\u7edf\u4e2d\u5df2\u5b89\u88c5\u8f6f\u4ef6\u5305\u4fe1\u606f -p \u5378\u8f7d\u8f6f\u4ef6\u5305\u53ca\u5176\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u4f9d\u8d56\u5173\u7cfb \uff083\uff09\u8f85\u52a9\u9009\u9879 \u00b6 --force-all \u5f3a\u5236\u5b89\u88c5\u4e00\u4e2a\u5305(\u5ffd\u7565\u4f9d\u8d56\u53ca\u5176\u5b83\u95ee\u9898) apt \u5305\u5b89\u88c5 \u5378\u8f7d \u00b6 \uff081\uff09\u683c\u5f0f \u00b6 apt [options] [command] [package ...] \uff082\uff09\u7528\u6cd5 \u00b6 apt install -y package_name //\u5b89\u88c5 apt remove package_name //\u5378\u8f7d apt update //\u5217\u51fa\u6240\u6709\u53ef\u66f4\u65b0\u7684\u8f6f\u4ef6\u6e05\u5355\u547d\u4ee4 \uff083\uff09\u6848\u4f8b \u00b6 // \u8fc7\u6ee4\u51fa\u6765\u4ee5rc\u5f00\u5934\u548cnvidia\u7684\u5305\u5e76\u5378\u8f7d dpkg -l | grep nvidia | grep \"^rc\" | awk '{print $2}' | grep -E 'nvidia' | xargs dpkg --purge dpkg -l | grep nvidia | grep \"^ii\" | awk '{print $2}' | grep -E '^nvidia' | xargs dpkg --force-all -r \u9644\u4ef6: \u00b6 \u6587\u7ae0\u5730\u5740: ubuntu\u5b89\u88c5\u53c2\u8003:","title":"Ubuntu system"},{"location":"ubuntu/ubuntu-system/#_1","text":"\u6b63\u6240\u8c13\u4e0d\u4f1a\u88c5\u7cfb\u7edf\u7684\u8fd0\u7ef4\u5c31\u4e0d\u662f\u597d\u8fd0\u7ef4\u7684\u7406\u5ff5\uff0c\u4e0b\u9762\u4ecb\u7ecd\u4e00\u4e0bubuntu\u7cfb\u7edf\u5b89\u88c5 \u51c6\u5907\u5de5\u4f5c \u6b65\u9aa4\u4e00: \u4e0b\u8f7diso\u955c\u50cf \u4e0b\u8f7d\u5730\u5740: https://mirrors.aliyun.com/ubuntu-releases/ \u6b65\u9aa4\u4e8c: \u5236\u4f5c\u7cfb\u7edf\u76d8 \u53ef\u4ee5\u53c2\u8003\u4f7f\u7528\u6280\u5de7\u4e2d\u7684Mac\u5236\u4f5c\u7cfb\u7edf\u76d8\u8fd9\u7bc7\u6587\u7ae0 \u6b65\u9aa4\u4e09: \u88c5\u5c31\u5b8c\u4e8b\u4e86 1.1. \u9009\u62e9\u8bed\u8a00 1.2. \u9009\u62e9\u952e\u76d8\uff08\u672c\u6b65\u9aa4\u76f4\u63a5\u9ed8\u8ba4\u6309\u56de\u8f66\u5373\u53ef\u3002\uff09 1.3. \u914d\u7f6e\u7f51\u7edc\uff08\u4e00\u822c\u60c5\u51b5\u4f1a\u76f4\u63a5\u8df3\u8fc7\u8fd9\u4e00\u6b65\uff09 1.4. \u9009\u62e9\u4ee3\u7406\uff08\u9ed8\u8ba4\u56de\u8f66\u8df3\u8fc7\uff09 1.5. \u914d\u7f6e\u955c\u50cf\u6e90\uff08\u8df3\u8fc7\uff09 1.6. \u9009\u62e9\u78c1\u76d8\uff08\u8fd9\u4e2a\u6b65\u9aa4\u6bd4\u8f83\u5173\u952e\uff09 \u9009\u62e9\u78c1\u76d8\u8fd9\u4e00\u6b65\u9700\u8981\u6ce8\u610f\uff0c\u9700\u8981\u6240\u6709\u78c1\u76d8\u7a7a\u95f4\u5206\u7ed9\u6839\u5206\u533a 1.7. \u7528\u6237\u4fe1\u606f 1.8. openssh server \u5207\u8bb0\u8981\u9009\u62e9\u4e0a ( \u5207\u8bb0 )","title":"\u7cfb\u7edf\u5b89\u88c5"},{"location":"ubuntu/ubuntu-system/#_2","text":"\u521d\u59cb\u5316\u6b65\u9aa4 \u6dfb\u52a0hosts\u4fe1\u606f \u4fee\u6539\u56fd\u5185apt\u6e90 \u6e05\u534e\u6e90 \u963f\u91cc\u6e90 \u6dfb\u52a0\u7ba1\u7406\u5458\u7528\u6237 \u901a\u5e38\u51e0\u4e2a\u7ba1\u7406\u4eba\u5458\u51e0\u4e2a\u7ba1\u7406\u7528\u6237 \u4fee\u6539\u5185\u6838\u53c2\u6570 \u5b89\u88c5\u57fa\u7840\u8f6f\u4ef6\uff0cgpu\u9a71\u52a8 \u5b89\u88c5docker \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u65f6 \u5b89\u88c5kubernetes \u5b89\u88c5\u6307\u5b9akubeadm\u7248\u672c \u4ee5\u4e0a\u521d\u59cb\u5316\u53ef\u4ee5\u901a\u8fc7\u8dd1ansible\u6765\u5b9e\u73b0\uff0c\u4e0b\u9762\u5177\u4f53\u62c6\u5206\u6765\u914d\u7f6e\u4e00\u4e0b \u6e29\u99a8\u63d0\u793a \u4ee5\u4e0b\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u662f\u4ee5\u6700\u65b0\u7684ubuntu22.04 \u4e3a\u4f8b\u5b50\u6765\u6f14\u793a","title":"\u7cfb\u7edf\u521d\u59cb\u5316"},{"location":"ubuntu/ubuntu-system/#_3","text":"ubuntu22.04 \u6e05\u534e\u6e90 \u9700\u8981\u6ce8\u610f\u4e00\u4e0b\uff0c\u5982\u679capt update \u62a5\u9519\uff0c\u5c31\u5c06https\u6539\u6210http \u5982\u679c\u9700\u8981\u6dfb\u52a0\u5176\u4ed6\u7684\u7248\u672c\u7684\u6e90\u53ef\u4ee5\u8bbf\u95ee: https://mirrors.tuna.tsinghua.edu.cn/help/ubuntu/ # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-updates main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-backports main restricted universe multiverse deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-security main restricted universe multiverse # \u9884\u53d1\u5e03\u8f6f\u4ef6\u6e90\uff0c\u4e0d\u5efa\u8bae\u542f\u7528 # deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse # deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ jammy-proposed main restricted universe multiverse","title":"\u66f4\u6362\u56fd\u5185\u6e90"},{"location":"ubuntu/ubuntu-system/#_4","text":"","title":"\u5b89\u5168"},{"location":"ubuntu/ubuntu-system/#_5","text":"\u901a\u5e38\u5728\u4f01\u4e1a\u4e2d\uff0c\u670d\u52a1\u5668\u4f1a\u906d\u53d7\u5916\u6765\u7684\u5f88\u591a\u7684\u6076\u610f\u653b\u51fb\uff0c\u90a3\u4e48\u670d\u52a1\u5668\u7684\u5b89\u5168\u5c31\u663e\u5f97\u683c\u5916\u7684\u91cd\u8981\u3002\u9996\u5148\u80af\u5b9a\u60f3\u5230\u7684\u662f\u670d\u52a1\u5668\u7684\u5e10\u53f7\u548c\u5bc6\u7801\u7ba1\u7406\uff0c\u901a\u5e38\u7684\u60c5\u51b5\u4e0b\u4f1a\u7981\u6b62root\u8fd9\u6837\u7684\u7ba1\u7406\u5458\u7528\u6237\u767b\u9646\uff0c\u4e5f\u4f1a\u7981\u6b62\u5bc6\u7801\u8fd9\u6837\u7684\u65b9\u5f0f\u767b\u9646\u3002 \u539f\u56e0: root\u7528\u6237\u7684\u6743\u9650\u592a\u9ad8\uff0c\u5982\u679c\u4e00\u65e6\u5e10\u53f7\u5bc6\u7801\u6cc4\u6f0f\uff0c\u5c31\u4f1a\u9020\u6210\u5f88\u4e25\u91cd\u7684\u540e\u679c\u3002 \u7981\u6b62\u5bc6\u7801\u65b9\u5f0f\u767b\u9646\u4e5f\u662f\u4e3a\u4e86\u5b89\u5168\u8003\u8651\uff0c\u6bd5\u7adf\u5bc6\u7801\u4e22\u5931\u4e5f\u662f\u5f88\u5e73\u5e38\u7684\u4e8b\u60c5\u3002\u63a8\u8350\u4f7f\u7528\u516c\u94a5\u7684\u65b9\u5f0f\u6765\u767b\u9646\u670d\u52a1\u5668\u3002","title":"\u4e94: \u7cfb\u7edf\u5b89\u5168"},{"location":"ubuntu/ubuntu-system/#51-root-centosubuntu","text":"\u53ef\u4ee5\u4fee\u6539 /etc/ssh/sshd_config \u914d\u7f6e\u6587\u4ef6 \u6dfb\u52a0: PermitRootLogin yes \u914d\u7f6e\uff08\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u5b8c\u6210\u521d\u59cb\u5316\u5c31\u7981\u6b62root\u767b\u9646\u4e86\uff09 yes \u4e3a\u5141\u8bb8root\u767b\u9646 no \u4e3a\u7981\u6b62root\u767b\u9646 \u91cd\u65b0\u542f\u52a8sshd\u670d\u52a1\u3002 systemctl restart sshd \u5f53\u7136\u4e5f\u53ef\u4ee5\u52a0\u5165\u7cfb\u7edf\u521d\u59cb\u5316\u6b65\u9aa4\u4e2d\uff0c\u7565\uff5e","title":"5.1: \u7981\u6b62root\u7528\u6237: \uff08centos/ubuntu\u90fd\u9002\u7528\uff09"},{"location":"ubuntu/ubuntu-system/#52","text":"\u751f\u6210\u516c\u94a5\u548c\u79c1\u94a5 root@user:~# ssh-keygen //\u4e00\u8def\u56de\u8f66 Generating public/private rsa key pair. Enter file in which to save the key ( /root/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa Your public key has been saved in /root/.ssh/id_rsa.pub The key fingerprint is: SHA256:J0s/ZHIRTj/UCcDQLHtxd5Qa0p3r2CYlcz7lPS7VaXU root@user The key ' s randomart image is: +--- [ RSA 3072 ] ----+ | . = +.+o.o+ | | .o == .o++. | | ooo+.o.. | | . .. = +E | | S. = X.B | | . X o @+ | | . o * o | | . . . | | . | +---- [ SHA256 ] -----+ \u8fd9\u4e2a\u65f6\u5019\u5728.ssh\u76ee\u5f55\u4e0b\u751f\u6210\u51e0\u4e2a\u6587\u4ef6 root@user:~# ll .ssh/ total 16 drwx------ 2 root root 4096 Sep 20 09 :46 ./ drwx------ 5 root root 4096 Sep 20 09 :35 ../ -rw------- 1 root root 0 May 24 15 :30 authorized_keys // \u8fd9\u4e2a\u662f\u6388\u6743\u6587\u4ef6 -rw------- 1 root root 2590 Sep 20 09 :46 id_rsa // \u8fd9\u4e2a\u662f\u79c1\u94a5\u6587\u4ef6 -rw-r--r-- 1 root root 563 Sep 20 09 :46 id_rsa.pub //\u8fd9\u4e2a\u662f\u516c\u94a5\u6587\u4ef6 \u5c06\u516c\u94a5\u52a0\u5165user\u7528\u6237\u4e0b: .ssh/authorized_keys root@user:/home/user# ls -a .ssh/ . .. authorized_keys root@user:/home/user# cat .ssh/authorized_keys ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDwWSc73tyq4TAkXxt3rWmGggbpgdm+egc8mOSDu0hauuvPdieIe1qUbKsIKC1O93KyDPlsfP5gcwqdEmf5Di0S6CCxRh6ENyZ9mtN+s1pCDeHiKbjhPyG4o71tafIDOjhcbpEtCwPA0YTrp5i1oO466qYHeFmTCmkcDFhuEKZx78EZdTwbFH0vhOGTymLFgUVauzmd45ZxpTzaZHrd093nFHWg6FeZWk2axkDiijLALNxiAAaECn2S69y5SxXgKSqpe4Z25b2cKKySlM1lBv1eI7CSxAUoxuXSpcgoRiVUx5VgJwkixKvq8NpihYEkV5pFRjB8W0ssu1YF6d+3MlzOkwa+kir9JJlLq+F/rrBTfF2mCLBgg0KE+voDd8vjEkqSmweNs2gEO7Gi/fUEfcabNAOuNNPL2dhdFl+BH2TCofDYvZcWd8Wrl/0qoW5nbUdCaC7aznb0lpVgseB/gj6ah3adCzfA/W8S+1znD9VMHDdMNy+AN8eeQQ6d2t05SOc = \u8bdd\u4e0d\u591a\u8bf4\u6d4b\u8bd5\u767b\u9646 $ ssh user@172.30.42.244 //\u8fd9\u662f\u6211\u4eec\u4f7f\u7528user\u7528\u6237\u767b\u9646\uff0c\u5c31\u4e0d\u9700\u8981\u5bc6\u7801\u4e86 Welcome to Ubuntu 20 .04.4 LTS ( GNU/Linux 5 .4.0-113-generic x86_64 ) \u7981\u6b62\u7528\u6237\u5bc6\u7801\u767b\u9646 \u4e3a\u4e86\u5b89\u5168\u7684\u8003\u8651\uff0c\u6211\u4eec\u9700\u8981\u5173\u95ed\u7528\u6237\u5bc6\u7801\u767b\u9646\u7684\u8fd9\u79cd\u65b9\u5f0f PubkeyAuthentication yes # \u542f\u7528\u516c\u544a\u5bc6\u94a5\u914d\u5bf9\u8ba4\u8bc1\u65b9\u5f0f RSAAuthentication yes # \u5141\u8bb8RSA\u5bc6\u94a5 PasswordAuthentication no # \u7981\u6b62\u5bc6\u7801\u9a8c\u8bc1\u767b\u5f55,\u5982\u679c\u542f\u7528\u7684\u8bdd,RSA\u8ba4\u8bc1\u767b\u5f55\u5c31\u6ca1\u6709\u610f\u4e49\u4e86 PermitRootLogin no # \u7981\u7528root\u8d26\u6237\u767b\u5f55\uff0c\u975e\u5fc5\u8981\uff0c\u4f46\u4e3a\u4e86\u5b89\u5168\u6027\uff0c\u8bf7\u914d\u7f6e \u8fd9\u6837\u7ed3\u5408\u4e0a\u4e00\u6b65\u9aa4\uff0c\u5173\u95ed\u7528\u6237\u8d26\u53f7\u5bc6\u7801\u9a8c\u8bc1\u65b9\u5f0f\uff0c\u53ea\u91c7\u7528\u5bc6\u94a5\u5bf9\u4f1a\u5b89\u5168\u5f88\u591a\u3002 SSH \u65e0\u6cd5\u767b\u9646 \u53ef\u4ee5ping\u901a\u4f46\u65e0\u6cd5ssh ssh -v ip \u65e0\u660e\u663e\u62a5\u9519 \u8003\u8651\u662f\u5426\u662f\u670d\u52a1\u7aef\u7981\u6b62\u5ba2\u6237\u7aef \u5728/etc/hosts.allow\u6587\u4ef6\u4e2d\u52a0\u4e0a sshd: ALL \uff0c\u91cd\u542fsshd \u4fee\u6539\u7f51\u5361\u914d\u7f6e root@ubuntu:/home/ubuntu# cat /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens18: addresses: - 192.168.1.114/24 gateway4: 192.168.1.1 nameservers: addresses: - 192.168.1.1 search: - 202.106.46.151 version: 2 \u6e05\u9664\u5185\u6838\u7f13\u5b58 https://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/","title":"5.2: \u5bc6\u94a5\u5bf9\u6765\u767b\u9646\u670d\u52a1\u5668"},{"location":"ubuntu/ubuntu-system/#ubuntu","text":"","title":"Ubuntu \u7cfb\u7edf\u7ba1\u7406 &amp;&amp; \u5b89\u88c5\u53ca\u7ba1\u7406\u7a0b\u5e8f:"},{"location":"ubuntu/ubuntu-system/#dpkg","text":"","title":"dpkg \u5305\u5b89\u88c5"},{"location":"ubuntu/ubuntu-system/#1","text":"dpkg [\u9009\u9879] \u5305\u6587\u4ef6","title":"\uff081\uff09\u683c\u5f0f"},{"location":"ubuntu/ubuntu-system/#2","text":"\u53c2\u6570 Description - i \u5b89\u88c5 deb \u8f6f\u4ef6\u5305 - r \u5220\u9664 deb \u8f6f\u4ef6\u5305 -r --purge \u8fde\u540c\u914d\u7f6e\u6587\u4ef6\u4e00\u8d77\u5220\u9664 -l \u67e5\u770b\u7cfb\u7edf\u4e2d\u5df2\u5b89\u88c5\u8f6f\u4ef6\u5305\u4fe1\u606f -p \u5378\u8f7d\u8f6f\u4ef6\u5305\u53ca\u5176\u914d\u7f6e\u6587\u4ef6\uff0c\u4f46\u65e0\u6cd5\u89e3\u51b3\u4f9d\u8d56\u5173\u7cfb","title":"\uff082\uff09\u7528\u6cd5"},{"location":"ubuntu/ubuntu-system/#3","text":"--force-all \u5f3a\u5236\u5b89\u88c5\u4e00\u4e2a\u5305(\u5ffd\u7565\u4f9d\u8d56\u53ca\u5176\u5b83\u95ee\u9898)","title":"\uff083\uff09\u8f85\u52a9\u9009\u9879"},{"location":"ubuntu/ubuntu-system/#apt","text":"","title":"apt \u5305\u5b89\u88c5 \u5378\u8f7d"},{"location":"ubuntu/ubuntu-system/#1_1","text":"apt [options] [command] [package ...]","title":"\uff081\uff09\u683c\u5f0f"},{"location":"ubuntu/ubuntu-system/#2_1","text":"apt install -y package_name //\u5b89\u88c5 apt remove package_name //\u5378\u8f7d apt update //\u5217\u51fa\u6240\u6709\u53ef\u66f4\u65b0\u7684\u8f6f\u4ef6\u6e05\u5355\u547d\u4ee4","title":"\uff082\uff09\u7528\u6cd5"},{"location":"ubuntu/ubuntu-system/#3_1","text":"// \u8fc7\u6ee4\u51fa\u6765\u4ee5rc\u5f00\u5934\u548cnvidia\u7684\u5305\u5e76\u5378\u8f7d dpkg -l | grep nvidia | grep \"^rc\" | awk '{print $2}' | grep -E 'nvidia' | xargs dpkg --purge dpkg -l | grep nvidia | grep \"^ii\" | awk '{print $2}' | grep -E '^nvidia' | xargs dpkg --force-all -r","title":"\uff083\uff09\u6848\u4f8b"},{"location":"ubuntu/ubuntu-system/#_6","text":"\u6587\u7ae0\u5730\u5740: ubuntu\u5b89\u88c5\u53c2\u8003:","title":"\u9644\u4ef6:"},{"location":"ubuntu/network/1-network-docs/","text":"1.1 \u67e5\u770b\u53ca\u6d4b\u8bd5\u7f51\u7edc \u00b6 \u67e5\u770b\u53ca\u6d4b\u8bd5\u7f51\u7edc\u914d\u7f6e\u662f\u7ba1\u7406 Linux \u7f51\u7edc\u670d\u52a1\u7684\u7b2c\u4e00\u6b65\uff0c\u672c\u8282\u5c06\u5b66\u4e60 Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684 \u7f51\u7edc\u67e5\u770b\u53ca\u6d4b\u8bd5\u547d\u4ee4\u3002\u5176\u4e2d\u8bb2\u89e3\u7684\u5927\u591a\u6570\u547d\u4ee4\u4ee5\u666e\u901a\u7528\u6237\u6743\u9650\u5c31\u53ef\u4ee5\u5b8c\u6210\u64cd\u4f5c\uff0c\u4f46\u666e\u901a\u7528\u6237 \u5728\u6267\u884c/sbin/\u76ee\u5f55\u4e2d\u7684\u547d\u4ee4\u65f6\u9700\u8981\u6307\u5b9a\u547d\u4ee4\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\u3002 1.1.1 \u67e5\u770b\u7f51\u7edc\u914d\u7f6e \u00b6 1. \u67e5\u770b\u7f51\u7edc\u63a5\u53e3\u5730\u5740 \u00b6 \uff081\uff09\u67e5\u770b\u6d3b\u52a8\u7684\u7f51\u7edc\u63a5\u53e3\u8bbe\u5907 \u82e5\u91c7\u7528 mini \u7248 CentOS 7 \u5b89\u88c5\u7684\u7cfb\u7edf\uff0c\u9ed8\u8ba4\u662f\u6ca1\u6709 ifconfig \u547d\u4ee4\u7684\uff0c\u9700\u8981\u5148\u901a\u8fc7 yum \u65b9\u5f0f\u5b89\u88c5 net-tools \u8f6f\u4ef6\u5305\uff0c\u624d\u6709 ifconfig \u547d\u4ee4\u3002\u5728\u4e0d\u5e26\u4efb\u4f55\u9009\u9879\u548c\u53c2\u6570\u6267\u884c ifconfig \u547d\u4ee4\u65f6\uff0c \u5c06\u663e\u793a\u5f53\u524d\u4e3b\u673a\u4e2d\u5df2\u542f\u7528\uff08\u6d3b\u52a8\uff09\u7684\u7f51\u7edc\u63a5\u53e3\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u76f4\u63a5\u6267\u884c ifconfig \u547d\u4ee4\u540e\u53ef\u4ee5\u770b \u5230 ens33\u3001lo \u8fd9\u4e24\u4e2a\u7f51\u7edc\u63a5\u53e3\u7684\u4fe1\u606f\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a [ root@VM-16-9-centos ] # ifconfig docker0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 172 .17.0.1 netmask 255 .255.0.0 broadcast 172 .17.255.255 ether 02 :42:7c:2d:55:50 txqueuelen 0 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .0.16.9 netmask 255 .255.252.0 broadcast 10 .0.19.255 inet6 fe80::5054:ff:fe16:3a9 prefixlen 64 scopeid 0x20<link> ether 52 :54:00:16:03:a9 txqueuelen 1000 ( Ethernet ) RX packets 51791902 bytes 9684145526 ( 9 .0 GiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 49095710 bytes 7697835155 ( 7 .1 GiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 14076 bytes 1715488 ( 1 .6 MiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14076 bytes 1715488 ( 1 .6 MiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 \uff082\uff09\u67e5\u770b\u6307\u5b9a\u7684\u7f51\u7edc\u63a5\u53e3\u4fe1\u606f \u5f53\u53ea\u9700\u8981\u67e5\u770b\u5176\u4e2d\u67d0\u4e00\u4e2a\u7f51\u7edc\u63a5\u53e3\u7684\u4fe1\u606f\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u7f51\u7edc\u63a5\u53e3\u7684\u540d\u79f0\u4f5c\u4e3a ifconfig \u547d \u4ee4\u7684\u53c2\u6570\uff08\u4e0d\u8bba\u8be5\u7f51\u7edc\u63a5\u53e3\u662f\u5426\u5904\u4e8e\u6fc0\u6d3b\u72b6\u6001\uff09\u3002\u4f8b\u5982\uff0c\u6267\u884c\u201cifconfig ens33\u201d\u547d\u4ee4\u540e\u53ef\u4ee5 \u53ea\u67e5\u770b\u7f51\u5361 ens33 \u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a [ root@VM-16-9-centos ~ ] # ifconfig eth0 eth0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .0.16.9 netmask 255 .255.252.0 broadcast 10 .0.19.255 inet6 fe80::5054:ff:fe16:3a9 prefixlen 64 scopeid 0x20<link> ether 52 :54:00:16:03:a9 txqueuelen 1000 ( Ethernet ) RX packets 51792964 bytes 9684253645 ( 9 .0 GiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 49096762 bytes 7698009938 ( 7 .1 GiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 \u4ece\u4e0a\u8ff0\u547d\u4ee4\u663e\u793a\u7684\u7ed3\u679c\u4e2d\uff0c\u53ef\u4ee5\u83b7\u77e5 ens33 \u7f51\u5361\u7684\u4e00\u4e9b\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u4e0b\u6240\u8ff0\u3002 inet\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684 IP \u5730\u5740\uff0c\u5982\u201c192.168.4.11\u201d\u3002 netmask\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684\u5b50\u7f51\u63a9\u7801\uff0c\u5982\u201c255.255.255.0\u201d\u3002 broadcast\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u6240\u5728\u7f51\u7edc\u7684\u5e7f\u64ad\u5730\u5740\uff0c\u5982\u201c192.168.4.255\u201d\u3002 ether\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684\u7269\u7406\u5730\u5740\uff08MAC \u5730\u5740\uff09\uff0c\u5982\u201c00:0c:29:3a:81:cc\u201d\u3002\u7f51\u7edc\u63a5 \u53e3\u7684\u7269\u7406\u5730\u5740\u901a\u5e38\u4e0d\u80fd\u66f4\u6539\uff0c\u662f\u7f51\u5361\u5728\u751f\u4ea7\u65f6\u786e\u5b9a\u7684\u5168\u7403\u552f\u4e00\u7684\u786c\u4ef6\u5730\u5740\u3002 \u9664\u6b64\u4ee5\u5916\uff0c\u8fd8\u80fd\u591f\u901a\u8fc7\u201cTX\u201d\u548c\u201cRX\u201d\u7b49\u4fe1\u606f\u4e86\u89e3\u901a\u8fc7\u8be5\u7f51\u7edc\u63a5\u53e3\u53d1\u9001\u548c\u63a5\u6536\u7684\u6570\u636e\u5305\u4e2a \u6570\u3001\u6d41\u91cf\u7b49\u66f4\u591a\u5c5e\u6027\u3002 2. \u67e5\u770b\u4e3b\u673a\u540d\u79f0 \u00b6 \u5728 Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u76f8\u5f53\u4e00\u90e8\u5206\u7f51\u7edc\u670d\u52a1\u90fd\u4f1a\u901a\u8fc7\u4e3b\u673a\u540d\u6765\u8bc6\u522b\u4e3b\u673a\uff0c\u5982\u679c\u4e3b\u673a\u540d\u914d \u7f6e\u4e0d\u5f53\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u529f\u80fd\u51fa\u73b0\u6545\u969c\u3002\u4f7f\u7528 hostname \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u4e3b\u673a\u7684\u4e3b\u673a\u540d\uff0c \u4e0d\u7528\u6dfb\u52a0\u4efb\u4f55\u9009\u9879\u6216\u53c2\u6570\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a(\u53e6\u5916\u9664\u4e86\u67e5\u770b\u4e3b\u673a\u540d\u8fd8\u53ef\u4ee5\u4f7f\u7528-i/-I\u53c2\u6570\u6765\u67e5\u770bip) [ root@VM-16-9-centos ~ ] # hostname VM-16-9-centos [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # hostname -i ::1 127 .0.0.1 [ root@VM-16-9-centos ~ ] # hostname -I 10 .0.16.9 172 .17.0.1 3. \u67e5\u770b\u8def\u7531\u8868\u6761\u76ee \u00b6 Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u8def\u7531\u8868\u51b3\u5b9a\u7740\u4ece\u672c\u673a\u5411\u5176\u4ed6\u4e3b\u673a\u3001\u5176\u4ed6\u7f51\u7edc\u53d1\u9001\u6570\u636e\u7684\u53bb\u5411\uff0c\u662f\u6392 \u9664\u7f51\u7edc\u6545\u969c\u7684\u5173\u952e\u4fe1\u606f\u3002\u76f4\u63a5\u6267\u884c\u201croute\u201d\u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u4e3b\u673a\u4e2d\u7684\u8def\u7531\u8868\u4fe1\u606f\uff0c\u5728\u8f93\u51fa\u7ed3 \u679c\u4e2d\uff0cDestination \u5217\u5bf9\u5e94\u76ee\u6807\u7f51\u6bb5\u7684\u5730\u5740\uff0cGateway \u5217\u5bf9\u5e94\u4e0b\u4e00\u8df3\u8def\u7531\u5668\u7684\u5730\u5740\uff0cIface \u5217 \u5bf9\u5e94\u53d1\u9001\u6570\u636e\u7684\u7f51\u7edc\u63a5\u53e3\u3002\u82e5\u7ed3\u5408\u201c-n\u201d\u9009\u9879\u4f7f\u7528\uff0c\u53ef\u4ee5\u5c06\u8def\u7531\u8bb0\u5f55\u4e2d\u7684\u5730\u5740\u663e\u793a\u4e3a\u6570\u5b57\u5f62\u5f0f\uff0c\u8fd9\u53ef\u4ee5\u8df3\u8fc7\u89e3\u6790\u4e3b\u673a \u540d\u7684\u8fc7\u7a0b\uff0c\u5728\u8def\u7531\u8868\u6761\u76ee\u8f83\u591a\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u52a0\u5feb\u6267\u884c\u901f\u5ea6\u3002 [ root@VM-16-9-centos ~ ] # route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default gateway 0 .0.0.0 UG 0 0 0 eth0 10 .0.16.0 0 .0.0.0 255 .255.252.0 U 0 0 0 eth0 link-local 0 .0.0.0 255 .255.0.0 U 1002 0 0 eth0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 10 .0.16.1 0 .0.0.0 UG 0 0 0 eth0 10 .0.16.0 0 .0.0.0 255 .255.252.0 U 0 0 0 eth0 169 .254.0.0 0 .0.0.0 255 .255.0.0 U 1002 0 0 eth0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 4. \u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u60c5\u51b5 \u00b6 \u901a\u8fc7 netstat \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u64cd\u4f5c\u7cfb\u7edf\u7684\u7f51\u7edc\u8fde\u63a5\u72b6\u6001\u3001\u8def\u7531\u8868\u3001\u63a5\u53e3\u7edf\u8ba1\u7b49\u4fe1\u606f\uff0c \u5b83\u662f\u4e86\u89e3\u7f51\u7edc\u72b6\u6001\u53ca\u6392\u9664\u7f51\u7edc\u670d\u52a1\u6545\u969c\u7684\u6709\u6548\u5de5\u5177\u3002\u4ee5\u4e0b\u662f netstat \u547d\u4ee4\u5e38\u7528\u7684\u51e0\u4e2a\u9009\u9879 -a\uff1a\u663e\u793a\u4e3b\u673a\u4e2d\u6240\u6709\u6d3b\u52a8\u7684\u7f51\u7edc\u8fde\u63a5\u4fe1\u606f\uff08\u5305\u62ec\u76d1\u542c\u3001\u975e\u76d1\u542c\u72b6\u6001\u7684\u670d\u52a1\u7aef\u53e3\uff09 -n\uff1a\u4ee5\u6570\u5b57\u7684\u5f62\u5f0f\u663e\u793a\u76f8\u5173\u7684\u4e3b\u673a\u5730\u5740\u3001\u7aef\u53e3\u7b49\u4fe1\u606f\u3002 -r\uff1a\u663e\u793a\u8def\u7531\u8868\u4fe1\u606f\u3002 -l\uff1a\u663e\u793a\u5904\u4e8e\u76d1\u542c\uff08Listening\uff09\u72b6\u6001\u7684\u7f51\u7edc\u8fde\u63a5\u53ca\u7aef\u53e3\u4fe1\u606f\u3002 -t\uff1a\u67e5\u770b TCP\uff08Transmission Control Protocol\uff0c\u4f20\u8f93\u63a7\u5236\u534f\u8bae\uff09\u76f8\u5173\u7684\u4fe1\u606f -u\uff1a\u663e\u793a UDP\uff08User Datagram Protocol\uff0c\u7528\u6237\u6570\u636e\u62a5\u534f\u8bae\uff09\u534f\u8bae\u76f8\u5173\u7684\u4fe1\u606f\u3002 -p\uff1a\u663e\u793a\u4e0e\u7f51\u7edc\u8fde\u63a5\u76f8\u5173\u8054\u7684\u8fdb\u7a0b\u53f7\u3001\u8fdb\u7a0b\u540d\u79f0\u4fe1\u606f\uff08\u8be5\u9009\u9879\u9700\u8981 root \u6743\u9650\uff09\u3002 \u901a\u5e38\u4f7f\u7528\u201c-anpt\u201d\u7ec4\u5408\u9009\u9879\uff0c\u4ee5\u6570\u5b57\u5f62\u5f0f\u663e\u793a\u5f53\u524d\u7cfb\u7edf\u4e2d\u6240\u6709\u7684 TCP \u8fde\u63a5\u4fe1\u606f\uff0c\u540c\u65f6\u663e \u793a\u5bf9\u5e94\u7684\u8fdb\u7a0b\u4fe1\u606f\u3002\u7ed3\u5408\u7ba1\u9053\u547d\u4ee4\u4f7f\u7528\u201cgrep\u201d\u547d\u4ee4\uff0c\u8fd8\u53ef\u4ee5\u5728\u7ed3\u679c\u4e2d\u8fc7\u6ee4\u51fa\u6240\u9700\u8981\u7684\u7279\u5b9a\u8bb0 \u5f55\u3002\u4f8b\u5982\uff0c\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u53ef\u4ee5\u67e5\u770b\u672c\u673a\u4e2d\u662f\u5426\u6709\u76d1\u542c\u201cTCP 22\u201d\u7aef\u53e3\uff08\u5373\u6807\u51c6 Web \u670d\u52a1\uff09\u7684 \u670d\u52a1\u7a0b\u5e8f\uff0c\u8f93\u51fa\u4fe1\u606f\u4e2d\u5305\u62ec PID \u53f7\u548c\u8fdb\u7a0b\u540d\u79f0\u3002 [ root@VM-16-9-centos ~ ] # netstat -antp |grep 22 tcp 0 0 0 .0.0.0:22 0 .0.0.0:* LISTEN 4344 /sshd \u9664\u4e86 netstat\uff0css \u547d\u4ee4\u4e5f\u53ef\u4ee5\u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u60c5\u51b5\uff0c\u5b83\u662f Socket Statistics \u7684\u7f29\u5199\uff0c\u4e3b\u8981 \u7528\u4e8e\u83b7\u53d6 socket \u7edf\u8ba1\u4fe1\u606f\uff0c\u5b83\u53ef\u4ee5\u663e\u793a\u548c netstat \u547d\u4ee4\u7c7b\u4f3c\u7684\u8f93\u51fa\u5185\u5bb9\u3002\u4f46 ss \u7684\u4f18\u52bf\u5728\u4e8e \u5b83\u80fd\u591f\u663e\u793a\u66f4\u591a\u66f4\u8be6\u7ec6\u7684\u6709\u5173 TCP \u548c\u8fde\u63a5\u72b6\u6001\u7684\u4fe1\u606f\uff0c\u800c\u4e14\u6bd4 netstat \u66f4\u5feb\u901f\u66f4\u9ad8\u6548\u3002\u8981\u60f3 \u4f7f\u7528 ss \u547d\u4ee4\uff0c\u9996\u5148\u786e\u4fdd iproute \u7a0b\u5e8f\u5305\u5df2\u88ab\u5b89\u88c5\uff0c\u53ef\u4ee5\u901a\u8fc7 yum \u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\u3002(\u7528\u6cd5\u7c7b\u4f3c) [ root@VM-16-9-centos ~ ] # ss -antp State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* users: (( \"sshd\" ,pid = 4344 ,fd = 3 )) LISTEN 0 100 127 .0.0.1:25 *:* 1.1.2 \u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5 \u00b6 \u7528\u6237\u8bbf\u95ee\u7f51\u7edc\u670d\u52a1\u7684\u524d\u63d0\u662f\u7f51\u7edc\u8fde\u63a5\u5904\u4e8e\u6b63\u5e38\u72b6\u6001\u3002\u82e5\u7f51\u7edc\u8fde\u63a5\u4e0d\u7a33\u5b9a\uff0c\u751a\u81f3\u65e0\u6cd5\u8fde\u63a5\uff0c \u7528\u6237\u5219\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u670d\u52a1\u3002\u56e0\u6b64\uff0c\u5f53\u7f51\u7edc\u8fde\u63a5\u51fa\u73b0\u95ee\u9898\u65f6\uff0c\u9700\u8981\u901a\u8fc7\u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5\u7684\u547d \u4ee4\u6765\u786e\u5b9a\u6545\u969c\u70b9\u3002\u4e0b\u9762\u4ecb\u7ecd\u51e0\u4e2a\u5e38\u7528\u7684\u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5\u7684\u547d\u4ee4\u3002 1. \u6d4b\u8bd5\u7f51\u7edc\u8fde\u901a\u6027 \u00b6 [ root@VM-16-9-centos ~ ] # ping baidu.com 2. \u6d4b\u8bd5DNS\u89e3\u6790 \u00b6 [ root@VM-16-9-centos ~ ] # nslookup www.google.com Server: 183 .60.83.19 Address: 183 .60.83.19#53 Non-authoritative answer: Name: www.google.com Address: 174 .37.54.20 Name: www.google.com Address: 2001 ::68f4:2e15 [ root@VM-16-9-centos ~ ] # dig www.google.com ; <<>> DiG 9 .11.4-P2-RedHat-9.11.4-26.P2.el7_9.7 <<>> www.google.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 44445 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 0 ;; QUESTION SECTION: ; www.google.com. IN A ;; ANSWER SECTION: www.google.com. 133 IN A 199 .59.149.206 ;; Query time: 1 msec ;; SERVER: 183 .60.83.19#53 ( 183 .60.83.19 ) ;; WHEN: \u4e94 11\u6708 18 15 :51:47 CST 2022 ;; MSG SIZE rcvd: 48 3. iperf \u7f51\u7edc\u6d4b\u8bd5 \u00b6 server: iperf -s -p 1234 -i 1 clent: iperf -c 172 .16.240.204 -p 1234 -i 1 -t 20 -w 20w","title":"Linux \u7f51\u7edc\u670d\u52a1"},{"location":"ubuntu/network/1-network-docs/#11","text":"\u67e5\u770b\u53ca\u6d4b\u8bd5\u7f51\u7edc\u914d\u7f6e\u662f\u7ba1\u7406 Linux \u7f51\u7edc\u670d\u52a1\u7684\u7b2c\u4e00\u6b65\uff0c\u672c\u8282\u5c06\u5b66\u4e60 Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684 \u7f51\u7edc\u67e5\u770b\u53ca\u6d4b\u8bd5\u547d\u4ee4\u3002\u5176\u4e2d\u8bb2\u89e3\u7684\u5927\u591a\u6570\u547d\u4ee4\u4ee5\u666e\u901a\u7528\u6237\u6743\u9650\u5c31\u53ef\u4ee5\u5b8c\u6210\u64cd\u4f5c\uff0c\u4f46\u666e\u901a\u7528\u6237 \u5728\u6267\u884c/sbin/\u76ee\u5f55\u4e2d\u7684\u547d\u4ee4\u65f6\u9700\u8981\u6307\u5b9a\u547d\u4ee4\u6587\u4ef6\u7684\u7edd\u5bf9\u8def\u5f84\u3002","title":"1.1 \u67e5\u770b\u53ca\u6d4b\u8bd5\u7f51\u7edc"},{"location":"ubuntu/network/1-network-docs/#111","text":"","title":"1.1.1 \u67e5\u770b\u7f51\u7edc\u914d\u7f6e"},{"location":"ubuntu/network/1-network-docs/#1","text":"\uff081\uff09\u67e5\u770b\u6d3b\u52a8\u7684\u7f51\u7edc\u63a5\u53e3\u8bbe\u5907 \u82e5\u91c7\u7528 mini \u7248 CentOS 7 \u5b89\u88c5\u7684\u7cfb\u7edf\uff0c\u9ed8\u8ba4\u662f\u6ca1\u6709 ifconfig \u547d\u4ee4\u7684\uff0c\u9700\u8981\u5148\u901a\u8fc7 yum \u65b9\u5f0f\u5b89\u88c5 net-tools \u8f6f\u4ef6\u5305\uff0c\u624d\u6709 ifconfig \u547d\u4ee4\u3002\u5728\u4e0d\u5e26\u4efb\u4f55\u9009\u9879\u548c\u53c2\u6570\u6267\u884c ifconfig \u547d\u4ee4\u65f6\uff0c \u5c06\u663e\u793a\u5f53\u524d\u4e3b\u673a\u4e2d\u5df2\u542f\u7528\uff08\u6d3b\u52a8\uff09\u7684\u7f51\u7edc\u63a5\u53e3\u4fe1\u606f\u3002\u4f8b\u5982\uff0c\u76f4\u63a5\u6267\u884c ifconfig \u547d\u4ee4\u540e\u53ef\u4ee5\u770b \u5230 ens33\u3001lo \u8fd9\u4e24\u4e2a\u7f51\u7edc\u63a5\u53e3\u7684\u4fe1\u606f\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a [ root@VM-16-9-centos ] # ifconfig docker0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 172 .17.0.1 netmask 255 .255.0.0 broadcast 172 .17.255.255 ether 02 :42:7c:2d:55:50 txqueuelen 0 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 eth0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .0.16.9 netmask 255 .255.252.0 broadcast 10 .0.19.255 inet6 fe80::5054:ff:fe16:3a9 prefixlen 64 scopeid 0x20<link> ether 52 :54:00:16:03:a9 txqueuelen 1000 ( Ethernet ) RX packets 51791902 bytes 9684145526 ( 9 .0 GiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 49095710 bytes 7697835155 ( 7 .1 GiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 14076 bytes 1715488 ( 1 .6 MiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 14076 bytes 1715488 ( 1 .6 MiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 \uff082\uff09\u67e5\u770b\u6307\u5b9a\u7684\u7f51\u7edc\u63a5\u53e3\u4fe1\u606f \u5f53\u53ea\u9700\u8981\u67e5\u770b\u5176\u4e2d\u67d0\u4e00\u4e2a\u7f51\u7edc\u63a5\u53e3\u7684\u4fe1\u606f\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u7f51\u7edc\u63a5\u53e3\u7684\u540d\u79f0\u4f5c\u4e3a ifconfig \u547d \u4ee4\u7684\u53c2\u6570\uff08\u4e0d\u8bba\u8be5\u7f51\u7edc\u63a5\u53e3\u662f\u5426\u5904\u4e8e\u6fc0\u6d3b\u72b6\u6001\uff09\u3002\u4f8b\u5982\uff0c\u6267\u884c\u201cifconfig ens33\u201d\u547d\u4ee4\u540e\u53ef\u4ee5 \u53ea\u67e5\u770b\u7f51\u5361 ens33 \u7684\u914d\u7f6e\u4fe1\u606f\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a [ root@VM-16-9-centos ~ ] # ifconfig eth0 eth0: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 inet 10 .0.16.9 netmask 255 .255.252.0 broadcast 10 .0.19.255 inet6 fe80::5054:ff:fe16:3a9 prefixlen 64 scopeid 0x20<link> ether 52 :54:00:16:03:a9 txqueuelen 1000 ( Ethernet ) RX packets 51792964 bytes 9684253645 ( 9 .0 GiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 49096762 bytes 7698009938 ( 7 .1 GiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 \u4ece\u4e0a\u8ff0\u547d\u4ee4\u663e\u793a\u7684\u7ed3\u679c\u4e2d\uff0c\u53ef\u4ee5\u83b7\u77e5 ens33 \u7f51\u5361\u7684\u4e00\u4e9b\u57fa\u672c\u4fe1\u606f\uff0c\u5982\u4e0b\u6240\u8ff0\u3002 inet\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684 IP \u5730\u5740\uff0c\u5982\u201c192.168.4.11\u201d\u3002 netmask\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684\u5b50\u7f51\u63a9\u7801\uff0c\u5982\u201c255.255.255.0\u201d\u3002 broadcast\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u6240\u5728\u7f51\u7edc\u7684\u5e7f\u64ad\u5730\u5740\uff0c\u5982\u201c192.168.4.255\u201d\u3002 ether\uff1a\u8868\u793a\u7f51\u7edc\u63a5\u53e3\u7684\u7269\u7406\u5730\u5740\uff08MAC \u5730\u5740\uff09\uff0c\u5982\u201c00:0c:29:3a:81:cc\u201d\u3002\u7f51\u7edc\u63a5 \u53e3\u7684\u7269\u7406\u5730\u5740\u901a\u5e38\u4e0d\u80fd\u66f4\u6539\uff0c\u662f\u7f51\u5361\u5728\u751f\u4ea7\u65f6\u786e\u5b9a\u7684\u5168\u7403\u552f\u4e00\u7684\u786c\u4ef6\u5730\u5740\u3002 \u9664\u6b64\u4ee5\u5916\uff0c\u8fd8\u80fd\u591f\u901a\u8fc7\u201cTX\u201d\u548c\u201cRX\u201d\u7b49\u4fe1\u606f\u4e86\u89e3\u901a\u8fc7\u8be5\u7f51\u7edc\u63a5\u53e3\u53d1\u9001\u548c\u63a5\u6536\u7684\u6570\u636e\u5305\u4e2a \u6570\u3001\u6d41\u91cf\u7b49\u66f4\u591a\u5c5e\u6027\u3002","title":"1. \u67e5\u770b\u7f51\u7edc\u63a5\u53e3\u5730\u5740"},{"location":"ubuntu/network/1-network-docs/#2","text":"\u5728 Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u76f8\u5f53\u4e00\u90e8\u5206\u7f51\u7edc\u670d\u52a1\u90fd\u4f1a\u901a\u8fc7\u4e3b\u673a\u540d\u6765\u8bc6\u522b\u4e3b\u673a\uff0c\u5982\u679c\u4e3b\u673a\u540d\u914d \u7f6e\u4e0d\u5f53\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u7a0b\u5e8f\u529f\u80fd\u51fa\u73b0\u6545\u969c\u3002\u4f7f\u7528 hostname \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u4e3b\u673a\u7684\u4e3b\u673a\u540d\uff0c \u4e0d\u7528\u6dfb\u52a0\u4efb\u4f55\u9009\u9879\u6216\u53c2\u6570\uff0c\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b\uff1a(\u53e6\u5916\u9664\u4e86\u67e5\u770b\u4e3b\u673a\u540d\u8fd8\u53ef\u4ee5\u4f7f\u7528-i/-I\u53c2\u6570\u6765\u67e5\u770bip) [ root@VM-16-9-centos ~ ] # hostname VM-16-9-centos [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # hostname -i ::1 127 .0.0.1 [ root@VM-16-9-centos ~ ] # hostname -I 10 .0.16.9 172 .17.0.1","title":"2. \u67e5\u770b\u4e3b\u673a\u540d\u79f0"},{"location":"ubuntu/network/1-network-docs/#3","text":"Linux \u64cd\u4f5c\u7cfb\u7edf\u4e2d\u7684\u8def\u7531\u8868\u51b3\u5b9a\u7740\u4ece\u672c\u673a\u5411\u5176\u4ed6\u4e3b\u673a\u3001\u5176\u4ed6\u7f51\u7edc\u53d1\u9001\u6570\u636e\u7684\u53bb\u5411\uff0c\u662f\u6392 \u9664\u7f51\u7edc\u6545\u969c\u7684\u5173\u952e\u4fe1\u606f\u3002\u76f4\u63a5\u6267\u884c\u201croute\u201d\u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u4e3b\u673a\u4e2d\u7684\u8def\u7531\u8868\u4fe1\u606f\uff0c\u5728\u8f93\u51fa\u7ed3 \u679c\u4e2d\uff0cDestination \u5217\u5bf9\u5e94\u76ee\u6807\u7f51\u6bb5\u7684\u5730\u5740\uff0cGateway \u5217\u5bf9\u5e94\u4e0b\u4e00\u8df3\u8def\u7531\u5668\u7684\u5730\u5740\uff0cIface \u5217 \u5bf9\u5e94\u53d1\u9001\u6570\u636e\u7684\u7f51\u7edc\u63a5\u53e3\u3002\u82e5\u7ed3\u5408\u201c-n\u201d\u9009\u9879\u4f7f\u7528\uff0c\u53ef\u4ee5\u5c06\u8def\u7531\u8bb0\u5f55\u4e2d\u7684\u5730\u5740\u663e\u793a\u4e3a\u6570\u5b57\u5f62\u5f0f\uff0c\u8fd9\u53ef\u4ee5\u8df3\u8fc7\u89e3\u6790\u4e3b\u673a \u540d\u7684\u8fc7\u7a0b\uff0c\u5728\u8def\u7531\u8868\u6761\u76ee\u8f83\u591a\u7684\u60c5\u51b5\u4e0b\u80fd\u591f\u52a0\u5feb\u6267\u884c\u901f\u5ea6\u3002 [ root@VM-16-9-centos ~ ] # route Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface default gateway 0 .0.0.0 UG 0 0 0 eth0 10 .0.16.0 0 .0.0.0 255 .255.252.0 U 0 0 0 eth0 link-local 0 .0.0.0 255 .255.0.0 U 1002 0 0 eth0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0 [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # [ root@VM-16-9-centos ~ ] # route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0 .0.0.0 10 .0.16.1 0 .0.0.0 UG 0 0 0 eth0 10 .0.16.0 0 .0.0.0 255 .255.252.0 U 0 0 0 eth0 169 .254.0.0 0 .0.0.0 255 .255.0.0 U 1002 0 0 eth0 172 .17.0.0 0 .0.0.0 255 .255.0.0 U 0 0 0 docker0","title":"3. \u67e5\u770b\u8def\u7531\u8868\u6761\u76ee"},{"location":"ubuntu/network/1-network-docs/#4","text":"\u901a\u8fc7 netstat \u547d\u4ee4\u53ef\u4ee5\u67e5\u770b\u5f53\u524d\u64cd\u4f5c\u7cfb\u7edf\u7684\u7f51\u7edc\u8fde\u63a5\u72b6\u6001\u3001\u8def\u7531\u8868\u3001\u63a5\u53e3\u7edf\u8ba1\u7b49\u4fe1\u606f\uff0c \u5b83\u662f\u4e86\u89e3\u7f51\u7edc\u72b6\u6001\u53ca\u6392\u9664\u7f51\u7edc\u670d\u52a1\u6545\u969c\u7684\u6709\u6548\u5de5\u5177\u3002\u4ee5\u4e0b\u662f netstat \u547d\u4ee4\u5e38\u7528\u7684\u51e0\u4e2a\u9009\u9879 -a\uff1a\u663e\u793a\u4e3b\u673a\u4e2d\u6240\u6709\u6d3b\u52a8\u7684\u7f51\u7edc\u8fde\u63a5\u4fe1\u606f\uff08\u5305\u62ec\u76d1\u542c\u3001\u975e\u76d1\u542c\u72b6\u6001\u7684\u670d\u52a1\u7aef\u53e3\uff09 -n\uff1a\u4ee5\u6570\u5b57\u7684\u5f62\u5f0f\u663e\u793a\u76f8\u5173\u7684\u4e3b\u673a\u5730\u5740\u3001\u7aef\u53e3\u7b49\u4fe1\u606f\u3002 -r\uff1a\u663e\u793a\u8def\u7531\u8868\u4fe1\u606f\u3002 -l\uff1a\u663e\u793a\u5904\u4e8e\u76d1\u542c\uff08Listening\uff09\u72b6\u6001\u7684\u7f51\u7edc\u8fde\u63a5\u53ca\u7aef\u53e3\u4fe1\u606f\u3002 -t\uff1a\u67e5\u770b TCP\uff08Transmission Control Protocol\uff0c\u4f20\u8f93\u63a7\u5236\u534f\u8bae\uff09\u76f8\u5173\u7684\u4fe1\u606f -u\uff1a\u663e\u793a UDP\uff08User Datagram Protocol\uff0c\u7528\u6237\u6570\u636e\u62a5\u534f\u8bae\uff09\u534f\u8bae\u76f8\u5173\u7684\u4fe1\u606f\u3002 -p\uff1a\u663e\u793a\u4e0e\u7f51\u7edc\u8fde\u63a5\u76f8\u5173\u8054\u7684\u8fdb\u7a0b\u53f7\u3001\u8fdb\u7a0b\u540d\u79f0\u4fe1\u606f\uff08\u8be5\u9009\u9879\u9700\u8981 root \u6743\u9650\uff09\u3002 \u901a\u5e38\u4f7f\u7528\u201c-anpt\u201d\u7ec4\u5408\u9009\u9879\uff0c\u4ee5\u6570\u5b57\u5f62\u5f0f\u663e\u793a\u5f53\u524d\u7cfb\u7edf\u4e2d\u6240\u6709\u7684 TCP \u8fde\u63a5\u4fe1\u606f\uff0c\u540c\u65f6\u663e \u793a\u5bf9\u5e94\u7684\u8fdb\u7a0b\u4fe1\u606f\u3002\u7ed3\u5408\u7ba1\u9053\u547d\u4ee4\u4f7f\u7528\u201cgrep\u201d\u547d\u4ee4\uff0c\u8fd8\u53ef\u4ee5\u5728\u7ed3\u679c\u4e2d\u8fc7\u6ee4\u51fa\u6240\u9700\u8981\u7684\u7279\u5b9a\u8bb0 \u5f55\u3002\u4f8b\u5982\uff0c\u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u53ef\u4ee5\u67e5\u770b\u672c\u673a\u4e2d\u662f\u5426\u6709\u76d1\u542c\u201cTCP 22\u201d\u7aef\u53e3\uff08\u5373\u6807\u51c6 Web \u670d\u52a1\uff09\u7684 \u670d\u52a1\u7a0b\u5e8f\uff0c\u8f93\u51fa\u4fe1\u606f\u4e2d\u5305\u62ec PID \u53f7\u548c\u8fdb\u7a0b\u540d\u79f0\u3002 [ root@VM-16-9-centos ~ ] # netstat -antp |grep 22 tcp 0 0 0 .0.0.0:22 0 .0.0.0:* LISTEN 4344 /sshd \u9664\u4e86 netstat\uff0css \u547d\u4ee4\u4e5f\u53ef\u4ee5\u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u60c5\u51b5\uff0c\u5b83\u662f Socket Statistics \u7684\u7f29\u5199\uff0c\u4e3b\u8981 \u7528\u4e8e\u83b7\u53d6 socket \u7edf\u8ba1\u4fe1\u606f\uff0c\u5b83\u53ef\u4ee5\u663e\u793a\u548c netstat \u547d\u4ee4\u7c7b\u4f3c\u7684\u8f93\u51fa\u5185\u5bb9\u3002\u4f46 ss \u7684\u4f18\u52bf\u5728\u4e8e \u5b83\u80fd\u591f\u663e\u793a\u66f4\u591a\u66f4\u8be6\u7ec6\u7684\u6709\u5173 TCP \u548c\u8fde\u63a5\u72b6\u6001\u7684\u4fe1\u606f\uff0c\u800c\u4e14\u6bd4 netstat \u66f4\u5feb\u901f\u66f4\u9ad8\u6548\u3002\u8981\u60f3 \u4f7f\u7528 ss \u547d\u4ee4\uff0c\u9996\u5148\u786e\u4fdd iproute \u7a0b\u5e8f\u5305\u5df2\u88ab\u5b89\u88c5\uff0c\u53ef\u4ee5\u901a\u8fc7 yum \u65b9\u5f0f\u8fdb\u884c\u5b89\u88c5\u3002(\u7528\u6cd5\u7c7b\u4f3c) [ root@VM-16-9-centos ~ ] # ss -antp State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* users: (( \"sshd\" ,pid = 4344 ,fd = 3 )) LISTEN 0 100 127 .0.0.1:25 *:*","title":"4. \u67e5\u770b\u7f51\u7edc\u8fde\u63a5\u60c5\u51b5"},{"location":"ubuntu/network/1-network-docs/#112","text":"\u7528\u6237\u8bbf\u95ee\u7f51\u7edc\u670d\u52a1\u7684\u524d\u63d0\u662f\u7f51\u7edc\u8fde\u63a5\u5904\u4e8e\u6b63\u5e38\u72b6\u6001\u3002\u82e5\u7f51\u7edc\u8fde\u63a5\u4e0d\u7a33\u5b9a\uff0c\u751a\u81f3\u65e0\u6cd5\u8fde\u63a5\uff0c \u7528\u6237\u5219\u65e0\u6cd5\u6b63\u5e38\u8bbf\u95ee\u7f51\u7edc\u670d\u52a1\u3002\u56e0\u6b64\uff0c\u5f53\u7f51\u7edc\u8fde\u63a5\u51fa\u73b0\u95ee\u9898\u65f6\uff0c\u9700\u8981\u901a\u8fc7\u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5\u7684\u547d \u4ee4\u6765\u786e\u5b9a\u6545\u969c\u70b9\u3002\u4e0b\u9762\u4ecb\u7ecd\u51e0\u4e2a\u5e38\u7528\u7684\u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5\u7684\u547d\u4ee4\u3002","title":"1.1.2 \u6d4b\u8bd5\u7f51\u7edc\u8fde\u63a5"},{"location":"ubuntu/network/1-network-docs/#1_1","text":"[ root@VM-16-9-centos ~ ] # ping baidu.com","title":"1. \u6d4b\u8bd5\u7f51\u7edc\u8fde\u901a\u6027"},{"location":"ubuntu/network/1-network-docs/#2-dns","text":"[ root@VM-16-9-centos ~ ] # nslookup www.google.com Server: 183 .60.83.19 Address: 183 .60.83.19#53 Non-authoritative answer: Name: www.google.com Address: 174 .37.54.20 Name: www.google.com Address: 2001 ::68f4:2e15 [ root@VM-16-9-centos ~ ] # dig www.google.com ; <<>> DiG 9 .11.4-P2-RedHat-9.11.4-26.P2.el7_9.7 <<>> www.google.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 44445 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 0 ;; QUESTION SECTION: ; www.google.com. IN A ;; ANSWER SECTION: www.google.com. 133 IN A 199 .59.149.206 ;; Query time: 1 msec ;; SERVER: 183 .60.83.19#53 ( 183 .60.83.19 ) ;; WHEN: \u4e94 11\u6708 18 15 :51:47 CST 2022 ;; MSG SIZE rcvd: 48","title":"2. \u6d4b\u8bd5DNS\u89e3\u6790"},{"location":"ubuntu/network/1-network-docs/#3-iperf","text":"server: iperf -s -p 1234 -i 1 clent: iperf -c 172 .16.240.204 -p 1234 -i 1 -t 20 -w 20w","title":"3. iperf \u7f51\u7edc\u6d4b\u8bd5"},{"location":"vm/promox/","text":"Proxmox \u00b6 Proxmox VE \u662f\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u5f00\u6e90\u7684\u4f01\u4e1a\u865a\u62df\u5316\u670d\u52a1\u5668\u7ba1\u7406\u5e73\u53f0\u3002\u5b83\u5c06 KVM \u7ba1\u7406\u7a0b\u5e8f\u548c Linux \u5bb9\u5668 (LXC)\u3001\u8f6f\u4ef6\u5b9a\u4e49\u7684\u5b58\u50a8\u548c\u7f51\u7edc\u529f\u80fd\u7d27\u5bc6\u96c6\u6210\u5728\u4e00\u4e2a\u5e73\u53f0\u4e0a\u3002\u501f\u52a9\u57fa\u4e8e Web \u7684\u96c6\u6210\u7528\u6237\u754c\u9762\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u7ba1\u7406 VM \u548c\u5bb9\u5668\u3001\u96c6\u7fa4\u7684\u9ad8\u53ef\u7528\u6027\u6216\u96c6\u6210\u7684\u707e\u96be\u6062\u590d\u5de5\u5177\u3002 \u4f18\u52bf: \u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528UI\u754c\u9762\u5bf9\u865a\u62df\u5316\u5df2\u7ecf\u64cd\u4f5c \u65e2\u53ef\u4ee5\u865a\u62dfwindows\u673a\u5668\uff0c\u4e5f\u53ef\u4ee5\u865a\u62dfLinux\u673a\u5668 \u4f7f\u7528\u7b80\u5355\uff0c\u4e0a\u624b\u8f83\u5feb \u90e8\u7f72Pve \u00b6 \u5236\u4f5c\u7cfb\u7edf\u76d8 \u00b6 \u5de5\u5177\u5206\u4eab: rufus\u5de5\u5177\u4e0b\u8f7d\u94fe\u63a5 \u5982\u679c\u7f51\u76d8\u5931\u6548\u53ef\u4ee5\u8bbf\u95ee\u5b98\u7f51: https://rufus.ie/zh/ \u8fdb\u884c\u4e0b\u8f7d image \u5206\u4eab: https://www.proxmox.com/en/downloads \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u5236\u4f5c\u7cfb\u7edf\u76d8\u4e00\u5b9a\u8981\u9009\u62e9dd\u7684\u65b9\u5f0f \u5230\u8fd9\u91cc\u7cfb\u7edf\u76d8\u5c31\u641e\u5b9a\u4e86\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u5b89\u88c5Pve\u8fd9\u4e2a\u7cfb\u7edf\u4e86\u3002 \u5176\u5b9e\u4ed6\u7684\u64cd\u4f5c\u7cfb\u7edf\u4ece\u4e0b\u56fe\u5c31\u80fd\u770b\u51fa\u662f\u4e00\u4e2adebian root@node1:~# cat /etc/os-release PRETTY_NAME = \"Debian GNU/Linux 11 (bullseye)\" NAME = \"Debian GNU/Linux\" VERSION_ID = \"11\" VERSION = \"11 (bullseye)\" VERSION_CODENAME = bullseye ID = debian HOME_URL = \"https://www.debian.org/\" SUPPORT_URL = \"https://www.debian.org/support\" BUG_REPORT_URL = \"https://bugs.debian.org/\" \u90a3\u4e48\u63a5\u4e0b\u6765\u5c31\u662f\u5b89\u88c5\u7cfb\u7edf\uff0c\u5982\u679c\u9700\u8981\u914d\u7f6eraid\u5c31\u53ef\u4ee5\u5148\u914d\u7f6e\u4e00\u4e0b\uff0c\u8fd9\u91cc\u5c31\u4e0d\u8fc7\u591a\u7684\u89e3\u91caraid\u7684\u6982\u5ff5\u4e86\u3002 \u5b89\u88c5\u7cfb\u7edf\u5176\u5b9e\u5c31\u662f\u666e\u901a\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u91cc\u5c31\u7565\u8fc7\uff5e \u5982\u679c\u5b9e\u5728\u4e0d\u61c2\u53ef\u4ee5\u53c2\u8003\uff1ahttps://www.jianshu.com/p/a2ad1aed6a92 \u7684\u5b89\u88c5\u6d41\u7a0b\uff0c\u57fa\u672c\u5c31\u662f\u8fd9\u6837 PVE \u7cfb\u7edf\u521d\u59cb\u5316 \u00b6 \u66f4\u6362\u56fd\u5185\u6e90: \u00b6 \u6e05\u534e\u6e90 # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free install package: (\u975e\u5fc5\u9700) \u00b6 ifupdown2,openvswitch-switch,vim apt update apt install ifupdown2 openvswitch-switch -y \u5b58\u50a8 \u00b6 \u5206\u533a\u683c\u5f0f\u5316\uff0c\u6302\u8f7d fdisk /dev/sdb mke2fs -t ext4 /dev/sdb1 mount /dev/sdb1 /mnt/pve/node5_sdb \u4ee5\u4e0a\u8fd9\u91cc\u662f\u4e34\u65f6\u6302\u8f7d\u7684\uff0c\u9700\u8981\u5c06\u5176\u5199\u5165/etc/fstab \u683c\u5f0f\u5316\u62a5\u9519 root@node18:~# mke2fs -t ext4 /dev/sdb1 mke2fs 1.44.5 (15-Dec-2018) /dev/sdb1 is apparently in use by the system; will not make a filesystem here! \u89e3\u51b3\u65b9\u6cd5 root@node18:~# dmsetup remove_all root@node18:~# mke2fs -t ext4 /dev/sdb1 mke2fs 1.44.5 (15-Dec-2018) Creating filesystem with 244055808 4k blocks and 61014016 inodes Filesystem UUID: dd408faf-92ff-467a-baf4-58d653ec3e40 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done UI\u754c\u9762\u6dfb\u52a0\u78c1\u76d8 \u00b6 \u6839\u636e\u5b9e\u9645\u60c5\u51b5\u586b\u5199\uff0c\u8fd9\u91cc\u622a\u56fe\u4e0d\u51c6\u786e \u6dfb\u52a0\u4ee5\u4e0a\u4fe1\u606f\uff0c\u53ef\u4ee5\u770b\u5230\u786c\u76d8\u5df2\u7ecf\u6302\u8f7d\u4e0a \u7f51\u7edc \u00b6 \u914d\u7f6e\u7f51\u7edc \u914d\u7f6e\u7ec4\u5efa\u6210\u96c6\u7fa4\u7684\u7f51\u5361 \u547d\u4ee4\u884c\u64cd\u4f5c\u6216\u8005\u5e94\u7528\u914d\u7f6e cd /etc/network/ mv interfaces.new interfaces reboot \u6fc0\u6d3b\u7f51\u7edc \u96c6\u7fa4\u7ba1\u7406: \u00b6 \u7ba1\u7406\u547d\u4ee4 # \u505c\u6b62\u865a\u62df\u673a qm stop <vmid> [ OPTIONS ] # \u5220\u9664 qm destroy <vmid> [ OPTIONS ] # \u89e3\u9501 qm unlink <vmid> --idlist <string> [ OPTIONS ] \u9a71\u9010\u6545\u969c\u673a\u5668 \uff1a cd /etc/pve/nodes # \u5220\u9664\u6545\u969c\u8282\u70b9node\u6587\u4ef6 rm -rf /etc/pve/nodes/pve2 # \u6539\u6210\u6545\u969c\u8282\u70b9\u5bf9\u5e94\u8def\u5f84 root@node17:/etc/pve/nodes# pvecm delnode node12 # \u767b\u5f55\u96c6\u7fa4\u4e2d\u4efb\u610f\u6b63\u5e38\u8282\u70b9\uff0c\u6267\u884c\u5982\u4e0b\u6307\u4ee4\u8fdb\u884c\u9a71\u9010\u64cd\u4f5c","title":"Proxmox\u865a\u62df\u5316"},{"location":"vm/promox/#proxmox","text":"Proxmox VE \u662f\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u5f00\u6e90\u7684\u4f01\u4e1a\u865a\u62df\u5316\u670d\u52a1\u5668\u7ba1\u7406\u5e73\u53f0\u3002\u5b83\u5c06 KVM \u7ba1\u7406\u7a0b\u5e8f\u548c Linux \u5bb9\u5668 (LXC)\u3001\u8f6f\u4ef6\u5b9a\u4e49\u7684\u5b58\u50a8\u548c\u7f51\u7edc\u529f\u80fd\u7d27\u5bc6\u96c6\u6210\u5728\u4e00\u4e2a\u5e73\u53f0\u4e0a\u3002\u501f\u52a9\u57fa\u4e8e Web \u7684\u96c6\u6210\u7528\u6237\u754c\u9762\uff0c\u60a8\u53ef\u4ee5\u8f7b\u677e\u7ba1\u7406 VM \u548c\u5bb9\u5668\u3001\u96c6\u7fa4\u7684\u9ad8\u53ef\u7528\u6027\u6216\u96c6\u6210\u7684\u707e\u96be\u6062\u590d\u5de5\u5177\u3002 \u4f18\u52bf: \u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528UI\u754c\u9762\u5bf9\u865a\u62df\u5316\u5df2\u7ecf\u64cd\u4f5c \u65e2\u53ef\u4ee5\u865a\u62dfwindows\u673a\u5668\uff0c\u4e5f\u53ef\u4ee5\u865a\u62dfLinux\u673a\u5668 \u4f7f\u7528\u7b80\u5355\uff0c\u4e0a\u624b\u8f83\u5feb","title":"Proxmox"},{"location":"vm/promox/#pve","text":"","title":"\u90e8\u7f72Pve"},{"location":"vm/promox/#_1","text":"\u5de5\u5177\u5206\u4eab: rufus\u5de5\u5177\u4e0b\u8f7d\u94fe\u63a5 \u5982\u679c\u7f51\u76d8\u5931\u6548\u53ef\u4ee5\u8bbf\u95ee\u5b98\u7f51: https://rufus.ie/zh/ \u8fdb\u884c\u4e0b\u8f7d image \u5206\u4eab: https://www.proxmox.com/en/downloads \u6e29\u99a8\u63d0\u793a \u8fd9\u4e2a\u5236\u4f5c\u7cfb\u7edf\u76d8\u4e00\u5b9a\u8981\u9009\u62e9dd\u7684\u65b9\u5f0f \u5230\u8fd9\u91cc\u7cfb\u7edf\u76d8\u5c31\u641e\u5b9a\u4e86\uff0c\u90a3\u4e48\u5c31\u53ef\u4ee5\u5b89\u88c5Pve\u8fd9\u4e2a\u7cfb\u7edf\u4e86\u3002 \u5176\u5b9e\u4ed6\u7684\u64cd\u4f5c\u7cfb\u7edf\u4ece\u4e0b\u56fe\u5c31\u80fd\u770b\u51fa\u662f\u4e00\u4e2adebian root@node1:~# cat /etc/os-release PRETTY_NAME = \"Debian GNU/Linux 11 (bullseye)\" NAME = \"Debian GNU/Linux\" VERSION_ID = \"11\" VERSION = \"11 (bullseye)\" VERSION_CODENAME = bullseye ID = debian HOME_URL = \"https://www.debian.org/\" SUPPORT_URL = \"https://www.debian.org/support\" BUG_REPORT_URL = \"https://bugs.debian.org/\" \u90a3\u4e48\u63a5\u4e0b\u6765\u5c31\u662f\u5b89\u88c5\u7cfb\u7edf\uff0c\u5982\u679c\u9700\u8981\u914d\u7f6eraid\u5c31\u53ef\u4ee5\u5148\u914d\u7f6e\u4e00\u4e0b\uff0c\u8fd9\u91cc\u5c31\u4e0d\u8fc7\u591a\u7684\u89e3\u91caraid\u7684\u6982\u5ff5\u4e86\u3002 \u5b89\u88c5\u7cfb\u7edf\u5176\u5b9e\u5c31\u662f\u666e\u901a\u7684\u64cd\u4f5c\u6d41\u7a0b\uff0c\u8fd9\u91cc\u5c31\u7565\u8fc7\uff5e \u5982\u679c\u5b9e\u5728\u4e0d\u61c2\u53ef\u4ee5\u53c2\u8003\uff1ahttps://www.jianshu.com/p/a2ad1aed6a92 \u7684\u5b89\u88c5\u6d41\u7a0b\uff0c\u57fa\u672c\u5c31\u662f\u8fd9\u6837","title":"\u5236\u4f5c\u7cfb\u7edf\u76d8"},{"location":"vm/promox/#pve_1","text":"","title":"PVE \u7cfb\u7edf\u521d\u59cb\u5316"},{"location":"vm/promox/#_2","text":"\u6e05\u534e\u6e90 # \u9ed8\u8ba4\u6ce8\u91ca\u4e86\u6e90\u7801\u955c\u50cf\u4ee5\u63d0\u9ad8 apt update \u901f\u5ea6\uff0c\u5982\u6709\u9700\u8981\u53ef\u81ea\u884c\u53d6\u6d88\u6ce8\u91ca deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-backports main contrib non-free deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free","title":"\u66f4\u6362\u56fd\u5185\u6e90:"},{"location":"vm/promox/#install-package","text":"ifupdown2,openvswitch-switch,vim apt update apt install ifupdown2 openvswitch-switch -y","title":"install package:(\u975e\u5fc5\u9700)"},{"location":"vm/promox/#_3","text":"\u5206\u533a\u683c\u5f0f\u5316\uff0c\u6302\u8f7d fdisk /dev/sdb mke2fs -t ext4 /dev/sdb1 mount /dev/sdb1 /mnt/pve/node5_sdb \u4ee5\u4e0a\u8fd9\u91cc\u662f\u4e34\u65f6\u6302\u8f7d\u7684\uff0c\u9700\u8981\u5c06\u5176\u5199\u5165/etc/fstab \u683c\u5f0f\u5316\u62a5\u9519 root@node18:~# mke2fs -t ext4 /dev/sdb1 mke2fs 1.44.5 (15-Dec-2018) /dev/sdb1 is apparently in use by the system; will not make a filesystem here! \u89e3\u51b3\u65b9\u6cd5 root@node18:~# dmsetup remove_all root@node18:~# mke2fs -t ext4 /dev/sdb1 mke2fs 1.44.5 (15-Dec-2018) Creating filesystem with 244055808 4k blocks and 61014016 inodes Filesystem UUID: dd408faf-92ff-467a-baf4-58d653ec3e40 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, 4096000, 7962624, 11239424, 20480000, 23887872, 71663616, 78675968, 102400000, 214990848 Allocating group tables: done Writing inode tables: done Creating journal (262144 blocks): done Writing superblocks and filesystem accounting information: done","title":"\u5b58\u50a8"},{"location":"vm/promox/#ui","text":"\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u586b\u5199\uff0c\u8fd9\u91cc\u622a\u56fe\u4e0d\u51c6\u786e \u6dfb\u52a0\u4ee5\u4e0a\u4fe1\u606f\uff0c\u53ef\u4ee5\u770b\u5230\u786c\u76d8\u5df2\u7ecf\u6302\u8f7d\u4e0a","title":"UI\u754c\u9762\u6dfb\u52a0\u78c1\u76d8"},{"location":"vm/promox/#_4","text":"\u914d\u7f6e\u7f51\u7edc \u914d\u7f6e\u7ec4\u5efa\u6210\u96c6\u7fa4\u7684\u7f51\u5361 \u547d\u4ee4\u884c\u64cd\u4f5c\u6216\u8005\u5e94\u7528\u914d\u7f6e cd /etc/network/ mv interfaces.new interfaces reboot \u6fc0\u6d3b\u7f51\u7edc","title":"\u7f51\u7edc"},{"location":"vm/promox/#_5","text":"\u7ba1\u7406\u547d\u4ee4 # \u505c\u6b62\u865a\u62df\u673a qm stop <vmid> [ OPTIONS ] # \u5220\u9664 qm destroy <vmid> [ OPTIONS ] # \u89e3\u9501 qm unlink <vmid> --idlist <string> [ OPTIONS ] \u9a71\u9010\u6545\u969c\u673a\u5668 \uff1a cd /etc/pve/nodes # \u5220\u9664\u6545\u969c\u8282\u70b9node\u6587\u4ef6 rm -rf /etc/pve/nodes/pve2 # \u6539\u6210\u6545\u969c\u8282\u70b9\u5bf9\u5e94\u8def\u5f84 root@node17:/etc/pve/nodes# pvecm delnode node12 # \u767b\u5f55\u96c6\u7fa4\u4e2d\u4efb\u610f\u6b63\u5e38\u8282\u70b9\uff0c\u6267\u884c\u5982\u4e0b\u6307\u4ee4\u8fdb\u884c\u9a71\u9010\u64cd\u4f5c","title":"\u96c6\u7fa4\u7ba1\u7406:"}]}